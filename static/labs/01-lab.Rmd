---
title: 'Lab 01 - Exploring data on COVID-19 in the U.S.'
author: "Your Name HERE"
date: "Last Updated `r format(Sys.Date())`"
output:
  html_document:
    theme: journal
    toc: TRUE
    toc_float: TRUE
    toc_depth: 2
    number_sections: TRUE
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE, 
                      comment = NA, 
                      warnings = FALSE, 
                      errors = FALSE, 
                      messages = FALSE, 
                      cache = TRUE,
                      tidy = FALSE)
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>

# Overview {-}

Today, we'll continuing exploring the COVID-19 data for the U.S.

We covered a lot of ground in our last lecture. Conceptually, we covered how to

- Write and code in R Markdown
- Install and load packages
- Download and inspect data
- Clean and recode data
- Calculate simple descriptive statistics with that data

To cover all that ground, we copied and pasted a lot of code. Today, we'll get practice writing our own code. 

# Getting set up

1. Save this document in a folder for the class
2. Knit the document

# Install the some additiona packages

- In the code chunk below, use the `install.packages()` function to install the `lubridate` package
- Run the code "Live" by sending the command to your console
- Once `lubridate` is installed, comment the code out by placing a \# at the start of the line

```{r install}
install.packages(c("lubridate","DT"))
```

The `'lubridate` package is a collection of functions that makes working with dates and time more simple.

# Load the packages we'll be using today

- Create a code chunk
- Label your code chunk "packages"
- Use the `library()` to load the following packages
  - `tidyverse`
  - `COVID19`
  - `lubridate`

```{r packages}

library(tidyverse)
library(COVID19)
library(lubridate)
library(DT)

```


# Download the COVID19 Data for U.S. 

Open up the [slides](https://pols1600.paultesta.org/slides/01-slides.html#81) from last class and copy the relevant code 

```{r covid}
# Download the COVID-19 data

covid <- COVID19::covid19(
  country = "US",
  level = 2,
  verbose = F
)


```


# Replicate the data cleaning and recoding from class

Specifically please do the following:

1. Create a object called `territories` that is a vector containing the names of U.S. territories
  - Use `<-` and `c()` ([slides](https://pols1600.paultesta.org/slides/01-slides#107))
2. Create a new dataframe, called `covid_us`, by filtering out observations from the U.S. territories
`- Use `<-`, `%>%`, `filter()`, `!` and `%in%` ([slides](hhttps://pols1600.paultesta.org/slides/01-slides#108))
3. Create a `state` variable that is a copy of the `administrative_area_level_2` 
  - Use `%>%`, `mutate()` and `->`
4. Create a variable called `new_cases` from the `confirmed`. Create a variable called `new_cases_pc` that is the number of new Covid-19 cases per 100,000 citizens
  - Use `%>%`,`group_by()`,`mutate()` and `lag()`, ``/` and `*` ([slides](hhttps://pols1600.paultesta.org/slides/01-slides#110))
5. Create a variable called `face_masks` from the `facial_coverings` variable. ([slides](hhttps://pols1600.paultesta.org/slides/01-slides#111))
  - Treat negative and positive values the same. 
  - Recode numeric values so that they have meaningful labels
  - Save as a factor variable so that the labels are ordered in terms of increasing levels of severity/restrictions

```{r recode}
# ---- 1. Create territories object

territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )

# ---- 2. Create covid_us data frame

covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)

# ---- 3-5. Recode variables in covid_us

covid_us %>%
  mutate(
    state = administrative_area_level_2,
  ) %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000
    ) %>%
  mutate(
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ) %>% factor(.,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -> covid_us

```

Finally let's compare our new `face_mask` variable to the original `facial_coverings` variable using the table command.

Uncomment the following code and replace the generic terms `data`, `variable1`, and `variable2` with appropriate terms.

```{r crosstab}
# table(data$variable1, data$variable2, useNA="ifany")
```


```{r crosstab_sol}
table(covid_us$face_masks, covid_us$facial_coverings,useNA="ifany")
```


You've just created a "crosstab" a frequency table showing the joint distribution of two variables.

<div class="blue">
>  Crosstabs are powerful tools for getting a sense of your data and for checking whether a recode did what you wanted it to do. 
</div>


Finally, let's create variable called `year_month` from the `date` variable and a variable describing the percent of a state's population that is fully vaccinated, which we'll use later in the lab.

## Uncomment and run the following code

Highlight the commented code below from `# covid_us %>%` to `#   ) -> covid_us` and press `shift + cmd + C` on a mac or `shift + ctrl + C` on PC to uncomment the code. 

```{r dates}
covid_us %>%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = "-"),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```


- The `year(date)` extracts the year from our `date` variable and saves it in new column called `year`
- Similarly, the `month(date)` extracts the month from our `date` variable and saves it in a new column called `month`
- Finally the `paste()` command pastes these two variables together, with the `str_pad()` adding a leading 0 to single digit months.


# Exploring our data

From the [documentation](https://covid19datahub.io/articles/docs.html) of the COVID19 package, we see that the numeric values of the `facial_coverings` correspond to following substantive policy regimes:

- 0 - No policy
- 1 - Recommended
- 2 - Required in some specified shared/public spaces outside the home with other people present, or some situations when social distancing not possible
- 3 - Required in all shared/public spaces outside the home with other people present or all situations when social distancing not possible
- 4 - Required outside the home at all times regardless of location or presence of other people

These data come from the [Oxford Covid-19 Government Response Tracker](https://github.com/OxCGRT/covid-policy-tracker). Oxford distinguishes between policies that are in effect for the entire administrative unit (e.g. the State of New York) and policies that may be in effect in only parts of the administrative unit (e.g. New York city)

> In short: positive integers identify policies applied to the entire administrative area. Negative integers are used to identify policies that represent a best guess of the policy in force, but may not represent the real status of the given area. The negative sign is used solely to distinguish the two cases, it should not be treated as a real negative value.

Let's get some practice using the `filter()`, `select()` `group_by()` and `summarize()`, and `n()` commands from `dplyr` package to understand how common each these negative values are in our data.

## Uncomment and run the code below, 

```{r negvals}
covid_us %>%
  filter(facial_coverings == -4) %>%
  select(date, state) %>%
  group_by(state) %>%
  summarize(
    n = n(),
    earliest_date = min(date),
    latest_date = max(date),
  )
```

## Please explain in words your best understanding of what each line of code is doing:

- `covid_us %>%` tells R that every line of code after will use `covid_us` dataframe
- `filter(facial_coverings == -4) %>%` tells R to filter out only the rows where the facial coverings variable equals -4
- `select(date, state) %>%` tells R to select the columns named `date` and `state`
- `group_by(state) %>%` tells R that subsequent commands should be done separately for each unique value of `state`
- `summarize(` tells R we want to summarize the output of susequent commands
- `n = n(),` tells R to count the number of observations (state-dates) for each state that had a value of -4 on the `facial_coverings` variable
- `earliest_date = min(date),` tells R to report the earliest date that each state had a value of -4
- `latest_date = max(date),` tells R to report the latest date that each state had a value of -4
- `)` tells R we're finished with the `summarize()` function



Substantively, what does the previous chunk of code tell us?

- So there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. 

<div class="blue">

> Filtering data, selecting specific variables, and summarizing variables are important skills that let us "know our data"

</div>

# Look at the source data for face mask policies

To make sure we understand what this policy variable `facial_coverings` is measuring, let's download the source data from Oxford


```{r}
oxford_us <- read_csv("https://github.com/OxCGRT/USA-covid-policy/raw/master/data/OxCGRT_US_latest.csv")
```


Now let's look at the policy on face masks for Illinois. From Oxford's [codebook](https://github.com/OxCGRT/covid-policy-tracker/blob/master/documentation/codebook.md), we learn that variables describing face mask policies all begin with the prefix `H6_`

<div class="blue">
> Using common prefixes for a variable is a good habit that will help you organize and work with your data
</div>

## Please run the following code:

```{r ilfacemask}
oxford_us %>%
  mutate(
    date = ymd(Date)
  )%>%
  filter(RegionName == "Illinois", 
         date > "2020-08-01", 
         date < "2021-01-01",
         !is.na(H6_Notes)) %>%
  select(date,starts_with("H6_")) -> il_facemasks
il_facemasks
```

## Again, explain in words, what the components of this code are doing:

- `oxford_us %>%`
- `mutate(date = ymd(Date))%>%`
- `filter(RegionName == "Illinois",` 
-  `       date > "2020-08-01", `
-  `       date < "2021-01-01",`
-  `        !is.na(H6_Notes)) %>%`
-  `select(date,starts_with("H6_")) -> il_facemasks`
- `il_facemasks`

Let's take a look at the `H6_Notes` variable for 2020-09-18

```{r ilpol1}
il_facemasks$H6_Notes[3]
```

Now update the code to select `H6_Notes` variable for 2020-10-01

```{r ilpol2}
# il_facemasks$H6_Notes[???]
```

## What have we learned about our variables measuring `face_mask` policy

In Illinois, the -4's seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of  `facial_coverings` to construct our `face_mask` variable, we're probably over stating the extent the extensiveness of policies in effect.

So we should be cautious in how we interpret our collapsed variable, `face_mask`. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy.

# Explore R's functions for generating summary statistics

In class, we kind of rushed through our discussion of descriptive statistics. Let's take a little time to review these concepts in more detail and see how to calculate them in R.

## Measures of Central Tendency

Measures of central tendency describe what a typical value of some variable looks like

### Mean

One of the most frequent measures of central tendency we'll use in this course is a mean or average.

Suppose we have $n$ observations of some variable $x$. We can calculate the mean of $\bar{x}$ ("x bar), by adding up all the values of x

\[
\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i
\]

We'll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we'll calculate below) are central to thinking about linear regression.

For now, please calculate the mean (average) number of new cases per 100,000 residents in our data:


```{r mean}
mean(covid_us$new_cases_pc, na.rm=T)
```


### Median


The median is another measure of what's typical of a numeric variable.

Imagine, we took our data new Covid-19 cases and arranged them in ascending order, from smallest to highest.

The median would be the value in the exact middle of that sequence, also known as the 50th percentile.

Formally, we can define that median as:


\[
M_x = X_i : \int_{-\infty}^{x_i} f_x(X)dx=\int_{x_i}^\infty f_x(X)dx=1/2
\]

Which might look like greek to you, which is fine. Just think of it as the middle value.

Please calculate the median number of new Covid-19 cases per 100,000 using the `median()` function. How does it compare to the mean?


```{r median}
median(covid_us$new_cases_pc, na.rm=T)
```

Interesting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the `new_cases_pc` has a "long tail" or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.


<div class="blue">

> Medians are less influenced by outliers than means

</div>

```{r hist}
hist(covid_us$new_cases_pc, breaks = 100)
```




### Modes

Conceptual, a mode describes the most frequent outcome. Models are useful for describing what's typical of "nominal" or categorical data like our measure of face mask policy. 

To calculate the mode of our `face_masks` variable, wrap the output of `table()` with the `sort()` function

```{r mode}
sort(table(covid_us$face_masks))
```

For numeric data, modes correspond to the peak of a variables density function (more on this later in the class). But you can get a sense of the relationship between, means, median's and modes from this helpful figure from [Wikipedia](https://en.wikipedia.org/wiki/Mode_(statistics)):

```{r wikifig, out.width=200}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Visualisation_mode_median_mean.svg/240px-Visualisation_mode_median_mean.svg.png")
```



## Measures of Dispersion

Measures of dispersion describe how much the data "vary" 


### Range

The range of a variable is simply it's minimum and maximum value

Please calculate the range of our `new_cases_pc`. What states on what dates observed these minimum and maximum values?


```{r}
range(covid_us$new_cases_pc,na.rm = T)
min(covid_us$new_cases_pc,na.rm = T)
max(covid_us$new_cases_pc,na.rm = T)

covid_us %>%
  filter(
    new_cases_pc < -188 |
    new_cases_pc > 1500
  )%>%
  select(state, date,new_cases_pc)

```


### Percentiles Ranges

The $p$-th percentile is the value of the observation such that 100\*p percent of the data are to the left and 100-100\*p are two the right.

\[
p_x = X_i : \int_{-\infty}^{x_i} f_x(X)dx= p; \int_{x_i}^\infty f_x(X)dx=1-p
\]


The median is just the 50th percentile

In R we calculate the $p$-th percentile using the `quantile()` setting the `probs` argument to the $p/100$ that we we want.

Please use the `quantile()` to calculate the 25th and 75th percentiles of the `new_cases_pc` variable.


```{r}
quantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)
```


The 25th and 75th percentile define the "Interquartile Range" where 50 percent of the observations lie within this range, and 50 percent lie outside the range.

### Variance

Conceptually, variance describes how much observations of a given variable vary around their mean.

The variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a variables mean.

\[
\sigma^2_x=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2
\]

- $x_i-\bar{x}$ is the deviation
- $(x_i-\bar{x})^2}$ squaring this ensures that we treat positive and negative deviations the same
- $\sum_{i=1}$ sums up all the differences
- $\frac{1}{n-1}$ takes the average of these differences (we divide by $n-1$ instead of $n$ for [statistical reasons](https://web.ma.utexas.edu/users/mks/M358KInstr/SampleSDPf.pdf) that we'll return two when we talk about estimation)

Use the `var()` function to calculate the variance of the `new_cases_pc` variable.

```{r}
var(covid_us$new_cases_pc,na.rm=T)

# Calculate by hand

sum(
  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, 
  na.rm=T
  )/(sum(!is.na(covid_us$new_cases_pc))-1)

```


Variance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)

### Standard Deviations


A standard deviation is simply the square root of variable's variance. 

\[
\sigma_x=\sqrt{\sigma^2_x}=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2}
\]

Standard deviations are easier to interpet because their units are the same as variable. Think of them as a measure of the typical amount of variation for variable.

Again, let's use the `sd()` function to calculate the standard deviation of the `new_cases_pc` variable

```{r}
sd(covid_us$new_cases_pc,na.rm=T)
```


## Measures of Association

Measures of association describe how variables relate to each other.

### Covariance

Covariance describes how two variables "covary". When $x$ is above its mean, $y$ also tends to be above it's mean, these variables have a positive covariance. If when $x$ tends to be high, $y$ tends to be low, these variables have a negative variance

\[
cov(x,y)=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})
\]

#### Please calculate the covariance between the percent of state's population that is fully vaccinated (`percent_vaccinated`) and `new_cases_pc` using the `var()` function

```{r}
var(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)
```


### Correlation

Like variances, covariances don't really have intrinsic meaning, since x and y can be measured on very different scales.

The correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.


#### Calculate the correlation between the percent of state's population that is fully vaccinated (`percent_vaccinated`) and `new_cases_pc` using the `cor()` function.

You'll need to set the argument `use="complete.obs`

```{r}
cor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = "complete.obs")



```

Hmm... That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021


#### Uncomment and interpret the output of the code below

```{r}

covid_us %>%
  filter(year > 2020)%>%
  ungroup() %>%
  group_by(year,month)%>%
  summarise(
    mn_per_vax = mean(percent_vaccinated, na.rm=T),
    cor = cor(new_cases_pc, percent_vaccinated, use = "complete.obs")
  )


```




# Facemasks and New Cases of Covid-19

```{r}
covid_us %>%
  filter(year_month == "2020-09")%>%
  group_by(face_masks) %>%
  summarise(
    total_cases = mean(confirmed,na.rm=T),
    new_cases = mean(new_cases_pc,na.rm=T)
  )

covid_us %>%
  filter(!is.na(face_masks))%>%
  group_by(year_month,face_masks) %>%
  summarise(
    n = length(unique(state)),
    new_cases = round(mean(new_cases_pc,na.rm=T)),
    total_cases = round(mean(confirmed,na.rm=T))
  ) -> cases_by_month_and_policy
```


```{r}
DT::datatable(cases_by_month_and_policy,
              filter = "top")
```




```{r}
cases_by_month_and_policy %>%
  ggplot(aes(year_month, 
             new_cases,
             size = n,
             col=face_masks))+
  geom_point()+coord_flip()
```


