---
title: "POLS 1600: Random Variables, Probability Distributions, and Expected Values and Variances"
date: Updated '`r format(Sys.Date(), "%B %d, %Y")`'
always_allow_html: yes
output: 
    html_document:
        toc: yes
        toc_float: yes
---



```{r init,echo=F, results=F,message=F}
## Easy way to look for and install missing packages and load them
if (!require("pacman")){ install.packages("pacman") }
pacman::p_load("knitr","mosaic","tidyverse","foreign","car","grid","gridExtra","scatterplot3d","rethinking","plotly","reshape2")

## Set some default options R Markdown
opts_chunk$set(tidy=F,echo=TRUE,results='markup',strip.white=TRUE,cache=F,highlight=TRUE,width.cutoff=132,size='footnotesize',message=FALSE,warning=TRUE,comment=NA)

# Default options in R
options(digits=4,width=100,scipen=8)

```

# Oveview

In these notes we'll introduce the concept of random variables and probability distributions

- Random variables are how we describe events in numeric terms
- Probability distributions are how we summarize the probability of events (described by random variables) in terms of parameters that define the shape of the probability distirbution. 

Then, we'll take a more detailed look at concepts we've used in this course:

- Expected Values, which are probability weighted averages describe the "center" or the "balancing point" of a probability distribution
- Variances, which describe the spread of distribution (around it's expected value)

# Random Variables

- Random variables assign numeric values to each event in an experiment.
    - Mutually exclusive and exhaustive, together cover the entire sample space.
- Discrete random variables take on finite, or [countably infinite](http://mathworld.wolfram.com/CountablyInfinite.html) distinct values.
- Continuous variables can take on an uncountably infinite number of values.


## Example: Toss Two Coins

- $S={TT,TH,HT,HH}$
- Let $X$ be the number of heads
    - $X(TT)=0$
    - $X(TH)=1$
    - $X(HT)=1$
    - $X(HH)=2$

    

# Probability Distributions

- Broadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events. They come in two forms, which as we'll see really contain the same information

- Probability Mass/Density Functions
    - Discrete variables have probability mass functions (PMF)
    - Continuous variables have probability density functions (PDF)
- Cumulative Density Functions
    - Discrete: Summation of discrete probabilities
    - Continuous: Integration over a range of values

## Discrete distributions

- **Probability Mass Function (pmf):** $f(x)=p(X=x)$
- Assigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply 
  - **Positivity** For any event A, P(A) ≥ 0. In English, that's “For any event A, the probability of A is greater or equal to 0”.
  - **Certainty**  For S, the sample space, or set of all possible outcomes, the Pr(S)=1. That is, anytime this experiment is performed, something happens.
  - **Additivity** If events A and B are mutually exclusive outcomes (if A happens, B can't happen, If B happens, A can't happen), then the probabilty of A or B happening is $Pr(A \cup B) = Pr(A) + Pr(B)$, is the probability of A plus the probability of B
- **Cumulative Distribution Function (cdf)** $F(x_j)=p(X\leq x)=\sum_{i=1}^{j}p(x_i)$
- Sum of the probability mass for events less than or equal to $x_j$

## Example: Toss Two coins

- $S={TT,TH,HT,HH}$
- Let $X$ be the number of heads
    - $X(TT)=0$
    - $X(TH)=1$
    - $X(HT)=1$
    - $X(HH)=2$
- $f(X=0)=p(X=0)=1/4$
- $f(X=1)=p(X=1)=1/2$
- $F(X\leq 1) = p(X \leq 1)= 3/4$


## PMF and CDF of a die

Below I graph the pmf and cdf of a die


```{r,echo=F}
df <- data.frame(x=seq(0, 7), 
                 y=c(0,cumsum(rep(1,6)/6),1),
                 p=c(NA,rep(1,6)/6,NA))
df$xend <- c(df$x[2:nrow(df)], NA)
df$yend <- df$y

p.pmf <- ggplot(df, aes(x=x, y=p)) +
      geom_segment(aes(xend = x, yend = 0), size = 1)+geom_point()+
    ylim(0,1)+labs(title="PMF of Die")+xlim(0,7)
#p.pmf
p.cdf <- ggplot(df, aes(x=x, y=y, xend=xend, yend=yend)) +
      geom_vline(aes(xintercept=x), linetype=2, color="grey") +
      geom_point() +  # Solid points to left
      geom_point(aes(x=xend, y=y), shape=1) +  # Open points to right
      geom_segment()+  # Horizontal line segments
    ylim(0,1)+labs(title="CDF of Die")+xlim(0,7)
grid.arrange(p.pmf,p.cdf)

```

Each side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X<=2) = 1/6 + 1/6 = 1/3


## Continuous distributions

- **Probability Density Functions (PDF):** $f(x)$
    - Assigns probabilities to events in the sample space such that Kolmogorov Axioms still apply 
    - But... since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.
- **Cumulative Distribution Function (CDF)** $F(x)=p(X\leq x)=\int_{-\infty}^{x}f(x)dx$
    - Instead of summing up to a specific value (discrete) we integrate over all possible values up to $x$
    - Probability of having a value less than x

First, a brief aside  on integral calculus:

What's the area of the rectangle? $base\times height$

```{r,echo=F}
df<-data.frame(x=c(0,1),y=c(1,1))

p.rect<-ggplot(data.frame(x1=0,x2=1,y0=0,y1=1),
               aes(xmin=x1,xmax=x2,ymin=y0,ymax=y1)
               )+geom_rect(col="black",fill="black",alpha=.2)+xlim(-.5,1.5)+ylim(0,1.5)
p.rect
```

How would we find the area under a curve?

```{r,echo=F}
x<-rnorm(1000000)
plot(density(x),xlab="",ylab="",main="")
```

Well suppose we added up the areas of a bunch of rectangles roughly whose height's approximated the height of the curve?

```{r,echo=F}
hist(x,freq=F,xlab="",ylab="",main="")
lines(density(x))
```

Can we do any better? Let's make the rectangles smaller

```{r,echo=F}
hist(x,freq=F,xlab="",ylab="",main="",breaks=100)
lines(density(x))
```

What happens as the width of rectangles get even smaller, approaches 0? Our approximation get's even better:



## Link between PDF and CDF
If 
\[
F(x)=p(X\leq x)=\int_{-\infty}^{x}f(x)dx
\]
Then by the fundamental theorem of calculus

\[
\frac{d}{dx}F(x)=f(x)
\]

In words
    - the PDF ($f(x)$) is the derivative (rate of change) of the CDF ($F(X)$)
    - the CDF describes the area under the curve defined by f(x) up to x 



## Properties of the CDF

- $0\leq F(x) \leq 1$
- $F$ is non-decreasing and right continuous
- $\lim_{x\to-\infty}F(x)=0$
- $\lim_{x\to\infty}F(x)=1$
- For all $a,b \in \mathbb{R}$ s.t. $a<b$

\[
p(a < X \leq b) = F(b)- F(a) = \int_a^b f(x)dx 
\]

## Recall the PMF and CDF of a die

```{r,echo=F}
grid.arrange(p.pmf,p.cdf)
```

## What's the probability 

- $p(X=1)...p(X=6) =$\pause $1/6$
- $p( 2 < X \leq 5) =$\pause  $F(5)-F(2)=5/6-2/6=3/6=1/2$


# Common Probablity Distirbutions

In this course, we'll use probability distributions to 

- Model the data generating process as a function of parameters we can estimate
- To perform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)

There are a lot of probability distributions:
![](http://www.math.wm.edu/~leemis/chart/UDR/BaseImage.png)

Fortunately, the distributions you need to know to really master data science, is probably more something like
![](https://miro.medium.com/max/4854/1*szMCjXuMDfKu6L9T9c34wg.png)

And the distributions we'll work with the most in this class are an even smaller subset.

## Bernoulli Random Variables

Let's start with our old friend the coin flip


A coin flip is an example of a **Bernoulli random variable** defined by 1 parameter $p$, the probability of success. It has a pmf of

\[
  f(x) =
    \left\{
        \begin{array}{cc}
                p & \mathrm{if\ } x=1 \\
                1-p & \mathrm{if\ } x=0 \\
        \end{array} 
    \right.
\]


And a CDF of 

\[
  F(x) =
    \left\{
        \begin{array}{cc}
                0 & \mathrm{if\ } x<1 \\
                1-p & \mathrm{if\ } 0\leq x<1 \\
                1& \mathrm{if\ } x\geq1 \\
        \end{array} 
    \right.
\]

Note that in our coin flip example $p=0.5$ but it need not. Just imagine a weighted coin like the Patriots use at Foxborough

## Uniform Distribution

Our fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range $[a,b]$, the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of

\[
  f(x) =
    \left\{
        \begin{array}{cc}
                \frac{1}{b-a}& \mathrm{if\ } a \leq x\leq b \\
                0 & \text{otherwise} \\
        \end{array} 
    \right.
\]

And a CDF:

\[
  F(x) =
    \left\{
        \begin{array}{cc}
                        0 & x <a \\
                \frac{x-a}{b-a}& \mathrm{if\ } a \leq x < b \\
                1 & x \geq b \\
        \end{array} 
    \right.
\]

We won't run into uniform distributions all that often except in examples like rolling a fair sided die, but often they're used in Bayesian analysis as a form of uninformative prior.


## Binomial Distributions

The binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.

The key parameters are the number of trials $n$ and the probability of success for each trial $p$ and the pdf of a binomial distribution is:

\[
f(x)=\binom{n}{x}p^x (1-p) ^{1-x} \ \text{for x 0,1,2},\dots n
\]

So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads

```{r}
sum(df[1:11,2])
p <- .2
df <- data.frame(x = 0:1000, px = dbinom(0:1000,size=1000, prob=p))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  labs(title="PMF of Binomial Distribution (N=20,P=.2)")
```

The binomial distribution will come in handy when trying to model binary outcomes. 

## Poisson Distributions

What would happen if you let the $n$ in a binomial distribution go to infinity and $p$ go to 0 so that $np$ stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter $\lambda$ which represents rate at which events occur.


\[
f(x)=\frac{\lambda^x}{x!}e^{-\lambda}

\]


```{r}
# PDF of Poisson, lambda = 4
# Try changing lambda
lambda <- 20
df <- data.frame(x = 0:20, px = dpois(0:20, lambda))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  labs(title="PMF of Poisson Distribution (Lambda=4)")
```


## Geometric Distributions

What if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:

\[
f(x)=p(1-p) ^{x}
\]

```{r}
df <- data.frame(x = 0:20, px = dgeom(0:20, .5))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  labs(title="PMF of Geometric Distribution, (p=.5)")

```

## Exponential Distributions

Taking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a $\lambda = \frac{1}{\beta}$ rate parameter

\[
f(x)=\frac{1}{\beta}\exp\left[-x/\beta\right]
\]

```{r}
p.pdf.exp<-ggplot(data.frame(x = c(0, 5)), aes(x)) + stat_function(fun = dexp)+ylim(0,1)+labs(title="PDF of Exponential Distribution (Lambda=1)",y="")
p.cdf.exp<-ggplot(data.frame(x = c(0, 5)), aes(x)) + stat_function(fun = pexp)+ylim(0,1)+labs(title="CDF of Exponential Distribution (Lambda=1)",y="")
grid.arrange(p.pdf.exp,p.cdf.exp)
```


We often use exponential distributions to model things like "time until failure" where failure might be another war or the ending of an Italian cabinet.

## Normal Distribution

Finally, there's the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter $\mu$ that determines the center of a distribution and a scale parameter $\sigma^2$ that determines the spread of a distribution

\[
f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp \left[
-\frac{1}{2\sigma^2}(x-\mu)^2
\right]
\]

Standard normal: $X \sim N(\mu =0,\sigma^2=1)$

```{r}
p.pdf.norm<-ggplot(data.frame(x = c(-3, 3)), aes(x)) + 
  stat_function(fun = dnorm,)+ylim(0,1)+
  labs(title="PDF of Standard Normal Distribution",y="")
p.cdf.norm<-ggplot(data.frame(x = c(-3, 3)), aes(x)) + stat_function(fun = pnorm)+ylim(0,1)+labs(title="CDF of Standard Normal Distribution",y="")
grid.arrange(p.pdf.norm,p.cdf.norm)
```

- As we'll normal distributions tend to arise when ever you're summing variables. That is sum together a bunch of values from almost any distribution and the **distribution of their sums** tends to follow a normal distribution. Since lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)

Consider a binomial distribution with N=100 and p=.5. The pmf of this variable (black lollipops) follows a distribution that's closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.

```{r}
p <- .5
df <- data.frame(x = 0:100, px = dbinom(0:100,size=100, prob=p))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  stat_function(fun = dnorm,args = list(50,sqrt(100*.5*(1-.5))), col="red")+
  labs(title="PMF of Binomial Distribution (N=20,P=.5)")
```

A relationship explained more generally by the Central Limit Theorem, which we'll cover next week.

### What's the p(X$\leq$ 0) for a normal distirbution with mean 0 and sd 1

Since the normal distribution is so common, it's useful to get practice working with it's pdf and cdf.

Consider the following question: If X is normally distributed variable with $\mu=0$ and $\sigma=1$, what's the probability that X is less than 0  $p(X\leq0)=?$ We could solve:

\[
\int_{-\infty}^{0}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx=0.5
\]



But R's `pnorm()` function will quickly tell us

- $p(X\leq0)=$ `r pnorm(0)`

And we can visualize this as follows:

```{r,echo=F}
normal <- function(mu=0, sigma=1, x){
1/(sigma*sqrt(2*pi))*exp(-(x-mu)^2/(2*sigma^2))
}
normal_shade <- function(mu=0, sigma=1, x,l=-3,r=0){
y <- normal(mu=mu, sigma=sigma, x)
y[x < l | x > r] <- NA
return(y)
}


p.pdf.norm.1<-p.pdf.norm+
      stat_function(data=data.frame(x=c(-2.99, 0)), fun=normal_shade, geom = 'area', fill = 'red', alpha = 0.2,
                args=list(mu=0,sigma=1,l=-3,r=0))

p.cdf.norm.1<-p.cdf.norm+geom_segment(aes(x=-3,xend=0,y=.5,yend=.5,col="red"))+scale_color_discrete(guide=F)
   
    
grid.arrange(p.pdf.norm.1,p.cdf.norm.1)

```

Consider some other questions?

- $p(X=0)=0$
  - The probability that a continuous variable is exactly some value is always 0.
- $p(X<0)=0.5$
- $p(-1< X< 1)$
- $p(-2< X< 2)$

### p(-1 < X <  1)


- $p(-1< X< 1)=pr(X<1)-pr(X<-1)$ 

\[
\int_{-1}^{1}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx=0.841-0.158=0.682
\]


```{r,echo=F}
p.pdf.norm.2<-p.pdf.norm+
      stat_function(data=data.frame(x=c(-2.99, 0)), fun=normal_shade, geom = 'area', fill = 'red', alpha = 0.2,
                args=list(mu=0,sigma=1,l=-1,r=1))

p.cdf.norm.2<-p.cdf.norm+
    geom_segment(aes(x=-3,xend=-1, y=pnorm(-1),yend=pnorm(-1),col="red"))+
    geom_segment(aes(x=-3,xend=1, y=pnorm(1),yend=pnorm(1),col="red"))+
    scale_color_discrete(guide=F)

grid.arrange(p.pdf.norm.2,p.cdf.norm.2)

```


### p(-2 < X < 2)

- $p(-2< X\leq 2)=$ `r pnorm(2)-pnorm(-2)`

```{r,echo=F}
p.pdf.norm.3<-p.pdf.norm+
      stat_function(data=data.frame(x=c(-2.99, 0)), fun=normal_shade, geom = 'area', fill = 'red', alpha = 0.2,
                args=list(mu=0,sigma=1,l=-2,r=2))

p.cdf.norm.3<-p.cdf.norm+
    geom_segment(aes(x=-3,xend=-2, y=pnorm(-2),yend=pnorm(-2),col="red"))+
    geom_segment(aes(x=-3,xend=2, y=pnorm(2),yend=pnorm(2),col="red"))+
    scale_color_discrete(guide=F)

grid.arrange(p.pdf.norm.3,p.cdf.norm.3)

```

We'll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0.

# Expected values and variances


## Expected Value

A (probability) weighted average of the possible outcomes of a random variable, often labeled $\mu$ 

Discrete:

\[
\mu_X=E(X)=\sum xp(x) 
\]

Continuous
\[
\mu_X=E(X)=\int_{-\infty}^{\infty}xf(x) dx
\] 

## What's the expected value of a 1 roll of fair die?

\begin{align*}
E(X)&=\sum_{i=1}^{6}x_ip(x_i)\\
     &=1/6\times(1+2+3+4+5+6)\\
     &= 21/6\\
     &=3.5
\end{align*}

## Properties of Expected Values

- $E(c)=c$
- $E(a+bX)=a+bE[X]$
- $E[E[X]]=X$
- $E[E[Y|X]]=E[Y]$
- $E[g(X)]=\int_{-\infty}^\infty g(x)f(x)dx$
- $E[g(X_1)+\dots+g(X_n)]=E[g(X_1)]+\dots E[g(X_n)$
- $E[XY]=E[X]E[Y]$ if $X$ and $Y$ are independent

## How many times would you have to roll a fair die to get all six sides?

We can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success. The expected value of a geometric variable is:

\[
\begin{align*}E(X)&=\sum_{k=1}^{\infty}kp(1-p)^{k-1} \\
&=p\sum_{k=1}^{\infty}k(1-p)^{k-1} \\
&=p\left(-\frac{d}{dp}\sum_{k=1}^{\infty}(1-p)^k\right) \text{(Chain rule)} \\
&=p\left(-\frac{d}{dp}\frac{1-p}{p}\right) \text{(Geometric Series)} \\
&=p\left(\frac{d}{dp}\left(1-\frac{1}{p}\right)\right)=p\left(\frac{1}{p^2}\right)=\frac1p\end{align*}
\]

For this question, we need to calculate the probability of success, p, after getting a side we need.

The probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:

```{r}
ev <- c()
for(i in 6:1){
  ev[i] <- 6/i
  
}
# Expected rolls for each 1 through 6th side
rev(ev)
# Total 
sum(ev)
```


## Variance

If $X$ has a finite mean $E[X]=\mu$, the $E[(X-\mu)^2]$ is finite and called the variance of $X$ which we write as $\sigma^2$ or $Var[X]$.

Note:
\[
\begin{align*}
\sigma^2=E[(X-\mu)^2]&=E[(X^2-2\mu X+\mu^2)]\\
&= E[X^2]-2\mu E[X]+\mu^2\\
&= E[X^2]-2\mu^2+\mu^2\\
&= E[X^2]-\mu^2\\
&= E[X^2]-E[X]^2
\end{align*}
\]

- "The variance of X is equal to the expected value of X-squared, minus the square of X's expected value."
- $\sigma^2=E[X^2]-E[X]^2$ is a useful identity in proofs and derivations

## Variance and Standard Deviations

We often think of variances $Var[X]$ as describing the spread of a distribution

\[
\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2
\]

A standard deviation is just the square root of the variance

\[
\sigma=\sqrt{Var[X]}
\]

## Covariance

Covariance measures the degree to which two random variables vary together. 

- $Cov[X,Y] \to +$ An increase in $X$ tends to be larger than its mean when $Y$ is larger than its mean

\[
Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]
\]

## Properties of Variance and Covariance

- $Cov[X,Y]=E[XY]-E[X]E[Y]$
- $Var[X]=E[X^2]-(E[X])^2$
- $Var[X|Y]=E[X^2|Y]-(E[X|Y])^2$
- $Cov[X,Y]=Cov[X,E[Y|X]]$
- $Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]$
- $Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]$


## Correlation

- The correlation between $X$ and $Y$ is simply the covariance of $X$ and $Y$ divided by the standard deviation of each.

\[
\rho=\frac{Cov[X,Y]}{\sigma_X\sigma_Y}
\]

- Normalize covariance to a scale that runs between [-1,1]

## Question: If two variables have zero covariance, are they independent?


No

A trivial example

- $p(X=x)=1/3$ for $x = -1,0,1$ and let $Y=X^2$
- Y clearly depends on X even though their covariance is 0

```{r,results="markdown"}
p=1/3
x=-1:1
y=x^2
cor(x,y)
```


