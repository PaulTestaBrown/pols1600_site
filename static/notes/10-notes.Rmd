---
title: "POLS 1600: Uncertainty: Sampling Distributions, Standard Errors and Confidence Intervals"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
always_allow_html: yes
output: 
  html_document:
      toc: true
      toc_float: yes
---


```{r init,echo=F,message=F}
# Set repo manually
options(repos="https://cran.rstudio.com" )

# Load required packages
if (!require("pacman")){ install.packages("pacman") }
pacman::p_load("tidyverse","mosaic","knitr","Hmisc","weights","ggplot2","devtools","scales","car","gridExtra","xtable","texreg","kableExtra","reshape2","readstata13")

# Set markdown options
opts_chunk$set(eval=T,echo=T,message=F,warning=F,cache=T)



```


# Annoucments

# Overview:

Over the next few weeks, we'll develop tools to help us assess uncertainty and draw statistical inference from our models.

We'll consider two broad approaches:

1. Characterizing uncertainty in estimation via sampling distributions (this week)
2. Characterizing uncertainty in testing via reference distributions (next week)

Our goals today are 

1. To understand the concept of a sampling distribution
2. See how we can simulate a sampling distribution by sampling from our sample as our sample was drawn from the population
3. Characterize sampling distributions via standard errors and confidence intervals
4. Produce standard errors and confidence intervals for linear models via this re-sampling approach.
5. See how statistical theory lets us calculate these quantities analytitically (with simple formulas).


# Definitions

To begin, let's lay out some definitions that we'll use through out these notes

- **Population**: All the cases from which you could have sampled
- **Parameter:** A quantity or quantities of interest often generically called $\theta$ ("theta"). Something we'd like to know about our population
- **Sample:** A (random) draw from that population
- **Sample Size:** The number of observations in your draw (without replacement)
- **Estimator:** A rule for calculating an *estimate* of our parameter of interest. 
- **Estimate:** The value produced by some estimator for some parameter from some data. Often called $\hat{\theta}$ 
- **Unbiased estimators:** $E(\hat{\theta})=E(\theta)$ On average, the estimates produced by some estimator will be centered around the truth
- **Consistent estimates:** $\lim_{n\to \infty} \hat{\theta_N} = \theta$ As the sample size increases, the estimates from an estimator converge in probability to the parameter value
- **Sampling Distribution:** How some estimate would vary if you took repeated samples from the population
- **Standard Error:** The standard deviation of the sampling distribution
- **Resampling Distribution:** How some estimate would vary if you took repeated samples **from your sample WITH REPLACEMENT** 
    - "Sampling from our sample, as the sample was sampled from the population."
- **Statistic:** A summary of the data (mean, regression coefficient, $R^2$). An estimator without a specified target of inference (E.g. the the sample mean is a statistic. It is an unbiased estimate of the population mean, but in general, a biased estimate of the population variance.)

# Sampling distriubtions

Now let's illustrate these concepts using data from the CCES

```{r}
# Load CCES data
load(url("https://raw.github.com/PTesta/POLS_1600/master/cces.rda"))

```

The CCES is a **sample** of size `r dim(cces)[1]` drawn from a known population (Residents of the U.S.).

For our purposes, we'll assume the data are a simple random sample from the population^[In fact, the CCES engages in a complex sampling and weighting procedure, which while important for analysis, is not our central concern here.]

Suppose we wanted to know the average income of residents of the United States. We could use the **sample mean** as an **unbiased estimate** of some unknown true value of the mean if we had been able to ask this question of every person in the U.S.

That value is easy to calculate from our data:

```{r}
summary(cces$income)
mean(cces$income,na.rm = T)
```

So in our sample with a measure of income that runs from 1 to 16, we see an average value of 6.328 which, if we looked up the codebook for the CCES corresponds  to an income between \$50,000 - \$59,999 on that 16-point scale.

**How different, might our estimate have been, had we had a different CCES with a different sample of size `r dim(cces)[1]` ?**

The answer is characterized by a hypothetical **sampling distribution** of the sample means for income for an infinite number CCES surveys, conducted in the same manner but with different respondents in the survey.

But we only have one sample! 

How can we characterize what would happen in repeated sampling?

The approach we'll take today is to is to **sample from our sample with replacement as our sample was sampled from the population.** 

# Simulating Sampling Distributions

## Treating the CCES as the Population

Let's suppose the CCES was our population of interest. So we know our parameter of interest is 

```{r}
mu_income <- mean(cces$income,na.rm = T)
sd_income <- sd(cces$income,na.rm = T)

```

That is, our population mean income $\mu=$`r mu_income` and the standard deviation of income in the popuation is $\sigma=$`sd_income` 

These quantities provide a rough summary of the distribution in the data

```{r,echo=F}
plot(density(cces$income,na.rm=T))
rug(cces$income)
abline(v=mu_income,col="red")
segments(x0=mu_income-sd_income,x1=mu_income+sd_income,y0=.05,y1=.05,col="blue")
```


## Draw a sample from the  CCES

Now lets take a sample of size 100 from our population and calculate its mean

```{r,echo=T}
set.seed(12345)
samp1<-mosaic::sample(x=cces,size = 100,replace=F)
mu_samp1_inc<-mean(samp1$income,na.rm=T)
mu_samp1_inc # Sample Mean
```

And standard deviation:
```{r,echo=T}
sd_samp1_inc<-sd(samp1$income,na.rm=T)
sd_samp1_inc # Sample SD
```

So ` r mu_samp1_inc` is pretty close to `r mu_income` but what if we had taken a different sample?

## Draw another sample from the  CCES

If we drew another sample of size 100, we'd get another sample mean:

```{r}
set.seed(123456)
samp2<-mosaic::sample(x=cces,size = 100,replace=F)
mu_samp2_inc<-mean(samp2$income,na.rm=T)
cbind(mu_samp1_inc,mu_samp2_inc,mu_income)
```

This time our sample mean is somewhat lower than the population value.

## Simulating our sampling distribution

Imagine we could draw a 1,000 or 10,000, or an infinite number of samples of size 100 from the 2016 CCES. How would the average level of income vary in each sample?

We'll let's find out. 

```{r}
set.seed(123)
# Take sample of size 100 from CCES and calculate mean
mu_inc_samp_dist<-do(1000)*with(mosaic::sample(x=cces,size = 100,replace=F),
                                mean(income,na.rm=T))

# Look at "bootstrapped" means
mu_inc_samp_dist[998:1000,] 

```

Make sure you understand what's going in the code above:

- `set.seed(123)` sets the "random" seed so that we all get the same results from this simulation
- `mu_inc_samp_dist` is the outcome of our simulation. (The sampling distribution)
- `do(1000)*` is a function that does whatever comes after the asterisk, the number of times you specify inside the parentheses
- `with(mosaic::sample(x=cces,size = 100,replace=F),` this is how we're generating a sample of size 100 from the CCES **without replacement**. The reason we're doing this without replacement is become we're assuming the population is the CCES. So this is like running a 1000 surveys of size 100. As we'll see later on, when we have only a sample and not the population, we'll sample **with replacement** to approximate this idea of repeated sampling.
- `mean(income,na.rm=T)` calculates the mean for that sample
- `mu_inc_samp_dist[998:1000,] ` shows us the the last three results of our simulation (three points in our sampling distribution)

## Summarzing our sampling distribution 

- We can take the standard deviation of this sampling distribution to describe the range of plausible values for the average value of income that we could have seen.

```{r}
# Calculate standard error of average income
mu_inc_se<-sd(mu_inc_samp_dist[,1])
mu_inc_se
```

- Note that 95 percent of the means in this distribution have values within $1.96\times S.E.$ of the population mean.

```{r,echo=T}
# Use SE to construct 95 percent coverage interval
ll<-mu_income-1.96*mu_inc_se
ul<-mu_income+1.96*mu_inc_se
mean(mu_inc_samp_dist[,1]>=ll&mu_inc_samp_dist[,1]<=ul)
```

- Which we can visualize as follows

```{r,echo=F}
plot(density(cces$income,na.rm=T),col="grey",ylim=c(0,1.5))
lines(density(mu_inc_samp_dist[,1]),ylim=c(0,1.3))
abline(v=mu_income,col="red")
segments(x0=ll,x1=ul,y0=1.24,y1=1.24)
```

- The grey curve shows the density of the income values in the population
- The black curve shows the distribution of sample means from 1000 samples of size 100 from this population.
- The red line shows population mean 
- The black bar shows the 95 percent coverage interval defined by the 2 times the standard deviation of the Sampling distribution.

## Resampling Distribution

Suppose we only had one sample. We can use that sample to obtain an unbiased estimate of the standard error (the standard deviation of our sampling distribution).

We do so by sampling **with replacement,** to create a re-sampling distribution. We call this bootstrapping, because we're using our sample to lift ourselves up by the bootstraps to estimate some unknown quantity like the standard deviation of a hypothetical sampling distribution

Below, are two **resampling** distributions generate using two different samples of size 100.

```{r}
table(resample(samp1$income))
set.seed(123)
mu_inc_resamp1_dist<-do(1000)*mean(resample(samp1$income,replace=T),na.rm = T)
mu_inc_resamp2_dist<-do(1000)*mean(resample(samp2$income,replace=T),na.rm = T)
```

We calculate the **standard errors** (Again, just the standard deviation of a sampling distribution) for each, and compare them to what the standard errors of our simulated sampling distribution (`mu_inc_se`, that is the standard error calculated from actually taking repeated samples from the population and not sampling with replacement from a sample), as well as what the asymptotic theory (e.g. the Central Limit Theorem), would lead us to expect ($\sigma/\sqrt{n}$)

```{r}
mu_inc_resamp1_se<-sd(mu_inc_resamp1_dist[,1])
mu_inc_resamp2_se<-sd(mu_inc_resamp2_dist[,1])
mu_inc_assymp_se<-sd_income/sqrt(100)
# SEs from bootstrapping vs theoretical sampling distribution and assymptotic
rbind(mu_inc_resamp1_se,mu_inc_resamp2_se,mu_inc_se,mu_inc_assymp_se)
```

So even though each individual sample had a different mean, the standard errors from those re-sampling distributions are close (unbiased) estimates to the standard error of the hypothetical sampling distribution

## Visually Comparing Standard Errors

We can see this below, by plotting the sampling distribution in black, and comparing it the re-sampling distributions of our two size 100 samples.

```{r,echo=F}
# Confidence intervals
ll1<-mu_samp1_inc-1.96*mu_inc_resamp1_se
ul1<-mu_samp1_inc+1.96*mu_inc_resamp1_se
ll2<-mu_samp2_inc-1.96*mu_inc_resamp2_se
ul2<-mu_samp2_inc+1.96*mu_inc_resamp2_se

plot(density(mu_inc_samp_dist[,1]),ylim=c(0,1.3))
abline(v=mu_income)
segments(x0=ll,x1=ul,y0=1.24,y1=1.24)

lines(density(mu_inc_resamp1_dist[,1]),col="blue")
abline(v=mu_samp1_inc,col="blue")
segments(x0=ll1,x1=ul1,y0=1.2,y1=1.2,col="blue")
lines(density(mu_inc_resamp2_dist[,1]),col="red")
abline(v=mu_samp2_inc,col="red")
segments(x0=ll2,x1=ul2,y0=1.22,y1=1.22,col="red")


```

The actual sampling distribution is centered around the truth.

The re-sampling distributions are **centered around the sample means of each sample**, but the widths of these distributions are approximately the same as width of the true sampling distribution. This is the foundation for how we think about confidence intervals


# Confidence Intervals

Recall the intervals constructed from our two samples. The first sample was quite close to the true value. The second sample was fairly far away and, in fact, the population mean falls just outside the upper limit of second sample's 95 percent coverage interval.

```{r}
mu_income

mu_income>=ll2&mu_income<=ul2
```

This is a key point, the interval for any sample, either does or does not contain the truth.

When we talk about a 95 percent confidence interval or confidence is about the properties of the interval, not a particular estimate.

- Specifically, in repeated sampling, we would expect 95 percent of the intervals constructed in the same manner to contain the "true" value (the population value of our parameter)

To illustrate this concept

- Suppose we took 100 samples of size 100 from the CCES. 
- For each sample, we constructed 95 percent confidence intervals around the sample mean for income, by sampling with replacement a 1000 times, taking that standard deviation of that re-sampling distribution and using that to construct a 95 percent confidence interval
- On average, we would expect 95 percent of those intervals to contain the "true" value of the average income (here the average income for the full CCES).

To do this let's write a function that do this process once

```{r,echo=F}
# Construct 100, 95 percent confidence intervals for sample of size 100
my_ci_fn<-function(x,nsamp=100,nsim=1000,level=.95,bs=F){
    # Take a sample of size "nsamp"
    y<-sample(x=na.omit(x),size=nsamp,replace=F)
    # Calculate the mean
    mu<-mean(y,na.rm=T)
    # If bs=TRUE do bootstrapped SEs 
    if(bs==T){
    mu_dist<-do(nsim)*mean(resample(y),na.rm=T)
    se<-sd(mu_dist[,1])}else{
        # Otherwise, just use assymptotic result (Quicker)
        se<-sd(y,na.rm=T)/sqrt(nsamp-1)
    }
    # Significance level
    the.p<-1-(1-level)/2
    # Calculate lower and upper limits of interval
    ll<-mu-qt(p=the.p,df=nsamp-1)*se
    ul<-mu+qt(p=the.p,df=nsamp-1)*se
    results<-cbind(mu=mu,ll=ll,ul=ul,se=se)
    return(results)
    
}
```

Then lets repeat this process a 100 times:

```{r}
set.seed(123)
CIs<-do(100)*my_ci_fn(cces$income,nsamp=100)
CIs$sample<-as.numeric(rownames(CIs))

CIs[1,]
```

And plot the results
```{r, echo=T}
ggplot(CIs,aes(x=mu,
               y=sample,
               xmin=ll,
               xmax=ul,
               col=ll<mu_income&ul>mu_income))+
  geom_point()+
  geom_vline(xintercept = mu_income)+
  geom_errorbarh()+
  scale_color_discrete(guide=F)+
  labs(title="95% CIs for 100 samples")
```

Each point is the mean from one sample of size 100. Each bar is a 95 percent confidence interval for that sample. In this particular case, 94 out of the 100 intervals we constructed contained the truth.

Note the how some sample estimates are fairly far from the true population. This is why we interpret intervals as providing a range of plausible values, rather than telling us anything about the certainty of a particular point estimate. 

We're not 95 percent certain that the true average value of income is `CIs[1,1]`. 

Rather any value between `CIs[1,2]` and `CIs[1,3]` is a plausible estimate for the true value of $\mu$, the average income in the population, because 95 percent of the time, intervals constructed in this manner contain the truth.

## Standard Errors and Sample Size

As we saw in last week's lab, standard errors (the width of the sampling distribution)  decreases as a function of the $\sqrt{N}$ (the square root of the sample size)

```{r}
nsamp10<-do(100)*my_ci_fn(cces$income,nsamp=10)
nsamp100<-do(100)*my_ci_fn(cces$income,nsamp=100)
nsamp1000<-do(100)*my_ci_fn(cces$income,nsamp=1000)
nsamp10000<-do(100)*my_ci_fn(cces$income,nsamp=10000)
```


```{r}
# Standard errors decrease as sample size increases
mean(nsamp10$se)
mean(nsamp100$se)
mean(nsamp1000$se)
mean(nsamp10000$se)
# Specifically, by the square root of the sample size
sd_income/sqrt(10)
sd_income/sqrt(100)
sd_income/sqrt(1000)
sd_income/sqrt(10000)
```

And while the width of the interval deceases as the sample size increases, the properties of the confidence interval remain the same: 95 percent of the intervals expected to contain the truth. The range of plausible values however gets much smaller.

```{r,echo=F}
nsamp10$sample<-1:100
nsamp100$sample<-1:100
nsamp1000$sample<-1:100
nsamp10000$sample<-1:100

p10<-ggplot(nsamp10,aes(x=mu,y=sample,xmin=ll,xmax=ul,col=ll<mu_income&ul>mu_income))+geom_point()+geom_vline(xintercept = mu_income)+
    geom_errorbarh()+scale_color_discrete(guide=F)+labs(title="95% CIs for 100 samples of\nSize 10")+xlim(0,12)
p100<-ggplot(nsamp100,aes(x=mu,y=sample,xmin=ll,xmax=ul,col=ll<mu_income&ul>mu_income))+geom_point()+geom_vline(xintercept = mu_income)+
    geom_errorbarh()+scale_color_discrete(guide=F)+labs(title="95% CIs for 100 samples of\nSize 100")+xlim(4,8.5)
p1000<-ggplot(nsamp1000,aes(x=mu,y=sample,xmin=ll,xmax=ul,col=ll<mu_income&ul>mu_income))+geom_point()+geom_vline(xintercept = mu_income)+
    geom_errorbarh()+scale_color_discrete(guide=F)+labs(title="95% CIs for 100 samples of\nSize 1,000")+xlim(4,8)
p10000<-ggplot(nsamp10000,aes(x=mu,y=sample,xmin=ll,xmax=ul,col=ll<mu_income&ul>mu_income))+geom_point()+geom_vline(xintercept = mu_income)+
    geom_errorbarh()+scale_color_discrete(guide=F)+labs(title="95% CIs for 100 samples of\nSize 10,000")+xlim(4,8)

grid.arrange(p10,p100,p1000,p10000)
```


# Confidence Intervals for Coefficients from Linear Models

- We can use re-sampling to construct confidence intervals around coefficients from our models. The same principles apply:
    - Fit our model on our sample
    - Fit a number of (~1000) additional models sampling with replacement from our sample
    - Calculate the the standard deviations of the estimates using this (bootstrapped) sampling distribution
    - Use this standard error  to construct confidence intervals
- We can also use formulas to describe the asymptotic behavior of these coefficients (what we would expect from infinite samples)

## Bootstrapped SEs

```{r}
nboots<-1000 # number of bootstraps
# Model
m1<-lm(voted01~contact01+female01+nonwhite01+income+
                education+anyunion01+pid+ideology+voted2012,data=cces)
# Data frame with coefficients from 200 simulations
m1_bs<-do(nboots)*lm(voted01~contact01+female01+nonwhite01+income+
                         education+anyunion01+pid+ideology+voted2012,
                     data=sample(cces,replace=T))

```

## Look at bootstrap simulations for contact and ideology

```{r}
m1_bs[10:13,c("contact01","ideology")]
dim(m1_bs)
sd(m1_bs$contact01)
```

What's a plausible range of coefficients for contact? Let's construct a 95 percent confidence interval and see!

```{r}
contact01_se<-sd(m1_bs$contact01)
contact01_CI<-coef(m1)["contact01"]+contact01_se*c(-2,2)
contact01_CI
confint(m1,"contact01")

```

What's a plausible range of coefficients for ideology?

```{r}
ideology_se<-sd(m1_bs$ideology)
ideology_CI<-coef(m1)["ideology"]+ideology_se*c(-2,2)
ideology_CI
```

# Assymptotic Standard Errors

## Variance of OLS estimators

Your textbook on page 378-380 walks through how you would calculate the standard error for a coefficient in a simple linear regression--that is a model with one outcome and one predictor. The same principals hold when you have multiple predictors, you just need a little linear algebra:

First, we'll define our estimated coefficients $\hat{\beta}$ in terms of their true values $\beta$ and some estimation error ($(X^\prime X)^{-1}X^\prime\epsilon$)

\[
\hat{\beta} = \beta+ (X^\prime X)^{-1}X^\prime\epsilon
\]

So the variance of $\hat{\beta}$ is a function of the estimation error (since the true values $\beta$ are fixed in the population)

\[
E[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^\prime]=E[((X^\prime X)^{-1}X^\prime\epsilon)((X^\prime X)^{-1}X^\prime\epsilon)^\prime]
\]

Recalling from the class in linear algebra we may or may not have had that, $(AB)^\prime=B^\prime A^\prime$, we can write

\[
E[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^\prime]=E[((X^\prime X)^{-1}X^\prime\epsilon)((X^\prime X)^{-1}X^\prime\epsilon)^\prime]
\]

As:

\[
E[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^\prime]=E[((X^\prime X)^{-1}X^\prime\epsilon\epsilon^\prime X(X^\prime X)^{-1}]
\]

Assuming a fixed $X$, $E[\epsilon\epsilon^\prime]=\sigma^2I$ and so

\[
\begin{align*}
E[(\hat{\beta}-\beta)(\hat{\beta}-\beta)^\prime]&=(X^\prime X)^{-1}X^\prime E[\epsilon\epsilon^\prime ]X(X^\prime X)^{-1}\\
&=(X^\prime X)^{-1}X^\prime[\sigma^2 I]X(X^\prime X)^{-1}\\
&=\sigma^2 I(X^\prime X)^{-1}X^\prime X(X^\prime X)^{-1}\\
&=\sigma^2 (X^\prime X)^{-1}
\end{align*}
\]

Where we can estimate $\sigma^2$ from the data

\[
\hat{\sigma}^2=\frac{\epsilon^\prime\epsilon}{n-k}
\]

Don't worry if you don't follow all the math. 

The key takeaway here is that statistical theory gives us an analytic solution (a formula) for calculating the variance of coefficients in our model. 

Conceptually, the variance of a coefficient is a function of the sum of squared residuals ($\sigma^2$) weighted by a matrix that captures the variance of our predictors ($(X^\prime X)^{-1}$


## Calculating Assymptotic SEs by hand.

Now let's see this formula in action. Again, we'll use data from the 2016 CCES

```{r}
# Load CCES data
load(url("https://raw.github.com/PTesta/POLS_1600/master/cces.rda"))

```


The steps are as follows

0. Estimate the model
1. Obtain the "model matrix", X
2. Calculate $(X^\prime X)^{-1}$
3. Determine the degrees of freedom ($n-k$)
4. Estimate $\hat{\sigma}^2=\frac{\epsilon\epsilon^\prime}{n-k}$
5. Obtain $\sigma^2 (X^\prime X)^{-1}$, the "Variance-Covariance Matrix of the model"
6. Calculate the standard errors of the coefficients (the square root of the diagonals)

## 0. Estimate the model

First we fit a model

```{r}
m1<-lm(voted01~contact01+female01+nonwhite01+income+
                education+anyunion01+pid+ideology+voted2012,data=cces)
```

##  1. Obtain the "model matrix", X

Next we extract the model matrix -- basically the predictors from our model excluding any rows with NAs and converted into a matrix with a column of 1s for the intercept so that R can manipulate it with linear algebra

```{r}
X<-model.matrix(m1)
class(X)
head(X)
```

## 2. Calculate $(X^\prime X)^{-1}$

Then we use some built in functions to take the inverse of "X transpose X". 

```{r}
XtXinv<-solve(t(X)%*%X)
dim(XtXinv)
```

Think of "X transpose X" as the matrix analog to summing the squared terms in our model, and the the inverse as the matrix algebra version of division. 

```{r}
sum(X[,2]^2)
(t(X)%*%X)[2,2]
```


## 3. Determine the degrees of freedom ($n-k$)

Don't worry to much about this, just know that when estimate terms in our model, we use up degrees of freedom (pieces of information), and so we adjust our estimate of $\sigma^2$ to account for the fact that we've already calculated $k$ other quantities in our model.

```{r}
n<-dim(X)[1]
k<-dim(X)[2]
df<-n-k
df
m1$df.residual
```

## 4. Estimate sum of squared residuals

Again recall that residuals are just the observed minus the predicted values from the model.

```{r}
e<-resid(m1)
sigma_sq<-t(e)%*%e/df
sigma_sq
# Same as
sum(e^2)/df
```

## 5. Obtain $\sigma^2 (X^\prime X)^{-1}$, the "Variance-Covariance Matrix of the model"

The diagonals of this matrix contain the variance of the coefficients

```{r}
m1_vcov<-as.vector(sigma_sq)*XtXinv
# Same as
all.equal(m1_vcov,vcov(m1))
```


## 6. Calculate the standard errors of the coefficients (the squareroot of the diagonals)

Taking the square root of the variance gives us the standard errors

```{r}
m1_se<-sqrt(diag(m1_vcov))
```

## Compare Bootstrapped SE to Analytical SE

Which we can compare to those obtained from bootstrapping:

```{r}
nboots<-1000 # number of bootstraps
# Model
# Data frame with coefficients from 200 simulations
m1_bs<-do(nboots)*lm(voted01~contact01+female01+nonwhite01+income+
                         education+anyunion01+pid+ideology+voted2012,
                     data=sample(cces,replace=T))
m1_se
apply(m1_bs[,1:10],2,sd)

```

## Finally, SEs the easy way

Now that you've gone through all that work to understand the concept of what a standard error is (the standard deviation of a sampling distribution), and see how theory let's us calculate this quantity analytically (with math), you are free to let R's `summary()` function do the work for you

```{r}
summary(m1)
# Just the standard errors:
summary(m1)$coef[,2]
```



# Assympotitc versus Bootstrapped Standard Errors

OMG Paul, why didn't you tell us we could just use `summary()` on the output of lm to get standard errors. Why go through all that work?!

Well, first, doing the simulations helps clarify conceptually what it is that a standard error is quantifying.

Second, sometimes there isn't a nice analytic formula. We can still produce valid standard errors by simulating the sampling process with replacement.

Finally, the formulas for asymptotic standard errors require slightly more stringent assumptions; things like the errors are homoskedastic (have a constant variance), and that your sample is sufficiently large for things to start converging by the central limit theorem. In finite samples, we don't know if those assumptions are appropriate.

Consider the following simulation:

We'll simulate a population where the variance of the errors is non-constant. 

```{r}
set.seed(123)
x <- abs(rnorm(100000))
y <- x + rnorm(100000)*x
pop <- data.frame(y,x)
summary(lm(y~x,pop))

plot(x, resid(lm(y~x,pop)))
```

Let's calculate the true value of the coefficient on x in the population and save it as beta
```{r}
beta <- coef(lm(y~x,pop))[2]
```

Now let's take a look at the standard errors from asymptotic versus simulation based approaches for a sample of say size 200. 

First let's generate an actual sampling distribution, by sampling without replacement from our population

```{r}
sample_dist <- do(1000)*coef(lm(y~x,sample(pop,200,replace=F)))
se_pop <- sd(sample_dist$x)
```

So the hypothetical standard for our population should be close to `r se_pop`


Now let's consider the standard error our analytic formula would give us for one sample of size 200

```{r}
samp <- sample(pop, 200, replace = F)
summary(lm(y~x,samp))
se_asymptotic <- summary(lm(y~x,samp))$coef[2,2]
se_asymptotic
```

And compare that to the standard error from bootstrapping

```{r}
bs_dist <- do(1000)*coef(lm(y~x,sample(samp,replace=T)))
sd(bs_dist$x)
```

The analytic standard error is about half the size of what we know the true standard error, while the standard error from bootstrapping is much closer.


IF you're interested, you can uncomment the code below, and simulate the confidence intervals using both approaches:

```{r}
# compare_asym_bs <- function(){
#   samp <- sample(pop, 200, replace = F)
#   fit <- summary(lm(y~x,samp))
#   beta <- fit$coefficients[2,1]
#   ll <- confint(fit)[2,1]
#   ul <- confint(fit)[2,2]
#   bs <- do(1000)*coef(lm(y~x,sample(samp,replace=T)))
#   se_bs <- sd(bs$x)
#   ll_bs <- beta - 1.96*se_bs
#   ul_bs <- beta + 1.96*se_bs
#   results <- c(beta=beta, ll=ll,ul=ul, ll_bs = ll_bs,ul_bs= ul_bs)
#   return(results)
#   }
# 
# compare_dist <- do(100)*compare_asym_bs()
# 
# compare_dist$n_sim <- 1:100
# 
# ggplot(compare_dist,aes(V1,n_sim))+
#   geom_point()+
#   geom_errorbarh(aes(xmin=V2, xmax=V3, col=V2<beta&V3>beta))+
#   geom_vline(xintercept =beta)
# 
# ggplot(compare_dist,aes(V1,n_sim))+
#   geom_point()+
#   geom_errorbarh(aes(xmin=V4, xmax=V5, col=V4<beta&V5>beta))+
#   geom_errorbarh(aes(xmin=V2, xmax=V3, col=V2<beta&V3>beta))+
#   geom_vline(xintercept =beta)


```


The confidence intervals based off of standard errors using analytic, large-sample, asymptotic assumptions tend to be too small -- providing incorrect coverage (i.e. they bracket the truth less than 95 percent of the time).^[So-called robust-standard errors, provide analytical estimates that are robust to violations like non-constant error variance, and produce estimates closer to the bootstrapped standard errors.]

> TLDR: Asympotitic theory allows for quick analytical calculations of standard errors. If the assumptions necessary for the asymptotics to work aren't met, then the standard errors will be wrong, and inferences may be incorrect.


# Summary

- Statistical inference involves quantifying our uncertainty about what could have happened
- We can describe this uncertainty in terms of a sampling distribution, characterized by their standard errors 
- We can estimate standard errors via simulation or analytically. Simulations require fewer assumptions, but take more time. Analytic estimates are are quick, but require more assumptions.
- We use standard errors to construct confidence intervals which describe a range of plausible values for the thing we're trying to estimate.
- Any one 95% confidence interval may or may not contain the truth, but in repeated sampling, 95% of the intervals we construct would contain the truth.

