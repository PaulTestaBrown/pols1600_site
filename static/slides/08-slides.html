<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 08:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Paul Testa" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/brown.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Week 08:
## Probably too much Probability Theory
### Paul Testa

---

















---
class: inverse, center, middle
# Overview

---
## General Plan

- Setup
- Feedback
- Review
- Lecture
  - Probability Theory
  - Conditional Probability 
  - Random variables and probability distributions
  - Expected values and variances


---
class:inverse, middle, center
# ðŸ’ª
## Get set up to work

---
## New packages

And tools for doing Permutations and Combinations


```r
install.packages("gtools")
```



---
## Packages for today



```r
the_packages &lt;- c(
  ## R Markdown
  "kableExtra","DT","texreg", "htmltools",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Graphics:
* "scatterplot3d",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "zoo", 
* "gtools"
)
```

---
## Define a function to load (and if needed install) packages



```r
ipak &lt;- function(pkg){
    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}
```

---
## Load packages for today


```r
ipak(the_packages)
```

```
   kableExtra            DT        texreg     htmltools     tidyverse 
         TRUE          TRUE          TRUE          TRUE          TRUE 
    lubridate       forcats         haven      labelled         ggmap 
         TRUE          TRUE          TRUE          TRUE          TRUE 
      ggrepel      ggridges      ggthemes        ggpubr        GGally 
         TRUE          TRUE          TRUE          TRUE          TRUE 
       scales       dagitty         ggdag       ggforce scatterplot3d 
         TRUE          TRUE          TRUE          TRUE          TRUE 
      COVID19          maps       mapdata           qss    tidycensus 
         TRUE          TRUE          TRUE          TRUE          TRUE 
    dataverse DeclareDesign           zoo        gtools 
         TRUE          TRUE          TRUE          TRUE 
```


---
class:inverse, center, middle
# ðŸ’ª
## Load Data for today

---
# ðŸ“¢
## Feedback

---
## Feedback

Only 8 responses...

---
## What we'll cover today


- Probability

  - Probability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur

  - Probability follows three simple rules (from which numerous theorems follow)

- Conditional Probability

  - Conditional probability allow us to describe how our beliefs about one event change after observing another event(s)

  - We update beliefs using Bayes Rule

- Random Variables and Probability Distributions

  - Random variables *map* events in the world onto numbers

  - Probability distributions describe the likelihood that random variables take certain values

  - The expected value of a random variable is a *probability weighted average* that tells us the most likely value a variable will take


 

---
class: inverse, center, middle
# ðŸ’¡
# Probability


---
## Probability

- Probability describes the likelihood of an event happening.

- Statistics uses probability to quantify uncertainty about estimates and hypotheses.

-  To do this, we will need to understand:
  
  - Definitions (**experiment**, **sample space**, **events**,)
  
  - Three *rules* of probability (**Kolmogorov axioms**)

  - Two interpretations interpreting probabilities (**Frequentist** and **Bayesian**)
    
    - Some rules for counting
  

---
## Experiments, sample spaces, sets, end events 

- In probability thoery, an **experiment** describes a repeatable process where the outcome is uncertain

  - Processes where the outcomes are uncertaint are called *non-deterministic* or *stochastisc*

- The **sample space** of an experiment is the **set** `\((\Omega\)` "omega", or `\(S\)`) of all the possible outcomes of an experiment 

- Sets can be:
  - empty `\(( A: \{\emptyset\}\)`
  - a single event `\(( Coin: \{\text{Heads}\}\)`
  - multiple events `\(( Odd\, \#s: \{\text{1,3,5}\}\)`
  - infinite `\((\mathbb{R}: \text{ The set of real numbers}\{ -\infty \dots +\infty\}\)`)

- An **event**, `\((E\)` or `\(A)\)`  is a **subset** of outcomes in the sample space
  - The sample space for a coin flip is `\(\Omega = \{\text{Heads, Tails}\}\)`
  - The event Heads is a subset of `\(\Omega\)`

---
bacground-image:url("https://cdn.shopify.com/s/files/1/0684/1207/products/700F4BB8-7FE1-461D-844D-52CB302E6E0C.png?v=1595004762)
background-size:contain


---
## Operations on Sets

- Empty Set: `\(\emptyset\)` a set with no elements
- Subset: 
    - Let `\(D\)` be the set outcomes for a 6-side die: `\(D=\{1,2,3,4,5,6\}\)`
    - `\(Primes=\{2,3,5\}\)`
    - `\(Primes \subset D \iff \forall X \in Primes, X \in D\)`
- Unions
    - `\(A \cup B = \{X:X \in A \lor X \in B \}\)`
    - Either `\(A\)`, `\(B\)` or both `\(A and B\)` occur
- Intersections
    - `\(A \cap B = \{X:X \in A \land X \in B \}\)`
    - Both `\(A\)` and `\(B\)` occur
- Complements
    - `\(A'=A^\complement = \{X:X\notin A\}\)`
    - `\(A'=A^\complement\)` means `\(A\)` does not occur
    - `\(\emptyset^\complement=S\)` and `\(S^\complement=\emptyset\)` 
  

---
class: bottom,right
background-image:url("https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png")
background-size:contain

[Source](https://www.onlinemathlearning.com/union-set.html)

---
## Three Rules of Probability

- Probability is defined by three *rules* or assumptions called the **[Kolmogorov Axioms](https://win-vector.com/2020/09/19/kolmogorovs-axioms-of-probability-even-smarter-than-you-have-been-told/)**


1. The probability of any event `\(A\)` is nonnegative 

$$Pr(A) \geq 0 $$

2. The probability that one of the outcomes in the same space occurs is 1


$$Pr(\Omega) = 1 $$

3. If events `\(A\)` and `\(B\)` are mutually exclusive, then:

`$$Pr(A \text{ or } B) = Pr(A) + Pr(B)$$`



---
## Two interpretations of probablity

- Probabilities are defined by these three axioms

- The are two broad ways of interpreting what probabilities mean:

  - Frequentist
  
  - Bayesian


---
## Frequentist interpretations of probability

- Probability describes how likely it is that some event happens.

  - Flip a fair coin, the probability of heads is Pr(Heads) = 0.5

- **Frequentist:** view this probability is the limit of the relative frequency of an event over repeated trials.

$$ Pr(E) = \lim_{n \to \infty} \frac{n_{E}}{n} \approx \frac{ \text{# of Times E happened}}{\text{Total # of Trials}}$$ 

- Thinking about probability as a relative frequency, requires us to know how to the number of times an event occurred. 


---
## How many elements in a set?

A set can be:

- **Countably Finite:** 
  
  - Roll a die, there six possible outcomes `\(\{1,2,3,4,5,6\}\)`


- **Countably Infinite:** 

  - Number of rolls until a six appears `\({1,2,3,\dots}\)`

--

- **Uncountably Infinite:** 

  - All the real numbers between 0.25 and 0.75 


---
## The Fundamental Counting Principle 


The Fundamental Counting Principle says that if there `\(x\)` ways to do one thing and `\(z\)` ways to do another then, then are their are `\(x \times z\)` total ways of doing both tasks.

More generally, if there are 

- If there are `\(j\)` tasks or decision stages

- And `\(k\)` choices at each decision stage `\(j\)` such that `\(n_{j,k} = k\)`

- The total number of possible outcomes is the product of the number number of choices `\(k\)` and each stage, `\(j\)` `\(\prod_{i=1}^{j} n_k = n_{i,k}*n_{2,k}*\dots*n_{j,k}\)` 


---
## How many different outfits could you have made me wear?

- **Palette**: `\(\{\text{Fall, Winter, Spring, Summer}\}\)`
- **Jacket**: `\(\{\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\}\)`
- **Top**: `\(\{\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\}\)`
- **Pant**: `\(\{\text{Slacks, Khakis, Jeans, Shorts}\}\)`
- **Shoe**: `\(\{\text{Dress Shoe, Dress boot, Jordans, Dunks, Basketball, Crocs}\}\)`
- **Tie**: `\(\{\text{Repp, Pattern, Knit, Bow, No tie}\}\)`
- **Pattern**: `\(\{\text{Simple, Stripes, Checks, Graphic}\}\)`

- How many decision stages?

- How many choices at each stage?

- How many possible outfits?

---
## How many different suggested outfits could you have made me wear?

- How many decision stages? `\(j=7\)`

- How many choices at each stage? 

- **Palette**: $\{\text{Fall, Winter, Spring, Summer}\} = n_{jk} = 4 $ 
- **Jacket**: `\(\{\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\} = n_{jk} = 5\)`
- **Top**: `\(\{\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\} = n_{jk} = 4\)`
- **Pant**: `\(\{\text{Slacks, Khakis, Jeans, Shorts}\} = n_{jk} = 4\)`
- **Shoe**: `\(\{\text{Dress Shoe, Dress boot, Jordans, Dunks, Basketball, Crocs}\} = n_{jk} = 6\)`
- **Tie**: `\(\{\text{Repp, Pattern, Knit, Bowtie, No tie}\} = n_{jk} = 5\)`
- **Pattern**: `\(\{\text{Simple, Stripes, Checks, Graphic}\} = n_{jk} = 4\)`

- How many possible suggestions? `\(\prod_{i=1}^{j} n_{jk} = 4 \times 5 \times 4 \times 4 \times 6 \times 5 \times 4 = 38,400 \text{ outfits}\)`

---
## And yet you chose this:

&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;I send students in my undergraduate research class a weekly survey to get a sense of what&amp;#39;s working and what&amp;#39;s not. &lt;br&gt;&lt;br&gt;I also ask them silly questions just for fun. Last week, I asked them to help me with my fit. This was the result: &lt;a href="https://t.co/w1t8Cr04Ap"&gt;pic.twitter.com/w1t8Cr04Ap&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paul Testa (@ProfPaulTesta) &lt;a href="https://twitter.com/ProfPaulTesta/status/1501291241404833795?ref_src=twsrc%5Etfw"&gt;March 8, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;


---
## Permutations and Combinations

- A **Permutation** is a selection of objects in which the **order matters.**

  - There are *six* ways to arrange the letters "a", "b", "c" using each letter only once ("without replacement")
  - There are *27* ways to arrange the letters "a", "b", "c" using each letter as many times as we want ("with replacement")


- A **Combination** is a selection of objects without regard to order

  - There is *1* ways to choose a group of three letters from the letters "a", "b", "c" using each letter only once ("without replacement")
  - There are *10* ways to arrange the letters "a", "b", "c" using each letter only once ("with replacement")



---



---
## Permutations in R

.pull-left[
`\({}_{3}P_2\)` order matters, repeats allowed

```r
gtools::permutations(3, 2, v = c("a","b","c"),repeats.allowed = T)
```

```
      [,1] [,2]
 [1,] "a"  "a" 
 [2,] "a"  "b" 
 [3,] "a"  "c" 
 [4,] "b"  "a" 
 [5,] "b"  "b" 
 [6,] "b"  "c" 
 [7,] "c"  "a" 
 [8,] "c"  "b" 
 [9,] "c"  "c" 
```
]

.pull-right[
`\({}_{3}P_2\)` order matters, no repeats 

```r
gtools::permutations(3, 2, v = c("a","b","c"),repeats.allowed = F)
```

```
     [,1] [,2]
[1,] "a"  "b" 
[2,] "a"  "c" 
[3,] "b"  "a" 
[4,] "b"  "c" 
[5,] "c"  "a" 
[6,] "c"  "b" 
```
]

---
## Combinations in R

.pull-left[
`\({}_{3}C_2\)` no order, no repeats

```r
gtools::combinations(3, 2, v = c("a","b","c"),repeats.allowed = F)
```

```
     [,1] [,2]
[1,] "a"  "b" 
[2,] "a"  "c" 
[3,] "b"  "c" 
```
]

.pull-right[
`\({}_{3}C_2\)` no order, repeats allowed

```r
gtools::combinations(3, 2, v = c("a","b","c"),repeats.allowed = T)
```

```
     [,1] [,2]
[1,] "a"  "a" 
[2,] "a"  "b" 
[3,] "a"  "c" 
[4,] "b"  "b" 
[5,] "b"  "c" 
[6,] "c"  "c" 
```
]

---
## Permutations and Combinations


---
## Frequentist interpretations of probability

- Probabilities from a Frequentist perspective are defined by *fixed* and *unknown*  **parameters**

- The goal of statistics for a frequentist is to learn about these parameters from data.

- Frequentist statistics often ask questions like "What is the probability of observing some data `\(Y\)`, given a hypothesis about the true value of  parameter(s), `\(\theta\)`, that generated it. 

---
## Frequentist interpretations of probability

For example, suppose we wanted to test whether a coin is "fair"  `\((p = Pr(Heads) = .5; q = Pr(Tails) = 1-p = .5).\)` We could:

- Flip a fair coin 10 times. Our estimate of the  `\(Pr(H)\)` is the number of heads divided by 10. It could be 0.5, but also 0 or 1, or some number in between.
  
- Flip a coin 100 times and our estimate will be closer to the true `\(paramter\)`. 
  
- Flip a coin an `\(\infty\)` amount of times and the relative frequency will converge to the true parameter/ `\((Pr(H) = \lim_{n \to \infty} \frac{n_{H}}{n} = p = 0.5 \text{ for a fair coin})\)` 




---
## Bayesian interpretations of probability

- Frequentist interpretations make sense for describing processes that we could easily repeat (e.g. Coin flips, Surveys, Experiments) 

- But feel more convoluted when trying to describe events like "the probability of that Biden wins reelection."

- Bayesian interpretations of probability view probabilities as subjective beliefs. 

- The task for a Bayesian statistics is to update these *prior* beliefs () based on a model of the *likelihood* of observing some data to form new beliefs after observing the data (called *posterior beliefs*).

- Bayesians do this using *Bayes Rule*, which says:

$$\text{posterior} \propto \text{likelihood} \times \text{prior} $$
More formally:

$$\underbrace{Pr(\theta|Y)}_{\text{Posterior}} \propto \underbrace{Pr(Y|\theta)}_{\text{Likelihood}}) \times \underbrace{Pr(\theta)}_{\text{Prior}} $$
---
background-image:url("https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png")
background-size:contain


---
## Bayesian vs Frequentists


- Our two main tools for doing statistical inference in this course

  - Hypothesis Testing
  - Interval Estimation

- Follow largely from frequentist interpretations of probability

- The differences between Bayesian and Frequentist frameworks, are both philosophical and technical in nature

  - Is probability a relative frequency or subjective belief? How do we form and use prior beliefs
  - Bayesian statistics relies heavily on algorhithms for [Markov Chain Monte-Carlo](http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf) simulations made possible by advances in computing.

- For most of the questions in this course, these two frameworks will yield similar (even identical) conclusions.
  
  - Sometimes it's helpful to think like a Bayesian, others, like a frequentist
  - All models are wrong, some models are useful.



---
class: inverse, center, middle
# ðŸ’¡
# Conditional Probability

---
## Conditional Probability

The condiotional probability of event `\(A\)` given that event `\(B\)` occurred is

`$$Pr(A|B) = \frac{Pr(A\text{ and }B)}{Pr(B)}$$`
Where

- `\(Pr(A\text{ and }B)\)` is the joint probability of both events occurring

- `\(P(B)\)` is the marginal probability of `\(B\)`

---
## The Multiplication Rule:

We can re-arrange:

`$$Pr(A|B) = \frac{Pr(A\text{ and }B)}{Pr(B)}$$`


To get:

$$Pr(A\text{ and }B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A) $$


---
## QSS Example (p. 256): Race and Gender in the FL Voter File 

`FLVoters` contains a random sample of 10,000 voters


```r
library(qss)
data("FLVoters")
dim(FLVoters)
```

```
[1] 10000     6
```

```r
FLVoters &lt;- na.omit(FLVoters)
dim(FLVoters)
```

```
[1] 9113    6
```

```r
head(FLVoters)
```

```
     surname county VTD age gender  race
1     PIEDRA    115  66  58      f white
2      LYNCH    115  13  51      m white
4    LATHROP    115  80  54      m white
5     HUMMEL    115   8  77      f white
6 CHRISTISON    115  55  49      m white
7      HOMAN    115  84  77      f white
```

---
## Marginal Probabilities: Gender


```r
# Pr(Gender)
table(FLVoters$gender)
```

```

   f    m 
4883 4230 
```

```r
margin_gender &lt;- prop.table(table(FLVoters$gender))
margin_gender
```

```

        f         m 
0.5358279 0.4641721 
```

---
## Marginal Probabilities: Race



```r
# Pr(Race)
table(FLVoters$race)
```

```

   asian    black hispanic   native    other    white 
     175     1194     1192       29      310     6213 
```

```r
margin_race &lt;- prop.table(table(FLVoters$race))
margin_race
```

```

      asian       black    hispanic      native       other       white 
0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 
```


---
## Cross Tab of race and gender


```r
table(FLVoters$gender,FLVoters$race, useNA = "ifany")
```

```
   
    asian black hispanic native other white
  f    83   678      666     17   158  3281
  m    92   516      526     12   152  2932
```

---
## Joint probability of race and gender


```r
joint_p &lt;- prop.table(table(FLVoters$gender,FLVoters$race))
joint_p
```

```
   
          asian       black    hispanic      native       other       white
  f 0.009107868 0.074399210 0.073082410 0.001865467 0.017337869 0.360035115
  m 0.010095468 0.056622408 0.057719741 0.001316800 0.016679469 0.321738176
```

---
## Marginal probabilities from joint probabilities:


```r
prob_tab&lt;-addmargins(table(FLVoters$gender,FLVoters$race))
prob_tab
```

```
     
      asian black hispanic native other white  Sum
  f      83   678      666     17   158  3281 4883
  m      92   516      526     12   152  2932 4230
  Sum   175  1194     1192     29   310  6213 9113
```

---
## Marginal probabilities from joint probabilities: Gender

```r
# Marginal probability of Gender
prob_tab[,"Sum"]/prob_tab["Sum","Sum"]
```

```
        f         m       Sum 
0.5358279 0.4641721 1.0000000 
```

```r
prop.table(table(FLVoters$gender))
```

```

        f         m 
0.5358279 0.4641721 
```

---
## Marginal probabilities from joint probabilities: Race

```r
# Marginal probability of Race
prob_tab["Sum",]/prob_tab["Sum","Sum"]
```

```
      asian       black    hispanic      native       other       white 
0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 
        Sum 
1.000000000 
```

```r
prop.table(table(FLVoters$race))
```

```

      asian       black    hispanic      native       other       white 
0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 
```

---
## Total Probability: Pr(Black)

`$$Pr(Black)=Pr(Black \cap female) + Pr(Black \cap male)$$`


```r
prop.table(table(FLVoters$gender,FLVoters$race))["f","black"] +
  prop.table(table(FLVoters$gender,FLVoters$race))["m","black"]
```

```
[1] 0.1310216
```

```r
prop.table(table(FLVoters$race))["black"]
```

```
    black 
0.1310216 
```

---
## Conditional Probability: Pr(Black|Female)

`$$Pr(Black|Female) = \frac{Pr(Black \text{ and } Female)}{Pr(Female)} \approx \frac{0.074}{0.536} \approx 0.139$$`



---
## Independence

Events `\(A\)` and `\(B\)` are independent if 

$$ Pr(A|B) = Pr(A) \text{ and } Pr(B|A) = Pr(B)$$

Conceputally, If `\(A\)` and `\(B\)` are **independent** knowing whether `\(B\)` occurred, tells us nothing about `\(A\)`, and so the conditional probability of `\(A\)` given `\(B\)`, `\(Pr(A|B)\)` is equal to the unconditional, or marginal probability, `\(Pr(A)\)`

Formally, two events are statistically independent if and only if the joint probability is equal to product of the marginal probabilities

`$$Pr(A\text{ and }B) = Pr(A)Pr(B)$$`
---
Race and gender are approximately independent in these data

.pull-left[

```r
plot(x = c(margin_race*margin_gender["f"]),
     y = joint_p["f",],
     xlim = c(0,.4),
     ylim = c(0,.4),
     xlab = "Pr(race)*Pr(female)",
     ylab = "Pr(race and gender)"
             )
abline(0,1)
```
]
.pull-right[

&lt;img src="08-slides_files/figure-html/unnamed-chunk-20-1.png" width="80%" style="display: block; margin: auto;" /&gt;

]

---
## Conditional Independence

We can extend the concept of independence to situations with more than two events:

If events `\(A\)`, `\(B\)`, and `\(C\)` are jointly independent then:

`$$Pr(A \cap B \cap C) = Pr(A)Pr(B)Pr(C)$$`

Joint independence implies pairwise independence and conditional independence:

`$$Pr(A \cap B | C) = Pr(A|C)Pr(B|C)$$`
But not the reverse.


---
## Bayes Rule

Bayes rule is theorem for how we should update our beliefs about `\(A\)` given that `\(B\)` occurred:

`$$Pr(A|B) = \frac{Pr(B|A)Pr(A)}{Pr(B)} = \frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\complement)Pr(A^\complement)}$$`
Where

- `\(Pr(A)\)` is called the prior probability of A (our initial belief)

- `\(Pr(A|B)\)` is called the posterior probability of A given B (our updated belief after observing B)

---
###  What's the probability you have Covid-19 given a positive test

`$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$`

|               | Has Covid      | Does Not Have Covid |
|---------------|----------------|---------------------|
| Test Positive | True Positive  | False Positive      |
| Test Negative | False Negative | True Negative       |


---
### What's the probability you have Covid-19 given a positive test

Let's assume:

- 1 out 1000 people has Covid
- Our test correctly identifies true positives 95 percent of the time (sensitivity = True Positive Rate)
- Our test correctly identifies true negatives 95 percent of the time (specifity = True Negative Rate)

---
### What's the probability you have Covid-19 given a positive test

In a sample of 100,000 people then:

`$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$`

|               | Has Covid      | Does Not Have Covid |
|---------------|----------------|---------------------|
| Test Positive | 950 | 4950      |
| Test Negative | 50 | 94050       |

- `\(Pr(+|Covid) = 950/(1000) \approx 0.95\)`  
- `\(Pr(Covid) = 1000/100000 \approx 0.01\)`  
- `\(Pr(+) = Pr(+|Covid) + Pr(+|Covid)= .95*.01 + .05*.99  \approx  0.059\)`  

---
### What's the probability you have Covid-19 given a positive test

Converting this table into marginal probabilities

`$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$`

|               | Has Covid      | Does Not Have Covid | Prob(Test)
|---------------|----------------|---------------------|---------------------|
| Test Positive | 0.0095            | 0.0495               |0.059
| Test Negative | 0.0005            | 0.9505               | 0.941 
| Prob(Covid)   | 0.01          | 0.99               | 1

`$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$`
`$$Pr(Covid|Test +) = \frac{0.95 \times 0.01}{0.059} \approx 0.16$$`
---
What if we took goot a second test? We could use posterior belief as our prior:


`$$Pr(Covid|2nd +) = \frac{0.95 \times 0.16}{0.16\times0.95 + (1-0.16)\times 0.95 } \approx 0.783$$`
Now we're much more confident that we have Covid-19


---
class: inverse, center, middle
# ðŸ’¡
# Random Variables and Probability Distributions


---
## Random Variables
- Random variables assign numeric values to each event in an experiment.

    - Mutually exclusive and exhaustive, together cover the entire sample space.

- Discrete random variables take on finite, or [countably infinite](http://mathworld.wolfram.com/CountablyInfinite.html) distinct values.

- Continuous variables can take on an uncountably infinite number of values.

---
## Example: Toss Two Coins

- `\(S={TT,TH,HT,HH}\)`
- Let `\(X\)` be the number of heads
    - `\(X(TT)=0\)`
    - `\(X(TH)=1\)`
    - `\(X(HT)=1\)`
    - `\(X(HH)=2\)`

    
---
## Probability Distributions

- Broadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events. 

- The can be represented in terms of:

  - Probability Mass/Density Functions
    - Discrete variables have probability mass functions (PMF)
    - Continuous variables have probability density functions (PDF)

  - Cumulative Density Functions
    - Discrete: Summation of discrete probabilities
    - Continuous: Integration over a range of values

---
## Discrete distributions

- **Probability Mass Function (pmf):** `\(f(x)=p(X=x)\)`

- Assigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply 

- **Cumulative Distribution Function (cdf)** `\(F(x_j)=p(X\leq x)=\sum_{i=1}^{j}p(x_i)\)`

  - Sum of the probability mass for events less than or equal to `\(x_j\)`

---
## Example: Toss Two coins

- `\(S={TT,TH,HT,HH}\)`
- Let `\(X\)` be the number of heads
    - `\(X(TT)=0\)`
    - `\(X(TH)=1\)`
    - `\(X(HT)=1\)`
    - `\(X(HH)=2\)`
- `\(f(X=0)=p(X=0)=1/4\)`
- `\(f(X=1)=p(X=1)=1/2\)`
- `\(F(X\leq 1) = p(X \leq 1)= 3/4\)`


---

&lt;img src="08-slides_files/figure-html/unnamed-chunk-21-1.png" width="80%" style="display: block; margin: auto;" /&gt;

Each side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X&lt;=2) = 1/6 + 1/6 = 1/3


---
## Continuous distributions


- **Probability Density Functions (PDF):** `\(f(x)\)`
    - Assigns probabilities to events in the sample space such that Kolmogorov Axioms still apply 
    - But... since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.

- **Cumulative Distribution Function (CDF)** `\(F(x)=p(X\leq x)=\int_{-\infty}^{x}f(x)dx\)`
    - Instead of summing up to a specific value (discrete) we integrate over all possible values up to `\(x\)`
    - Probability of having a value less than x


---
## Integrals

First, a brief aside  on integral calculus:

What's the area of the rectangle? `\(base\times height\)`

&lt;img src="08-slides_files/figure-html/unnamed-chunk-22-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
## Integrals

How would we find the area under a curve?

&lt;img src="08-slides_files/figure-html/unnamed-chunk-23-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Integrals

Well suppose we added up the areas of a bunch of rectangles roughly whose height's approximated the height of the curve?

&lt;img src="08-slides_files/figure-html/unnamed-chunk-24-1.png" width="80%" style="display: block; margin: auto;" /&gt;

Can we do any better? 

---
## Integrals

Let's make the rectangles smaller

&lt;img src="08-slides_files/figure-html/unnamed-chunk-25-1.png" width="80%" style="display: block; margin: auto;" /&gt;

What happens as the width of rectangles get even smaller, approaches 0? Our approximation get's even better:


---
## Link between PDF and CDF

If 
$$F(x)=p(X\leq x)=\int_{-\infty}^{x}f(x)dx $$

Then by the [fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus)

`$$\frac{d}{dx}F(x)=f(x)$$`

In words

- the PDF ($f(x)$) is the derivative (rate of change) of the CDF ($F(X)$)

- the CDF describes the area under the curve defined by f(x) up to x 


---
## Properties of the CDF

- `\(0\leq F(x) \leq 1\)`

- `\(F\)` is non-decreasing and right continuous

- `\(\lim_{x\to-\infty}F(x)=0\)`

- `\(\lim_{x\to\infty}F(x)=1\)`

- For all `\(a,b \in \mathbb{R}\)` s.t. `\(a&lt;b\)`

$$p(a &lt; X \leq b) = F(b)- F(a) = \int_a^b f(x)dx $$

---
## Recall the PMF and CDF of a die

&lt;img src="08-slides_files/figure-html/unnamed-chunk-26-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## What's the probability 

- `\(p(X=1)...p(X=6) =\)`\pause `\(1/6\)`
- `\(p( 2 &lt; X \leq 5) =\)`\pause  `\(F(5)-F(2)=5/6-2/6=3/6=1/2\)`


---
# Common Probablity Distirbutions

In this course, we'll use probability distributions to 

- Model the data generating process as a function of parameters we can estimate

- To perform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)

---
There are a lot of probability distributions:
![](http://www.math.wm.edu/~leemis/chart/UDR/BaseImage.png)


---
Fortunately, the distributions you need to know to really master data science, is probably more something like
![](https://miro.medium.com/max/4854/1*szMCjXuMDfKu6L9T9c34wg.png)

And the distributions we'll work with the most in this class are an even smaller subset.

---
## Bernoulli Random Variables

Let's start with our old friend the coin flip


A coin flip is an example of a **Bernoulli random variable** defined by 1 parameter `\(p\)`, the probability of success. It has a pmf of

`$$f(x) =
    \left\{
        \begin{array}{cc}
                p &amp; \mathrm{if\ } x=1 \\
                1-p &amp; \mathrm{if\ } x=0 \\
        \end{array} 
    \right.$$`


And a CDF of 

`$$F(x) =
    \left\{
        \begin{array}{cc}
                0 &amp; \mathrm{if\ } x&lt;1 \\
                1-p &amp; \mathrm{if\ } 0\leq x&lt;1 \\
                1&amp; \mathrm{if\ } x\geq1 \\
        \end{array} 
    \right.$$`

Note that in our coin flip example `\(p=0.5\)` but it need not. Just imagine a weighted coin like the Patriots use at Foxborough

---
## Uniform Distribution

Our fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range `\([a,b]\)`, the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of

`$$f(x) =
    \left\{
        \begin{array}{cc}
                \frac{1}{b-a}&amp; \mathrm{if\ } a \leq x\leq b \\
                0 &amp; \text{otherwise} \\
        \end{array} 
    \right.$$`

And a CDF:

`$$F(x) =
    \left\{
        \begin{array}{cc}
                        0 &amp; x &lt;a \\
                \frac{x-a}{b-a}&amp; \mathrm{if\ } a \leq x &lt; b \\
                1 &amp; x \geq b \\
        \end{array} 
    \right.$$`

We won't run into uniform distributions all that often except in examples like rolling a fair sided die, but often they're used in Bayesian analysis as a form of uninformative prior.

---
## Binomial Distributions

The binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.

The key parameters are the number of trials `\(n\)` and the probability of success for each trial `\(p\)` and the pdf of a binomial distribution is:

`$$f(x)=\binom{n}{x}p^x (1-p) ^{1-x} \ \text{for x 0,1,2},\dots n$$`
So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads

---

&lt;img src="08-slides_files/figure-html/unnamed-chunk-27-1.png" width="80%" style="display: block; margin: auto;" /&gt;

The binomial distribution will come in handy when trying to model binary outcomes. 

---
## Poisson Distributions

What would happen if you let the `\(n\)` in a binomial distribution go to infinity and `\(p\)` go to 0 so that `\(np\)` stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter `\(\lambda\)` which represents rate at which events occur.


`$$f(x)=\frac{\lambda^x}{x!}e^{-\lambda}$$`

---

&lt;img src="08-slides_files/figure-html/unnamed-chunk-28-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
## Geometric Distributions

What if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:

`$$f(x)=p(1-p) ^{x}$$`
---

&lt;img src="08-slides_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Exponential Distributions

Taking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a `\(\lambda = \frac{1}{\beta}\)` rate parameter

`$$f(x)=\frac{1}{\beta}\exp\left[-x/\beta\right]$$`

---

&lt;img src="08-slides_files/figure-html/unnamed-chunk-30-1.png" width="80%" style="display: block; margin: auto;" /&gt;


We often use exponential distributions to model things like "time until failure" where failure might be another war or the ending of an Italian cabinet.

---
## Normal Distribution

Finally, there's the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter `\(\mu\)` that determines the center of a distribution and a scale parameter `\(\sigma^2\)` that determines the spread of a distribution

`$$f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp \left[
-\frac{1}{2\sigma^2}(x-\mu)^2
\right]$$`

---
Standard normal: `\(X \sim N(\mu =0,\sigma^2=1)\)`

&lt;img src="08-slides_files/figure-html/unnamed-chunk-31-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
- As we'll see normal distributions tend to arise when ever you're summing variables. 

- That is sum together a bunch of values from almost any distribution and the **distribution of their sums** tends to follow a normal distribution. 

- Since lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)


---
Consider a binomial distribution with N=100 and p=.5. The pmf of this variable (black lollipops) follows a distribution that's closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.

A relationship explained more generally by the Central Limit Theorem, which we'll cover next week.

---
&lt;img src="08-slides_files/figure-html/unnamed-chunk-32-1.png" width="80%" style="display: block; margin: auto;" /&gt;



---
### What's the p(X$\leq$ 0) for a normal distirbution with mean 0 and sd 1

Since the normal distribution is so common, it's useful to get practice working with it's pdf and cdf.

Consider the following question: If X is normally distributed variable with `\(\mu=0\)` and `\(\sigma=1\)`, what's the probability that X is less than 0  `\(p(X\leq0)=?\)` We could solve:

`$$\int_{-\infty}^{0}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx=0.5$$`



---
But R's `pnorm()` function will quickly tell us

- `\(p(X\leq0)=\)` 0.5

And we can visualize this as follows:

---
&lt;img src="08-slides_files/figure-html/unnamed-chunk-33-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
Consider some other questions?

- `\(p(X=0)=0\)`
  - The probability that a continuous variable is exactly some value is always 0.
- `\(p(X&lt;0)=0.5\)`
- `\(p(-1&lt; X&lt; 1)\)`
- `\(p(-2&lt; X&lt; 2)\)`

---
### p(-1 &lt; X &lt;  1)


- `\(p(-1&lt; X&lt; 1)=pr(X&lt;1)-pr(X&lt;-1)\)` 

`$$\int_{-1}^{1}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx=0.841-0.158=0.682$$`


&lt;img src="08-slides_files/figure-html/unnamed-chunk-34-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
### p(-2 &lt; X &lt; 2)

- `\(p(-2&lt; X\leq 2)=\)` 0.9544997

&lt;img src="08-slides_files/figure-html/unnamed-chunk-35-1.png" width="80%" style="display: block; margin: auto;" /&gt;

We'll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0.



---
## Expected Value

A (probability) weighted average of the possible outcomes of a random variable, often labeled `\(\mu\)` 

Discrete:

`$$\mu_X=E(X)=\sum xp(x)$$`

Continuous

`$$\mu_X=E(X)=\int_{-\infty}^{\infty}xf(x) dx$$`

---
## What's the expected value of a 1 roll of fair die?

`$$\begin{align*}
E(X)&amp;=\sum_{i=1}^{6}x_ip(x_i)\\
     &amp;=1/6\times(1+2+3+4+5+6)\\
     &amp;= 21/6\\
     &amp;=3.5
\end{align*}$$`

---
## Properties of Expected Values

- `\(E(c)=c\)`

- `\(E(a+bX)=a+bE[X]\)`

- `\(E[E[X]]=X\)`

- `\(E[E[Y|X]]=E[Y]\)`

- `\(E[g(X)]=\int_{-\infty}^\infty g(x)f(x)dx\)`

- `\(E[g(X_1)+\dots+g(X_n)]=E[g(X_1)]+\dots E[g(X_n)\)`

- `\(E[XY]=E[X]E[Y]\)` if `\(X\)` and `\(Y\)` are independent

---
## How many times would you have to roll a fair die to get all six sides?

We can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success. The expected value of a geometric variable is:

`$$\begin{align*}E(X)&amp;=\sum_{k=1}^{\infty}kp(1-p)^{k-1} \\
&amp;=p\sum_{k=1}^{\infty}k(1-p)^{k-1} \\
&amp;=p\left(-\frac{d}{dp}\sum_{k=1}^{\infty}(1-p)^k\right) \text{(Chain rule)} \\
&amp;=p\left(-\frac{d}{dp}\frac{1-p}{p}\right) \text{(Geometric Series)} \\
&amp;=p\left(\frac{d}{dp}\left(1-\frac{1}{p}\right)\right)=p\left(\frac{1}{p^2}\right)=\frac1p\end{align*}$$`

---
For this question, we need to calculate the probability of success, p, after getting a side we need.

The probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:


```r
ev &lt;- c()
for(i in 6:1){
  ev[i] &lt;- 6/i
  
}
# Expected rolls for each 1 through 6th side
rev(ev)
```

```
[1] 1.0 1.2 1.5 2.0 3.0 6.0
```

```r
# Total 
sum(ev)
```

```
[1] 14.7
```

---
## Variance

If `\(X\)` has a finite mean `\(E[X]=\mu\)`, the `\(E[(X-\mu)^2]\)` is finite and called the variance of `\(X\)` which we write as `\(\sigma^2\)` or `\(Var[X]\)`.

Note:

`$$\begin{align*}
\sigma^2=E[(X-\mu)^2]&amp;=E[(X^2-2\mu X+\mu^2)]\\
&amp;= E[X^2]-2\mu E[X]+\mu^2\\
&amp;= E[X^2]-2\mu^2+\mu^2\\
&amp;= E[X^2]-\mu^2\\
&amp;= E[X^2]-E[X]^2
\end{align*}$$`

- "The variance of X is equal to the expected value of X-squared, minus the square of X's expected value."
- `\(\sigma^2=E[X^2]-E[X]^2\)` is a useful identity in proofs and derivations

---
## Variance and Standard Deviations

We often think of variances `\(Var[X]\)` as describing the spread of a distribution

`$$\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2$$`

A standard deviation is just the square root of the variance

`$$\sigma=\sqrt{Var[X]}$$`

---
## Covariance

Covariance measures the degree to which two random variables vary together. 

- `\(Cov[X,Y] \to +\)` An increase in `\(X\)` tends to be larger than its mean when `\(Y\)` is larger than its mean

`$$Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]$$`

---
## Properties of Variance and Covariance

- `\(Cov[X,Y]=E[XY]-E[X]E[Y]\)`

- `\(Var[X]=E[X^2]-(E[X])^2\)`

- `\(Var[X|Y]=E[X^2|Y]-(E[X|Y])^2\)`

- `\(Cov[X,Y]=Cov[X,E[Y|X]]\)`

- `\(Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]\)`

- `\(Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]\)`


---
## Correlation

- The correlation between `\(X\)` and `\(Y\)` is simply the covariance of `\(X\)` and `\(Y\)` divided by the standard deviation of each.

`$$\rho=\frac{Cov[X,Y]}{\sigma_X\sigma_Y}$$`

- Normalize covariance to a scale that runs between [-1,1]

---
## Question: If two variables have zero covariance, are they independent?


No

A trivial example

- `\(p(X=x)=1/3\)` for `\(x = -1,0,1\)` and let `\(Y=X^2\)`
- Y clearly depends on X even though their covariance is 0


```r
p=1/3
x=-1:1
y=x^2
cor(x,y)
```

```
[1] 0
```







    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-lakeside-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
