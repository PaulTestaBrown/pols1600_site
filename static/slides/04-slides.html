<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 04:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Paul Testa" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <script src="libs/jquery/jquery-3.6.0.min.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="css/brown.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Week 04:
## Casual Inference in Observational Designs
### Paul Testa

---












class: inverse, center, middle
background-image:url(https://images.gnwcdn.com/2021/articles/2021-11-02-21-59/overwatch-2-and-diablo-4-have-been-delayed-1635890355123.jpg/EG11/resize/1200x-1/overwatch-2-and-diablo-4-have-been-delayed-1635890355123.jpg)
background-size:cover

# Overview

---
## General Plan

- Setup
  - Packages
  - Data
- Feedback
- Review
  - Statistical programming
  - Descriptive statistics
  - Data visualization
  - Causal inference
- Experimental vs Observational Studies
- Three Approaches to Covariate Adjustment
  - Subclassification
  - Matching
  - Regression
  
Tuesday:

- Three Designs for Causal Inference in Observational Studies
  - Difference in Differences
  - Regression Discontinuity
  - Instrumental Variables


## What you'll learn

- Causal inference in observational and experimental studies is about counterfactual comparisons
- In observational studies, to make causal claims we generally make some  assumption of conditional independence:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

- The credibility of this assumption depends less on the data, and more on how the data were generated.
  - **Selection on Observables** is rarely a credible assumption
- Observational designs that produce credible causal inference, leverage aspects of the world that create *natural experiments*
- You should be able to describe the logic and assumptions of common designs in social science 
  - **Difference-in-Differences:** *Parallel Trends* 
  - **Regression Discontiniuity:** *Continuity at the cutoff*
  - **Instrumental Variables:** Instruments need to be *Relevant and Exogenous*


---
class:inverse, middle, center
# üí™
## Get set up to work

---
## New packages

Let's try installing these packages manually: 


```r
install.packages("dataverse")
```


```r
install.packages("tidycensus")
```


```r
install.packages("easystats", repos = "https://easystats.r-universe.dev")
```


```r
install.packages("DeclareDesign")
```

- Some further guidance available [here](https://pols1600.paultesta.org/slides/04-packages.html)

- Particular, for next week, please follow the instructions to [install a Census API key in R](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package)

---
## Packages for today




---
## Define a function to load (and if needed install) packages



```r
ipak &lt;- function(pkg){
    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}
```

---
## Load packages for today


```r
ipak(the_packages)
```

```
   kableExtra            DT     tidyverse     lubridate       forcats 
         TRUE          TRUE          TRUE          TRUE          TRUE 
        haven      labelled         ggmap       ggrepel      ggridges 
         TRUE          TRUE          TRUE          TRUE          TRUE 
     ggthemes        ggpubr        GGally        scales       dagitty 
         TRUE          TRUE          TRUE          TRUE          TRUE 
        ggdag       ggforce       COVID19          maps       mapdata 
         TRUE          TRUE          TRUE          TRUE          TRUE 
          qss    tidycensus     dataverse DeclareDesign     easystats 
         TRUE          TRUE          TRUE          TRUE          TRUE 
```


---
class:inverse, center, middle
# üí™
## Load Data for today

---
## Load Data for today

We'll load the data we need in some of the examples below as well:


```r
## Lab data
load(url("https://pols1600.paultesta.org/files/data/03_lab.rda"))

## RDD data
library(qss)
data(MPs)
```


---
class: inverse, center, left
background-image:url("https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/17632380-high-res-fleabag-1555319632.jpg")
background-size:cover
# üì¢
## Feedback



---
## What we liked

--

- We liked the lab (or liked it better)

- Working in groups 

- Extensive notes

- Reasonable pacing 

- Real world data

- Mix of substantive and technical questions

---
## What we liked

<div id="htmlwidget-80412ee25bc1d619ff93" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-80412ee25bc1d619ff93">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12"],["I liked working in lab groups and understanding where the data is coming from","I liked that we got time to work on the lab in groups and then came together as a class and discussed our progress","The extensive notes/explanations included in the documents we used\nWorking through the steps of coding something specific as a class","I thought the subject we were investigating was interesting. I like the mix of descriptive questions and questions which we had to answer with code.","The mutate function has been pretty useful so far.","I really enjoyed how the lab instructions were laid out in a way that allowed us to ~practice~ writing the code ourselves, while also ensuring we didn't get too lost in the sauce.","I like working in groups.","I really enjoyed the lab this week - I thought it was fun and very well paced. Specifically, I liked how each question was allotted a certain amount of time, and that we touched base as a class between questions.","The lab went much better than last week! really felt like I was learning a lot this week","I really enjoyed reading about the study we used in this week's lab. The content was interesting and it was great to learn something new. Also because I learned a lot from the lab today and synthesize why being able to manipulate source data is so helpful in better understanding and accessing the outcomes of a study.","Honestly class this week was a little bit confusing, but I thought going over last week's material briefly was very useful and provided a good segue into this week's material.","I was a big fan of switching to the groups as opposed to doing it in pairs or individually within the class setting."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Likes<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":4,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[4,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>


---
## What we disliked

--

- Labs still feel rushed (at the end)

- When code breaks it can be hard to fix

- Lecture was a bit confusing/long

- The space

---
## What we disliked

<div id="htmlwidget-91156d66e466b9db5093" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-91156d66e466b9db5093">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12"],["The end of labs feel a little rushed and I get lost in the more complicated last steps. Also, the lecture had a lot of big math-y words.","N/A","I think it'd be useful to do summaries of what we learned that day at the end of class. It might be difficult to do, though, when we're crunched for time.","Broadly speaking, I don't like computers. I don't like bending over the small desks, staring at a screen, etc. The class is fine: it's something I have to do, all things worth doing are difficult, and I'm learning to think in new ways. That said, I tend to doubt that computer programing is conducive to human flourishing. Would the world be a better place if everyone learned how to code? I tend to think no. Would the world be a better place if everyone read Shakespeare? Probably. This isn't really a practical concern, but rather a question about, y'know, what we're doing here.  \n\nMore concretely, I found the lab hard to complete, but I'll look over the notes and review. Part of me thinks I'd learn better if I started the labs before class. I'd ask better questions, be less confused, etc. \n\nIn sum: the class is fine. It's good to learn to think in new ways and good to be challenged, even if I have some broader questions about coding as a human activity.","I'm not sure if this is possible but releasing the labs earlier (even maybe the night before) would be cool so I can familiarize myself with the steps.  When I get to class I always feel like I am a bit behind because I am just trying to figure out what to do.","The stats. Although I did the reading and took notes on the stats notes provided, conceptually, I am...lost.","I feel like once the code fails in one place there is no point in moving on until you fix it, so then I have to choose between falling behind or having functional code","On Tuesday, I found it difficult to sit for the lecture without a break halfway through. I think a 3-5 minute break on days like Tuesday when most of the class is lecture would be helpful.","I thought the classroom were in and the way we were all sitting during the lab was not very inducive of collaborative work. It was really hard to hear my group mates and focus since its such a small space. Not sure if anything can be done about that.\nAlso, would it be possible to set up Canvas so that it registers when our groupmates have submitted the lab. I have other classes that have put me in a \"group\" with others on Canvas and it submitted my work for them and vice versa","I think that lecture was a little bit confusing jumping between QSS and class content but honestly I think that's just getting into the flow of it","The more complicated formulae we've looked at this week have been on the difficult end - more clarification on how practically we're going to use them could be useful.","The music could be a bit louder while we're working."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Dislikes<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---
## How I read these comments

**Likes**

- What's working? What interests you? What can I try to do more of

--

**Dislikes**

- What's the issue?
  - Specific
  - General

- What's the cause?

- What can I do to help?

- What can I expect you to do


---
class: middle

&gt; The stats. Although I did the reading and took notes on the stats notes provided, conceptually, I am...lost.

---

## What's the issue?



.pull-left[
- General problem
- Done the readings but still feeling lost

]

.pull-right[
&gt; The stats. Although I did the reading and took notes on the stats notes provided, conceptually, I am...lost.

]


---
## What's the cause



.pull-left[
My intuition:

- I'm trying to do to much in lectures and labs

- Lack of correspondence between lecture, labs, and textbook 

- Feeling overwhelmed can obscure how much you actually know

]

.pull-right[
&gt; The stats. Although I did the reading and took notes on the stats notes provided, conceptually, I am...lost.

]

---
## How can I help?

.pull-left[

- Review

- Revise

- Reassure


]

.pull-right[
&gt; The stats. Although I did the reading and took notes on the stats notes provided, conceptually, I am...lost.

]

---
## Review: What have we covered

 
- Statistical Programming

- Descriptive statistics

- Data visualization as tool for descriptive inference

- Causal inference in experimental designs


---
## Review: Where have we covered


- Statistical Programming
  - All the Slides (üí™ sections) and Labs
  - [Slides 02](https://pols1600.paultesta.org/slides/02-slides.html#72) on Data Wrangling
  - QSS: Chapter 1.3, Chapter 2.2
  - [Cheat sheets](https://www.rstudio.com/resources/cheatsheets/)
    - [Transforming](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf)
    - [Reshaping](https://raw.githubusercontent.com/rstudio/cheatsheets/main/tidyr.pdf)
 
---
## Review: Where have we covered

- Descriptive statistics

  - [Slides 01](https://pols1600.paultesta.org/slides/01-slides.html#112), [Lab 01](https://pols1600.paultesta.org/labs/01-lab-comments.html#8_Explore_R%E2%80%99s_functions_for_generating_summary_statistics)
  - QSS: Chapter 2.6, Chapter 3,6, Exercises 1.5,

---
## Review: Where have we covered


- Data visualization as tool for descriptive inference
  
  - [Slides 02](https://pols1600.paultesta.org/slides/02-slides.html#16), [Lab 02](https://pols1600.paultesta.org/labs/02-lab-comments.html#Outline_the_steps_you_will_need_to_complete_this_process)
  - QSS: Chapter 1.3, Chapter 2.2, Chapter 3.3, 3.6
  - [ggplot2](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf) cheat sheet

---
## Review: Where have we covered

- Causal inference in experimental designs
  
  - [Slides 03](https://pols1600.paultesta.org/slides/03-slides.html#39), [Lab 03](https://pols1600.paultesta.org/labs/03-lab-comments.html#8_Calculate_the_Average_Treatment_Effect)
  - QSS: Chapter 2.3, 2.4


---
## Review: What should you know right now?

 
- Statistical Programming

- Descriptive statistics

- Data visualization as tool for descriptive inference

- Causal inference in experimental designs

 


---
## What you don't need to know

- I view QSS as a supplement to the course. 

- Similar topical structure, but if we don't talk about it in class, it's not something you need to know for the course:

  - Q-Q plots
  - K-means clustering
  - Gini Coefficients
  - `swirl()`
 
- If there are specific sections of QSS that are particularly confusing, let me know!


---
class:inverse, middle, center
# üîç
## Statistical Programming

---
##  Mapping Concepts to Code

&lt;table&gt;
&lt;caption&gt;You're learning how to map conceptual tasks to commands in R&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Skill &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Common Commands &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Setup R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; library(), ipak() &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Load data &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; read_csv(), load() &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Get HLO of data &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; df$x, glimpse(), table(), summary() &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Transform data &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; &amp;lt;-, mutate(), ifelse(), case_when() &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Reshape data &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; pivot_longer(), left_join() &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Summarize data numerically &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; mean(), median(), summarise(), group_by() &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Summarize data graphically &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ggplot(), aes(), geom_ &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
##  Mapping Concepts to Code

- Takes time and practice

- Don't be afraid to FAAFO

- Don't worry about memorizing everything. 

- Statistical programming is necessary to actually **do** empirical research

- Learning to code will help us understand statistical concepts.

- Learning to think programmatically and algorithmically will help us tackle complex problems



---
class: inverse, middle, center
# üîç
## Descriptive statistics


---
## Descriptive statistics

- Descriptive statistics help us describe what's typical of our data

--
- What's a typical value in our data
  - [Mean](https://pols1600.paultesta.org/labs/01-lab-comments.html#81_Measures_of_Central_Tendency)
  - [Median](https://pols1600.paultesta.org/labs/01-lab-comments.html#81_Measures_of_Central_Tendency)
  - [Mode](https://pols1600.paultesta.org/labs/01-lab-comments.html#81_Measures_of_Central_Tendency)

--
- How much do our data vary?
  - [Variance](https://pols1600.paultesta.org/labs/01-lab-comments.html#82_Measures_of_Dispersion)
  - [Standard deviation](https://pols1600.paultesta.org/labs/01-lab-comments.html#82_Measures_of_Dispersion)

--
- As one variable changes how does another change?
  - [Covariance](https://pols1600.paultesta.org/labs/01-lab-comments.html#83_Measures_of_Association)
  - [Correlation](https://pols1600.paultesta.org/labs/01-lab-comments.html#83_Measures_of_Association)

--
- Descriptive statistics are:
  - Diagnostic
  - Generative


---
## Descriptive statistics: Levels of understanding

- Conceptual

- Practical

- Definitional

- Theoretical

---
## Descriptive statistics: Levels of understanding

- **Conceptual**

- **Practical**

- Definitional

- Theoretical


---
## Mean: Conceptual Understanding

A mean is:
  
- A common and important measure of central tendency (what's typical)

- It's the arithmetic average you learned in schooll

- We can think of it as the balancing point of a distribution

- A conditional mean is the average of one variable `\(X\)`, when some other variable, `\(Z\)` takes a value `\(z\)`

  - Think about the average height in our class (unconditional mean) vs the average height among men and women (conditional means)


---
## Mean as a balancing point


![](https://mathbitsnotebook.com/Algebra1/StatisticsData/balancepoint1.jpg)

[Source](https://mathbitsnotebook.com/Algebra1/StatisticsData/STCenter.html)

---
## Mean: Practical

There are lots of ways to calculate means in `R`

- The simplest is to use the `mean()` function
  
  - If our data have missing values, we need to to tell `R` to remove them 
  

```r
mean(df$x, na.rm=T)
```

---
## Conditional Means: Practical

  
  - To calculate a conditional mean we could us a logical index `[df$z == 1]` 


```r
mean(df$x[df$z == 1], na.rm=T)
```
  
- If we wanted to a calculate a lot of conditional means we could use the `mean()` in combination with `group_by()` and `summarise()`


```r
df %&gt;% 
  group_by(z)%&gt;%
  summarise(
    x = mean(x, na.rm=T)
  )
```


---
## Mean: Definitional

Formally, we can define the arithmetic mean of `\(x\)` as `\(\bar{x}\)`:

$$
\bar{x} = \frac{1}{n}\left (\sum_{i=1}^n{x_i}\right ) = \frac{x_1+x_2+\cdots +x_n}{n}
$$

In words, this formula says, to calculate the average of x, we sum up all the values of `\(x_i\)` from observation `\(i=1\)` to `\(i=n\)` and then divide by the total number of observations `\(n\)`


---
## Mean: Definitional

- In this class, I don't put a lot of weight on memorizing definitions (that's what Google's for).

- But being comfortable with "the math" is important and useful

- Definitional knowledge is a prerequisite for understanding more theoretical claims.

---
## Mean: Theoretical

Suppose I asked you to show that the sum of deviations from a mean equals 0?

$$
\text{Claim:} \sum_{i=1}^n (x_i -\bar{x}) = 0
$$

---
## Mean: Theoretical

Knowing the definition of an arithmetic mean, we could write: 


`$$\begin{aligned}
\sum_{i=1}^n (x_i -\bar{x}) &amp;= \sum_{i=1}^n x_i - \sum_{i=1}^n\bar{x} &amp; \text{Distribute Summation}\\
              &amp;= \sum_{i=1}^n x_i - n\bar{x} &amp; \text{Summing a constant, } \bar{x}\\
              &amp;= \sum_{i=1}^n x_i - n\times \left ( \frac{1}{n} \sum_{i=1}^n{x_i}\right ) &amp; \text{Definition of } \bar{x}\\
              &amp;= \sum_{i=1}^n x_i - \sum_{i=1}^n{x_i} &amp; n \times \frac{1}{n}=1\\
              &amp;= 0             
\end{aligned}$$`


---
## Mean: Theoretical

Why do we care?

- Showing the deviations sum to 0 is another way of saying the mean is a "balancing point." 

- This turns out to be a useful property of means that will reappear throughout the course

- If I asked you to make a prediction, `\(\hat{x}\)` of a random person's height in this class, the mean would have the lowest "mean squared error" (MSE `\(=\frac{1}{n}\sum (x_i - \hat{x_i})^2)\)` 



---
## Mean: Theoretical

Occasionally, you'll read or here me say  say things like:

&gt; The sample mean is an unbiased estimator of the population mean

In a statistics class, we would take time to prove this.


---
## The sample mean is an unbiased estimator of the population mean

Claim:

Let `\(x_1, x_2, \dots x_n\)` from a random sample from a population with mean `\(\mu\)` and variance `\(\sigma^2\)`

Then:

$$
\bar{x} = \frac{1}{n}\left (\sum_{i=1}^n x_i\right )
$$

is an unbiased estimator of `\(\mu\)`

$$
E[\bar{x}] = \mu
$$

---
## The sample mean is an unbiased estimator of the population mean


Proof:

`$$\begin{aligned}
E\left [\bar{x} \right] &amp;= E\left [\frac{1}{n}\left (\sum_{i=1}^n x_i \right) \right] &amp; \text{Definition of } \bar{x} \\
&amp;= \frac{1}{n} \sum_{i=1}^nE\left [ x_i \right]  &amp; \text{Linearity of Expectations} \\
&amp;= \frac{1}{n} \sum_{i=1}^n \mu  &amp; E[x_i] = \mu \\
&amp;= \frac{n}{n}  \mu  &amp; \sum_{i=1}^n \mu = n\mu \\
&amp;= \mu  &amp; \blacksquare \\
\end{aligned}$$`


---
## Levels of understanding

In this course, we tend to emphasize the 

- **Conceptual**

- **Practical**

Over

- Definitional

- Theoretical


In an intro statistics class, the ordering might be reversed.

Trade offs:

- Pro: We actually get to *work with data* and *do empirical research* much sooner
- Cons: We substitute intuitive understandings for more rigorous proofs


---
class:inverse, middle, center
# üîç
## Data visualization


---
## Data visualization as a tool for descriptive inference

A statistical graphic is a mapping of `data` variables to `aes` thetic attributes of `geom` etric objects.

At a minimum, a graphic contains three core components:

- `data:` the dataset containing the variables of interest.
- `aes`: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.
- `geom:` the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars.

[Ismay and Kim (2022)](https://moderndive.com/2-viz.html#grammarofgraphics)

---
## You're about to be reincarnated: HLO


```r
df_wk03$reincarnation
```

```
&lt;labelled&lt;double&gt;[12]&gt;: You're about to be reincarnated. Would you like to come back as a
 [1] 1 3 2 2 3 2 3 1 3 2 2 2

Labels:
 value                                    label
     1     Land-dweller (like a lion or lizard)
     2   Air-dweller (like a bird or butterfly)
     3      Sea-dweller (like a fish or kraken)
     4       Dirt-dweller (like a plant or ant)
     5 Single-celled organism (like a protozoa)
```

```r
table(df_wk03$reincarnation)
```

```

1 2 3 
2 6 4 
```


---
## You're about to be reincarnated: Basic Plot


```r
df_wk03 %&gt;%
  ggplot(aes(x = reincarnation, 
             fill = reincarnation))+
  geom_bar(
    stat = "count"
  )
```

---

&lt;img src="04-slides_files/figure-html/plot0-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
##  Use a factor to label and order responses


```r
df_wk03 %&gt;%
  mutate(
    # Turn numeric values into factor labels 
    Reincarnation = forcats::as_factor(reincarnation),
    # Order factor in decreasing frequency of levels
    Reincarnation = forcats::fct_infreq(Reincarnation),
    # Reverse order so levels are increasing in frequency
    Reincarnation = forcats::fct_rev(Reincarnation)
  ) -&gt; df_wk03
```

---
## Check recoding


```r
table(recode= df_wk03$Reincarnation, original = df_wk03$reincarnation)
```

```
                                          original
recode                                     1 2 3
  Single-celled organism (like a protozoa) 0 0 0
  Dirt-dweller (like a plant or ant)       0 0 0
  Land-dweller (like a lion or lizard)     2 0 0
  Sea-dweller (like a fish or kraken)      0 0 4
  Air-dweller (like a bird or butterfly)   0 6 0
```


---
##  You're about to be reincarnated: Revised plot


```r
df_wk03 %&gt;% # Data
  # Aesthetics
  ggplot(aes(x = Reincarnation, 
             fill = Reincarnation))+
  # Geometry
  geom_bar(stat = "count")+ # Statistic
  ## Include levels of Reincarnation w/ no values
  scale_x_discrete(drop=FALSE)+
  # Don't include a legend
  scale_fill_discrete(drop=FALSE, guide="none")+
  # Flip x and y
  coord_flip()+
  # Remove lines
  theme_classic() -&gt; fig1

# Display figure
fig1
```

---

&lt;img src="04-slides_files/figure-html/plot1-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## What creature and why?


<div id="htmlwidget-76f9d0c70fac8acaa766" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-76f9d0c70fac8acaa766">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12"],["Land-dweller (like a lion or lizard)","Sea-dweller (like a fish or kraken)","Air-dweller (like a bird or butterfly)","Air-dweller (like a bird or butterfly)","Sea-dweller (like a fish or kraken)","Air-dweller (like a bird or butterfly)","Sea-dweller (like a fish or kraken)","Land-dweller (like a lion or lizard)","Sea-dweller (like a fish or kraken)","Air-dweller (like a bird or butterfly)","Air-dweller (like a bird or butterfly)","Air-dweller (like a bird or butterfly)"],["horse because they have cool hair and can run around in meadows","A dolphin because they're majestic","A bird that is elusive and hard to find/capture, whether because it lives in remote areas or is nocturnal. Why? Flying is fun, and there is lower risk of me being hunted or brought back to a zoo!","Who doesn't want to fly, man?","A dolphin because I love the ocean and swimming fast would be fun","A goose. I could find friendship in my gaggle, have a low potential for divorce (they mate for life), and get to see the moon up close and personal when I migrate.","Blue whale üêã","I'd actually like to be an animal that goes between land and water, like a walrus!","Manatee specifically because I really admire them and their lifestyle but I would take any fish because they have a good vibe","Water makes me feel like I am suffocating. I have always wanted to be able to fly. When I meditate, sometimes I pretend I am a bird soaring over myself and the world I am in now. So coming back as a bird would be cool. Plus, life is simpler. I could camp out by my mom's bird feeders and enjoy myself.","A falcon or albatross, depends on the vibes.","Boring, but flying would be pretty awesome. I'd probably be a duck, since they have all that nice down that keeps them warm.  I want to be a toasty bird that gets to fly around."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Reincarnation<\/th>\n      <th>creature<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>




---
## You're about to be reincarnated: Add some labels


```r
df_wk03 %&gt;%
  mutate(
    # Create numeric id
    id = 1:n(),
    # Create a label with 3 answers and NA elsewhere
    Why = case_when(
      id == 1 ~ str_wrap(creature[1],30),
      id == 2 ~ str_wrap(creature[2],30),
      id == 6 ~ str_wrap(creature[6],30),
      TRUE ~ NA_character_

    )

  ) -&gt; df_wk03
```

---
<div id="htmlwidget-f5cf7c5c779a934e48e6" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f5cf7c5c779a934e48e6">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12"],["horse because they have cool\nhair and can run around in\nmeadows","A dolphin because they're\nmajestic",null,null,null,"A goose. I could find\nfriendship in my gaggle, have\na low potential for divorce\n(they mate for life), and get\nto see the moon up close and\npersonal when I migrate.",null,null,null,null,null,null],["horse because they have cool hair and can run around in meadows","A dolphin because they're majestic","A bird that is elusive and hard to find/capture, whether because it lives in remote areas or is nocturnal. Why? Flying is fun, and there is lower risk of me being hunted or brought back to a zoo!","Who doesn't want to fly, man?","A dolphin because I love the ocean and swimming fast would be fun","A goose. I could find friendship in my gaggle, have a low potential for divorce (they mate for life), and get to see the moon up close and personal when I migrate.","Blue whale üêã","I'd actually like to be an animal that goes between land and water, like a walrus!","Manatee specifically because I really admire them and their lifestyle but I would take any fish because they have a good vibe","Water makes me feel like I am suffocating. I have always wanted to be able to fly. When I meditate, sometimes I pretend I am a bird soaring over myself and the world I am in now. So coming back as a bird would be cool. Plus, life is simpler. I could camp out by my mom's bird feeders and enjoy myself.","A falcon or albatross, depends on the vibes.","Boring, but flying would be pretty awesome. I'd probably be a duck, since they have all that nice down that keeps them warm.  I want to be a toasty bird that gets to fly around."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Why<\/th>\n      <th>creature<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>



---
## You're about to be reincarnated: 


```r
# Caclculate totals before calling ggplot
plot_df &lt;- df_wk03 %&gt;%
* group_by(Reincarnation)%&gt;%
* summarise(
*   Count = n(),
*   Why = na.omit(unique(Why))
  )
```

---
## You're about to be reincarnated:

<div id="htmlwidget-b180e140640e68e38ab9" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-b180e140640e68e38ab9">{"x":{"filter":"none","vertical":false,"data":[["1","2","3"],["Land-dweller (like a lion or lizard)","Sea-dweller (like a fish or kraken)","Air-dweller (like a bird or butterfly)"],[2,4,6],["horse because they have cool\nhair and can run around in\nmeadows","A dolphin because they're\nmajestic","A goose. I could find\nfriendship in my gaggle, have\na low potential for divorce\n(they mate for life), and get\nto see the moon up close and\npersonal when I migrate."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Reincarnation<\/th>\n      <th>Count<\/th>\n      <th>Why<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>


---
## You're about to be reincarnated: Revised Plot


```r
plot_df %&gt;%
  ggplot(aes(x = Reincarnation, 
             y = Count,
             fill = Reincarnation, 
             label=Why))+
* geom_bar(stat = "identity")+
  ## Include levels of Reincarnation w/ no values
  scale_x_discrete(drop=FALSE)+
  # Don't include a legend
  scale_fill_discrete(drop=FALSE, guide="none")+
  coord_flip()+
  labs(x = "",y="",title="You're about to be reincarnated.\nWhat do you want to come back as?")+
  theme_classic() -&gt; fig1
```

---
&lt;img src="04-slides_files/figure-html/plot2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Add labels:


```r
fig1 &lt;- fig1 +
  geom_label_repel(
    fill="white",
    nudge_y = 1, 
    hjust = "left",
    size=3,
    arrow = arrow(length = unit(0.015, "npc"))
    )+ 
  scale_y_continuous(
    breaks = c(0,2,4,6,8,10,12),
    expand = expansion(add =c(0,6))
    )

fig1
```


---

&lt;img src="04-slides_files/figure-html/plot3-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
## Data visualization is an iterative process

- Data visualization is an iterative process

- Good data viz requires lots of data transformations

- Start with a minimum working example and build from there

- Don't let the perfect be the enemy of the good enough.


---
class:inverse, middle, center
# üîç
## Causal inference


---
## Causal inference is about counterfactual comparisons

- Causal inference is about counterfactual comparisons
  
  - What would have happened if some aspect of the world either had or had not been present

---
## Causal inference is about counterfactual comparisons

Two ways to represent causal claims

- DAGs helped illustrate types of bias
  - **Confounder bias:** Failing to control for a common cause (aka Selection Bias, Omitted Variable Bias)
  - **Collider bias:** Controlling for a common consequence 

- Potential Outcomes Notation helped illustrate the **Fundamental Problem of Causal Inference**

---
## Fundamental Problem of Causal Inference

- For an individual, we only observe one of potential many potential outcomes.
  
  - Did taking an aspirin make your headache go away?
  
  - Need to compare you with aspirin and without, but only see one version of the world.

- Individual Causal Effects are **unidentified**

---
## Causal Identification


- Causal Identification: What do we need to assume for your causal claim to be credible

- Randomization offers a solution to the fundamental problem by ensuring that treatment is independent of potential outcomes `\((Y(1), Y(0))\)`, observed `\((X)\)`, and unobserved covariates `\((U)\)`.

$$
Y(1), Y(0),X,U, \perp D
$$

---
## Randomization creates credible counterfactual comparisons

If treatment has been randomly assigned, then:

- The only thing that differs between treatment and control is that one group got the treatment, and another did not.
- We can estimate the Average Treatment Effect (ATE) using the difference of sample means


`$$\begin{aligned}
E \left[ \frac{\sum_1^m Y_i}{m}-\frac{\sum_{m+1}^N Y_i}{N-m}\right]&amp;=\overbrace{E \left[ \frac{\sum_1^m Y_i}{m}\right]}^{\substack{\text{Average outcome}\\
\text{among treated}\\ \text{units}}}
-\overbrace{E \left[\frac{\sum_{m+1}^N Y_i}{N-m}\right]}^{\substack{\text{Average outcome}\\
\text{among control}\\ \text{units}}}\\
&amp;= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]
\end{aligned}$$`

---
## What's the counterfactual?

&gt; Broadly speaking, I don't like computers. I don't like bending over the small desks, staring at a screen, etc. The class is fine: it's something I have to do, all things worth doing are difficult, and I'm learning to think in new ways. That said, I tend to doubt that computer programing is conducive to human flourishing. Would the world be a better place if everyone learned how to code? I tend to think no. Would the world be a better place if everyone read Shakespeare? Probably. This isn't really a practical concern, but rather a question about, y'know, what we're doing here. 

&gt; More concretely, I found the lab hard to complete, but I'll look over the notes and review. Part of me thinks I'd learn better if I started the labs before class. I'd ask better questions, be less confused, etc. 

&gt; In sum: the class is fine. It's good to learn to think in new ways and good to be challenged, even if I have some broader questions about coding as a human activity."


---
## What's the counterfactual?

&gt; Broadly speaking, I don't like computers. I don't like bending over the small desks, staring at a screen, etc. The class is fine: it's something I have to do, all things worth doing are difficult, and I'm learning to think in new ways. That said, I tend to doubt that computer programing is conducive to human flourishing. **Would the world be a better place if everyone learned how to code?** I tend to think no. Would the world be a better place if everyone read Shakespeare? Probably. This isn't really a practical concern, but rather a question about, y'know, what we're doing here. 

&gt; More concretely, I found the lab hard to complete, but I'll look over the notes and review. Part of me thinks I'd learn better if I started the labs before class. I'd ask better questions, be less confused, etc. 

&gt; In sum: the class is fine. It's good to learn to think in new ways and good to be challenged, even if I have some broader questions about coding as a human activity."

---
## Would the world be a better place if everyone learned how to code?

--

- How should we define/measure human flourishing?

--

- What are some counterfactuals  we might make?
  - Everyone learns to code:
  - You learning to code:
  - Learning to code vs reading Shakespeare

--

- Why might these comparisons be misleading?
  - CS concentrators to POLS concentrators
  - POLS 1600 takers to POLS 0500

--

- How could we assess the "effects" of POLS 1600?

---
class:,bottom, center
background-image:url("https://media.giphy.com/media/dUaHl1MDcaGLGtVBbI/giphy-downsized-large.gif")
background-size:contain

## Learning to code


---
class: inverse, bottom, center
background-image:url("https://media.giphy.com/media/h5pRkbOXAH66zlZVML/giphy.gif")
background-size:contain
## Thinking Programatically







---
class: inverse, center, middle
# üí°
# Casual Inference in Experimental and Observational Designs

---
## Experimental and Observational Designs

- **Experimental designs** are studies in which a causal variable of interest, the *treatement*, is manipulated by the researcher to examine its causal effects on some *outcome* of interest

  - **Randomized Controlled Trial** (RCTs) each unit is **randomly** assigned to a treatment(s) or control group

- **Observational designs** are studies in which a causal variable of interest is assigned by someone other than the researcher (nature, governments, people)

---
## Causal Identification

- **Casual Identification** refers to "the assumptions needed for statistical estimates to be given a causal interpretation" [Keele (2015)](http://lukekeele.com/wp-content/uploads/2016/03/causal.pdf)

- **What's  Your Casual Identification Strategy:** What are the assumptions that make your research design credible?

- Identification &gt; Estimation

---
## Causal Identification with Experimental Designs

Causal identification for an experiment, requires relatively few assumptions:

- **Independence** (Satisfied by Randomization)
  - `\(Y(1), Y(0),X,U, \perp D\)`
- **SUTVA** Stable Unit Treatment Value Assumption (Depends on features of the design)
  - No interference between units `\(Y_i(d_1, d_2, \dots, d_N) = Y_i(d_i)\)`
  - No hidden values of the treatment/Variation in the treatment 



---
## Internal vs External Validity



- **Internal validity** the extent to which causal assumptions are satisfied in a study

- **External validity** the extent to which conclusions can be generalized beyond a particular study

---
## Internal vs External Validity

Experimental designs are said to have high **internal** validity, but may lack **external** validity

  - [The college sophomore problem](https://psycnet.apa.org/doiLanding?doi=10.1037%2F0022-3514.51.3.515)
  
  - [The weirdest people in the world](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/weirdest-people-in-the-world/BF84F7517D56AFF7B7EB58411A554C17)

  - **The Hawthorne Effect** refers to the phenomenon where study subjects behave differently because they know they are being observed by researchers. (QSS Ch 2 p. 52)

---
## Causal Identification in Observational Designs

- In an *observational study* the researcher does not control the treatment assignment

- No guarantee that treatment (D=1) and control groups (D=0) are comparable (That is that we're comparing like with like)

- Instead, we have justify our claims by theory and assumption rather than direct manipulation through random assignment.

---
## Conditional Independence (ignorability)

If treatment is not randomly assigned then in general:

$$
Y_i(1),Y_i(0) \text{ is not} \perp D_i
$$


However, in some situations, it may be plausible to claim that *conditional* on some variable(s) `\(X\)`, the distribution of potential outcomes `\(Y\)` is the same (independent) across levels of treatment `\(D\)` (**conditional ignorability**)


$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

- Conditional on a some covariate(s) `\(X_i\)` our treatment is **as-if randomized**

---
## As-if randomized

![](https://media.makeameme.org/created/dx-59c7bd.jpg)


---
## Causal Identification in Observational Designs

- The claim that treatment is as-if randomized requires further justification in the theory and design of your study

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

- While we can't "prove" this assumption, we typically can test some observable implications of this claim,
  
  - For example, in [lab 03](https://pols1600.paultesta.org/labs/03-lab-comments.html#7_Examine_the_difference_in_covariates_between_those_assigned_to_each_treatment_condition_in_the_study) we tested for **covariate balance** by comparing the average values of pre-treatment covariates from the voter file.


---
## The Experimental Ideal

&gt; ‚ÄúThe planner of an observational study should always ask himself: How would the study be conducted if it were possible to do it by controlled experimentation‚Äù (Cochran 1965)

- What would you have to randomly assign to answer your question as posed?

- Is it feasible to imagine changing just the treatment and nothing else? 

  - If yes, maybe we should do an experiment

  - If not, maybe we should rethink our question and ask what about the design of observational study allows us to make the credible comparisons that an experiment would generate?
  
---
## Observational Designs

Observational designs all involve covariate adjustment

We will discuss three approaches to covariate adjustment

- Subclassification
- Matching
- Regression

Then we will consider three types of observational desings that can produce credible causal claims:

- Difference in Difference
- Regression Discontinuities
- Instrumental Variables


---
class: inverse, center, middle
background-image:url("https://m.media-amazon.com/images/I/71lsG3fCaOL._SL1200_.jpg")
background-size:cover
# Break
## [Take the survey](https://brown.co1.qualtrics.com/jfe/form/SV_8GHAmuPswJfaSyi)

---
class: inverse, center, middle

# üí°  
## Covariate Adjustment

---
##  Covariate Adjustment

Covariate adjustment refers a broad class of procedures that try to make a comparison more credible or meaningful by adjusting for some other potentially confounding factor.

---
##  Covariate Adjustment

When you hear people talk about

- Controlling for age
- Conditional on income
- Holding age and income constant
- Ceteris paribus (All else equal)

They are typically talking about some sort of covariate adjustment.

---
##  Three approaches to for covariate adjustment

- Subclassification

- Matching 

- Regression 





---
##  Causal Identification through Subclassification

Motivation: Treatment, `\(D\)` is not randomly assigned

$$
Y_i(1),Y_i(0) \text{ is not} \perp D_i
$$

Identifying assumptions

`$$\begin{aligned}
Y_i(1),Y_i(0) \perp D_i |X_i &amp;&amp; \text{Selection on Observables}\\
0 &lt; Pr(D_i = 1|X_i) &lt; 1  &amp;&amp; \text{Common Support}
\end{aligned}$$`

---
##  Causal Identification through Subclassification

If these assumptions hold, we can estimate the average treatment effect:

`$$\begin{aligned}
ATE = E[Y_i(1)- Y_i(0)|X_i] &amp;= E[Y_i(1)- Y_i(0)|X_i, D_i=1]\\
&amp; = E[Y_i |X_i, D_i=1] - E[Y_i |X_i, D_i=0]
\end{aligned}$$`

The average treatment effect is identified by the observed difference of means between treatment and control conditional on the values of `\(X\)`

---
##  Causal Identification through Subclassification

- Economists call `\(Y_i(1),Y_i(0) \perp D_i |X_i\)` an assumption of **Selection on Observables**
  
  - Controlling for what we can observe `\(X\)`, `\(D\)` is conditionally independent of Potential Outcomes
  
  - Violated if there were some other factor, `\(U\)` that influenced both `\(D\)` and `\(Y\)`

- `\(0 &lt; Pr(D = 1|X) &lt; 1\)` is called an assumption of **Common Support**
  
  - There is a non-zero probability of receiving the treatment for all values of X
  
  - Violated if only one subgroup had access to the treatment (Vaccine by age group comparisons)

---
##  Example of Subclassification

- We used subclassification when we compared the unconditional rates of new Covid-19 cases by face mask policy to the conditional rates new cases by policy regime in each month of our data.
  - Overall rates are misleading. 
  - Lots of things differ between January 2020 and January 2022
  - Subclassification by month provides a "fairer" comparison
  - But is it "causal"

--
- Even controlling for "month" there are other omitted variables:
  - Other policies in place?
  - Socio-economic differences between states
  - Others?

--
- Trying to subclassify (stratify) comparisons on more than one or two variables gets hard
  - **The Curse of Dimensionality**

---
## The Curse of Dimensionality

- As we try to control for more factors, the number of observations per dimension declines rapidly
  - Men vs Women
  - Men, ages 20-30 vs Men ages 30-40
  - Men, ages 20-30 with college degrees and blue eyes vs Men  ages 20-30 with college degrees and green eyes

- Subclassification with more than a few variables, will often produce a lack of common support: 
  - Not enough observations to make credible counterfactual comparisons

---
## Matching

- Matching refers to a broad set of procedures that essentially try to generalize subclassification to 
  - address to **curse of dimensionality**
  - achieve balance on a range of observable covariates between **treated** and **control** groups

- Many different types of matching procedures:

  - **Exact matching:** Find exact matches between treatment and control observations for all covariates `\(X\)`
  - **Coarsened exact matching:** Find approximate matches within ranges of values for `\(X\)`
  - **Distance-metric matching:** Calculate a distance metric between observations based on their values of `\(X\)`, and match treated and control to minimize that distance
  - **Propensity score matching:** Calculate the propsenity to receive treatment using `\(X\)` to predict `\(D\)` and treated and control based on their *propensity scores*

---

&lt;img src="https://cdn2.hubspot.net/hubfs/355318/images/Propensity-Score-Graphic.jpg" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://www.summitllc.us/propensity-score-matching)

---
## Causal Identification with Matching

Matching again requires an assumption of **selection on observables**

`$$\begin{aligned}
Y_i(1),Y_i(0) \perp D_i |X_i &amp;&amp; \text{Selection on Observables}\\
0 &lt; Pr(D_i = 1|X_i) &lt; 1  &amp;&amp; \text{Common Support}
\end{aligned}$$`


Matching procedures like propensity score matching, allow us to match treated and control observations based on a *propensity score*, a predicted value of receiving the treatment, `\(D\)` based on observed variables, `\(X\)`.

$$
p(X_i) = Pr(D=1|X_i) = \pi_i
$$

Allowing us to estimate an ATE conditional on `\(\pi_i\)`

`$$\begin{aligned}
ATE &amp;= E[Y_i(1)- Y_i(0)|p(X_i) = \pi_i] \\
&amp;= E[Y_i(1)- Y_i(0)|p(X_i) = \pi_i, D_i=1]\\
&amp; = E[Y_i |p(X_i) = \pi_i, D_i=1] - E[Y_i |p(X_i) = \pi_i, D_i=0]
\end{aligned}$$`


---
## What to Know about Matching

- The mechanics of matching are beyond the scope of this course

- Just think of it as a generalization of subclassification when we want to condition on multiple variables

- "Solves" the curse of dimensionality, creating Treatment-Control comparisons between groups that are similar on **observed covariates**

- But **no guarantee** that matching produces balance on **unobserved covariates**.





---
## Regression

- We will spend the next two weeks talking in detail about regression, in general and linear regression in particular.

- Today we'll introduce some basic notation and simple examples

- Conceptually, think of regression as 
  - a tool to make predictions
  - by fitting lines to data

- Theoretically, we will build towards an understanding of linear regression as a "linear estimate of the conditional expectation function `\((CEF = E[Y|X])\)`

---
## Regression

Broadly regression is a statistical procedural to help us model relationships that consists of:

- `\(Y\)` an **outcome variable** or thing we're trying to explain
  - AKA: The Dependent Variable, The Response Variable, The Lefthand side of the model

- `\(X\)` a set of **predictor variables** or things we think explain variation in our outcome
  - AKA: The independent variable, covariates, the right hand side of the model.

- `\(\beta\)` a set of **unknown parameters** that describe the relationship between our outcome `\(Y\)` and our predictors `\(X\)`

- `\(\epsilon\)` the **error term** representing variation in `\(Y\)` not explained by our model.

---
## Linear Regression

Next week, we will start to consider simple (bivariate) linear regressions of the form:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- We call this a bivariate regression, because there are only two variables.

- We call this a linear regression, because `\(y_i = \beta_0 + \beta_1 x_i\)` is the equation for a line, where:
  - `\(\beta_0\)` corresponds to the `\(y\)` intercept, or the model's prediction when `\(x = 0\)`.
  - `\(\beta_1\)` corresponds to the slope, or how `\(y\)` is predicted to change as `\(x\)` changes.

---
## Linear Regression 
  
- If you find this notation confusing, try plugging in substantive concepts for what `\(y\)` and `\(x\)` represent
- Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.

The generic linear model

`$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$`

Reflects:

`$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$`

---
## Estimating a Linear Regression

- We estimate linear regressions in `R` using the `lm()` function.
- `lm()` requires two arguments:
  - a formula of the general form `y ~ x` read as "Y modeled by X" or below "Transgender Feeling Thermometer's modeled by Age
  - a `data` telling R 
  


```r
load(url("https://pols1600.paultesta.org/files/data/03_lab.rda"))
```


```r
m1 &lt;- lm(therm_trans_t0 ~ vf_age, data = df)
m1
```

```

Call:
lm(formula = therm_trans_t0 ~ vf_age, data = df)

Coefficients:
(Intercept)       vf_age  
    62.8196      -0.2031  
```

---
&lt;img src="04-slides_files/figure-html/figlm1code-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
&lt;img src="04-slides_files/figure-html/figlm1-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
## Multiple Regression

In two weeks, we will generalize this to include multiple predictors

`$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2  x_{2i} + \epsilon$$`

- `\(\beta_0\)` in this model still corresponds to the y intercept the model's prediction when everything else is 0 
- `\(\beta_1\)` now describes the relationship between `\(x\)`


`$$\text{FT}_{Trans} = \beta_0 + \beta_1 \text{Age}_{1i} + \beta_2  \text{Party ID}_{2i} + \epsilon$$`

---
## Multiple Regression with Matrix Notation

With more than two predictors, we will write equations using Matrix notation

`$$y_i = X\beta + \epsilon$$`

.pull-left[
Where X is a matrix of variables:

`$$X=
\begin{bmatrix}
	 1  &amp; X_{1,1}&amp;\dots &amp; X_{1,k}\\
 1  &amp; X_{2,1}&amp;\dots &amp; X_{2,k}\\
 \vdots&amp;\vdots&amp;  &amp; \vdots \\
 1  &amp; X_{n,1}&amp;\dots &amp; X_{n,k}\\
\end{bmatrix}$$`
]
.pull-right[
And `\(\beta\)` is a vector of coefficients

`$$\beta=
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots \\
\beta_k\\
\end{bmatrix}$$`
]

---
## Causal Identification with Regression

Like subclassification and matching, causal identification with Linear Regression requires us to believe the assumption of **Selection on Observables**

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

And additionally asks us to make assumptions about the "functional form": 

$$
Y = \beta_0 + \tau D + X\beta + \epsilon
$$


---
## Causal Identification with Regression

- To interpret a linear regression's estimate of `\(\tau\)` as the causal effect of `\(D\)` on `\(Y\)`, we need to believe we've got the *right model*
  
  - The only factors that predict both `\(Y\)` and `\(D\)` are those factors `\(X\)` which we control for in our model.
  
  - The relationship between `\(Y\)` and these predictors is *linear*

--

- We almost never have the right model.
  - Do we think age is the only factor that influences attitudes toward Trangendered folk? Probably not. 

--

- Regression is still a robust and useful tool
  - Why do we see negative relationship between age and transgender attitudes?

--

- But for causal inference, there's nothing inherent about estimating a regression, or controlling for a lot of variables that makes for particularly credible causal claims.



---
##  Methods for covariate adjustment

- Subclassification
  - üëç: Easy to implement and interpret
  - üëé: Curse of dimensionality, Selection on observables

- Matching 
  - üëç: Balance on multiple covariates, Mirrors logic of experimental design 
  - üëé: Selection on observables, Only provides on observed variables, Lot's of technical details...

- Regression 
  - üëç: Easy to implement, control for many factors (good and bad)
  - üëé: Selection on observables, easy to fit "bad" models

---
class: inverse, center, middle

# üí°  
# Three Designs for Causal Inference in Observational Studies


---
## Credible Cauasal Inference in Observational Studies

Subclassification, matching, and regression all require an assumption of selection on observables:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

But how do we know if we've got the right model or we've controlled for the right variables?

--

Typically, we don't

---
## Credible Cauasal Inference in Observational Studies

Instead, social scientists  look for situations where the credibility of

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

depends less on how much data you have and much more on how your data were generated.



---
class: middle

.pull-left[
&gt; Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact. ... [T]he primary engine driving improvement has been a focus on the **quality of empirical research designs.** (p. 4)

]

.pull-right[
&lt;img src="./images/04_cred.png" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)

]

---
class: middle

.pull-left[
&gt; Design-based studies are distinguished by their prima facie credibility and by the attention investigators devote to making both an **institutional** and a **data-driven** case for causality (p. 5)

]

.pull-right[
&lt;img src="./images/04_cred.png" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)
]

---
class: middle

.pull-left[
&gt; The econometric methods that feature most prominently in quasi-experimental studies are **instrumental variables**, **regression discontinuity** methods,and **differences-in-differences**-style policy analysis. ... The best of today‚Äôs design-based studies make a strong institutional case, backed up with empirical evidence, for the variation thought to generate a useful **natural experiment**.(p. 12)

]

.pull-right[
&lt;img src="./images/04_cred.png" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)
]

---
## Three Designs for Causal Inference in Observational Studies

- Difference in Differences

- Regression Discontinuity

- Instrumental Variables


---
class: inverse, center, middle
# üí° Difference in Differences


---
class: inverse, center, middle
background-image:url(https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&amp;itok=Mn-K5rQc)
background-size: cover
## London in the Time of Cholera

---
## Motivating Example: What causes Cholera?

- In the 1800s, cholera was thought to be transmitted through the air.

- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.

- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. 

---
## Notation

Let's adopt a little notation to help us think about the logic of Snow's design:

- `\(D\)`: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)

- `\(T\)`: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).

- `\(Y_{di}(t)\)` the potential outcome of unit `\(i\)` 

  - `\(Y_{1i}(t)\)` the potential outcome of unit `\(i\)` when treated between the two periods 

  - `\(Y_{0i}(t)\)` the potential outcome of unit `\(i\)` when control between the two periods 

---
## Causal Effects

The individual causal effect for unit i at time t is:

`$$\tau_{it} = Y_{1i}(t) ‚àí Y_{0i}(t)$$`

What we observe is 

`$$Y_i(t) = Y_{0i}(t)\cdot(1 ‚àí D_i(t)) + Y_{1i}(t)\cdot D_i(t)$$`

`\(D\)` only equals 1, when `\(T\)` equals 1, so we never observe `\(Y_0i(1)\)` for the treated units. 

In words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.

---
## Average Treatment on Treated

Our goal is to estimate the average effect of treatment on treated (ATT):


`$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$`

That is, what would have happened in Lambeth, had their water company not moved their pipes

---
## Average Treatment on Treated

Our goal is to estimate the average effect of treatment on treated (ATT):

We we can observe is:

|               | Post-Period (T=1)  | Pre-Period (T=0)  |
|---------------|--------------------|-------------------|
| Treated `\(D_{i}=1\)`  | `\(E[Y_{1i}(1)\vert D_i = 1]\)`  | `\(E[Y_{0i}(0)\vert D_i = 1]\)` |
| Control `\(D_i=0\)`  | `\(E[Y_{0i}(1)\vert D_i = 0]\)`  | `\(E[Y_{0i}(0)\vert D_i = 0]\)` |

---
## Data

Because potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Company &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1854 (T=1) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1849 (T=0) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Lambeth (D=1) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 85 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Southwark and Vauxhall (D=0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 147 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 135 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## How can we estimate the effect of moving pumps upstream?

Recall, our goal is to estimate the effect of the the treatment on the treated:

`$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$`

Let's conisder some strategies Snow could take to estimate this quantity:

---
## Before vs after comparisons:

- Snow could have compared Labmeth in 1854 `\((E[Y_i(1)|D_i = 1] = 19)\)` to Lambeth in 1849 `\((E[Y_i(0)|D_i = 1]=85)\)`, and claimed that moving the pumps upstream led to 66 fewer cholera deaths. 

- This comparison assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved ($E[Y_{0i}(1)|D_i = 1]$).

- A skeptic might argue that Lambeth in 1849 `\(\neq\)` Lambeth in 1854


---
## Treatment-Control comparisons in the Post Period.

- Snow could have compared outcomes between Lambeth and S&amp;V in 1954  ($E[Yi(1)|Di = 1] ‚àí E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to 128 fewer deaths.

- Here the assumption is that the outcomes in S&amp;V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved ($E[Y_{0i}(1)|D_i = 1]$)

- Again, our skeptic could argue  Lambeth `\(\neq\)` S&amp;V 

---
## Difference in Differences

To address these concerns, Snow employed what we now call a difference-in-differences design, 

There are two, equivalent ways to view this design. 

`$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}‚àí \overbrace{\{E[Y_{i}(0)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$`

- Difference 1: Average change between Treated and Control  in Post Period
- Difference 2: Average change between Treated and Control  in Pre Period

Which is equivalent to:

`$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i} = 1]\}}_{\text{Post - Pre |Treated }}‚àí \overbrace{\{E[Y_{i}(1)|D_{i} = 0] ‚àí E[Y_{i}(0)|D_{i}=0 ]}^{\text{Post-Pre|Control}}$$`


- Difference 1: Average change between Treated over time
- Difference 2: Average change between Control over time

---
## Difference in Differences


You'll see the DiD design represented both ways, but they produce the same result:

$$
\tau_{ATT} = (19-147) - (85-135) = -78
$$

$$
\tau_{ATT} = (19-85) - (147-135) = -78
$$

---
## Identifying Assumption of a Difference in Differences Design

The key assumption in this design is what's known as the parallel trends assumption: `\(E[Y_{0i}(1) ‚àí Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) ‚àí Y_{0i}(0)|D_i = 0]\)` 

- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&amp;V

---
&lt;img src="04-slides_files/figure-html/paralleltrends-1.png" width="80%" style="display: block; margin: auto;" /&gt;


Where:

1. `\(E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(1)|D_{i} = 0]\)`
2. `\(E[Y_{i}(0)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i}\} = 0]\)`
3. `\(E[Y_{1i}(1) ‚àí Y_{0i}(1)|D_{i} = 1]\)`


---
## Summary

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
  - Taking the pre-post difference removes any fixed differences between the units
  
  - Then taking the difference between treated and control differences removes any common differences over time

- The key identifying assumption of a DiD design is the "assumption of parallel trends"
  - Absent treatment, treated and control groups
would see the same changes over time.
  - Hard to prove, possible to test


---
## Extensions and limitations

- DiD easy to estimate with linear regression
- Generalizes to multiple periods and treatment interventions
  - More pre-treatment periods allow you assess "parallel trends" assumption
- Alternative methods 
  - Synthetic control
  - Event Study Designs
- What if you have multiple treatments or treatments that come and go?
  - Panel Matching
  - Generalized Synthetic control

---
## Applications

- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment

- [Abadie, Diamond, &amp; Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany

- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?

---
class: inverse, center, middle
# üí° Regression Discontinuity Design

---
## Motivating Example

.pull-left[
- Do Members of Parliament in the UK get richer from holding office (QSS Chapter 4.3.4)
]
.pull-right[
&lt;img src="./images/04_eggers.png" width="80%" style="display: block; margin: auto;" /&gt;
[Eggers and Hainmueller (2009)](https://www.cambridge.org/core/journals/american-political-science-review/article/abs/mps-for-sale-returns-to-office-in-postwar-british-politics/E4C2B102194AA1EA0D2F1F777EAE3C08)
]


---
## Logic of the Regression Discontinuity Design (RDD) 

- What's the effect of holding elected office in the UK on personal wealth?

- People who win elections differ in many ways from people who lose elections.

- Logic of an RDD: 

  - Just look at the wealth of individuals who either narrowly won or lost elections.

  - Candidates close to 50 percent cutoff (discontinuity) should be more comparable (better counterfactuals)

---
## Data from Eggers and Hainmueller (2009)


```r
library(qss)
data(MPs)
glimpse(MPs)
```

```
Rows: 427
Columns: 10
$ surname    &lt;chr&gt; "Llewellyn", "Morris", "Walker", "Walker", "Waring", "Brown‚Ä¶
$ firstname  &lt;chr&gt; "David", "Claud", "George", "Harold", "John", "Ronald", "Le‚Ä¶
$ party      &lt;chr&gt; "tory", "labour", "tory", "labour", "tory", "labour", "tory‚Ä¶
$ ln.gross   &lt;dbl&gt; 12.13591, 12.44809, 12.42845, 11.91845, 13.52022, 12.46052,‚Ä¶
$ ln.net     &lt;dbl&gt; 12.135906, 12.448091, 10.349009, 12.395034, 13.520219, 9.63‚Ä¶
$ yob        &lt;int&gt; 1916, 1920, 1914, 1927, 1923, 1921, 1907, 1912, 1905, 1920,‚Ä¶
$ yod        &lt;int&gt; 1992, 2000, 1999, 2003, 1989, 2002, 1987, 1984, 1998, 2004,‚Ä¶
$ margin.pre &lt;dbl&gt; NA, NA, -0.057168204, -0.072508894, -0.269689620, 0.3409586‚Ä¶
$ region     &lt;chr&gt; "Wales", "South West England", "North East England", "Yorks‚Ä¶
$ margin     &lt;dbl&gt; 0.05690404, -0.04973833, -0.04158868, 0.02329524, -0.230005‚Ä¶
```

---
<div id="htmlwidget-6d66e466b9db509376f9" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-6d66e466b9db509376f9">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10"],["surname","firstname","party","ln.gross","ln.net","yob","yod","margin.pre","region","margin"],["surname of the candidate","first name of the candidate","party of the candidate (labour or tory)","log gross wealth at the time of death","log net wealth at the time of death","year of birth of the candidate","year of death of the candidate","margin of the candidate‚Äôs party in the previous election electoral","region","margin of victory (vote share)"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Variable<\/th>\n      <th>Description<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>



---

```r
MPs %&gt;%
  ggplot(aes(margin, ln.net))+
  geom_point(shape=1)+
  facet_grid(~party)+
  geom_smooth(data =MPs %&gt;%
                filter(margin &lt;0),
              method = "lm")+
  geom_smooth(data =MPs %&gt;%
                filter(margin &gt;0),
              method = "lm")+
  theme_bw() -&gt; fig_rdd
fig_rdd
```

---
&lt;img src="04-slides_files/figure-html/rddfig-1.png" width="80%" style="display: block; margin: auto;" /&gt;
---
## RDD Notation

- `\(X\)` is a **forcing** variable
- Treatment `\(D\)` is a determined by `\(X\)`

$$
D_i = 1\{X_i &gt; c\}
$$

- `\(X\)` is the `margin` variable in the example data, and `\(D=1\)` if `margin` is greater than 0 (i.e. the candidate won the election)

- Interested in the differences in the outcome at the threshold

`$$\lim_{x \downarrow  c} E[Y_i|X=x] - \lim_{x \uparrow  c} E[Y_i|X=x]$$`

---
## Causal Identification with an RDD

If we assume `\(E[Y_i(0)|X=x]\)` and `\(E[Y_i(1)|X=x]\)` are continuous in x, then we can estimate a (local) ATE at the threshold:

`$$\begin{align}
ATE_{RDD} &amp;= E[Y(1)-Y(0)|X_i=c] \\
&amp;=  E[Y(1)|X_i=c] -  E[Y(0)|X_i=c]\\
&amp;= \lim_{x \downarrow  c} E[Y_i|X=x] - \lim_{x \uparrow  c} E[Y_i|X=x] \\
\end{align}$$`

---
## Continuity Assumption

&lt;img src="https://mixtape.scunning.com/graphics/rdd_simul_ex.jpg" width="80%" style="display: block; margin: auto;" /&gt;

[Cunningham (2022)](https://mixtape.scunning.com/regression-discontinuity.html#continuity-assumption)

---
## Causal Identification with an RDD

- The continuity assumption is a formal way of saying that observations close to the threshold are good counterfactuals for each other

- We can't prove this assumption

- But if it holds, we should observe
 
  - no sorting around the cutoff (no self selection)
 
  - similar distributions of covariates around the cutoff (balance tests)
  
  - no effect of treatment on things measured pre-treatment (placebo tests)



---
class: inverse, center, middle
# üí° 
## Instrumental Variables


---
## Instrumental Variables

Instrumental variables are an economists favorite tool for dealing with **omitted variable bias**

- We have some non random treatment whose effects we'd like to assess
- We're worried that these effects are **confounded** by some unobserved, omitted variable, that influences both the treatment and the outcome
- We find an **instrumental variable** that satisfies the following:
  - Randomization
  - Excludability
  - First-stage relatioship
  - Monotonicity
- Allowing us estimate a Local Average Treatment Effect (LATE) using the only the variation in our treatment is **exogenous** (uncorrelated with ommited variables)

---
class: center
## IV Assumption: Randomization


.left-column[
- No path from `\(U\)` to `\(Z\)`
]
.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption: Excludability


.left-column[
- No path from `\(Z\)` to `\(Y\)`
]

.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption:  First Stage

.left-column[
- Path from `\(Z\)` to `\(D\)`
]
.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption: Monotonicity


.left-column[

-  `\(D_i(Z=1)\geq D_i(Z=0)\)`
- "No Defiers"

]

.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)
]

---
class: center
## Compliance

With a binary treatment, `\(D\)` and binary instrument `\(Z\)` there are four types of compliance

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Type &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(D_i(Z=1)\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(D_i(Z=0)\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Always Takers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Never Takers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Compliers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Defiers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Assuming Monotonicity means there are "No Defiers"

---
## Estimating the Local Average Treatment Effect

If we believe our assumptions of:

- Randomization
- Excludability
- First-stage relationship
- Monotonicity

Then we can estimate Local Average Treatment Effect (LATE) sometimes called the Complier Average Treatment Effect CATE) 

---
## Estimating the Local Average Treatment Effect

It can be [shown](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf) that the LATE:

`$$LATE = \frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1]-E[D|Z=0]}= \frac{ATE_{Z\to Y}}{ATE_{Z\to D}}$$`


---

## Example: Earnings and Military Service

Adapted from [Edward Rubin](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#42)

*Example:* If we want to estimate the effect of veteran status on earnings,
`$$\begin{align}
  \text{Earnings}_i = \beta_0 + \beta_1 \text{Veteran}_i + u_i \tag{1}
\end{align}$$`

--

We would love to calculate `\(\color{#e64173}{\text{Earnings}_{1i}} - \color{#6A5ACD}{\text{Earnings}_{0i}}\)`, but we can't.

--

And OLS will likely be biased for `\((1)\)` due to selection/omitted-variable bias.

---

## Introductory example

Imagine that we can split veteran status into an exogenous (as-if random, unbiased) part and an endogenous (non-random, biased) part...

--

`$$\begin{align}
  \text{Earnings}_i
  &amp;= \beta_0 + \beta_1 \text{Veteran}_i + u_i \tag{1} \\
  &amp;= \beta_0 + \beta_1 \left(\text{Veteran}_i^{\text{Exog.}} + \text{Veteran}_i^{\text{Endog.}}\right) + u_i \\
  &amp;= \beta_0 + \beta_1 \text{Veteran}_i^{\text{Exog.}} + \underbrace{\beta_1 \text{Veteran}_i^{\text{Endog.}} + u_i}_{w_i} \\
  &amp;= \beta_0 + \beta_1 \text{Veteran}_i^{\text{Exog.}} + w_i
\end{align}$$`

--

We could use this exogenous variation in veteran status to consistently estimate `\(\beta_1\)`.

--

**Q:** What would exogenous variation in veteran status mean?

---

## Introductory example

**Q:** What would exogenous variation in veteran status mean?

--

**A.sub[1]:** Choices to enlist in the military that are essentially random‚Äîor at least uncorrelated with omitted variables and the disturbance.

--

**A.sub[2]:** .No selection bias:
`$$\begin{align}
  \color{#e64173}{\mathop{E}\left(\text{Earnings}_{0i}\mid\text{Veteran}_i = 1\right)} - \color{#6A5ACD}{\mathop{E}\left( \text{Earnings}_{0i} \mid \text{Veteran}_i = 0 \right)} = 0
\end{align}$$`

---
## Instruments

**Q:** How do we isolate this *exogenous variation* in our explanatory variable?
--
&lt;br&gt;**A:** Find an instrument (an instrumental variable).

--

**Q:** What's an instrument?
--
&lt;br&gt;**A:** An **instrument** is a variable that is

1. **correlated** with the **explanatory variable** of interest (*relevant*),
2. **uncorrelated** with the **error** term (*exogenous*).

---

## Instruments


So if we want an instrument `\(z_i\)` for endogenous veteran status in

`$$\begin{align}
  \text{Earnings}_i = \beta_0 + \beta_1 \text{Veteran}_i + u_i
\end{align}$$`

1. **Relevant:** `\(\mathop{\text{Cov}} \left( \text{Veteran}_i,\, z_i \right) \neq 0\)`
2. **Exogenous:** `\(\mathop{\text{Cov}} \left( z_i,\, u_i \right) = 0\)`

---
## Instruments: Relevance

**Relevance:** We need the instrument to cause a change in (correlate with) our endogenous explanatory variable.

We can actually test this requirement using regression and a *t* test.

--

***Example:*** For the veteran status, consider three potential instruments:

.pull-left[
1. Social security number

2. Physical fitness

3. Vietnam War draft
]

--

.pull-right[ 
- **Probably not relevant** uncorrelated with military service

- *Potentially relevant* service may correlate with fitness

- **Relevant** being drafted led to service
]

---


## Instruments: Exogeneity

.hi[Exogeneity:] The instrument to be independent of omitted factors that affect our outcome variable‚Äîas good as randomly assigned.

`\(z_i\)` must be uncorrelated with our disturbance `\(u_i\)`. .hi[Not testable.]

--

***Example:*** For the .pink[veteran status], consider three potential instruments:

.pull-left[
1. Social security number

2. Physical fitness

3. Vietnam War draft
]

--

.pull-right[ 
- **Exogenous** SSN essentially random

- *Not Exogenous* fitness correlated with many things

- **Exogenous** draft via lottery
]

---
## Relevant and Exogenous


&lt;img src="04-slides_files/figure-html/venn_iv-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Relevant, Not Exogenous

&lt;img src="04-slides_files/figure-html/venn_iv_endog-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Not Relevant and Not Exogenous


&lt;img src="04-slides_files/figure-html/venn_iv_irrelevant-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Relevant, Not Exogenous
&lt;img src="04-slides_files/figure-html/venn_iv_endog2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
# Venn diagram explanation

In these figures (Venn diagrams)

- Each circle illustrates a variable.
- Overlap gives the share of correlatation between two variables.
- Dotted borders denote *omitted* variables.

Take-aways

- Figure 1: .hi-pink[Valid instrument] (relevant; exogenous)
- Figure 2: .hi-slate[Invalid instrument] (relevant; not exogenous)
- Figure 3: .hi-slate[Invalid instrument] (not relevant; not exogenous)
- Figure 4: .hi-slate[Invalid instrument] (relevant; not exogenous)

---
## IV Applications

&lt;img src="https://pbs.twimg.com/media/EJGyHnyUYAA-yhM?format=jpg&amp;name=large" width="80%" style="display: block; margin: auto;" /&gt;

[@AndrewHeiss](https://twitter.com/andrewheiss/status/1193931226865901569?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1193931226865901569%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2Fandrewheiss2Fstatus2F1193931226865901569widget%3DTweet)



---
## IV Summary

Instrumental variables require a number of assumptions to yield credible causal claims:

- Randomization
- Excludability
- First-stage relationship
- Monotonicity

Estimation and inference of IVs is beyond the scope of this course. 

  - See Edward Rubin's excellent [slides](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#85)
  - And Matt Blackwells [notes](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf)

- Understanding the identifying assumptions of IV can help you critique a study (even if the you don't fully understand the math)

---
class: inverse, middle, center
# üí° 
## Summary

---
## What you need to know

- Causal inference in observational and experimental studies is about counterfactual comparisons
- In observational studies, to make causal claims we generally make some  assumption of conditional independence:

$$
Y_i(1),Y_i(0), \perp D_i |X_i
$$

- The credibility of this assumption depends less on the data, and more on how the data were generated.
- **Selection on Observables** is rarely a credible assumption
- Observational designs that produce credible causal inference, leverage aspects of the world that create *natural experiments*
- You should be able to describe the logic and assumptions of common designs in social science 
  - **Difference-in-Differences:** *Parallel Trends* 
  - **Regression Discontiniuity:** *Continuity at the cutoff*
  - **Instrumental Variables:** Instruments need to be *Relevant and Exogenous*




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-lakeside-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
