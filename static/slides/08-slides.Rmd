---
title: "Week 08:"
subtitle: "Probably too much Probability Theory"
author: "Paul Testa"
output:
  xaringan::moon_reader:
    css: ["default", "css/brown.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(blogdown.knit.serve_site = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300,
  fig.align = "center", out.width = "80%", cache = T
  )
library("tidyverse")
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```



```{r packages, include=F}
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg", "htmltools",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Graphics:
  "scatterplot3d", #<<
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "zoo", 
  "gtools" #<<
)
```

```{r ipak, include=F}
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

```


```{r loadpackages, cache=F, include=F}
ipak(the_packages)
```


class: inverse, center, middle
# Overview

---
## General Plan

- Setup
- Feedback
- Review
- Lecture
  - Probability Theory
  - Conditional Probability 
  - Random variables and probability distributions
  - Expected values and variances


---
class:inverse, middle, center
# ðŸ’ª
## Get set up to work

---
## New packages

And tools for doing Permutations and Combinations

```{r, eval = F}
install.packages("gtools")
```



---
## Packages for today


```{r, ref.label=c("packages")}

```

---
## Define a function to load (and if needed install) packages


```{r, ref.label="ipak"}
```

---
## Load packages for today

```{r ref.label="loadpackages"}
```


---
class:inverse, center, middle
# ðŸ’ª
## Load Data for today

---
class:inverse, center, middle
# ðŸ“¢
## Feedback

---
## Feedback

Only 8 responses...

---
## What we'll cover today


- Probability

  - Probability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur

  - Probability follows three simple rules (from which numerous theorems follow)

- Conditional Probability

  - Conditional probability allow us to describe how our beliefs about one event change after observing another event(s)

  - We update beliefs using Bayes Rule

- Random Variables and Probability Distributions

  - Random variables *map* events in the world onto numbers

  - Probability distributions describe the likelihood that random variables take certain values

  - The expected value of a random variable is a *probability weighted average* that tells us the most likely value a variable will take


 

---
class: inverse, center, middle
# ðŸ’¡
# Probability


---
## Probability

- Probability describes the likelihood of an event happening.

- Statistics uses probability to quantify uncertainty about estimates and hypotheses.

-  To do this, we will need to understand:
  
  - Definitions (**experiment**, **sample space**, **events**,)
  
  - Three *rules* of probability (**Kolmogorov axioms**)

  - Two interpretations interpreting probabilities (**Frequentist** and **Bayesian**)
    
    - Some rules for counting
  

---
## Experiments, sample spaces, sets, and events 

- In probability thoery, an **experiment** describes a repeatable process where the outcome is uncertain

  - Processes where the outcomes are uncertaint are called *non-deterministic* or *stochastisc*

- The **sample space** of an experiment is the **set** $(\Omega$ "omega", or $S$) of all the possible outcomes of an experiment 

- Sets can be:
  - empty $( A: \{\emptyset\}$
  - a single event $( Coin: \{\text{Heads}\}$
  - multiple events $( Odd\, \#s: \{\text{1,3,5}\}$
  - infinite $(\mathbb{R}: \text{ The set of real numbers}\{ -\infty \dots +\infty\}$)

- An **event**, $(E$ or $A)$  is a **subset** of outcomes in the sample space
  - The sample space for a coin flip is $\Omega = \{\text{Heads, Tails}\}$
  - The event Heads is a subset of $\Omega$

---
bacground-image:url("https://www.playmonster.com/wp-content/uploads/2019/09/1000_set_pkgcontents-1.png")
background-size:contain


---
## Operations on Sets

- Empty Set: $\emptyset$ a set with no elements
- Subset: 
    - Let $D$ be the set outcomes for a 6-side die: $D=\{1,2,3,4,5,6\}$
    - $Primes=\{2,3,5\}$
    - $Primes \subset D \iff \forall X \in Primes, X \in D$
- Unions
    - $A \cup B = \{X:X \in A \lor X \in B \}$
    - Either $A$, $B$ or both $A and B$ occur
- Intersections
    - $A \cap B = \{X:X \in A \land X \in B \}$
    - Both $A$ and $B$ occur
- Complements
    - $A'=A^\complement = \{X:X\notin A\}$
    - $A'=A^\complement$ means $A$ does not occur
    - $\emptyset^\complement=S$ and $S^\complement=\emptyset$ 
  

---
class: bottom,right
background-image:url("https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png")
background-size:contain

[Source](https://www.onlinemathlearning.com/union-set.html)

---
## Three Rules of Probability

- Probability is defined by three *rules* or assumptions called the **[Kolmogorov Axioms](https://win-vector.com/2020/09/19/kolmogorovs-axioms-of-probability-even-smarter-than-you-have-been-told/)**


1. The probability of any event $A$ is nonnegative 

$$Pr(A) \geq 0 $$

2. The probability that one of the outcomes in the same space occurs is 1


$$Pr(\Omega) = 1 $$

3. If events $A$ and $B$ are mutually exclusive, then:

$$Pr(A \text{ or } B) = Pr(A) + Pr(B)$$
---
## The Addition Rule

For any given events, $A$ and $B$, the **addition** rule says we can find the probability of either $A$ or $B$ occurring:

$$Pr(A \cup B) = Pr(A \text{ or } B) = Pr(A) + Pr(B) - \underbrace{Pr(A \text{ and } B)}_{\text{aka } Pr(A \cap B)}$$
In words: The probability of either A or B occurring is the probability that A occurs plus the probability that B occurs - minus the probability that both occur (so that we're not double counting...)

---
class: bottom,right
background-image:url("https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png")
background-size:contain

[Source](https://www.onlinemathlearning.com/union-set.html)


---
## The Law of Total Probability (Part 1)

For any event two events, $A$ and $B$, the probability of $A$ $(Pr(A)$ can be **decomposed** into the sum of the probabilities of two **mutually exclusive** events:

$$Pr(A) = Pr(A \text{ and } B) + Pr(A \text{ and } B^{\complement})$$

---
class: bottom,right
background-image:url("https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png")
background-size:contain

[Source](https://www.onlinemathlearning.com/union-set.html)


---
## Two interpretations of probablity

- Probabilities are defined by these three axioms

- The are two broad ways of interpreting what probabilities mean:

  - Frequentist
  
  - Bayesian


---
## Frequentist interpretations of probability

- Probability describes how likely it is that some event happens.

  - Flip a fair coin, the probability of heads is Pr(Heads) = 0.5

--

- **Frequentist:** view this probability is the limit of the relative frequency of an event over repeated trials.

$$Pr(E) = \lim_{n \to \infty} \frac{n_{E}}{n} \approx \frac{ \text{# of Times E happened}}{\text{Total # of Trials}}$$ 

- Thinking about probability as a relative frequency, requires us to know how to the number of times an event occurred. 


---
background-image:url("https://www.kindpng.com/picc/m/99-991302_transparent-sesame-street-count-clipart-count-sesame-street.png")
background-size:contain

---
## How many elements in a set?

A set can be:

- **Countably Finite:** 
  
  - Roll a die, there six possible outcomes $\{1,2,3,4,5,6\}$

--

- **Countably Infinite:** 

  - Number of rolls until a six appears ${1,2,3,\dots}$

--

- **Uncountably Infinite:** 

  - All the real numbers between 0.25 and 0.75 


---
## The Fundamental Counting Principle 


The Fundamental Counting Principle says that if there $x$ ways to do one thing and $z$ ways to do another then, then are their are $x \times z$ total ways of doing both tasks.

More generally, if there are 

- If there are $j$ tasks or decision stages

- And $k$ choices at each decision stage $j$ such that $n_{j,k} = k$

- The total number of possible outcomes is the product of the number number of choices $k$ and each stage, $j$ $\prod_{i=1}^{j} n_k = n_{i,k}*n_{2,k}*\dots*n_{j,k}$ 


---
## How many different outfits could you have made me wear?

- **Palette**: $\{\text{Fall, Winter, Spring, Summer}\}$
- **Jacket**: $\{\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\}$
- **Top**: $\{\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\}$
- **Pant**: $\{\text{Slacks, Khakis, Jeans, Shorts}\}$
- **Shoe**: $\{\text{Dress Shoe, Dress boot, Jordans, Dunks, Basketball, Crocs}\}$
- **Tie**: $\{\text{Repp, Pattern, Knit, Bow, No tie}\}$
- **Pattern**: $\{\text{Simple, Stripes, Checks, Graphic}\}$

- How many decision stages?

- How many choices at each stage?

- How many possible outfits?

---
## How many different suggested outfits could you have made me wear?

- How many decision stages? $j=7$

- How many choices at each stage? 

  - **Palette**: $\{\text{Fall, Winter, Spring, Summer}\} = n_{1,4} = 4$ 
  - **Jacket**: $\{\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\} = n_{2,5} = 5$
  - **Top**: $\{\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\} = n_{3,4} = 4$
  - **Pant**: $\{\text{Slacks, Khakis, Jeans, Shorts}\} = n_{4,4} = 4$
  - **Shoe**: $\{\text{Dress Shoe, Boots, Jordans, Dunks, Basketball, Crocs}\} = n_{5,6} = 6$
  - **Tie**: $\{\text{Repp, Pattern, Knit, Bowtie, No tie}\} = n_{6,7} = 5$
  - **Pattern**: $\{\text{Simple, Stripes, Checks, Graphic}\} = n_{7,4} = 4$

- How many possible suggestions? $\prod_{i=1}^{j} n_{jk} = 4 \times 5 \times 4 \times 4 \times 6 \times 5 \times 4 = 38,400 \text{ outfits}$

---
#### And yet you chose this:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I send students in my undergraduate research class a weekly survey to get a sense of what&#39;s working and what&#39;s not. <br><br>I also ask them silly questions just for fun. Last week, I asked them to help me with my fit. This was the result: <a href="https://t.co/w1t8Cr04Ap">pic.twitter.com/w1t8Cr04Ap</a></p>&mdash; Paul Testa (@ProfPaulTesta) <a href="https://twitter.com/ProfPaulTesta/status/1501291241404833795?ref_src=twsrc%5Etfw">March 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---
## Perumations and Combinations

When we're counting how many "events" we can construct from a set of elements, our answer depends on whether we:

- Can distinguish the **order** of elements

- Allow for elements to be **repeated**

Situations where we can distinguish the order of elements being selected are described by **Permutations**

Situations where we cannot distinguish the order of elements being selected are described by **Combinations**

In both cases, the total number of permutations or combinations, varies, depending on whether we allow repeats or not.


---
## Factorials

The symbol `!` is the factorial function. `5!` is read as "five factorial" and is a shorthand way of writing:

$$5! = 5\times 4\times 3\times 2 \times 1$$

Combined with the fundamental counting principle, factorials let us mathematically represent situations where 

  - order does or does not matter
  
  - repeats are or are not allowed.
  
---
## Perumations: With regard to order and with repeats

A **Permutation** is a selection of objects in which the **order matters.**

When order matters and repeats allowed there are:

- $N$ ways to make our first choice, $N$ ways to make our second choice, ... $N$ ways to make our $k^{th}$ and $N^k$ total permutations of choices
  
- There are *27* ways $(n^k=3^3)$ to arrange the letters "a", "b", "c" using each letter as many times as we want ("with replacement/repeats allowed)


---
## Perumations: With regard to order and without repeats

When order matters and repeats are not allowed:

- There are $N$ ways to make our first choice, $N-1$ ways to make our second choice, ... and $N-k$ ways to make our $k^{th}$ choice. 

The formula for a permutation without repeats then is:

$${}_{n}P_k = n\times (n-1) \dots (n-k) = \frac{n!}{(n-k)!}$$

Since dividing $n!$ by $(n-k)!$, leaves us with $n\times (n-1) \dots (n-k)$

$${}_{5}P_3  = \frac{5\times 4 \times 3 \times 2 \times 1}{ 2 \times 1}= 5\times 4 \times 3 = 60$$
- There are *six* ways to arrange the letters "a", "b", "c" using each letter only once (without replacement/no repeats)

$${}_{3}P_3  = \frac{3 \times 2 \times 1}{1}= 3\times 2 \times 1 = 6$$

---
## Combinations: without regard to order and without repeats

A **Combination** is a selection of objects **without regard to order**

Combinations without repeats are described using  $\binom{n}{k}$ (a "binomial coefficient), and is a shorthand way of writing:

$${}_{n}C_k = \binom{n}{k} = \frac{{}_{n}P_k}{k!} =  \frac{n!}{k!(n-k)!}$$


- We divide by $k!$ because given $k$ sampled elements, there are $k!$ ways of arranging them, but when order does not matter, we want to treat these arrangements as the same (e,g, $AB$ and $BA$ are both instances of getting an $A$ and $B$)


- There is *1* ways to choose a group of three letters from the letters "a", "b", "c" using each letter only once (without replacement/no repeats)
  
$${}_{3}C_3 = \binom{3}{3} = \frac{{}_{3}P_3}{3!} =  \frac{3!}{3!(0)!} = \frac{3*2*1}{3*2*1*1}=1$$


- There are *10* ways to arrange the letters "a", "b", "c" using each letter only once (with replacement/repeats allowed)

---
## Combinations: without regard to order and with repeats

Finally, we a combination where repeats are allowed is described by a [multiset coefficient](https://en.wikipedia.org/wiki/Multiset)

$${}_{n}C_k = \left(\binom{n + k - 1}{k}\right) =\frac{(n + k -1)!}{k!(n-1)!}$$

- There are *10* ways to arrange the letters "a", "b", "c" using each letter only once (with replacement/repeats allowed)


$${}_{3}C_3 = \left(\binom{3+3-1}{3}\right) = \frac{(3+3-1)!}{3!(3-1)!} =  \frac{5!}{3!(2)!} = \frac{5*4*3*2*1}{3*2*1*2*1}=\frac{5*4}{2*1}=10$$
- I find this one less intuitive too explain, and less commonly used. The classic example is something like picking donuts or filling up a sampler six pack.

---

```{r counttab, echo=F,results="asis", tidy=TRUE}

counting_tab <- tibble::tibble(
   ` `=  c("Permutation", "Perumation","Combination", "Combination"),
  `Order Matters` = c("Yes","Yes","No","No"),
  `Repetition Allowed` = c("Yes","No","No","Yes"),
  Formula = c('&nbsp; ${}_{n}P_k =  n^k$ ', 
              "&nbsp; ${}_{n}P_k = \\frac{n!}{(n-k)!}$",
              "&nbsp; ${}_{n}C_k = \\binom{n}{k} = \\frac{{}_{n}P_k}{k!} \\frac{n!}{k!(n-k)!}$",
              "&nbsp; ${}_{n}C_k = \\binom{n + k - 1}{k} =\\frac{(n + k -1)!}{k!(n-1)!}$"
              ),
  "Example = {abc}" = c('P(3,3) = Pick three letters in order with replacement = $3^3 = 27$ permutations',
              'P(3,3) = Pick three letters in order without replacement = $\\frac{3!}{(3-3)!} = \\frac{3*2*1}{1} = 6$ permutations',
              'C(3,3) = Choose three letters without replacement = $\\frac{3!}{3!(3-3)!} = \\frac{3*2*1}{3*2*1} = 1$ combination',
              'C(3,3) = Choose three letters with replacement = $\\frac{3+3 -1!}{3!(3-1)!} = \\frac{5*4*3*2*1}{(3*2*1)(2*1)} = 10$ combinations'
              )
  )


kable(counting_tab, format = "markdown")%>%
  kable_styling()
```





---
## Permutations in R

.pull-left[
${}_{3}P_2$ order matters, repeats allowed
```{r}
gtools::permutations(3, 2, v = c("a","b","c"),repeats.allowed = T)
```
]

.pull-right[
${}_{3}P_2$ order matters, no repeats 
```{r}
gtools::permutations(3, 2, v = c("a","b","c"),repeats.allowed = F)
```
]

---
## Combinations in R

.pull-left[
${}_{3}C_2$ no order, no repeats
```{r}
gtools::combinations(4, 2, v = c("a","b","c","d"),repeats.allowed = F)
```
]

.pull-right[
${}_{3}C_2$ no order, repeats allowed
```{r}
gtools::combinations(4, 2, v = c("a","b","c","d"),repeats.allowed = T)
```
]

---
## Permutations and Combinations

Counting turns out to be vary useful for a number of questions/problems in social science. 

For example, in a randomized experiment, how we randomize treatment assignments can have big implications for how we conduct statistical inference.

  - In short, we will "analyze as we randomize"
  - Compare what we observed from one possible way of assigning treatment, to what we could have observed under different assignments, if some claim (hypothesis) were true
  - Simple random assignment (equal probability, flipping a coin) yields a lot of possible assignments
  - Complete random assignment, fixes the \# treated (e.g. N/2), ruling cases where everyone or no-one gets the treatment
  - Similarly, paired random assignment randomizes within pairs.

```{r, echo=F}
exptab <- tibble(
  Randomization = c("Coin Flip", "Completely Randomized","Pair-Randomized"),
  `Total # of Treatment Assignments` = c("&nbsp; $N^2$","&nbsp; $\\binom{N}{N/2}$", "&nbsp; $2^{N/2}$"),
  `N=4` = c("16","6","4"),
  `N=8` = c("256","70","16"),
  `N=16` = c("65,536","12870","256"),
)

kable(exptab, format="markdown")%>%
  kable_styling()
```



---
## Frequentist interpretations of probability

- Probabilities from a Frequentist perspective are defined by *fixed* and *unknown*  **parameters**

- The goal of statistics for a frequentist is to learn about these parameters from data.

- Frequentist statistics often ask questions like "What is the probability of observing some data $Y$, given a hypothesis about the true value of  parameter(s), $\theta$, that generated it. 

---
## Frequentist interpretations of probability

For example, suppose we wanted to test whether a coin is "fair"  $(p = Pr(Heads) = .5; q = Pr(Tails) = 1-p = .5).$ We could:

- Flip a fair coin 10 times. Our estimate of the  $Pr(H)$ is the number of heads divided by 10. It could be 0.5, but also 0 or 1, or some number in between.
  
- Flip a coin 100 times and our estimate will be closer to the true $paramter$. 
  
- Flip a coin an $\infty$ amount of times and the relative frequency will converge to the true parameter/ $(Pr(H) = \lim_{n \to \infty} \frac{n_{H}}{n} = p = 0.5 \text{ for a fair coin})$ 




---
## Bayesian interpretations of probability

- Frequentist interpretations make sense for describing processes that we could easily repeat (e.g. Coin flips, Surveys, Experiments) 

- But feel more convoluted when trying to describe events like "the probability of that Biden wins reelection."

- Bayesian interpretations of probability view probabilities as subjective beliefs. 

- The task for a Bayesian statistics is to update these *prior* beliefs () based on a model of the *likelihood* of observing some data to form new beliefs after observing the data (called *posterior beliefs*).

- Bayesians do this using *Bayes Rule*, which says:

$$\text{posterior} \propto \text{likelihood} \times \text{prior}$$
More formally:

$$\underbrace{Pr(\theta|Y)}_{\text{Posterior}} \propto \underbrace{Pr(Y|\theta)}_{\text{Likelihood}}) \times \underbrace{Pr(\theta)}_{\text{Prior}} $$
---
background-image:url("https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png")
background-size:contain


---
## Bayesian vs Frequentists


- Our two main tools for doing statistical inference in this course

  - Hypothesis Testing
  - Interval Estimation

- Follow largely from frequentist interpretations of probability

- The differences between Bayesian and Frequentist frameworks, are both philosophical and technical in nature

  - Is probability a relative frequency or subjective belief? How do we form and use prior beliefs
  - Bayesian statistics relies heavily on algorhithms for [Markov Chain Monte-Carlo](http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf) simulations made possible by advances in computing.

- For most of the questions in this course, these two frameworks will yield similar (even identical) conclusions.
  
  - Sometimes it's helpful to think like a Bayesian, others, like a frequentist
  - All models are wrong, some models are useful.



---
class: inverse, center, middle
# ðŸ’¡
# Conditional Probability

---
## Conditional Probability: Definition

The conditional probability that event **A** occurred, given that event **B** occurred is written as $Pr(A|B)$ (*"The probability of A given B"*) and defined as:

$$Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)} = \frac{\text{Probability of Both A and B}}{\text{Probability of B}}$$

- $Pr(A \cap B)$ is the same as $Pr(A \text{ and } B)$  is the **joint probability** of both events occurring

- $Pr(B)$ is the **marginal probability** of B occuring

---
## Conditional Probability: Multiplication  Rule

Joint probabilities are **symmetrical**. $Pr(A \cap B) = Pr(B \cap A)$. 

By rearranging terms:

$$Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)}$$
We get the **multiplication rule**:

$$Pr(A \cap B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A)$$




---
## QSS Example (p. 256): Race and Gender in the FL Voter File 

`FLVoters` contains a random sample of 10,000 voters

```{r}
library(qss)
data("FLVoters")
dim(FLVoters)
FLVoters <- na.omit(FLVoters)
dim(FLVoters)
head(FLVoters)

```

---
## Marginal Probabilities: Gender

```{r}
# Pr(Gender)
table(FLVoters$gender)
margin_gender <- prop.table(table(FLVoters$gender))
margin_gender
```

---
## Marginal Probabilities: Race

```{r}
# Pr(Race)
table(FLVoters$race)
margin_race <- prop.table(table(FLVoters$race))
margin_race
```


---
## Cross Tab of race and gender

```{r}
table(FLVoters$gender,FLVoters$race, useNA = "ifany")
```

---
## Joint probability of race and gender

```{r}
joint_p <- prop.table(table(FLVoters$gender,FLVoters$race))
joint_p
```

---
## Marginal probabilities from joint probabilities:

```{r}
prob_tab<-addmargins(table(FLVoters$gender,FLVoters$race))
prob_tab
```

---
## Marginal probabilities from joint probabilities: Gender
```{r}
# Marginal probability of Gender
prob_tab[,"Sum"]/prob_tab["Sum","Sum"]
prop.table(table(FLVoters$gender))
```

---
## Marginal probabilities from joint probabilities: Race
```{r}
# Marginal probability of Race
prob_tab["Sum",]/prob_tab["Sum","Sum"]
prop.table(table(FLVoters$race))

```

---
## Total Probability: Pr(Black)

$$Pr(Black)=Pr(Black \cap female) + Pr(Black \cap male)$$

```{r}
prop.table(table(FLVoters$gender,FLVoters$race))["f","black"] +
  prop.table(table(FLVoters$gender,FLVoters$race))["m","black"]

prop.table(table(FLVoters$race))["black"]

```

---
## Conditional Probability: Pr(Black|Female)

$$Pr(Black|Female) = \frac{Pr(Black \text{ and } Female)}{Pr(Female)} \approx \frac{0.074}{0.536} \approx 0.139$$



---
## Independence

Events $A$ and $B$ are independent if 

$$Pr(A|B) = Pr(A) \text{ and } Pr(B|A) = Pr(B)$$

Conceputally, If $A$ and $B$ are **independent** knowing whether $B$ occurred, tells us nothing about $A$, and so the conditional probability of $A$ given $B$, $Pr(A|B)$ is equal to the unconditional, or marginal probability, $Pr(A)$

Formally, two events are statistically independent if and only if the joint probability is equal to product of the marginal probabilities

$$Pr(A\text{ and }B) = Pr(A)Pr(B)$$
---
Race and gender are approximately independent in these data

.pull-left[
```{r, eval=F}
plot(x = c(margin_race*margin_gender["f"]),
     y = joint_p["f",],
     xlim = c(0,.4),
     ylim = c(0,.4),
     xlab = "Pr(race)*Pr(female)",
     ylab = "Pr(race and gender)"
             )
abline(0,1)
```
]
.pull-right[

```{r, echo=F}
plot(x = c(margin_race*margin_gender["f"]),
     y = joint_p["f",],
     xlim = c(0,.4),
     ylim = c(0,.4),
     xlab = "Pr(race)*Pr(female)",
     ylab = "Pr(race and gender)"
             )
abline(0,1)
```

]

---
## Conditional Independence

We can extend the concept of independence to situations with more than two events:

If events $A$, $B$, and $C$ are jointly independent then:

$$Pr(A \cap B \cap C) = Pr(A)Pr(B)Pr(C)$$

Joint independence implies pairwise independence and conditional independence:

$$Pr(A \cap B | C) = Pr(A|C)Pr(B|C)$$
But not the reverse.


---
## Bayes Rule

Bayes rule is theorem for how we should update our beliefs about $A$ given that $B$ occurred:

$$Pr(A|B) = \frac{Pr(B|A)Pr(A)}{Pr(B)} = \frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\complement)Pr(A^\complement)}$$
Where

- $Pr(A)$ is called the prior probability of A (our initial belief)

- $Pr(A|B)$ is called the posterior probability of A given B (our updated belief after observing B)

---
###  What's the probability you have Covid-19 given a positive test

$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$

|               | Has Covid      | Does Not Have Covid |
|---------------|----------------|---------------------|
| Test Positive | True Positive  | False Positive      |
| Test Negative | False Negative | True Negative       |


---
### What's the probability you have Covid-19 given a positive test

Let's assume:

- 1 out 1000 people has Covid
- Our test correctly identifies true positives 95 percent of the time (sensitivity = True Positive Rate)
- Our test correctly identifies true negatives 95 percent of the time (specifity = True Negative Rate)

---
### What's the probability you have Covid-19 given a positive test

In a sample of 100,000 people then:

$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$

|               | Has Covid      | Does Not Have Covid |
|---------------|----------------|---------------------|
| Test Positive | 950 | 4950      |
| Test Negative | 50 | 94050       |

- $Pr(+|Covid) = 950/(1000) \approx 0.95$  
- $Pr(Covid) = 1000/100000 \approx 0.01$  
- $Pr(+) = Pr(+|Covid) + Pr(+|Covid)= .95*.01 + .05*.99  \approx  0.059$  

---
### What's the probability you have Covid-19 given a positive test

Converting this table into marginal probabilities

$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$

|               | Has Covid      | Does Not Have Covid | Prob(Test)
|---------------|----------------|---------------------|---------------------|
| Test Positive | 0.0095            | 0.0495               |0.059
| Test Negative | 0.0005            | 0.9505               | 0.941 
| Prob(Covid)   | 0.01          | 0.99               | 1

$$Pr(Covid|Test +) = \frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$
$$Pr(Covid|Test +) = \frac{0.95 \times 0.01}{0.059} \approx 0.16$$
---
What if we took goot a second test? We could use posterior belief as our prior:


$$Pr(Covid|2nd +) = \frac{0.95 \times 0.16}{0.16\times0.95 + (1-0.16)\times 0.95 } \approx 0.783$$
Now we're much more confident that we have Covid-19


---
class: inverse, center, middle
# ðŸ’¡
# Random Variables and Probability Distributions


---
## Random Variables
- Random variables assign numeric values to each event in an experiment.

    - Mutually exclusive and exhaustive, together cover the entire sample space.

- Discrete random variables take on finite, or [countably infinite](http://mathworld.wolfram.com/CountablyInfinite.html) distinct values.

- Continuous variables can take on an uncountably infinite number of values.

---
## Example: Toss Two Coins

- $S={TT,TH,HT,HH}$
- Let $X$ be the number of heads
    - $X(TT)=0$
    - $X(TH)=1$
    - $X(HT)=1$
    - $X(HH)=2$

    
---
## Probability Distributions

- Broadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events. 

- The can be represented in terms of:

  - Probability Mass/Density Functions
    - Discrete variables have probability mass functions (PMF)
    - Continuous variables have probability density functions (PDF)

  - Cumulative Density Functions
    - Discrete: Summation of discrete probabilities
    - Continuous: Integration over a range of values

---
## Discrete distributions

- **Probability Mass Function (pmf):** $f(x)=p(X=x)$

- Assigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply 

- **Cumulative Distribution Function (cdf)** $F(x_j)=p(X\leq x)=\sum_{i=1}^{j}p(x_i)$

  - Sum of the probability mass for events less than or equal to $x_j$

---
## Example: Toss Two coins

- $S={TT,TH,HT,HH}$
- Let $X$ be the number of heads
    - $X(TT)=0$
    - $X(TH)=1$
    - $X(HT)=1$
    - $X(HH)=2$
- $f(X=0)=p(X=0)=1/4$
- $f(X=1)=p(X=1)=1/2$
- $F(X\leq 1) = p(X \leq 1)= 3/4$


---

```{r,echo=F, fig.height=4}
df <- data.frame(x=seq(0, 7), 
                 y=c(0,cumsum(rep(1,6)/6),1),
                 p=c(NA,rep(1,6)/6,NA))
df$xend <- c(df$x[2:nrow(df)], NA)
df$yend <- df$y

p.pmf <- ggplot(df, aes(x=x, y=p)) +
      geom_segment(aes(xend = x, yend = 0), size = 1)+geom_point()+
    ylim(0,1)+labs(title="PMF of Die")+xlim(0,7)
#p.pmf
p.cdf <- ggplot(df, aes(x=x, y=y, xend=xend, yend=yend)) +
      geom_vline(aes(xintercept=x), linetype=2, color="grey") +
      geom_point() +  # Solid points to left
      geom_point(aes(x=xend, y=y), shape=1) +  # Open points to right
      geom_segment()+  # Horizontal line segments
    ylim(0,1)+labs(title="CDF of Die")+xlim(0,7)
ggarrange(p.pmf,p.cdf)

```

Each side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X<=2) = 1/6 + 1/6 = 1/3


---
## Continuous distributions


- **Probability Density Functions (PDF):** $f(x)$
    - Assigns probabilities to events in the sample space such that Kolmogorov Axioms still apply 
    - But... since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.

- **Cumulative Distribution Function (CDF)** $F(x)=p(X\leq x)=\int_{-\infty}^{x}f(x)dx$
    - Instead of summing up to a specific value (discrete) we integrate over all possible values up to $x$
    - Probability of having a value less than x


---
## Integrals

First, a brief aside  on integral calculus:

What's the area of the rectangle? $base\times height$

```{r,echo=F, fig.height=4}
df<-data.frame(x=c(0,1),y=c(1,1))

p.rect<-ggplot(data.frame(x1=0,x2=1,y0=0,y1=1),
               aes(xmin=x1,xmax=x2,ymin=y0,ymax=y1)
               )+geom_rect(col="black",fill="black",alpha=.2)+xlim(-.5,1.5)+ylim(0,1.5)
p.rect
```


---
## Integrals

How would we find the area under a curve?

```{r,echo=F, fig.height=4}
x<-rnorm(1000000)
plot(density(x),xlab="",ylab="",main="")
```

---
## Integrals

Well suppose we added up the areas of a bunch of rectangles roughly whose height's approximated the height of the curve?

```{r,echo=F, fig.height=4}
hist(x,freq=F,xlab="",ylab="",main="")
lines(density(x))
```

Can we do any better? 

---
## Integrals

Let's make the rectangles smaller

```{r,echo=F, fig.height=4}
hist(x,freq=F,xlab="",ylab="",main="",breaks=100)
lines(density(x))
```

What happens as the width of rectangles get even smaller, approaches 0? Our approximation get's even better:


---
## Link between PDF and CDF

If 
$$F(x)=p(X\leq x)=\int_{-\infty}^{x}f(x)dx $$

Then by the [fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus)

$$\frac{d}{dx}F(x)=f(x)$$

In words

- the PDF ($f(x)$) is the derivative (rate of change) of the CDF ($F(X)$)

- the CDF describes the area under the curve defined by f(x) up to x 


---
## Properties of the CDF

- $0\leq F(x) \leq 1$

- $F$ is non-decreasing and right continuous

- $\lim_{x\to-\infty}F(x)=0$

- $\lim_{x\to\infty}F(x)=1$

- For all $a,b \in \mathbb{R}$ s.t. $a<b$

$$p(a < X \leq b) = F(b)- F(a) = \int_a^b f(x)dx $$

---
## Recall the PMF and CDF of a die

```{r,echo=F}
ggarrange(p.pmf,p.cdf)
```

---
## What's the probability 

- $p(X=1)...p(X=6) =$\pause $1/6$
- $p( 2 < X \leq 5) =$\pause  $F(5)-F(2)=5/6-2/6=3/6=1/2$


---
# Common Probablity Distirbutions

In this course, we'll use probability distributions to 

- Model the data generating process as a function of parameters we can estimate

- To perform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)

---
There are a lot of probability distributions:
![](http://www.math.wm.edu/~leemis/chart/UDR/BaseImage.png)


---
Fortunately, the distributions you need to know to really master data science, is probably more something like
![](https://miro.medium.com/max/4854/1*szMCjXuMDfKu6L9T9c34wg.png)

And the distributions we'll work with the most in this class are an even smaller subset.

---
## Bernoulli Random Variables

Let's start with our old friend the coin flip


A coin flip is an example of a **Bernoulli random variable** defined by 1 parameter $p$, the probability of success. It has a pmf of

$$f(x) =
    \left\{
        \begin{array}{cc}
                p & \mathrm{if\ } x=1 \\
                1-p & \mathrm{if\ } x=0 \\
        \end{array} 
    \right.$$


And a CDF of 

$$F(x) =
    \left\{
        \begin{array}{cc}
                0 & \mathrm{if\ } x<1 \\
                1-p & \mathrm{if\ } 0\leq x<1 \\
                1& \mathrm{if\ } x\geq1 \\
        \end{array} 
    \right.$$

Note that in our coin flip example $p=0.5$ but it need not. Just imagine a weighted coin like the Patriots use at Foxborough

---
## Uniform Distribution

Our fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range $[a,b]$, the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of

$$f(x) =
    \left\{
        \begin{array}{cc}
                \frac{1}{b-a}& \mathrm{if\ } a \leq x\leq b \\
                0 & \text{otherwise} \\
        \end{array} 
    \right.$$

And a CDF:

$$F(x) =
    \left\{
        \begin{array}{cc}
                        0 & x <a \\
                \frac{x-a}{b-a}& \mathrm{if\ } a \leq x < b \\
                1 & x \geq b \\
        \end{array} 
    \right.$$

We won't run into uniform distributions all that often except in examples like rolling a fair sided die, but often they're used in Bayesian analysis as a form of uninformative prior.

---
## Binomial Distributions

The binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.

The key parameters are the number of trials $n$ and the probability of success for each trial $p$ and the pdf of a binomial distribution is:

$$f(x)=\binom{n}{x}p^x (1-p) ^{1-x} \ \text{for x 0,1,2},\dots n$$
So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads

---

```{r,echo=F}
p <- .2
df <- data.frame(x = 0:1000, px = dbinom(0:1000,size=1000, prob=p))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  labs(title="PMF of Binomial Distribution (N=20,P=.2)")
```

The binomial distribution will come in handy when trying to model binary outcomes. 

---
## Poisson Distributions

What would happen if you let the $n$ in a binomial distribution go to infinity and $p$ go to 0 so that $np$ stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter $\lambda$ which represents rate at which events occur.


$$f(x)=\frac{\lambda^x}{x!}e^{-\lambda}$$

---

```{r, echo=F}
# PDF of Poisson, lambda = 4
# Try changing lambda
lambda <- 20
df <- data.frame(x = 0:20, px = dpois(0:20, lambda))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  labs(title="PMF of Poisson Distribution (Lambda=4)")
```


---
## Geometric Distributions

What if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:

$$f(x)=p(1-p) ^{x}$$
---

```{r, echo=F}
df <- data.frame(x = 0:20, px = dgeom(0:20, .5))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  labs(title="PMF of Geometric Distribution, (p=.5)")

```

---
## Exponential Distributions

Taking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a $\lambda = \frac{1}{\beta}$ rate parameter

$$f(x)=\frac{1}{\beta}\exp\left[-x/\beta\right]$$

---

```{r, echo=F}
p.pdf.exp<-ggplot(data.frame(x = c(0, 5)), aes(x)) + stat_function(fun = dexp)+ylim(0,1)+labs(title="PDF of Exponential Distribution (Lambda=1)",y="")
p.cdf.exp<-ggplot(data.frame(x = c(0, 5)), aes(x)) + stat_function(fun = pexp)+ylim(0,1)+labs(title="CDF of Exponential Distribution (Lambda=1)",y="")
ggarrange(p.pdf.exp,p.cdf.exp)
```


We often use exponential distributions to model things like "time until failure" where failure might be another war or the ending of an Italian cabinet.

---
## Normal Distribution

Finally, there's the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter $\mu$ that determines the center of a distribution and a scale parameter $\sigma^2$ that determines the spread of a distribution

$$f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp \left[
-\frac{1}{2\sigma^2}(x-\mu)^2
\right]$$

---
Standard normal: $X \sim N(\mu =0,\sigma^2=1)$

```{r, echo=F}
p.pdf.norm<-ggplot(data.frame(x = c(-3, 3)), aes(x)) + 
  stat_function(fun = dnorm,)+ylim(0,1)+
  labs(title="PDF of Standard Normal Distribution",y="")
p.cdf.norm<-ggplot(data.frame(x = c(-3, 3)), aes(x)) + stat_function(fun = pnorm)+ylim(0,1)+labs(title="CDF of Standard Normal Distribution",y="")
ggarrange(p.pdf.norm,p.cdf.norm)
```


---
- As we'll see normal distributions tend to arise when ever you're summing variables. 

- That is sum together a bunch of values from almost any distribution and the **distribution of their sums** tends to follow a normal distribution. 

- Since lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)


---
Consider a binomial distribution with N=100 and p=.5. The pmf of this variable (black lollipops) follows a distribution that's closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.

A relationship explained more generally by the Central Limit Theorem, which we'll cover next week.

---
```{r, echo=F}
p <- .5
df <- data.frame(x = 0:100, px = dbinom(0:100,size=100, prob=p))
ggplot(df,aes(x=x,y=px))+
         geom_point()+
         geom_segment(aes(x=x,xend=x,y=0,yend=px))+
  stat_function(fun = dnorm,args = list(50,sqrt(100*.5*(1-.5))), col="red")+
  labs(title="PMF of Binomial Distribution (N=20,P=.5)")
```



---
### What's the p(X$\leq$ 0) for a normal distirbution with mean 0 and sd 1

Since the normal distribution is so common, it's useful to get practice working with it's pdf and cdf.

Consider the following question: If X is normally distributed variable with $\mu=0$ and $\sigma=1$, what's the probability that X is less than 0  $p(X\leq0)=?$ We could solve:

$$\int_{-\infty}^{0}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx=0.5$$



---
But R's `pnorm()` function will quickly tell us

- $p(X\leq0)=$ `r pnorm(0)`

And we can visualize this as follows:

---
```{r,echo=F}
normal <- function(mu=0, sigma=1, x){
1/(sigma*sqrt(2*pi))*exp(-(x-mu)^2/(2*sigma^2))
}
normal_shade <- function(mu=0, sigma=1, x,l=-3,r=0){
y <- normal(mu=mu, sigma=sigma, x)
y[x < l | x > r] <- NA
return(y)
}


p.pdf.norm.1<-p.pdf.norm+
      stat_function(data=data.frame(x=c(-2.99, 0)), fun=normal_shade, geom = 'area', fill = 'red', alpha = 0.2,
                args=list(mu=0,sigma=1,l=-3,r=0))

p.cdf.norm.1<-p.cdf.norm+geom_segment(aes(x=-3,xend=0,y=.5,yend=.5,col="red"))+scale_color_discrete(guide=F)
   
    
ggarrange(p.pdf.norm.1,p.cdf.norm.1)

```

---
Consider some other questions?

- $p(X=0)=0$
  - The probability that a continuous variable is exactly some value is always 0.
- $p(X<0)=0.5$
- $p(-1< X< 1)$
- $p(-2< X< 2)$

---
### p(-1 < X <  1)


- $p(-1< X< 1)=pr(X<1)-pr(X<-1)$ 

$$\int_{-1}^{1}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx=0.841-0.158=0.682$$


```{r,echo=F, fig.height=4}
p.pdf.norm.2<-p.pdf.norm+
      stat_function(data=data.frame(x=c(-2.99, 0)), fun=normal_shade, geom = 'area', fill = 'red', alpha = 0.2,
                args=list(mu=0,sigma=1,l=-1,r=1))

p.cdf.norm.2<-p.cdf.norm+
    geom_segment(aes(x=-3,xend=-1, y=pnorm(-1),yend=pnorm(-1),col="red"))+
    geom_segment(aes(x=-3,xend=1, y=pnorm(1),yend=pnorm(1),col="red"))+
    scale_color_discrete(guide=F)

ggarrange(p.pdf.norm.2,p.cdf.norm.2)

```

---
### p(-2 < X < 2)

- $p(-2< X\leq 2)=$ `r pnorm(2)-pnorm(-2)`

```{r,echo=F, fig.height=4}
p.pdf.norm.3<-p.pdf.norm+
      stat_function(data=data.frame(x=c(-2.99, 0)), fun=normal_shade, geom = 'area', fill = 'red', alpha = 0.2,
                args=list(mu=0,sigma=1,l=-2,r=2))

p.cdf.norm.3<-p.cdf.norm+
    geom_segment(aes(x=-3,xend=-2, y=pnorm(-2),yend=pnorm(-2),col="red"))+
    geom_segment(aes(x=-3,xend=2, y=pnorm(2),yend=pnorm(2),col="red"))+
    scale_color_discrete(guide=F)

ggarrange(p.pdf.norm.3,p.cdf.norm.3)

```

We'll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0.



---
## Expected Value

A (probability) weighted average of the possible outcomes of a random variable, often labeled $\mu$ 

Discrete:

$$\mu_X=E(X)=\sum xp(x)$$

Continuous

$$\mu_X=E(X)=\int_{-\infty}^{\infty}xf(x) dx$$

---
## What's the expected value of a 1 roll of fair die?

$$\begin{align*}
E(X)&=\sum_{i=1}^{6}x_ip(x_i)\\
     &=1/6\times(1+2+3+4+5+6)\\
     &= 21/6\\
     &=3.5
\end{align*}$$

---
## Properties of Expected Values

- $E(c)=c$

- $E(a+bX)=a+bE[X]$

- $E[E[X]]=X$

- $E[E[Y|X]]=E[Y]$

- $E[g(X)]=\int_{-\infty}^\infty g(x)f(x)dx$

- $E[g(X_1)+\dots+g(X_n)]=E[g(X_1)]+\dots E[g(X_n)$

- $E[XY]=E[X]E[Y]$ if $X$ and $Y$ are independent

---
## How many times would you have to roll a fair die to get all six sides?

We can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success. The expected value of a geometric variable is:

$$\begin{align*}E(X)&=\sum_{k=1}^{\infty}kp(1-p)^{k-1} \\
&=p\sum_{k=1}^{\infty}k(1-p)^{k-1} \\
&=p\left(-\frac{d}{dp}\sum_{k=1}^{\infty}(1-p)^k\right) \text{(Chain rule)} \\
&=p\left(-\frac{d}{dp}\frac{1-p}{p}\right) \text{(Geometric Series)} \\
&=p\left(\frac{d}{dp}\left(1-\frac{1}{p}\right)\right)=p\left(\frac{1}{p^2}\right)=\frac1p\end{align*}$$

---
For this question, we need to calculate the probability of success, p, after getting a side we need.

The probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:

```{r}
ev <- c()
for(i in 6:1){
  ev[i] <- 6/i
  
}
# Expected rolls for each 1 through 6th side
rev(ev)
# Total 
sum(ev)
```

---
## Variance

If $X$ has a finite mean $E[X]=\mu$, the $E[(X-\mu)^2]$ is finite and called the variance of $X$ which we write as $\sigma^2$ or $Var[X]$.

Note:

$$\begin{align*}
\sigma^2=E[(X-\mu)^2]&=E[(X^2-2\mu X+\mu^2)]\\
&= E[X^2]-2\mu E[X]+\mu^2\\
&= E[X^2]-2\mu^2+\mu^2\\
&= E[X^2]-\mu^2\\
&= E[X^2]-E[X]^2
\end{align*}$$

- "The variance of X is equal to the expected value of X-squared, minus the square of X's expected value."
- $\sigma^2=E[X^2]-E[X]^2$ is a useful identity in proofs and derivations

---
## Variance and Standard Deviations

We often think of variances $Var[X]$ as describing the spread of a distribution

$$\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2$$

A standard deviation is just the square root of the variance

$$\sigma=\sqrt{Var[X]}$$

---
## Covariance

Covariance measures the degree to which two random variables vary together. 

- $Cov[X,Y] \to +$ An increase in $X$ tends to be larger than its mean when $Y$ is larger than its mean

$$Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]$$

---
## Properties of Variance and Covariance

- $Cov[X,Y]=E[XY]-E[X]E[Y]$

- $Var[X]=E[X^2]-(E[X])^2$

- $Var[X|Y]=E[X^2|Y]-(E[X|Y])^2$

- $Cov[X,Y]=Cov[X,E[Y|X]]$

- $Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]$

- $Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]$


---
## Correlation

- The correlation between $X$ and $Y$ is simply the covariance of $X$ and $Y$ divided by the standard deviation of each.

$$\rho=\frac{Cov[X,Y]}{\sigma_X\sigma_Y}$$

- Normalize covariance to a scale that runs between [-1,1]

---
## Question: If two variables have zero covariance, are they independent?


No

A trivial example

- $p(X=x)=1/3$ for $x = -1,0,1$ and let $Y=X^2$
- Y clearly depends on X even though their covariance is 0

```{r,results="markdown"}
p=1/3
x=-1:1
y=x^2
cor(x,y)
```







