---
title: "Week 06:"
subtitle: "Linear Regression with Multiple Predictors"
author: "Paul Testa"
output:
  xaringan::moon_reader:
    css: ["default", "css/brown.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
---

class: inverse, center, middle
background-image:url("https://static.onecms.io/wp-content/uploads/sites/20/2020/04/09/abby-huntsman-1.jpg")
background-size:cover

# Overview

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      cache = T,
  comment = NA, dpi = 300,
  fig.align = "center", out.width = "80%")
library("tidyverse")
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


```{r packages, include=F}
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Graphics:
  "scatterplot3d", #<<
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)
```

```{r ipak, include=F}
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

```


```{r loadpackages, cache=F, include=F}
ipak(the_packages)
```



---
## General Plan

- Group Assignment 2: Data
- Setup
  - Packages
  - Data
- Feedback
- Review
  - Causal Inference in Observational Designs
  - Simple Linear Regression
- Multiple Regression 
  - Overview
  - Estimating and Interpreting Multiple Regression
  - Reading Regression Tables

---
class: inverse, center, middle
# Research Questions

---
## Assignment 1 Research Questions:

- Feedback posted to Canvas.

- General Comments:

  - Really great questions!
  
  - Feedback tries to balance interest/feasibility/data availability
  
  - "Not an experiment" $\to$ What would you have to randomize to answer your question
  
  - Not every question need to be causal
  
  - "Make your theories elaborate" - R.A. Fisher to [William Cochran](https://www.jstor.org/stable/2344179)
    - But keep your models simple - Testa to POLS 1600


---
## Assignment 2: Data Explorations


- Prompt posted [here](https://pols1600.paultesta.org/files/assignments/A1_research_questions.docx)

- Upload to [Canvas](https://canvas.brown.edu/courses/1087979/assignments/7870539) this Sunday

1. A revised description of your group's research project
2. A description of a linear model implied by your question
3. R code that loads some potentially relevant data to your question and at least one descriptive summary of that data.
4. Some information about your group such as:
    - A group name^[If you're Group 01 don't change your name to Group 4]
    - A group color or color scheme
    - A group motto, mascot, crest, etc.
    - Your group's theme song
    - Your group's astrological sign
    - Anything else that you think well help you form strong ingroup bounds that facilitate collaboration


---
class:inverse, middle, center
# 💪
## Get set up to work

---
## Packages

Hopefully, you were all able to install the following packages 

- `dataverse` (necessary for this week)
- `tidycensus` (necessary for this week)
- `easystats` (useful later)
- `DeclareDesign` (useful later)

---
## Census API:

Additionally, I hope you have all followed the steps [here](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package):

1. Install the `tidycensus` package
2. Load the installed package
3. Request an API key from the Census
4. Check your email
5. Activate your key
6. Install your API key in R
7. Check that everything worked

To install the an API key so we can download data directly from the US Census


---
## Packages for today


```{r, ref.label=c("packages")}

```

---
## Define a function to load (and if needed install) packages


```{r, ref.label="ipak"}
```

---
## Load packages for today

```{r ref.label="loadpackages"}
```


---
class:inverse, center, middle
# 💪
## Load Data for Thursday's Lab

---
## Load the Covid-19 Data

```{r covid}
covid <- COVID19::covid19(
  country = "US",
  level = 2,
  verbose = F
)
```

---
## Filter Covid-19 Data to US States (Now Excluding D.C. as well)

```{r covidus}
# Vector containing of US territories
territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands",
  "District of Columbia"
  )

# Filter out Territories and create state variable
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)%>%
  mutate(
    state = administrative_area_level_2
  )
```

---
## Mutate: Calculate New Deaths Per Capita and Percent Vaccinated


```{r newcases}
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = new_deaths / population *100000,
    new_deaths_pc_14da = zoo::rollmean(new_deaths_pc, k = 14, align = "right", fill=NA ),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```


---
## Load Data on Presidential Elections



```{r pres_data}
# Try this code first
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

pres_df <- get_dataframe_by_name(
  "1976-2020-president.tab",
  "doi:10.7910/DVN/42MVDX"
)

# If the code above fails, comment out and uncomment the code below:

# load(url("https://pols1600.paultesta.org/files/data/pres_df.rda"))
```

---
## HLO of Presidential Elections Data

```{r pres_hlo}
head(pres_df)
```

---
## Transform Data to get just 2020 Election

```{r pres_wrangle}
pres_df %>%
  mutate(
    year_election = year,
    state = str_to_title(state),
    # Fix DC
    state = ifelse(state == "District Of Columbia", "District of Columbia", state)
  ) %>%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%>%
  filter(year == 2020) %>%
  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes
         ) %>%
  pivot_wider(names_from = party_simplified,
              values_from = candidatevotes) %>%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes *100,
    rep_voteshare = REPUBLICAN/totalvotes*100,
    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,"Trump","Biden")))
  ) -> pres2020_df
```

---
## Transform Data to get just 2020 Election

```{r}
head(pres2020_df)
```


---
## Load Data on Median State Income from the Census

```{r acs_data}
acs_df <- get_acs(geography = "state", 
              variables = c(med_income = "B19013_001",
                            med_age = "B01002_001"), 
              year = 2019)

# Uncomment if get_acs() doesn't work:
# load(url("https://pols1600.paultesta.org/files/data/acs_df.rda"))

```

---
## HLO: Census Data 

```{r acs_hlo}
head(acs_df)
```


---
## Tidy Census Data

```{r acs_tidy}
acs_df %>%
  mutate(
    state = NAME,
  ) %>%
  select(state, variable, estimate) %>%
  pivot_wider(names_from = variable,
              values_from = estimate) -> acs_df
```

---
## Tidy Census Data

```{r}
head(acs_df)
```



---
## Merge election data into Covid data

```{r merge_pres}
# Always check dimensions before and after merging
dim(covid_us)
dim(pres2020_df)
# Merge covid_us with pres2020_df and save as tmp file
tmp <- covid_us %>% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(tmp) # Same number of rows as covid_us w/ 8 additional columns
```


---
## Merge Census data into Covid data

```{r acs_merge}
dim(tmp)
dim(acs_df)

# Merge tmp with acs_df and save as final covid_df file
covid_df <- tmp %>% left_join(
  acs_df,
  by = c("state" = "state")
)
dim(covid_df)  # Same number of rows as tmp w/ 2 additional columns

```



---
## Subset Merged data to include only the variables and observations we want

```{r}
the_vars <- c(
  # Covid variables
  "state","state_po","date","new_deaths_pc_14da", "percent_vaccinated",
  # Election variables
  "winner","rep_voteshare",
  # Demographic variables
  "med_age","med_income","population")


covid_lab <- covid_df %>%
  filter( date == "2021-09-23")%>%
  select(all_of(the_vars))%>%
  ungroup()

length(the_vars)
dim(covid_lab)
```


---
## A Preview of Where We're Headed:

Consider the following **multiple regression** (which we will on Thursday):

$$\text{New Covid Deaths} = \beta_0 +\beta_1 \text{Rep. Vote Share} +\beta_2 \text{Median Age} +\beta_3 \text{Median Income} + \epsilon$$
- The tell us how the outcome, *New Covid Deaths*, is expected to change with a **unit change** in a given predictor, controlling for/holding constant the other predictors in the model (more on "controlling for" shortly).
  - In the model above, $\beta_2$ tells us how *New Covid Deaths* is predicted to change, if the *Median Age* of a state increased by one year (i.e. a unit change).
  - Similarly, $\beta_3$ tells us how *New Covid Deaths* is predicted to change, if the *Median Income* of a state increased by one dollar (i.e. a unit change).  
  - Since vote share, median income and age are measured on very different scales, interpreting these coefficients in the same model can be cumbersome.

---
## Standardizing variables.


- When variables are measured in different units multiple regression coefficients can be hard to interpret
  - Coefficients, $\beta$, tell us about the predicted change in $y$ for a unit change in $x$ or $z$
  - For example $\beta_{age}$

- *z-scores* standardize variables so that their unit of measurement no longer matters (QSS p. 103).


- To calculate the *z-score* of a variable $x$, we simply, substract off the mean and divide by the standard deviaton

$$z\text{-score of x} = \frac{x_i - \mu_{x}}{\sigma_x}$$

- The *z-score* of Age is 

$$z\text{-score of Age} = \frac{\text{Age}_i - \mu_{Age}}{\sigma_{Age}}$$

---
## Standardizing Predictors in R

```{r}
covid_lab %>%
  mutate(
    rep_voteshare_std = (rep_voteshare - mean(rep_voteshare))/sd(rep_voteshare),
    med_age_std = ( med_age - mean( med_age))/sd( med_age),
    med_income_std = (med_income - mean(med_income))/sd(med_income),
    percent_vaccinated_std = (percent_vaccinated - mean(percent_vaccinated))/sd(percent_vaccinated)
  ) -> covid_lab



```




---
## Multiple Regression with Standardized Predictors

If we were to estimate a model with standardized predictors:

$$\text{New Covid Deaths} = \beta_0 +\beta_1 \text{Rep. Vote Share}_{std} +\beta_2 \text{Median Age}_{std} +\beta_3 \text{Median Income}_{std} + \epsilon$$
The coefficients still tell us predicted change in New Covid Deaths for a unit change in a predicotr, but now a unit change corresponds to a 1-standard deviation increase of each predictor:

```{r}
covid_lab %>%
  summarise(
    sd_rep_vote = sd(rep_voteshare),
    sd_med_age = sd(med_age),
    sd_med_income = sd(med_income)
  )
```

---
## Saving Data

Finally, I'll save the data for Thursday's lab

```{r}
# Don't run this code
save(covid_lab, file = "../files/data/06_lab.rda")

```

And on Thursday, we'll be able to load the `covid_lab` just by running:

```{r}
load(url("https://pols1600.paultesta.org/files/data/06_lab.rda"))
```







---
class: inverse, center, middle
background-image: url("http://static1.squarespace.com/static/5578d503e4b0bc473a0165e5/t/59e7b63a12abd9eb164d30bd/1508357693350/?format=1500w")
background-size: cover
# 📢
## Feedback

```{r feedback, echo=F, message=F}
df_feedback_full <- haven::read_spss("../files/data/wk05.sav")
df_feedback_full %>%
  filter(optout == 1)-> df_feedback
```

---
## What we liked

--

- The lab, or at least aspects of the lab like:
  - Working in groups
  - Building from copying code to writing code
  - Dealing with real world data/questions
- Pacing of lab/lecture
- Tutorials
- Math(!?)

---
## What we liked

```{r likes, echo=F, out.height='90%'}
DT::datatable(df_feedback %>% 
                select(Likes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 3
              )
              )

```



---
## What we disliked 


- Lab
  - Pacing/Length
  - Coding roadblocks/portals of discovery
- Connection between lecture and lab
- Math


---
## What we disliked

```{r dislikes, echo=F}
DT::datatable(df_feedback %>% 
                select(Dislikes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 3
              )
              )
2+2
```

---
## What we'll do

.pull-left[
### Me

- Keep the labs shorter
- Post the labs earlier
- Try different ways of teaching coding
  - Copy paste and run code
  - Run and comment code
  - Fill in the blank
  - Update and adapt code
  - Troubleshoot broken code
  - Write your own code
]

--

.pull-right[
### You

- Keep doing the tutorials
- Review the labs before class
- Use the structure of .Rmd file as your friend
- Use separate code chunks for exploratory code and final code
- Work through the commented labs
- Come to office hours?



]

---
class: inverse, center, middle
background-image:url("https://media.giphy.com/media/KWwPcQwGetlA0qKdY0/giphy.gif")
background-size: cover

## Help me with my fit



---
```{r, echo=F}
df_feedback_full %>%
  select(starts_with("fit_"))%>%
  mutate_all(forcats::as_factor)%>%
  mutate_all(as.character)%>%
  pivot_longer(
    cols = starts_with("fit"),
    names_to = "Fit",
    values_to = "Options"
  )%>%
  mutate(
    Fit = str_to_title(gsub("fit_","", Fit))
  )%>%
  group_by(Fit,Options)%>%
  summarise(
    Vote = n(),
  )%>%
  arrange(Fit,Vote)%>%
  mutate(
    Rank =n():1,
    Fit = factor(Fit, levels = c("Palette","Jacket","Pant","Top","Pattern","Tie","Shoe")),
    Options = forcats::fct_reorder(Options, Vote)
  )%>%
  ungroup()%>%
  na.omit()%>%
  ggplot(aes(Options,Vote, fill=Rank))+
  geom_bar(stat = "identity")+
  facet_wrap(~Fit, scale="free_y",ncol = 2)+
  coord_flip()+
  guides(fill="none")



```

---
class: inverse, center, middle
background-image:url("https://www.blackenterprise.com/wp-content/blogs.dir/1/files/2015/08/Michael-Jordan_nike.com-2-1280x720.jpg")
background-size: cover


---
class:inverse, middle, center
# 🔍
## Review


---
## Review

- Casual Inference in Observational Designs
  - Difference-in-Differences
  - Regression Discontinuity Design
  - Instrumental Variables

- Simple Linear Regression

---
class:inverse, middle, center
# 🔍
## Casual Inference in Observational Designs

---
## Review: Casual Inference in Observational Designs


- Observational designs that try to estimate causal effects need to justify assumptions about conditional independence:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

--

- This assumption goes by many, jargony names: Selection on Observables, Conditional Independence, No unmeasured confounders.

--

- Credibility of this assumption depends less on having a lot of data, and more on how your data were generated.

--

- Three common research designs:

  - Difference in Difference Design

  - Regression Discontinuity Designs
  
  - Instrumental Variable Designs


---
## Review: Difference in Difference Design

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
  - Taking the pre-post difference removes any fixed differences between the units
  
  - Then taking the difference between treated and control differences removes any common differences over time

- The key identifying assumption of a DiD design is the "assumption of parallel trends"

- Common Diff-in-Diffs in political science:
  - Basically any time there's staggered adoption of policy overtime $\to$ Diff-in-Diff.
  

---
## Review: Regression Discontinuity Design

- A Regression Discontinuity Design (RDD):
 - leverages a naturally occuring discontinuity in the probability of receiving some treatment
 - uses regression to estimate the effect of that treatment **at the discontinuity**

- RDD are identified under assumptions of either:
  - Continuity at the cutoff
  - Local randomization
  
- The key idea is that at (continuity) or in some window around (local randomization), variation in the outcome of interest is due only change in the treatment

- Common discontinuities in political science:
  - First past the post elections
  - Borders
  - Eligibility cutoffs for policies

---
## Review: Instrumental Variables

Instrumental variables are an economists favorite tool for dealing with **omitted variable bias**

- We're worried that these effects are **confounded** by some unobserved, omitted variable, that influences both the treatment and the outcome
- We find an **instrumental variable** that is **exogenous** and **relevant** satisfying the following assumptions
  - Randomization 
  - Excludability
  - First-stage relationship
  - Monotonicity
- Allowing us estimate a Local Average Treatment Effect (LATE) using the instrument to isolate the variation in our treatment that is **exogenous** (uncorrelated with ommitted variables)
- Common instruments in political science:
  - Nature (rainfall)
  - Geography (distance from the equator)
  - History (Different colonial legacies/institutions)


---
## IV Applications

```{r ivapps, echo=F}
knitr::include_graphics("https://pbs.twimg.com/media/EJGyHnyUYAA-yhM?format=jpg&name=large")
```

[@AndrewHeiss](https://twitter.com/andrewheiss/status/1193931226865901569?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1193931226865901569%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2Fandrewheiss2Fstatus2F1193931226865901569widget%3DTweet)


---
class:inverse, middle, center
# 🔍
# Simple Linear Regression
### Linear regression provides a linear estimate of the CEF

---
## Review: Simple Linear Regression

- **Conceptual**
  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function
  
```{r ols, eval=F}
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR): 

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$
  
---
## Linear Regression and the CEF

Recall the first model we fit in last week's lab

$$\text{new_deaths} = \beta_0 + \beta_1 \text{winner} + \epsilon$$

```{r}
m1 <- lm(new_deaths ~ winner, covid_df)
m1
```

`m1` is the equation for a very simple line defined by two points:

  - $\beta_0$ = E[new_deaths|winner = Trump]
  - $\beta_0 + \beta_1$ = E[new_deaths|winner = Biden]

---
## Linear Regression and the CEF

It's helpful to take a look at what's going on under the hood in `lm()`

We gave it the following data:

```{r}
head(m1$model)
```

---
## Linear Regression and the CEF

- R doesn't know how to calculate the mean of `Biden` (or any character or factor data). 
- Instead it creates an indicator variable `winnerBiden` which equals 1 when `winner` equals `Biden` and 0 otherwise.

```{r}
head(model.matrix(m1))
```

---
## Linear Regression and the CEF

```{r}
table(m1$model$winner)
table(model.matrix(m1)[,2])
```

---

```{r, eval=F}
covid_df%>%
  mutate(
    winner01 = ifelse(winner=="Biden",1,0)
  )%>%
  ggplot(aes(winner01, new_deaths))+
  geom_point()+
  stat_summary(geom="point",fun=mean,col= "red",size=2)+
  geom_segment(aes(x=0,xend =1, 
                   y=coef(m1)[1],
                   yend = coef(m1)[1]+coef(m1)[2]*1
                   ),
               col= "red")

```

---
```{r, echo=F}
covid_df%>%
  mutate(
    winner01 = ifelse(winner=="Biden",1,0)
  )%>%
  ggplot(aes(winner01, new_deaths))+
  geom_point()+
  stat_summary(geom="point",fun=mean,col= "red",size=2)+
  geom_segment(aes(x=0,xend =1, 
                   y=coef(m1)[1],
                   yend = coef(m1)[1]+coef(m1)[2]*1
                   ),
               col= "red")

```

---
## Linear Regression and the CEF

- When the CEF function is linear, `lm()` provides you coefficients that give you conditional means like in `m1`
  - The CEF is linear for *saturated models* in like  `m1` where the coefficients define every possible category of comparison.
- For continuous variables, like  `m3` from the lab below, `lm()` provides a linear approximation to the CEF

```{r}
m3 <- lm(new_deaths_pc_14da ~ percent_vaccinated, covid_df,
         subset = date == "2021-09-23")
m3
```


---

```{r, eval=F}
covid_df %>%
  filter(date == "2021-09-23") %>%
  ggplot(aes(percent_vaccinated, new_deaths_pc_14da))+
  geom_point(size=.5,alpha=.5)+
  stat_summary_bin(fun = "mean", 
                   geom = "point", 
                   col="red",
                   bins =10
                   )+
  geom_smooth(method ="lm")
```

---

```{r, echo=F}
covid_df %>%
  filter(date == "2021-09-23") %>%
  ggplot(aes(percent_vaccinated, new_deaths_pc_14da))+
  geom_point(size=.5,alpha=.5)+
  stat_summary_bin(fun = "mean", 
                   geom = "point", 
                   col="red",
                   bins =10
                   )+
  geom_smooth(method ="lm")
```


---
class: inverse, middle, center
# 💡 
## Multiple Regression

---
## Overiew: Multiple Regression

- **Conceptual**
  - Multiple linear regression generalizes simple regression to models with multiple predictors

$$y_i = \beta_0 + \beta_1x_{1,i} +\beta_2x_{2,i} + \epsilon_i$$

$$y_i = X\beta + \epsilon_i$$

- **Practical**
  - We estimate linear models in R using the `lm()` function using the `+` to add predictors
  - We use the `*` to include the main effects $(\beta_1 x, \beta_2z)$ and interactions $(\beta_3 (x\cdot z))$of two predictors
  
```{r, eval=F}
lm(y ~ x + z, data = df)
lm(y ~ x*z, data = df) # Is a shortcut for:
lm(y ~ x + z + x:z, data = df)

```

---
## Overiew: Multiple Regression


- *Technical/Definitional*
  - Simple linear regression chooses a $\hat{\beta_0}$ and $\hat{\beta_1}$  to minimize the Sum of Squared Residuals (SSR): 

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\hat{\beta_0}+\hat{\beta_1}x_i))^2$$
  - Multiple linear regression chooses a vector of coefficients $\hat{\beta}$ to minimize the Sum of Squared Residuals (SSR):
  
$$\textrm{Find }\widehat{\beta} \text{ argmin }_{\widehat{\beta}} \sum \epsilon^2=\epsilon^\prime\epsilon=(Y-X\widehat{\beta})^\prime(Y-X\widehat{\beta})$$


- *Theoretical*
  - Multiple Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$ where $Y$ is now a function of multiple predictors, $X$


---
class: inverse, middle, center
# 💡 
## Estimating and Interpreting Multiple Regression Regression Models

---
## Political Interest and Partisan Evaluations

Let's load some data from the [2016 NES](https://electionstudies.org/data-center/2016-time-series-study/) and explore the relationship between political interest and evaluations of presidential candidates:

- Political Interest: "How interested are you in in politics?
  - Very Interested
  - Somewhat interested
  - Not very Interested
  - Not at all Interested
- Feeling Thermometer: "... On the feeling thermometer scale of 0 to 100, how would you rate"
  - Donald Trump
  - Hillary Clinton

---
## Data

```{r}
load(url("https://pols1600.paultesta.org/files/data/nes.rda"))
# Look at data
dim(nes)
head(nes)
```

---
## Data: HLO

```{r}
table(nes$pol_interest)
summary(nes$ft_trump)
summary(nes$ft_hrc)

```



---
## Data Wrangling/Recoding

```{r}
nes %>%
  mutate(
    income = ifelse(faminc > 16, NA, faminc),
    interested = ifelse(pol_interest==3,T,F),
    pol_interest_f = factor(case_when(
      pol_interest == 0 ~ "Not at all Interested",
      pol_interest == 1 ~ "Not very Interested",
      pol_interest == 2 ~ "Somewhat Interested",
      pol_interest == 3 ~ "Very Interested"
    )),
    tc_diff = abs(ft_trump - ft_hrc)
  ) -> nes
```

---
## Roadmap

Let's estimate the following series of models, predicting the absolute value of the difference in feeling thermometer ratings between Trump and Clinton (`tc_diff`)

$$\text{tc_diff} = \beta_0$$
$$\text{tc_diff} = \beta_0 + \beta_1\text{interested}$$
$$\text{tc_diff} = \beta_0 + \beta_1\text{pol_interest_f}$$
$$\text{tc_diff} = \beta_0 + \beta_1\text{pol_interest}$$
$$\text{tc_diff} = \beta_0 + \beta_1\text{interested} + \beta_2\text{age}$$
$$\text{tc_diff} = \beta_0 + \beta_1\text{interested} + \beta_2\text{age} + \beta_3\text{interested} \times \text{age}$$
$$\text{tc_diff} = \beta_0 + \beta_1\text{age} + \beta_2\text{income}$$

---
## `m0`: "The Empty Model"

A linear model with just an intercept returns the mean of `tc_diff`

```{r}
m0 <- lm(tc_diff ~ 1, nes)
m0
mean(nes$tc_diff, na.rm=T)

# Save Sum of Squared Residuals
ssr0 <- sum(m0$residuals^2)

```

---
## `m1` A model with a single dichotomous predictor

Now let's model `tc_diff` as function of whether respondents are very interested in politics (`interested` = T) or not (`interested` = F)

```{r}
m1 <- lm(tc_diff ~ interested, nes)
coef(m1)
mean(nes$tc_diff[nes$interested == F],na.rm = T)

coef(m1)[1] + coef(m1)[2]
mean(nes$tc_diff[nes$interested == T],na.rm = T)
```


---
## `m1` A model with a single dichotomous predictor

- If we tell R not to include an intercept, it returns those conditional means exactly.

```{r}
m1_alt <- lm(tc_diff ~ -1+ interested, nes)
coef(m1_alt)
tapply(nes$tc_diff, nes$interested, mean,na.rm=T)
```


---
## `m1` A model with a single dichotomous predictor

- When we compare the SSR from `m1` we see that it has decreased from `m0` reflecting the fact some of the total variation in `tc_diff` is being explained by the variation in  `interested` 

```{r}
# Save Sum of Squared Residuals
ssr1 <- sum(m1$residuals^2)
ssr1 < ssr0
```

---
## `m2` A model with a single categoical predictor

Now let's model `tc_diff` as function of the categorical predictor `pol_interest_f`, which is a factor variable of the four unique levels of political interest

```{r}
m2 <- lm(tc_diff ~ pol_interest_f, nes)
m2
```

- Woah, why did R turn a single variable into model with four coefficients?

---

When we give `lm()` factor or character data, 


```{r}
head(m2$model)[1:4,]
```


It converts this data in separate binary indicators for each level excluding the first ("Not at all interested"), that take a value of 1 when `pol_interest_f` equals that level and 0 otherwise

```{r}
head(model.matrix(m2))[1:4,]
```


---
## `m2` A model with a single categoical predictor

The coefficients in `m2` tell us how the conditional mean of `tc_diff` for someone with a given level of interest is likely to very from the conditional mean of `tc_diff` for someone that is not at all interested in politics.

```{r}
tapply(nes$tc_diff, nes$pol_interest_f, mean,na.rm=T)
```

---
## `m2` A model with a single categoical predictor


```{r}
# Compare to
# Not at all
coef(m2)[1]
# Not very
coef(m2)[1] + coef(m2)[2]
# Somewhat
coef(m2)[1] + coef(m2)[3]
# Very
coef(m2)[1] + coef(m2)[4]
```

---
## `m2` A model with a single categorical predictor

- Again if we excluded the intercept `lm()` returns the conditional means

```{r}
m2_alt <- lm(tc_diff ~ -1+ pol_interest_f, nes)
coef(m2_alt)
tapply(nes$tc_diff, nes$pol_interest_f, mean,na.rm=T)
```

---
## `m2` A model with a single categorical predictor


- Finally, note that the SSR for `m2` is lower than that of `m1`. Including more information about varying levels of political interest helped us explain more variation in our outcome

```{r}
# Save Sum of Squared Residuals
ssr2 <- sum(m2$residuals^2)
ssr2 < ssr1
```


---
## `m3` A model with a single numeric predictor

What if we had instead modeled `tc_diff` as a function of `pol_interest`

```{r}
m3 <- lm(tc_diff ~ pol_interest, nes)
m3
```

`lm()` returns a single coefficient, because `pol_interest` is a numeric variable.


---
## Comparing `m2` to `m3`

```{r, eval=F}
nes %>%
  ggplot(aes(pol_interest,tc_diff))+
  geom_jitter(size=.5, alpha=.5)+
  geom_smooth(method ="lm", aes(col = "m3"),se=F)+
  stat_summary(geom ="point",
               fun = mean,
               aes(col = "m2")
               )+
  labs(col = "Model")
```


---
## Comparing `m2` to `m3`

```{r, echo=F}
nes %>%
  ggplot(aes(pol_interest,tc_diff))+
  geom_jitter(size=.5, alpha=.5)+
  geom_smooth(method ="lm", aes(col = "m3"),se=F)+
  stat_summary(geom ="point",
               fun = mean,
               aes(col = "m2")
               )+
  labs(col = "Model")
```


---
## `m4`: A model with both a dichtomous and continuous predictor

Now let's estimate a multiple regression model "controlling" for age

```{r}
m4 <- lm(tc_diff ~ interested + age, nes)
coef(m4)
```

Note the size of the coefficient on `interested` has decreased, compared to `m1`. Part of the variation `tc_diff` explained by `interested` reflected variation explained by `age`

```{r}
coef(m1)[1]
coef(m4)[2]
```

- Mechanically, we sometimes say the coefficient on `interested` shifts the intercept of the line describing the relationship between `tc_diff` and age.

---
```{r, echo=F}
nes%>%
  ggplot(aes(age, tc_diff))+
  geom_point()+
  geom_abline(aes(intercept = coef(m4)[1],
                  slope = coef(m4)[3],
                  col = "Not interested"))+
  geom_abline(aes(intercept = coef(m4)[1] + coef(m4)[2],
                  slope = coef(m4)[3],
                  col = "Interested"))+
  labs(
    col = "Interested?"
  )
```







---
## `m5`: A model with an interaction between a dichtomous and continuous predictor

What if we think the relationship between `age` and `tc_diff` varies depending on a person's level of political interest?

We could fit an interacton model:

```{r}
m5 <- lm(tc_diff ~ interested*age, nes)
coef(m5)
```


---

Mechanically, R takes the data

```{r}
head(m5$model)
```

And creates a model matrix:

```{r}
head(model.matrix(m5))
```

---
## `m5`: A model with an interaction between a dichtomous and continuous predictor

This interaction model essentially estimates separate models describing the relationship between `tc_diff` and `age`  for the uninterested...

```{r}
m5_uninterested <- lm(tc_diff ~ age, nes,
                    subset = interested == F)
coef(m5_uninterested)
coef(m5)[1]
coef(m5)[3]
```


---
## `m5`: A model with an interaction between a dichtomous and continuous predictor

And interestd

```{r}
m5_interested <- lm(tc_diff ~ age, nes,
                    subset = interested == T)
coef(m5_interested)
coef(m5)[1] +coef(m5)[2]
coef(m5)[3] +coef(m5)[4]

```

---

```{r}
nes %>%
  filter(!is.na(interested))%>%
  ggplot(aes(age, tc_diff, col = interested))+
  geom_point(size = .5, alpha = .5)+
  geom_smooth(method ="lm")
```


---
## `m6`: A model with an two continuous predictors

Now lets model `tc_diff` as a function of two predictors: `age` and `income`

```{r}
m6 <- lm(tc_diff ~ age + income, nes)
m6
```

We interpret this model as saying:

- Controlling for `income`, as `age` increases by 1 we can expect the predicted difference in `tc_diff` to increase by 0.40
- Controlling for `age`, as `income` increases by 1 we can expect the predicted difference in `tc_diff` to increase by 0.16

---
```{r,echo=F}

s3d <-with(nes, scatterplot3d(age,income,tc_diff ,
      pch=16,cex.symbols=.5, type="p",angle=70))
s3d$plane3d(m6,col="red")


```



---
## A model with an interaction between two continuous predictors

## Multiplicative Interactions with a continuous moderator

Suppose we wanted to fit an interaction between two continuous variables like `age` and `income`. 

The general form of this model is

$$
Y= \beta_0 + \beta_1X + \beta_2 +\beta_3X\times Z + \epsilon
$$

- The marginal effect of X on Y, now depends on the value of Z at which you evaluate the relationship

$$
\frac{\partial Y}{\partial X}=\beta_1+\beta_3Z
$$

---

Easier to show this with some simulated data:

```{r,echo=F}
set.seed(123)
X<-rnorm(200,5,2)
Z<-rnorm(200,5,2)
Y<-2+2*X+3*Z+-3*X*Z+rnorm(200,1,4)
m7<-lm(Y~X*Z)
nx <- seq(min(c(X,Z)), max(c(X,Z)), length.out = 50)
z <- outer(nx, nx, FUN = function(a, b) predict(m7, data.frame(X = a, Z = b)))
pmat<-persp(nx, nx, z, theta = 0, phi = 10, shade = 0.75, xlab = "X", ylab = "Z", 
    zlab = "Y")
depth3d <- function(x,y,z, pmat, minsize=0.2, maxsize=2) {

  # determine depth of each point from xyz and transformation matrix pmat
  tr <- as.matrix(cbind(x, y, z, 1)) %*% pmat
  tr <- tr[,3]/tr[,4]

  # scale depth to point sizes between minsize and maxsize
  psize <- ((tr-min(tr) ) * (maxsize-minsize)) / (max(tr)-min(tr)) + minsize
  return(psize)
}
psize = depth3d(X,Z,Y,pmat,minsize=0.1, maxsize = 1)
mypoints <- trans3d(X, Z, Y, pmat=pmat)
points(mypoints, cex=psize, col=4)

```


---
```{r,echo=F}
pmat<-persp(nx, nx, z, theta = 45, phi = 10, shade = 0.75, xlab = "X", ylab = "Z", 
    zlab = "Y")
psize = depth3d(X,Z,Y,pmat,minsize=0.1, maxsize = 1)
mypoints <- trans3d(X, Z, Y, pmat=pmat)
points(mypoints, cex=psize, col=4)
```


---
class: inverse, middle, center
# 💡 
## Reading Regression Tables


---
## Regression Tables

- Academic articles are littered with regression tables.

- Below we'll see how to produce regression tables in R

- Learn some heuristics for how to interpret regression tables
  - We'll cover the theory behind these heuristics in the coming weeks
  

---
## Making Regression Tables in R

We can make a very simple regression table using the `htmlreg` function from the `texreg` package

```{r, eval = F}
texreg::htmlreg(
  list(m0,m1,m3,m4,m5,m6)
)
```
  

---

```{r, echo = F}
texreg::htmlreg(
  list(m0,m1,m3,m4,m5,m6)
)
```
  
---
Yuck, we need to set a argument in the chunk header to results="asis"

````

```{r echo=F, results = "asis"}`r`''`
texreg::htmlreg(
  list(m0,m1,m3,m4,m5,m6)
)

```

````

---

```{r, echo = F, results="asis", out.height="60%"}
texreg::htmlreg(
  list(m0,m1,m3,m4,m5,m6),
    
)

```
  


---
## Interpreting Regression Tables (Stargazing)

- Each column is a model
- Each row is a coefficient from that model with its  **standard error** (more to come) in parantheses below
- The bottom of the table shows summary stastitics of the model ( $R^2$ = proportion of variance explained)
- Coefficients with asterisks `*` are **statistically significant**
  - It is unlikely that we would see a coefficient this big or bigger if the true coefficient were 0
- Rule of thumb:

$$\text{If }\frac{\beta}{se} > 2 \to \text{Statistically Significant}$$


---
class: inverse, middle, center
# 💡 
## Summary

---
## Summary

Today we estimated and interpreted a series of multiple regression models

- The coefficients in these models still describe slopes (partial derivatives), now for planes (and hyper planes)

- Controlling for variables is easy in `lm()`

```{r, eval=F}
m1 <- lm(y ~ x, data=df)
m2 <- lm(y ~ x + z, data=df)

```

In the lab and next week, we will start to explore

- What it means to "control for x"
- How can we compare different models
- How can we interpret predictions from multiple regression
