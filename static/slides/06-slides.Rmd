---
title: "Week 06:"
subtitle: "Linear Regression with Multiple Predictors"
author: "Paul Testa"
output:
  xaringan::moon_reader:
    css: ["default", "css/brown.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
---

class: inverse, center, middle
background-image:url("https://img1.hscicdn.com/image/upload/f_auto/lsci/db/PICTURES/CMS/332100/332134.4.jpg")
background-size:cover

# Overview

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      cache = T,
  comment = NA, dpi = 300,
  fig.align = "center", out.width = "80%")
library("tidyverse")
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


```{r packages, include=F}
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", #<<
  # Analysis
  "DeclareDesign", "easystats", "zoo"#<<
)
```

```{r ipak, include=F}
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

```


```{r loadpackages, cache=F, include=F}
ipak(the_packages)
```



---
## General Plan

- Group Assignment 2: Data
- Setup
  - Packages
  - Data
- Feedback
- Review
  - Causal Inference in Observational Designs
  - Simple Linear Regression
- Multiple Regression 
  - What does it mean to control for X
  - Residualized Regression
  - Interpreting Multiple Regression
  - Reading Regression Tables
- ICYI: The Matrix Algebra of Linear Regression 

---
class: inverse, center, middle
# Research Questions

---
## Assignment 1 Research Questions:

- Feedback posted to Canvas.

- General Comments:

  - Really great questions!
  
  - Feedback tries to balance interest/feasibility/data availability
  
  - "Not an experiment" $\to$ What would you have to randomize to answer your question
  
  - Not every question need to be causal
  
  - "Make your theories elaborate" - R.A. Fisher to [William Cochran](https://www.jstor.org/stable/2344179)
    - But keep your models simple - Testa to POLS 1600


---
## Assignment 2: Data Explorations


- Prompt posted [here](https://pols1600.paultesta.org/files/assignments/A1_research_questions.docx)

- Upload to [Canvas](https://canvas.brown.edu/courses/1087979/assignments/7870539) this Sunday

1. A revised description of your group's research project
2. A description of a linear model implied by your question
3. R code that loads some potentially relevant data to your question and at least one descriptive summary of that data.
4. Some information about your group such as:
    - A group name^[If you're Group 01 don't change your name to Group 4]
    - A group color or color scheme
    - A group motto, mascot, crest, etc.
    - Your group's theme song
    - Your group's astrological sign
    - Anything else that you think well help you form strong ingroup bounds that facilitate collaboration


---
class:inverse, middle, center
# 💪
## Get set up to work

---
## Packages

Hopefully, you were all able to install the following packages 

- `dataverse` (necessary for this week)
- `tidycensus` (necessary for this week)
- `easystats` (useful later)
- `DeclareDesign` (useful later)

---
## Census API:

Additionally, I hope you have all followed the steps [here](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package):

1. Install the `tidycensus` package
2. Load the installed package
3. Request an API key from the Census
4. Check your email
5. Activate your key
6. Install your API key in R
7. Check that everything worked

To install the an API key so we can download data directly from the US Census


---
## Packages for today


```{r, ref.label=c("packages")}

```

---
## Define a function to load (and if needed install) packages


```{r, ref.label="ipak"}
```

---
## Load packages for today

```{r ref.label="loadpackages"}
```


---
class:inverse, center, middle
# 💪
## Load Data for Thursday's Lab

---
## Load the Covid-19 Data

```{r covid}
covid <- COVID19::covid19(
  country = "US",
  level = 2,
  verbose = F
)
```

---
## Filter Covid-19 Data to US States (Now Excluding D.C. as well)

```{r covidus}
# Vector containing of US territories
territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands",
  "District of Columbia"
  )

# Filter out Territories and create state variable
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)%>%
  mutate(
    state = administrative_area_level_2
  )
```

---
## Mutate: Calculate New Deaths Per Capita and Percent Vaccinated


```{r newcases}
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = new_deaths / population *100000,
    new_deaths_pc_14da = zoo::rollmean(new_deaths_pc, k = 14, align = "right", fill=NA ),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```


---
## Load Data on Presidential Elections



```{r pres_data}
# Try this code first
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

pres_df <- get_dataframe_by_name(
  "1976-2020-president.tab",
  "doi:10.7910/DVN/42MVDX"
)

# If the code above fails, comment out and uncomment the code below:

# load(url("https://pols1600.paultesta.org/files/data/pres_df.rda"))
```

---
## HLO of Presidential Elections Data

```{r pres_hlo}
head(pres_df)
```

---
## Transform Data to get just 2020 Election

```{r pres_wrangle}
pres_df %>%
  mutate(
    year_election = year,
    state = str_to_title(state),
    # Fix DC
    state = ifelse(state == "District Of Columbia", "District of Columbia", state)
  ) %>%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%>%
  filter(year == 2020) %>%
  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes
         ) %>%
  pivot_wider(names_from = party_simplified,
              values_from = candidatevotes) %>%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes *100,
    rep_voteshare = REPUBLICAN/totalvotes*100,
    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,"Trump","Biden")))
  ) -> pres2020_df
```

---
## Transform Data to get just 2020 Election

```{r}
head(pres2020_df)
```


---
## Load Data on Median State Income from the Census

```{r acs_data}
acs_df <- get_acs(geography = "state", 
              variables = c(med_income = "B19013_001",
                            med_age = "B01002_001"), 
              year = 2019)

# Uncomment if get_acs() doesn't work:
# load(url("https://pols1600.paultesta.org/files/data/acs_df.rda"))

```

---
## HLO: Census Data 

```{r acs_hlo}
head(acs_df)
```


---
## Tidy Census Data

```{r acs_tidy}
acs_df %>%
  mutate(
    state = NAME,
  ) %>%
  select(state, variable, estimate) %>%
  pivot_wider(names_from = variable,
              values_from = estimate) -> acs_df
```

---
## Tidy Census Data

```{r}
head(acs_df)
```



---
## Merge election data into Covid data

```{r merge_pres}
# Always check dimensions before and after merging
dim(covid_us)
dim(pres2020_df)
# Merge covid_us with pres2020_df and save as tmp file
tmp <- covid_us %>% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(tmp) # Same number of rows as covid_us w/ 8 additional columns
```


---
## Merge Census data into Covid data

```{r acs_merge}
dim(tmp)
dim(acs_df)

# Merge tmp with acs_df and save as final covid_df file
covid_df <- tmp %>% left_join(
  acs_df,
  by = c("state" = "state")
)
dim(covid_df)  # Same number of rows as tmp w/ 2 additional columns

```



---
## Subset Merged data to include only the variables and observations we want

```{r}
the_vars <- c(
  # Covid variables
  "state","state_po","date","new_deaths_pc_14da", "percent_vaccinated",
  # Election variables
  "winner","rep_voteshare",
  # Demographic variables
  "med_age","med_income","population")


covid_lab <- covid_df %>%
  filter( date == "2021-09-23")%>%
  select(all_of(the_vars))%>%
  ungroup()

length(the_vars)
dim(covid_lab)
```


---
## A Preview of Where We're Headed:

Consider the following **multiple regression** (which we will on Thursday):

$$\text{New Covid Deaths} = \beta_0 +\beta_1 \text{Rep. Vote Share} +\beta_2 \text{Median Age} +\beta_3 \text{Median Income} + \epsilon$$
- The tell us how the outcome, *New Covid Deaths*, is expected to change with a **unit change** in a given predictor, controlling for/holding constant the other predictors in the model (more on "controlling for" shortly).
  - In the model above, $\beta_2$ tells us how *New Covid Deaths* is predicted to change, if the *Median Age* of a state increased by one year (i.e. a unit change).
  - Similarly, $\beta_3$ tells us how *New Covid Deaths* is predicted to change, if the *Median Income* of a state increased by one dollar (i.e. a unit change).  
  - Since vote share, median income and age are measured on very different scales, interpreting these coefficients in the same model can be cumbersome.

---
## Standardizing variables.


- When variables are measured in different units multiple regression coefficients can be hard to interpret
  - Coefficients, $\beta$, tell us about the predicted change in $y$ for a unit change in $x$ or $z$
  - For example $\beta_{age}$

- *z-scores* standardize variables so that their unit of measurement no longer matters (QSS p. 103).


- To calculate the *z-score* of a variable $x$, we simply, substract off the mean and divide by the standard deviaton

$$ z\text{-score of x} = \frac{x_i - \mu_{x}}{\sigma_x} $$

- The *z-score* of Median Age is 

$$ z\text{-score of Median Age} = \frac{\text{Age}_i - \mu_{Age}}{\sigma_{Age}} $$

---
## Standardizing Predictors in R

```{r}
covid_lab %>%
  mutate(
    rep_voteshare_std = (rep_voteshare - mean(rep_voteshare))/sd(rep_voteshare),
    med_age_std = ( med_age - mean( med_age))/sd( med_age),
    med_income_std = (med_income - mean(med_income))/sd(med_income),
    percent_vaccinated_std = (percent_vaccinated - mean(percent_vaccinated))/sd(percent_vaccinated)
  ) -> covid_lab



```




---
## Multiple Regression with Standardized Predictors

If we were to estimate a model with standardized predictors:

$$\text{New Covid Deaths} = \beta_0 +\beta_1 \text{Rep. Vote Share}_{std} +\beta_2 \text{Median Age}_{std} +\beta_3 \text{Median Income}_{std} + \epsilon$$
The coefficients still tell us predicted change in New Covid Deaths for a unit change in a predicotr, but now a unit change corresponds to a 1-standard deviation increase of each predictor:

```{r}
covid_lab %>%
  summarise(
    sd_rep_vote = sd(rep_voteshare),
    sd_med_age = sd(med_age),
    sd_med_income = sd(med_income)
  )
```

---
## Saving Data

Finally, I'll save the data for Thursday's lab

```{r}
# Don't run this code
save(covid_lab, file = "../files/data/06_lab.rda")

```

And on Thursday, we'll be able to load the `covid_lab` just by running:

```{r}
load(url("https://pols1600.paultesta.org/files/data/06_lab.rda"))
```







---
class: inverse, center, middle
background-image: url("https://i.pinimg.com/originals/a2/05/b6/a205b689caf19f3287b1544cbe0e6b7b.jpg")
background-size: contain
# 📢
## Feedback

```{r feedback, echo=F, message=F}
df_feedback_full <- haven::read_spss("../files/data/wk05.sav")
df_feedback_full %>%
  filter(optout == 1)-> df_feedback
```

---
## What we liked

--

- The lab, or at least aspects of the lab like:
  - Working in groups
  - Building from copying code to writing code
  - Dealing with real world data/questions
- Pacing of lab/lecture
- Tutorials
- Math(!?)

---
## What we liked

```{r likes, echo=F, out.height='90%'}
DT::datatable(df_feedback %>% 
                select(Likes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 3
              )
              )

```



---
## What we disliked 


- Lab
  - Pacing/Length
  - Coding roadblocks/portals of discovery
- Connection between lecture and lab
- Math


---
## What we disliked

```{r dislikes, echo=F}
DT::datatable(df_feedback %>% 
                select(Dislikes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 3
              )
              )
2+2
```

---
## What we'll do

.pull-left[
### Me

- Keep the labs shorter
- Post the labs earlier
- Try different ways of teaching coding
  - Copy paste and run code
  - Run and comment code
  - Fill in the blank
  - Update and adapt code
  - Troubleshoot broken code
  - Write your own code
]

--

.pull-right[
### You

- Keep doing the tutorials
- Review the labs before class
- Use the structure of .Rmd file as your friend
- Use separate code chunks for exploratory code and final code
- Work through the commented labs



]

---
class: inverse, center, middle
background-image:url("https://media.giphy.com/media/KWwPcQwGetlA0qKdY0/giphy.gif")
background-size: cover

## Help me with my fit



---
```{r, echo=F}
df_feedback_full %>%
  select(starts_with("fit_"))%>%
  mutate_all(forcats::as_factor)%>%
  mutate_all(as.character)%>%
  pivot_longer(
    cols = starts_with("fit"),
    names_to = "Fit",
    values_to = "Options"
  )%>%
  mutate(
    Fit = str_to_title(gsub("fit_","", Fit))
  )%>%
  group_by(Fit,Options)%>%
  summarise(
    Vote = n(),
  )%>%
  arrange(Fit,Vote)%>%
  mutate(
    Rank =n():1,
    Fit = factor(Fit, levels = c("Palette","Jacket","Pant","Top","Pattern","Tie","Shoe")),
    Options = forcats::fct_reorder(Options, Vote)
  )%>%
  ungroup()%>%
  na.omit()%>%
  ggplot(aes(Options,Vote, fill=Rank))+
  geom_bar(stat = "identity")+
  facet_wrap(~Fit, scale="free_y",ncol = 2)+
  coord_flip()+
  guides(fill="none")



```

---
class: inverse, center, middle
background-image:url("https://www.blackenterprise.com/wp-content/blogs.dir/1/files/2015/08/Michael-Jordan_nike.com-2-1280x720.jpg")
background-size: cover


---
class:inverse, middle, center
# 🔍
## Review


---
## Review

- Casual Inference in Observational Designs
  - Difference-in-Differences
  - Regression Discontinuity Design
  - Instrumental Variables

- Simple Linear Regression

---
class:inverse, middle, center
# 🔍
## Casual Inference in Observational Designs

---
## Review: Casual Inference in Observational Designs


- Observational designs that try to estimate causal effects need to justify assumptions about conditional independence:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

--

- This assumption goes by many, jargony names: Selection on Observables, Conditional Independence, No unmeasured confounders.

--

- Credibility of this assumption depends less on having a lot of data, and more on how your data were generated.

--

- Three of the most common observational designs for causal inference are:

  - Difference in Difference Design

  - Regression Discontinuity Designs
  
  - Instrumental Variable Designs


---
## Review: Difference in Difference Design

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
  - Taking the pre-post difference removes any fixed differences between the units
  
  - Then taking the difference between treated and control differences removes any common differences over time

- The key identifying assumption of a DiD design is the "assumption of parallel trends"

- Common Diff-in-Diffs in political science:
  - Basically any time there's staggered adoption of policy overtime $\to$ Diff-in-Diff.
  

---
## Review: Regression Discontinuity Design

- A Regression Discontinuity Design (RDD):
 - leverages a naturally occuring discontinuity in the probability of receiving some treatment
 - uses regression to estimate the effect of that treatment **at the discontinuity**

- RDD are identified under assumptions of either:
  - Continuity at the cutoff
  - Local randomization
  
- The key idea is that at (continuity) or in some window around (local randomization), variation in the outcome of interest is due only change in the treatment

- Common discontinuities in political science:
  - First past the post elections
  - Borders
  - Eligibility cutoffs for policies

---
## Review: Instrumental Variables

Instrumental variables are an economists favorite tool for dealing with **omitted variable bias**

- We're worried that these effects are **confounded** by some unobserved, omitted variable, that influences both the treatment and the outcome

- We find an **instrumental variable** that is **exogenous** and **relevant** satisfying the following assumptions
  - Randomization 
  - Excludability
  - First-stage relationship
  - Monotonicity
- Allowing us estimate a Local Average Treatment Effect (LATE) using the instrument to isolate the variation in our treatment that is **exogenous** (uncorrelated with ommitted variables)

---
## IV Applications

```{r ivapps, echo=F}
knitr::include_graphics("https://pbs.twimg.com/media/EJGyHnyUYAA-yhM?format=jpg&name=large")
```

[@AndrewHeiss](https://twitter.com/andrewheiss/status/1193931226865901569?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1193931226865901569%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2Fandrewheiss2Fstatus2F1193931226865901569widget%3DTweet)


---
class:inverse, middle, center
# 🔍
# Simple Linear Regression
### Linear regression provides a linear estimate of the CEF

---
## Review: Simple Linear Regression

- **Conceptual**
  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function
  
```{r ols, eval=F}
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR): 

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$
  
---
## Linear Regression and the CEF

Recall the first model we fit in last week's lab

$$\text{new_deaths} = \beta_0 + \beta_1 \text{winner} + \epsilon$$

```{r}
m1 <- lm(new_deaths ~ winner, covid_df)
m1
```

`m1` is the equation for a very simple line defined by two points:

  - $\beta_0$ = E[new_deaths|winner = Trump]
  - $\beta_0 + \beta_1$ = E[new_deaths|winner = Biden]

---
## Linear Regression and the CEF

It's helpful to take a look at what's going on under the hood in `lm()`

We gave it the following data:

```{r}
head(m1$model)
```

---
## Linear Regression and the CEF


R doesn't know how to calculate the mean of `Biden` (or any character or factor data). 

Instead it creates an indicator variable, that takes a value of 1, any where `winner` equals `Biden` and 0 when `winner` equals `Trump`

```{r}
head(model.matrix(m1))
table(m1$model$winner)
table(model.matrix(m1)[,2])
```

---

```{r}
covid_df%>%
  mutate(
    winner01 = ifelse(winner=="Biden",1,0)
  )%>%
  ggplot(aes(winner01, new_deaths))+
  geom_point()+
  stat_summary(geom="point",fun=mean,col= "red",size=2)+
  geom_segment(aes(x=0,xend =1, 
                   y=coef(m1)[1],
                   yend = coef(m1)[1]+coef(m1)[2]*1
                   ),
               col= "red")

```

---

```{r}
covid_df %>%
  filter(date == "2021-09-23") %>%
  ggplot(aes(percent_vaccinated, new_deaths_pc_14da))+
  geom_point(size=.5,alpha=.5)+
  stat_summary_bin(fun = "mean", 
                   geom = "point", 
                   col="red",
                   bins =10
                   )+
  geom_smooth(method ="lm")
```

---
class: inverse, middle, center
# 💡 
## Multiple Regression

---
## Overiew: Multiple Regression

- **Conceptual**
  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function
  
```{r ols, eval=F}
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR): 

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$



# Mutliple Regression


- We'll spend the bulk of this course we working with linear models
    - Relatively simple to estimate and interpret
    - Some nice statistical properties
    - Foundation for understanding more complicated models

## Some Notation for Today 

- $Y$ some (for today) continuous outcome
- $X$ a continuous predictor
- $Z$ another continuous predictor
- $D$ a dichotomous predictor
- $G$ a categorical predictor with more than two levels
- $\beta_i$ the coefficient for predictor $i$
- $u_i$ the residual for $i$ (difference between observed and predicted)

---
## Data

Let's load some data from the 2016 NES

```{r}
load(url("https://pols1600.paultesta.org/files/data/nes.rda"))
# Look at data
dim(nes)
head(nes)
```

Let's explore how self-reported interest in politics, relates to perceptions of presidentational candidates (Clinton and Trump) in 2016. 

Suppose we're interested in the absolute difference in affect toward candidates -- that is, whether someone feels strongly positive about one candidate and strongly negative about another candidate. 

For simplicity, let's compare differences in absolute affect between those who say they follow politics closely, to those who say they don't.

First, we'll recode the data:

```{r}
# Recode
nes$interest01<-ifelse(nes$pol_interest==3,1,0)
nes$tc_diff<-abs(nes$ft_trump-nes$ft_hrc)

plot(nes$ft_hrc,nes$ft_trump)
abline(0,1)
par(mfrow=c(1,1))


```

Next, we'll formalize our question into a model

\[
\text{|FT}_{T} - \text{FT}_H| = \beta_0 + \beta_1 \text{Interest}
\]

What do the coefficients tell us?

Why might we expet the coefficient on $\beta_1$ to be positive? Why might we expect it to be negative?

## Candidate Differences modeled by Interest

```{r}
themodel<-lm(tc_diff~interest01,nes)
themodel
```

## Displaying the model

```{r,echo=F}
nes_good<-na.omit(nes[,c("tc_diff","interest01")])
set.seed(12345)
# Produce scatter plot with some 
pred_values<-predict(themodel)
themeans<-with(nes_good,tapply(tc_diff,interest01,mean,na.rm=T))
diff(themeans)
themodel
plot(tc_diff~jitter(interest01),data=nes,
     col="gray",axes=FALSE,pch=19,cex=.5,
xlab="Political Interest\n(0=Not Interested, 1=Interested)",
ylab="Abs. Diff in Candidate FTs",ylim=range(nes_good$tc_diff,na.rm=T))
axis(1,at=c(0,1),labels=c(0,1))
axis(2)
points(c(0,1),themeans,cex=2,col=c("purple","blue")) ## open circles
set.seed(12345) # So the interest jitter is the same as before
points(pred_values~jitter(interest01),data=nes_good,cex=.7)
abline(themodel)
arrows(0,themeans[1],1,themeans[1],length=.1)
arrows(1,themeans[1],1,themeans[2],length=.1)
text(x = 1,y=themeans[2]-6,labels = round(coef(themodel)[2],2),pos=4)
```

Now let's think about building models more generally, starting with the simplest possible model


## The Empty Model

\[
Y = \beta_0 + u_i
\]

What's your best prediction for some outcome, knowing nothing else?

```{r}
lm0<-lm(tc_diff~1,nes)
coef(lm0) 
mean(nes$tc_diff,na.rm=T)
ssr0 <- sum(resid(lm0)^2)
ssr0

sum((nes$tc_diff - mean(nes$tc_diff, na.rm=T))^2,na.rm=T)
nes$tc_diff[1]- mean(nes$tc_diff, na.rm=T)
```

A prediction of the unconditional mean of the outcome, will minimize the sum of squared errors. 

Now let's add some information, like whether people say they're interested in politics

## A model with a single dichotomous predictor

\[
Y= \beta_0 + \beta_1D_i + u_i 
\]

This is just the model from above telling us how feelings toward candidates differ by levels of political interest?

```{r}
lm1<-lm(tc_diff~interest01,nes)
coef(lm1)
mean(tc_diff~interest01,data=nes,na.rm=T)
diff(mean(tc_diff~interest01,data=nes,na.rm=T))
ssr1 <- sum(resid(lm1)^2)
ssr1 < ssr0
```

Note the sum of squared residuals has decreased from lm0 to lm1. Adding predictors increases the portion of total variance explained by our model (and decreases the portion unexplained)

Let's review how we would handle categorical predictor, say modeling absolute differences in feelings toward the candidate by each level of political interests

## A model with a categorical predictor

\[
Y_i= \beta_0 + \beta_1 G_i + u_i 
\]


```{r}
table(nes$pol_interest)
lm2<-lm(tc_diff~factor(pol_interest),nes)
head(model.frame(lm2))
head(model.matrix(lm2))
t(t(coef(lm2)))
```


- What just happened?
    - The factor() tells R to treat pol\_interest as if it were a categorical variable (rather than numeric)
    - When you put a factor variable into a formula, R drops the first level of the factor (pol\_interest=0) and creates indicators for each subsequent level of the factor (pol\_interest = 0-3)
    - The coefficients tell us how the mean of each level of the pol\_interest variable differs from the mean of the excluded category (given to us by the intercept)

```{r}

mu_0<-with(nes,mean(tc_diff[pol_interest==0],na.rm=T))
mu_1<-with(nes,mean(tc_diff[pol_interest==1],na.rm=T))
mu_2<-with(nes,mean(tc_diff[pol_interest==2],na.rm=T))
mu_3<-with(nes,mean(tc_diff[pol_interest==3],na.rm=T))
```

```{r}
coef(lm2)[1]
mu_0
coef(lm2)[2]
mu_1-mu_0
coef(lm2)[3]
mu_2-mu_0
coef(lm2)[4]
mu_3-mu_0
```

What if we hadn't used the factor function?

Then R would treat the pol\_interest variable as a numeric variable and we'd have:

##  A model with a single continous predictor

\[
Y_i= \beta_0 + \beta_1X_i + u_i 
\]

```{r}
lm3<-lm(tc_diff~pol_interest,nes)
coef(lm3)
```

We interpret the coefficient on $pol\_interest$ like the slope of a line (i.e. a 1 unite change in political interest is associated with `r round(coef(lm3)[2],2)` unit increase in perceived candidate differences)

- What provides a better fit?

```{r,echo=F}
pred.df<-data_frame(pol_interest=0:3)
pred.df$lm2_fit<-predict(lm2,pred.df)
pred.df$lm3_fit<-predict(lm3,pred.df)

with(nes, plot(x=jitter(pol_interest),y=tc_diff,pch=19,col="grey",cex=.2))
points(pred.df$pol_interest,pred.df$lm2_fit,col=pred.df$pol_interest+3,pch=19)
points(pred.df$pol_interest,pred.df$lm3_fit,col="red",pch=4)
abline(lm3,col="red")
with(na.omit(nes[,c("tc_diff","pol_interest")]),
             abline(h=mean(tc_diff,na.rm=T),col="darkgrey",lty=2))
with(na.omit(nes[,c("tc_diff","pol_interest")]),
abline(v=mean(pol_interest,na.rm=T),col="darkgrey",lty=2))


sum(resid(lm2)^2)<sum(resid(lm3)^2)
sum(resid(lm2)^2)-sum(resid(lm3)^2)
# Model 2 explains about 4 percent (but with two additional variables)
var(predict(lm2))/var(lm2$model$tc_diff)
summary(lm2)$r.squared
# Model 3 explains about 3 percent of the variation
var(predict(lm3))/var(lm3$model$tc_diff)
summary(lm3)$r.squared

```


Let's see what happens when we add a continuous predictor to our model, like age


## A model with both a dichtomous and continous predictor


\[
Y_i= \beta_0 + \beta_1D_i+\beta_2X_i + u_i 
\]


```{r,}
lm4<-lm(tc_diff~interest01+age,nes)
lm4
```

What have we done? 

Well one way to think about this model, is that we have a belief that perceptions about candidates are related to both people's levels of political interest and their age. 

We also think that interest and age may be related. 

So by fitting a model with both predictors, we attempt to control for the variance explained by one variable when trying to identify the variatio explained by the other variable.

That is, once we know all there is to know about age's relationship to interest  and age's relationship to perceptions of candidate differece, does political interest explain any additional variation in candidate perceptions (and vice versa, once we know the relationship between interest and perceptions, does age explain any additional variation)

We can do this manually, by estimating a series of bivariate regressions of affect on age and interest on age, and then regressing the residuals from the first model on the residuals from the second:


```{r}
# 1. Regress affect on age
lm4_affect_on_age <- lm(tc_diff~age, data=lm4$model)

# 2. Save portion of affect not explained by age
tc_diff_res <- lm4_affect_on_age$residuals

# 3. Check: Are residuals associated with age
round(cor(tc_diff_res, lm4_affect_on_age$model$age),5) 

# 4. Regress interest on age
lm4_interest_on_age <- lm(interest01 ~ age, data = lm4$model)
interest01_res <- lm4_interest_on_age$res

# 5. Check: Are residuals associated with age
round(cor(interest01_res, lm4_interest_on_age$model$age),5) 

# 6. Regress remaining variation in affect not explained by age
#    on variation in interest not explained by age
lm4_res <- lm(tc_diff_res ~ interest01_res)

# 7. Same as coefficient on interest01 from lm4
coef(lm4_res)[2]
coef(lm4)[2]

```


In terms of our interpetring our model graphically, we can think of the dichotomous predictor as shifting the intercept of a line whose slope is defined by the coefficient on age.

```{r,echo=F}
# Data frame for predictions at each combination of age and interest
pred.df2<-expand.grid(interest01=c(0,1),age=sort(unique(nes$age)))
head(pred.df2)
# Fitted values
pred.df2$lm4_fit<-predict(lm4,pred.df2)

# Plot data, colors for age and interest
with(nes, plot(x=age,y=tc_diff,pch=19,col=ifelse(nes$interest01==1,"green","purple"),cex=.2))

# Plot lines defined by model
abline(coef(lm4)[1],coef(lm4)[3],col="purple")
abline(coef(lm4)[1]+coef(lm4)[2],coef(lm4)[3],col="green")
# And predicted values
points(pred.df2$age,pred.df2$lm4_fit,col=ifelse(pred.df2$interest01==1,"green","purple"))
```


## A model with an interaction between D and X

Suppose we thought the relationship between age and perceptions depended on levels of interest. We could represent this model with an interaction:

\[
Y_i= \beta_0 +\beta_1D_i + \beta_2X_i + \beta_3D_i\times X_i
\]

- $\beta_1$ shifts the intercept 
    - From $\beta_0$ (D=0) to  $\beta_0+\beta_1$ (D=1)
- $\beta_3$ changes the slope
    - From $\beta_2$ (D=0) to  $\beta_2+\beta_3$ (D=1)

Let's fit that model

```{r}
# * is a short cut for writing the full interaction
lm5<-lm(tc_diff~interest01*age,nes)
lm5

```

And plot the results to see if the relationship between age and perceptions differs conditional on interest?

```{r,echo=F}
pred.df2$lm5_fit<-predict(lm5,pred.df2)
with(nes, plot(x=age,y=tc_diff,pch=19,col=ifelse(nes$interest01==1,"green","purple"),cex=.2))

abline(coef(lm5)[1],coef(lm5)[3],col="purple")
abline(coef(lm5)[1]+coef(lm5)[2],coef(lm5)[3]+coef(lm5)[4],col="green")
points(pred.df2$age,pred.df2$lm5_fit,col=ifelse(pred.df2$interest01==1,"green","purple"))

```

We could get the same two lines, by fitting separate models on just the interested and uninterested:

```{r}

# Subsetting with an index
lm5_uninterested <- lm(tc_diff ~ age, data = nes[nes$interest01 == 0,])

coef(lm5_uninterested)["age"]
# Same as
coef(lm5)["age"]

# Subsetting with an argument
lm5_interested <- lm(tc_diff ~ age, data = nes, subset = interest01 == 1)

coef(lm5_interested)["age"]
# Same as
coef(lm5)["age"] + coef(lm5)["interest01:age"]


```


## A model with two continuous predictors

\[
Y_i\sim 1 + X_i + Z_i
\]

- Essentially adding another dimension to our model

```{r}
nes$income<-ifelse(nes$faminc>16,NA,nes$faminc)
lm6<-lm(tc_diff~age+income,nes)
lm6
```

Which we can visualize:

```{r,echo=F}

s3d <-with(nes, scatterplot3d(age,income,tc_diff ,
      pch=16,cex.symbols=.5, type="p",angle=70))
s3d$plane3d(lm6,col="red")


```

## Multiplicative Interactions with a continuous moderator

\[
Y= \beta_0 + \beta_1X + \beta_2 +\beta_3X\times Z
\]

- The marginal effect of X on Y, now depends on the value of Z at which you evaluate the relationship

\[
\frac{\partial Y}{\partial X}=\beta_1+\beta_3Z
\]

Easier to show this with some simulated data:

```{r,echo=F}
set.seed(123)
X<-rnorm(200,5,2)
Z<-rnorm(200,5,2)
Y<-2+2*X+3*Z+-3*X*Z+rnorm(200,1,4)
m4<-lm(Y~X*Z)
nx <- seq(min(c(X,Z)), max(c(X,Z)), length.out = 50)
z <- outer(nx, nx, FUN = function(a, b) predict(m4, data.frame(X = a, Z = b)))
pmat<-persp(nx, nx, z, theta = 0, phi = 10, shade = 0.75, xlab = "X", ylab = "Z", 
    zlab = "Y")
depth3d <- function(x,y,z, pmat, minsize=0.2, maxsize=2) {

  # determine depth of each point from xyz and transformation matrix pmat
  tr <- as.matrix(cbind(x, y, z, 1)) %*% pmat
  tr <- tr[,3]/tr[,4]

  # scale depth to point sizes between minsize and maxsize
  psize <- ((tr-min(tr) ) * (maxsize-minsize)) / (max(tr)-min(tr)) + minsize
  return(psize)
}
psize = depth3d(X,Z,Y,pmat,minsize=0.1, maxsize = 1)
mypoints <- trans3d(X, Z, Y, pmat=pmat)
points(mypoints, cex=psize, col=4)

```

Another Perspective

```{r,echo=F}
pmat<-persp(nx, nx, z, theta = 45, phi = 10, shade = 0.75, xlab = "X", ylab = "Z", 
    zlab = "Y")
psize = depth3d(X,Z,Y,pmat,minsize=0.1, maxsize = 1)
mypoints <- trans3d(X, Z, Y, pmat=pmat)
points(mypoints, cex=psize, col=4)
```

# ICYI Some of the Math Behind OLS Regression

This isn't really central to our task in course (hence the ICYI), but is useful if you plan to continue on in future courses. So below is a brief discussion of how to estimate OLS regression with multiple predictors, it requires a little bit of linear algebra.


Last week we saw that in a bivariate (two variable) world, OLS chose coefficients to minimize the sum of squared errors


\[
\textrm{Find }\widehat{\beta_0},\,\widehat{\beta_1} \text{ argmin }_{\beta_0, \beta_1} \sum (Y-(\beta_0+\beta_1X))^2
\]

Giving us:

\[
\beta_0 = \bar{Y} - \beta_1 \bar{X_{1}}
\]
And
\[
\beta_1 = \frac{\sum (Y_i -\bar{Y})(X_{1,i}-\bar{X_1})}{\sum (\bar{X_1}-X_{1,i})^2}=\frac{Cov{[X,Y]}}{Var[X]}
\]

## OLS with Multiple Predicors

That's great, but what if we want to predict $Y$ with more than just one $X$, like:

\[
Y=\beta_0+\beta_1X_1+\beta_2X_2 + \dots +\beta_kX_k +\epsilon
\]

The criterion remains the same--we want to minizime the sum of the squared residuals $\sum \epsilon^2$, but we'll need to use matix algebra to simplify the math.

## OLS in matrix form

Spefically, we can write:

\[
Y=\beta_0+\beta_1X_1+\beta_2X_2 + \dots +\beta_kX_k +\epsilon
\]

As
$$
Y= \begin{bmatrix}
Y_1 	\\
Y_2 	\\
\vdots \\
Y_n 	
\end{bmatrix}
$$
$$
X=
\begin{bmatrix}
	 1  & X_{1,1}&\dots & X_{1,k}\\
 1  & X_{2,1}&\dots & X_{2,k}\\
 \vdots&\vdots&  & \vdots \\
 1  & X_{n,1}&\dots & X_{n,k}\\
\end{bmatrix}
$$
$$\beta=
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots \\
\beta_k\\
\end{bmatrix}$$

$$
\epsilon=
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{bmatrix}
$$

\[
Y=X\beta+\epsilon
\]


## Residuals

Our residual (error), is simply

\[
Y=X\beta+\epsilon
\]

\[
\epsilon=Y-X\beta
\]


Note that our $\epsilon$ is a $n\times$ column vector:


\[
\epsilon=
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{bmatrix}
\]
And it's transpose, $\epsilon^\prime$ is a $1\times n$ row vector

\[
\epsilon^\prime=[\epsilon_1,\epsilon_2,\dots,\epsilon_n]
\]


## Minimizing the sum of Squared Residuals with Matrix Algebra

Taking the inner product of a vector ($\epsilon^\prime\epsilon$) is the equivalent of $\sum \epsilon^2$

So our task becomes:

\[
\textrm{Find }\widehat{\beta} \text{ argmin }_{\widehat{\beta}} \sum \epsilon^2=\epsilon^\prime\epsilon=(Y-X\widehat{\beta})^\prime(Y-X\widehat{\beta})
\]

Distributing the transpose and expanding that last expression we get:

$$
\begin{align}
\epsilon^\prime\epsilon &=(Y-X\widehat{\beta})^\prime(Y-X\widehat{\beta})\\
&=Y^\prime Y-\widehat{\beta}^\prime X^\prime Y-Y^\prime X \widehat{\beta} +\widehat{\beta}^\prime X^\prime X\widehat{\beta}
\end{align}
$$

At this point, it's probably useful to have some toy numbers to illustrate what's going on.

## Example data with 5 observations and two predictors

```{r}
y=c(1,2,4,6,10)
x1=c(-3,3,2,4,8)
x2=c(1,5,3,5,2)
X<-cbind(1,x1,x2)
X


```

## Claim: $-\widehat{\beta}^\prime X^\prime Y$=$-Y^\prime X \widehat{\beta}$

Both $X^\prime Y$ and $Y^\prime X$ yield vectors with the same values 

```{r}
# t() is R's transpose function (switch index of rows and columns)
t(X)%*%y
t(y)%*%X
```

Which when pre multiplied by some $\widehat{\beta}^\prime$ (e.g.  $\widehat{\beta}^\prime X^\prime Y$) or post-multipled by $\widehat{\beta}$ (ie.. $-Y^\prime X \widehat{\beta}$), give the same value.

```{r}
# Pick a random vector for illustration
beta_ex<-1:3
t(beta_ex)%*%t(X)%*%y
t(y)%*%X%*%beta_ex
```

## Minimizing the sum of Squared Residuals with Matrix Algebra

So now we can combine the second and third terms:

$$
\begin{align}
\epsilon^\prime\epsilon &=(Y-X\widehat{\beta})^\prime(Y-X\widehat{\beta})\\
&=Y^\prime Y-\widehat{\beta}^\prime X^\prime Y-Y^\prime X \widehat{\beta} +\widehat{\beta}^\prime X^\prime X\widehat{\beta}\\
&=Y^\prime Y-2\widehat{\beta}^\prime X^\prime Y+\widehat{\beta}^\prime X^\prime X\widehat{\beta}
\end{align}
$$

## Claim: $X^{\prime}X$ is symmetric 

A symmetric matrix is one where

\[
A^\prime=A
\]

Only square matrices can be symmetric

Let's look at our toy example

```{r}
XtX<-t(X)%*%X
XtX
t(XtX)
```


Let's look at our toy example

```{r}
XtX==t(XtX)
```

## First Order Conditions

Ok, so now we just need to take the derivative of $\epsilon^{\prime}\epsilon$ with respect to $\widehat{\beta}$ and set it 0

\[
\frac{\partial[\epsilon^{\prime}\epsilon]}{\partial\widehat{\beta}}=-2X^\prime Y+2X^{\prime}X\widehat{\beta}=0
\]

## Matrix Differentiation

For $k\times1$ vectors $a$ and $b$:

\[
\frac{\partial[a^\prime b]}{\partial b}=\frac{\partial[b^\prime a]}{\partial b}=a
\]

When $A$ is a symmetric matrix

\[
\frac{\partial[a^\prime b]}{\partial b}=2Ab=2b^\prime A
\]

\[
\frac{\partial[2\widehat{\beta} X^{\prime}Y ]}{\partial\widehat{\beta}}=2X^{\prime}Y
\]

And

\[
\frac{\partial[2\widehat{\beta} X^{\prime}X\widehat{\beta} ]}{\partial\widehat{\beta}}= \frac{\partial[\widehat{\beta}^\prime A \widehat{\beta}]}{\partial \widehat{\beta}}= 2X^{\prime}X\widehat{\beta}
\]

## First Order Conditions

Ok, so now we just need to take the derivative of $\epsilon^{\prime}\epsilon$ with respect to $\widehat{\beta}$ and set it 0

\[
\frac{\partial[\epsilon^{\prime}\epsilon]}{\partial \widehat{\beta}}=-2X^\prime Y+2X^{\prime}X\widehat{\beta}=0
\]


$$
\begin{align}
-2X^{\prime}Y+2X^{\prime}X\widehat{\beta}&=0\\
2X^{\prime}X\widehat{\beta}&= 2X^{\prime}Y\\
X^{\prime}X\widehat{\beta}&= X^\prime Y\\
\left[X^{\prime}X\right]^{-1}X^{\prime}X\widehat{\beta} &= \left[X^{\prime}X\right]^{-1}X^{\prime}Y\\
I\widehat{\beta}&= \left[X^{\prime}X\right]^{-1} X^{\prime}Y\\
\widehat{\beta}&= \left[X^{\prime}X\right]^{-1} X^{\prime}Y
\end{align}
$$

## Second Order Conditions

If we were to take the derviative of $\epsilon^{\prime}\epsilon$ a second time we'd get $2X^{\prime}X$ , which provided x has full rank is a positive definite matrix meaning $\widehat{\beta}$ provide a minium for $\epsilon^{\prime}\epsilon$

## Simple example

```{r}
# Math
solve(XtX)%*%t(X)%*%y
# R
lm1<-lm(y~x1+x2,data.frame(y,x1,x2))
lm1
```


## Properties of OLS

Recall

\[
Y=X\widehat{\beta}+\epsilon
\]

Plug in to (normal form) equqtions

$$
\begin{align}
X^{\prime}X\widehat{\beta}=X^{\prime}Y\\
X^{\prime}X\widehat{\beta}=X^{\prime}[X\widehat{\beta}+\epsilon]\\
X^{\prime}X\widehat{\beta}=X^{\prime}X\widehat{\beta}+X^{\prime}\epsilon\\
X^{\prime}\epsilon=0
\end{align}
$$

This is simply a result of our fitting procedure. That is it will always be true for the model we fit to some data


```{r}
e=resid(lm1)
```

From $X^{\prime}\epsilon=0$ a number of properties follow

1. Observed values of $X$ are uncorrelated with $\epsilon$
    
    ```{r}
round(cor(cbind(x1,x2,e)),2)
```


2. The $\sum \epsilon$ and sample mean of $\epsilon$ are 0

```{r}
sum(e);mean(e)
```


3. The models predictions include the conditional means of $Y$ and $X$
    
    ```{r}
predict(lm1,newdat=data.frame(x1=mean(x1),x2=mean(x2)))
mean(y)
```


4. The models prediction $\widehat{Y}$ are uncorrelated with $\epsilon$
    
    ```{r}
round(cor(cbind(yhat=fitted(lm1),e)),2)
```


## What are the assumptions necessary to justify OLS?

Read 10 textbooks, get 10 different answers about assumptions "necessary" for OLS

- Nature of the data
- About the structure of the relationship
- About the distribution of the errors

"Necessary" typically means something like necessary for OLS to be unbiased or provide correct standard errors.

Here's an example of the Assumtions for OLS from a PoliSci text (Lewis-Beck) 

1. No specification error
- The relationship between Y and X is linear
- We've got the correct model
2. No measurement error
- Measurement error in Y vs in X have different implications
3. A well-behaved error term, $\epsilon$
- $E[\epsilon]=0$ (zero mean)
- $E[\epsilon^2]=\sigma^2$ (homoscedasticity)
- $E[\epsilon_i\epsilon_j]=0 \forall i \neq j$ (no correlation in errors)
- $\epsilon \sim N(0,\sigma^2)$ (Normality)

Properties of OLS under these assumptions

- The estimated coefficients from OLS are the best linear unbiased estimator (BLUE) of the population parameters, where best means lowest variance (among linear unbiased estimators)

## An Econometric Statement

1. Linear in parameters and correct specification
2. X has full rank
3. Explanatory variables are exogenous
4. Errors are independent and identically distributed
5. In the population, errors are normally distributed

## Linear in parameteres and correct spefication

\[
y=X\beta+u
\]

The relationship between X and Y is linear in parameters (i.e. a change in X is associated with same change in Y)

Violations: non-linear relationship, omitted variables

## X has full rank

Essentially no variable can be expressed as a linear combination of some other variables (Why we had to drop one level of the treatment when estimating the ATE for the Findley data)

Violations: Including indicators for both male and female in a model with an intercept

## Explanatory variables are exogenous

\[
E[\epsilon|X]=0
\]

- X is fixed in repeated sampling and all estimates are conditional on X, or X is random and independent of population residuals
- Violations: omitted variables, measurement error and simultaneity

## Errors are independent and identically distributed

\[
\epsilon \sim iid(0,\sigma^2)
\]

- What it's saying:
    - $E[\epsilon]=0$ in the population
    - $E[\epsilon\epsilon^\prime]=\Omega=\sigma^2I$ in the population

- $Var[\epsilon]=\sigma^2$ for all $i$
    - $Cov[\epsilon_i,\epsilon_j]=0$ for all $i$
    - Violations: omitted variables, heteroscedasticity, auto-correlation (multiple observations of the same unit over time, spatial analysis)

#  ICYI Bias and Variance of OLS

## OLS is unbiased estimator of $\beta$

$$
\begin{align}
\widehat{\beta} &= \left[X^{\prime}X\right]^{-1}X^{\prime}Y\\
\widehat{\beta} &= \left[X^{\prime}X\right]^{-1}X^{\prime}(X\beta+\epsilon)\\
\widehat{\beta} &= \left[X^{\prime}X\right]^{-1}X^{\prime}X\beta+\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon\\
\widehat{\beta} &= \beta+\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon
\end{align}
$$


If $X$ is fixed, then $E[\epsilon]=0$ and

$$
\begin{align}
E[\widehat{\beta}]&=E[\beta]+E[\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon]\\
E[\widehat{\beta}]&=E[\beta]+\left[X^{\prime}X\right]^{-1}X^{\prime}E[\epsilon]\\
E[\widehat{\beta}]&=E[\beta]
\end{align}
$$

Or $X$ is random but independent of $\epsilon$ so $E[X^{\prime}\epsilon]=0$

$$    
\begin{align}
E[\widehat{\beta}]&=E[\beta]+E[\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon]\\
E[\widehat{\beta}]&=E[\beta]+\left[X^{\prime}X\right]^{-1}E[X^{\prime}\epsilon]\\
E[\widehat{\beta}]&=E[\beta]
\end{align}
$$

## Variance of OLS estimators

Recall:

\[
\widehat{\beta} = \beta+\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon
\]

So the variance of $\widehat{\beta}$

\[
E[(\widehat{\beta}-\beta)(\widehat{\beta}-\beta)^\prime]=E[(\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon)(\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon)^\prime]
\]

Recalling from previous math classes, $(AB)^\prime=B^\prime A^\prime$, we can write

\[
E[(\widehat{\beta}-\beta)(\widehat{\beta}-\beta)^\prime]=E[(\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon)(\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon)^\prime]
\]

As:

\[
E[(\widehat{\beta}-\beta)(\widehat{\beta}-\beta)^\prime]=E[(\left[X^{\prime}X\right]^{-1}X^{\prime}\epsilon\epsilon^\prime X\left[X^{\prime}X\right]^{-1}]
\]


Assuming a fixed $X$, $E[\epsilon\epsilon^\prime=\sigma^2I]$ and so

$$
\begin{align}
E[(\widehat{\beta}-\beta)(\widehat{\beta}-\beta)^\prime]&=\left[X^{\prime}X\right]^{-1}X^{\prime}E[\epsilon\epsilon^\prime ]X\left[X^{\prime}X\right]^{-1}\\
&=\left[X^{\prime}X\right]^{-1}X^{\prime}[\sigma^2 I]X\left[X^{\prime}X\right]^{-1}\\
&=\sigma^2 I\left[X^{\prime}X\right]^{-1}X^{\prime}X\left[X^{\prime}X\right]^{-1}\\
&=\sigma^2 \left[X^{\prime}X\right]^{-1}
\end{align}
$$

Where we can estimate $\sigma^2$ from the data

\[
\widehat{\sigma}^2=\frac{\epsilon^{\prime}\epsilon}{n-k}
\]


## Variance Covariance Matrix of OLS Estimator

$$
\begin{align}
Var[\widehat{\beta}]&=E[(\widehat{\beta}-\beta)(\widehat{\beta}-\beta)^\prime]\\
&=
\begin{bmatrix}
Var[\widehat{\beta}_1]& Cov[\widehat{\beta}_1,\widehat{\beta}_2] &\dots&Cov[\widehat{\beta}_1,\widehat{\beta}_k]	\\
Cov[\widehat{\beta}_2,\widehat{\beta}_1] &Var[\widehat{\beta}_2] &\dots&Cov[\widehat{\beta}_1,\widehat{\beta}_k]	\\
\vdots  &\vdots &\vdots &\vdots 	\\
Cov[\widehat{\beta}_k,\widehat{\beta}_1] &Cov[\widehat{\beta}_k,\widehat{\beta}_2] &\dots&Var[\widehat{\beta}_k,\widehat{\beta}_k]	
\end{bmatrix}
\end{align}
$$

## Omitted Variable Bias

What happens if we fit:

\[
y=X\beta
\]

When in truth, the real relationship is

\[
y=X\beta + {Z}\gamma+ u
\]

Doing the math

\[
\widehat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y
\]

Plugging the true values of y in

$$
\begin{align}
\widehat{\beta}&=(X^{\prime}X)^{-1}X^{\prime}X(\beta +{Z}\gamma+\epsilon)\\
&=(X^{\prime}X)^{-1}X^{\prime}X\beta+ +(X^{\prime}X)^{-1}X^{\prime}{Z}\gamma+(X^{\prime}X)^{-1}X^{\prime}\epsilon)\\
&=\beta +(X^{\prime}X)^{-1}X^{\prime}{Z}\gamma+(X^{\prime}X)^{-1}X^{\prime}\epsilon)
\end{align}
$$


Taking expecations, conditional on only $X$
$$
\begin{align}
E[\widehat{\beta}|X]&=\beta+(X^{\prime}X)^{-1}X^{\prime}{Z}\gamma\\
&\beta + OVB
\end{align}
$$


"Short equals long plus the effect of the ommitted times the regression of the ommitted on the included"

- OVB a violation of conditional independence, estimates are biased, and often we don't know the direction of that bias.

---
# Summary

- Multiple regression is a generalization of bivariate regression "to control" for more than one variable.
- Multiple regression tells us how our outcome of interest will change when a predictor changes, holding all the other predictors in the model constant.
- Once you've fit a model, to make predictions, simply plug in values for the variables, multiply them by the associated coefficients and evaluate the equation.
- Interaction models allow the relationship (slope) between Y and X to vary based on the value of some other variable Z.
- Omitted variable bias, means are estimated coefficients will be wrong. Generally, we don't know the direction of this error.


  

