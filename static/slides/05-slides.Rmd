---
title: "Week 05:"
subtitle: "Prediction with Linear Regression"
author: "Paul Testa"
output:
  xaringan::moon_reader:
    css: ["default", "css/brown.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300,
  fig.align = "center", out.width = "80%", cache = FALSE)
library("tidyverse")
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


```{r packages, include=F}
the_packages <- c(
  ## R Markdown
  "kableExtra","DT",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", #<<
  # Analysis
  "DeclareDesign", "easystats", "zoo"#<<
)
```

```{r ipak, include=F}
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

```


```{r loadpackages, include=F}
ipak(the_packages)
```

---
class: inverse, center, middle
# Overview

---
## General Plan

- Setup
  - Packages
  - Data
- Feedback
- Review
  - Casual inference
  - Covariate Adjustment
- Linear Models
  - The Basics
  - The Mechanics
  - The Intuitions
- Three Common Observational Designs
  - Difference in Difference
  - Regression Discontinuity
  - Instrumental Variable Designs
- Summary


---
class:inverse, middle, center
# üí™
## Get set up to work

---
## New packages

Hopefully, you were all able to install the following packages 

```{r, eval=FALSE}
install.packages("dataverse")
install.packages("tidycensus")
install.packages("easystats")
install.packages("DeclareDesign")

```



---
## Packages for today


```{r, ref.label=c("packages")}

```

---
## Define a function to load (and if needed install) packages


```{r, ref.label="ipak"}
```

---
## Load packages for today

```{r ref.label="loadpackages"}
```


---
class:inverse, center, middle
# üí™
## Load Data for today

---
## Load the Covid-19 Data

```{r covid}
covid <- COVID19::covid19(
  country = "US",
  level = 2,
  verbose = F
)
```

---
## Transform Covid-19 Data

```{r covidus}
# Vector containing of US territories
territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )

# Filter out Territories and create state variable
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)%>%
  mutate(
    state = administrative_area_level_2
  )
```

---
## New Cases


```{r newcases}
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000
    ) -> covid_us
```

---
## Facemask Policy

```{r}
covid_us %>%
mutate(
  # Recode facial_coverings to create face_masks
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ),
    # Turn face_masks into a factor with ordered policy levels
    face_masks = factor(face_masks,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -> covid_us
```

---
## Dates

```{r dates}
covid_us %>%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = "-"),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```


---
## Load Data on Presidential Elections

```{r}
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

pres_df <- get_dataframe_by_name(
  "1976-2020-president.tab",
  "doi:10.7910/DVN/42MVDX"
)
```

---
## HLO of Presidential Elections Data

```{r}
head(pres_df)
```

---
## Transform Data to get just 2020 Electoin

```{r}
pres_df %>%
  mutate(
    state = str_to_title(state)
  ) %>%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%>%
  filter(year == 2020) %>%
  select(state, year, party_simplified, candidatevotes, totalvotes
         ) %>%
  spread(party_simplified,candidatevotes) %>%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes,
    rep_voteshare = REPUBLICAN/totalvotes,
    winner = ifelse(rep_voteshare > dem_voteshare,"Trump","Biden")

  ) -> pres2020_df
```


---
## Load Data on Median State Income from the Census

```{r}
acs_income <- get_acs(geography = "state", 
              variables = c(medincome = "B19013_001"), 
              year = 2019)
```

---
## HLO: Census Data 

```{r}
glimpse(acs_income)
```


---
## Tidy Census Data

```{r}
acs_income %>%
  mutate(
    state = NAME,
    median_income = estimate
  ) -> acs_income
```


---
## Merge election data and covid data into single `df`

.pull-left[
- We're going to take our `covid_us` data and **merge** into this data on the 2020 election from `pres2020_df` using the common `state` variable in each data set for a `left_join()` 

- Always check the matches in your joining variable (i.e. `state`)

- Below we see that our recoding of state to title case in created a mismatch
]

.pull-right[
```{r}
# Should be 51
sum(pres2020_df$state %in% covid_us$state)
# Find the mismatch:
pres2020_df$state[!pres2020_df$state %in% covid_us$state]
# Fix
pres2020_df$state[pres2020_df$state == "District Of Columbia"] <- "District of Columbia"
# Problem Solved
sum(pres2020_df$state %in% covid_us$state)

```
]

---
## Merge election data into Covid data

```{r}
dim(covid_us)
dim(pres2020_df)
df <- covid_us %>% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(df) # Same number of rows as covid_us w/ 7 additional columns
```


---
## Merge census data into Covid data

```{r}
# Should be 51
dim(df)
dim(acs_income)
df <- df %>% left_join(
  acs_income,
  by = c("state" = "state")
)
dim(df)  # Same number of rows as covid_us w/ 6 additional columns

```


# üì¢
## Feedback

```{r, echo=F, message=F}
df <- haven::read_spss("../files/data/wk03.sav")
df %>%
  mutate(
    Reincarnation = reincarnation,
    Why = creature,
  ) -> df
```

---
## What we liked

--

- Likes here

---
## What we liked

```{r likes, echo=F, out.height='90%'}
DT::datatable(df %>% 
                select(Likes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 4
              )
              )
```


---
## What we disliked

--

- Dislikes here

---
## What we disliked

```{r dislikes, echo=F}
DT::datatable(df %>% 
                select(Dislikes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 3
              )
              )
```


---
class:inverse, middle, center
# üîç
## Review


---
## Review

- Casual inference
  
- Covariate Adjustment

---
class:inverse, middle, center
# üîç
## Causal Inference

---
## Review: Causal Inference 

- Causal inference is about counterfactual comparisons

- Some counterfactuals are easier to imagine or create than others

- Randomization solves the fundamental problem of causal inference allowing us to estimate average treatment effects free from selection bias

- Randomization is not always possible, desirable, or ethical

- Observational designs that try to estimate causal effects need to justify assumptions about conditional independence:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$
- This assumption goes by many, jargony names: Selection on Observables, Conditional Independence, No unmeasured confounders.
- Credibility of this assumption depends less on having a lot of data, and more on how your data were generated.

- Observational designs estimate the effect of $D$ conditional on $X$ using covariate adjustment.


---
class:inverse, middle, center
# üîç
## Covariate Adjustment

---
## Review: Covariate Adjustment

- Covariate adjustment are a set of statistical procedures that allow us to estimate conditional values 
  - $e.g.(E[Y|X=x]), E[Happiness| leetCoder == T]$

- We adjust for covariates to improve our predictions and make credible comparisons
  - $E[\text{New Covid-19 cases}|\text{Face mask policy}]$ vs $E[\text{New Covid-19 cases}|\text{Face mask policy |June 2020 },]$
  - $ATE = E[Y| D = 1] - E[Y | D=0]$

- Three approaches:
  - Subclassification
  - Matching
  - Regression

---
## Subclassification

- Subclassification is a simple way to adjust for a covariate
  - Subset the data to include only the values you want ($X=x$, $D=1$, $Interverntion = Treated$) and calculate the quantity of interest (e.g. a conditional mean, `mean(df$income[df$age <30])`)
  - But what if we want to control for more than one variable? 
  - What if our variables aren't categorical like sex, but continuous like height?

- **The Curse of Dimensionality** as you attempt to adjust for more covariates (add more dimensions), the space of possible combinations grows exponentially
  - Assumption of Common Support likely to be violated $0 < Pr(D_i = 1|X_i) < 1$ 

---
## Matching

- Matching refers to a broad set of procedures that try to recreate what randomization provides: **covariate balance**
  - **covariate balance** is a fancy term for saying that in the aggregate, two groups look similar accept on group received the *treatment* $(D=1)$ while another group did not $(D=0)$

- There are many ways to try to match observations:
  - **Exact matching:** Find exact matches between treatment and control observations for all covariates $X$. Only works for a few covariates.
  - **Coarsened exact matching:** Find approximate matches within ranges of values for $X$
  - **Matching on summaries of the covariates** calculating  a single measure of the similarility of observations and matching on this summary to produce covariate balance.

- Matching is 
  - conceptually appealing (mirrors the logic of an experiment)
  - technically complex (complicated algorhitms, finicky software) 
  - Only provides balance on **observed covariates**



---
class: inverse, center, middle
# üí°  
# Linear Regression
## The Basics

---
## Regression

Broadly regression is a statistical procedural to help us model relationships that consists of:

- $Y$ an **outcome variable** or thing we're trying to explain
  - AKA: The Dependent Variable, The Response Variable, The Lefthand side of the model

- $X$ a set of **predictor variables** or things we think explain variation in our outcome
  - AKA: The independent variable, covariates, the right hand side of the model.

- $\beta$ a set of **unknown paramters** that describe the relationship between our outcome $Y$ and our predictors $X$

- $\epsilon$ the **error term** representing variation in $Y$ not explained by our model.


---
## Linear Regression

Today let's return to the   simple (bivariate) linear regressions we introduced last week:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- We call this a bivariate regression, because there are only two variables.

- We call this a linear regression, because $y_i = \beta_0 + \beta_1 x_i$ is the equation for a line, where:
  - $\beta_0$ corresponds to the $y$ intercept, or the model's prediction when $x = 0$.
  - $\beta_1$ corresponds to the slope, or how $y$ is predicted to change as $x$ changes.

---
## Linear Regression 
  
- If you find this notation confusing, try plugging in substantive concepts for what $y$ and $x$ represent
- Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.

The generic linear model

$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$

Reflects:

$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$

---
## Estimating a Linear Regression

- We estimate linear regressions in `R` using the `lm()` function.
- `lm()` requires two arguments:
  - a formula of the general form `y ~ x` read as "Y modeled by X" or below "Transgender Feeling Thermometer's modeled by Age
  - a `data` telling R 
  

```{r lmdata}
load(url("https://pols1600.paultesta.org/files/data/03_lab.rda"))
```

```{r m1}
m1 <- lm(therm_trans_t0 ~ vf_age, data = df)
m1

```

---
```{r figlm1code, echo = F}
fig_lm <- df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  geom_abline(intercept = coef(m1)[1],
              slope = coef(m1)[2],
              col = "blue"
              )+
  geom_vline(xintercept = 0,linetype = 2)+
  xlim(0,100)+
  annotate("point",
           x = 0, y = coef(m1)[1],
           col= "red",
           )+
  annotate("text",
           label = expression(paste(beta[0],"= 62.81" )),
           x = 1, y = coef(m1)[1]+5,
           hjust = "left",
           )+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People"
  )+
  theme_classic()
fig_lm
```

---
```{r figlm1, echo=F}
# devtools::install_github("nicolash2/ggbrace")
library(ggbrace)
fig_lm +
  ylim(48, 52)+
  xlim(58, 62)+
  geom_segment(aes(
    y = coef(m1)[1] + coef(m1)[2]*61,
    yend = coef(m1)[1] + coef(m1)[2]*60,
    x = 60, xend = 60),
    arrow = arrow(ends = "first",length = unit(0.015, "npc")),
    col = "red"
    )+
    geom_segment(aes(
    y = coef(m1)[1] + coef(m1)[2]*61,
    yend = coef(m1)[1] + coef(m1)[2]*61,
    x = 60, xend = 61),
    arrow = arrow(ends = "last",length = unit(0.015, "npc")),
    col = "red"
    )+
   geom_curve(aes(
    y = 49.7,
    yend = 50.53,
    x = 59.43, xend = 59.8),
    curvature = -.2,
    arrow = arrow(ends = "last",length = unit(0.015, "npc")),
    col = "black",
    )+
    annotate("text",
           label = expression(paste(beta[1],"= -0.21" )),
           x = 59, y = 49.5,
           hjust = "left",
           )+
  geom_brace(aes(c(59.9, 59.98),c(50.43, 50.63)), 
             inherit.data=F,
             rotate = 270)
```


---
background-image:url("https://imgflip.com/i/66mx9d")
background-size:contain

---
class: inverse, center, middle
# üí°  
# Linear Regression
## The Mechanics of OLS

---
## How did `lm()` choose $\beta_0$ and $\beta_1$

P: By minimizing the sum of squared errors.

Q: Ok, that's not really that helpful...

Q: What's an error?

Q: Why would we square and sum them

Q: How do we minimize them.

P: Good questions!

---
## What's an error?

An error, $\epsilon_i$ is simply the difference between the observed value of $y_i$ and what our model would predict, $\hat{y_i}$ given some value of $x_i$. So for a model:

$$
y_i=\beta_0+\beta_1 x_{1,i} + \epsilon_i
$$

We simply subtract our model's prediction $\beta_0+\beta_1 x_{1,i}$ from the the observed value, $y_i$

$$
\hat{\epsilon_i}=y_i-\hat{y_i}=(Y_i-(\beta_0+\beta_1 x_{1,i}))
$$

To get $\epsilon_i$

---
## Why are we squaring and summing $\epsilon$

There are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)

  - Squaring $\epsilon$ treats positive and negative values equally.
  
  - Summing produces single value summarizing our models overall performance.


---
## How do we minimize $\sum \epsilon^2$

OLS chooses $\beta_0$ and $\beta_1$ to minimize $\sum \epsilon^2$, the Sum of Squared Residuals (SSR)

$$
\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (Y-(\beta_0+\beta_1X))^2
$$

---
## How do we minimize $\sum \epsilon^2$

To understand what's going on under the hood, you need a broad understanding of some basic calculus.

The next few slides provide a brief review of derivatives and differential calculus.


---
## Derivatives


The derivative of $f$ at $x$ is its rate of change at $x$

- For a line: the slope
- For a curve: the slope of a line tangent to the curve

You'll see two notations for derivatives:

1. Leibniz notation:

$$
\frac{df}{dx}(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

2. Lagrange: $f^{\prime}(x)$


---
## Some useful Facts about Derivatives

Derivative of a constant

$$
f^{\prime}(c)=0
$$

Derivative of a line f(x)=2x

$$
f^{\prime}(x)=2
$$

Chain rule: y= f(g(x)). The derivative of y with respect to x is

$$
\frac{d}{dx}(f(g(x)))=f^{\prime}(g(x))g^{\prime}(x)
$$

The derivative of the "outside" times the derivative of the "inside," remembering that the derivative of the outside function is evaluated at the value of the inside function.

---
## Finding a Local Minimums


Local minimum:

$$
f^{\prime}(x)=0 \text{ and } f^{\prime\prime}(x)>0 
$$

.pull-right[

```{r}
knitr::include_graphics("https://copingwithcalculus.com/SecondDeriv1.png")
```
[Source](https://copingwithcalculus.com/SecondDerivativeTest.html)
]


---
## Partial Derivatives

Let $f$ be a function of the variables $(X_1, \dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is

$$\begin{align*}
\frac{\partial f(X_1, \dots, X_n)}{\partial X_i}=\lim_{h\to0}\frac{f(X_1, \dots X_i+h \dots, X_n)-f(X_1, \dots X_i \dots, X_n)}{h}
\end{align*}$$

```{r}
knitr::include_graphics("https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png")
```
[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)

---
## Minimizing the sum of squared errors

Our model

$$
Y_i =\beta_0+\beta_1X_{1,i}+\epsilon_i
$$

Finds coefficients $\beta_0$ and $\beta_1$ to to minimize the sum of squared residuals, $\hat{\epsilon}_i$:

$$\begin{alignat*}{1}
\sum \hat{\epsilon_i}^2 &= \sum (Y_i-\beta_0-\beta_1 X_{1,i})^2
\end{alignat*}$$


---
## Minimizing the sum of squared errors

We solve for $\beta_0$ and $\beta_1$, by taking the partial derivatives with respect to $\beta_0$ and $\beta_1$, and setting them equal to zero

$$
\begin{alignat*}{1}
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial \beta_0} &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})=0\\
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial\beta_1} &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})X_{1,i}=0
\end{alignat*}
$$


---
## Solving for $\beta_0$

First, we'll solve for $\beta_0$, by multiplying both sides by -1/2 and distributing the $\sum$:

$$\begin{aligned}
0 &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})\\
\sum \beta_0 &= \sum Y_i - \sum \beta_1 X_{1,i}\\
N \beta_0 &= \sum Y_i -\sum \beta_1 X_{1,i}\\
\beta_0 &= \frac{\sum Y_i}{N} - \frac{\beta_1 \sum X_{1,i}}{N}\\
\beta_0 &= \bar{Y} - \beta_1 \bar{X_{1}}
\end{aligned}$$

---
## Solving for $\beta_1$

Now, we can solve for $\beta_1$ plugging in $\beta_0$.

$$\begin{aligned}
0 &= -2\sum [(Y_i-\beta_0-\beta_1 X_{1,i})X_{1,i}]\\
0 &= \sum [Y_iX_i-(\bar{Y} - \beta_1 \bar{X_{1}})X_{1,i}-\beta_1 X_{1,i}^2]\\
0 &= \sum [Y_iX_i-\bar{Y}X_{1,i} + \beta_1 \bar{X_{1}}X_{1,i}-\beta_1 X_{1,i}^2]
\end{aligned}$$

---
## Solving for $\beta_1$

Now we'll rearrange some terms and pull out an $X_{1,i}$ to get


$$\begin{aligned}
0 &= \sum [(Y_i -\bar{Y} + \beta_1 \bar{X_{1}}-\beta_1 X_{1,i})X_{1,i}]
\end{aligned}$$

Dividing both sides by $X_{1,i}$ and distributing the summation, we can isolate $\beta_1$

$$\begin{aligned}
\beta_1 \sum (X_{1,i}-\bar{X_1}) &= \sum (Y_i -\bar{Y})
\end{aligned}$$

Dividing by $\sum (X_{1,i}-\bar{X_1})$ to get

$$\begin{aligned}
\beta_1  &= \frac{\sum (Y_i -\bar{Y})}{\sum (X_{1,i}-\bar{X_1})}
\end{aligned}$$

---
## Solving for $\beta_1$

Finally, by multiplying by $\frac{(X_{1,i}-\bar{X_1})}{(X_{1,i}-\bar{X_1})}$ we get

$$\begin{aligned}
\beta_1  &= \frac{\sum (Y_i -\bar{Y})(X_{1,i}-\bar{X_1})}{\sum (\bar{X_1}-X_{1,i})^2}
\end{aligned}$$

Which has a nice interpretation: 

$$
\begin{aligned}
\beta_1 &= \frac{Cov(X_1,Y)}{Var(X_1)}
\end{aligned}
$$

So the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat! 


---
class: inverse, center, middle
# üí°  
# Linear Regression
## What you need to know.



Let's calculate the simple regression coefficients for age predicted by vaccine beliefs

```{r,echo=T}
wrk<-na.omit(nes16[,c("age","vaccines")])
# Variance Covariance of age and vaccines
vcov1<-var(wrk)
vcov1
# Means
age_mn<-mean(wrk$age)
vac_mn<-mean(wrk$vaccines)
beta1<-vcov1[1,2]/vcov1[2,2]
beta0<-age_mn-beta1*vac_mn
```




---
## Why minimize $\sum{\hat{\epsilon_i}^2}$


Substantively, by minimizing the sum of squared residuals, we're trying to minimize the distance between our models predictions and the observed data.

Suppose the true relationship between y and x is:

$$
y~1+2x + \epsilon
$$

If we simulated some data where this was true:

```{r}
set.seed(123)
x <- rnorm(20)
y <- 1 + 2*x + rnorm(20)
m0 <- lm(y~x)
m0
```

The least squares fitting criteria does a decent job recovering those true values^[And in expectation as $N\to \infty$ the estimates would converge on the true values)]. It does so by trying to minimize the distance (grey segments) between the model's predictions (black line) and the observed values (open dots)


```{r}
# Residual: Distance between predicted and observed
e <- resid(m0)
# Predicted (or fitted values)
yhat <- predict(m0)
# Same as plugging each value of x into equation
yhat1 <- coef(m0)[1] + coef(m0)[2]*x
all.equal(yhat,yhat)

plot(x,y)
abline(m0)
segments(x,y,x,yhat,col="grey",lwd=1)
```

Had we chosen some other line (intercept and slope) 

```{r}
yhat2 <- 1+1*x
e2 <- y - yhat2
plot(x,y)
abline(1,1,col="red")
segments(x,y,x,yhat2,col="red",lwd=2)
abline(m0)
segments(x,y,x,yhat,col="blue",lwd=1)
```


Our predictions would be worse (i.e. our residuals would be higher)

```{r}
mean(e)
mean(e2)
```


Furthermore, in addition to being unbiased (mean 0), the residuals from OLS are uncorrelated with the predictors and predictions in our model

```{r}
round(cor(e,x),2)
round(cor(e,yhat),2)
round(cor(e2,x),2)
round(cor(e2,yhat2),2)
```


Why not $\sum{\hat{\epsilon_i}}$

- Any line that passes through $\bar{Y}$ and $\bar{X}$ will satisfy this criteria

Why not $\sum{|\hat{\epsilon_i}|}$

- Sometimes we do! For example in quantile regression...


# The Math behind OLS Regression

To understand what's going on under the hood, you need a broad understanding of some basic calculus. The notes below provide a brief review of some basic calculus to show how to calculate by hand the coefficient for simple bivariate (Y modeled by X) model. We'll probably skip over this in class. The key takeaway is that if you do the math, you'll find that coefficient that minimizes the SSR is:

$$
\begin{aligned}
\beta_1 &= \frac{Cov(X_1,Y)}{Var(X_1)}
\end{aligned}
$$

## A Brief Review of Derivatives


A derivative of $f$ at $x$ is its rate of change at $x$

- For a line: the slope
- For a curve: the slope of a line tangent to the curve

You'll see too notations for derivatives:

1. Leibniz notation:

$$
\frac{df}{dx}(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

2. Lagrange: $f^{\prime}(x)$



## Derivatives

Derivative of a constant

$$
f^{\prime}(c)=0
$$

Derivative of a line f(x)=2x

$$
f^{\prime}(x)=2
$$

Chain rule: y= f(g(x)). The derivative of y with respect to x is

$$
\frac{d}{dx}(f(g(x)))=f^{\prime}(g(x))g^{\prime}(x)
$$

The derivative of the "outside" times the derivative of the "inside," remembering that the derivative of the outside function is evaluated at the value of the inside function.

## Finding Local Minimums

Local minimum:

$$
f^{\prime}(x)=0 \text{ and } f^{\prime\prime}(x)>0 
$$

## Partial Derivatives

Let $f$ be a function of the variables $(X_1, \dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is

$$\begin{align*}
\frac{\partial f(X_1, \dots, X_n)}{\partial X_i}=\lim_{h\to0}\frac{f(X_1, \dots X_i+h \dots, X_n)-f(X_1, \dots X_i \dots, X_n)}{h}
\end{align*}$$

## Minimizing the sum of squared errors

Our model

$$
Y_i =\beta_0+\beta_1X_{1,i}+\epsilon_i
$$

Finds coefficients $\beta_0$ and $\beta_1$ to to minimize the sum of squared residuals, $\hat{\epsilon}_i$:

$$\begin{alignat*}{1}
\sum \hat{\epsilon_i}^2 &= \sum (Y_i-\beta_0-\beta_1 X_{1,i})^2
\end{alignat*}$$

We solve for $\beta_0$ and $\beta_1$, by taking the partial derivatives with respect to $\beta_0$ and $\beta_1$, and setting them equal to zero

$$
\begin{alignat*}{1}
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial \beta_0} &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})=0\\
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial\beta_1} &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})X_{1,i}=0
\end{alignat*}
$$

First, we'll solve for $\beta_0$, by multiplying both sides by -1/2 and distributing the $\sum$:

$$
\begin{alignat*}{1}
0 &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})\\
\sum \beta_0 &= \sum Y_i - \sum \beta_1 X_{1,i}\\
N \beta_0 &= \sum Y_i -\sum \beta_1 X_{1,i}\\
\beta_0 &= \frac{\sum Y_i}{N} - \frac{\beta_1 \sum X_{1,i}}{N}\\
\beta_0 &= \bar{Y} - \beta_1 \bar{X_{1}}
\end{alignat*}
$$


Now, we can solve for $\beta_1$ plugging in $\beta_0$.

$$
\begin{alignat*}{1}
0 &= -2\sum [(Y_i-\beta_0-\beta_1 X_{1,i})X_{1,i}]\\
0 &= \sum [Y_iX_i-(\bar{Y} - \beta_1 \bar{X_{1}})X_{1,i}-\beta_1 X_{1,i}^2]\\
0 &= \sum [Y_iX_i-\bar{Y}X_{1,i} + \beta_1 \bar{X_{1}}X_{1,i}-\beta_1 X_{1,i}^2]
\end{alignat*}
$$

Now we'll rearrange some terms and pull out an $X_{1,i}$ to get

$$
\begin{alignat*}{1}
0 &= \sum [(Y_i -\bar{Y} + \beta_1 \bar{X_{1}}-\beta_1 X_{1,i})X_{1,i}]
\end{alignat*}
$$

Dividing both sides by $X_{1,i}$ and distributing the summation, we can isolate $\beta_1$
$$
\begin{alignat*}{1}
\beta_1 \sum (X_{1,i}-\bar{X_1}) &= \sum (Y_i -\bar{Y})
\end{alignat*}
$$

Dividing by $\sum (X_{1,i}-\bar{X_1})$ to get

$$
\begin{alignat*}{1}
\beta_1  &= \frac{\sum (Y_i -\bar{Y})}{\sum (X_{1,i}-\bar{X_1})}
\end{alignat*}
$$

Finally, by multiplying by $\frac{(X_{1,i}-\bar{X_1})}{(X_{1,i}-\bar{X_1})}$ we get

$$
\begin{alignat*}{1}
\beta_1  &= \frac{\sum (Y_i -\bar{Y})(X_{1,i}-\bar{X_1})}{\sum (\bar{X_1}-X_{1,i})^2}
\end{alignat*}
$$

Which has a nice interpretation: 

$$
\begin{aligned}
\beta_1 &= \frac{Cov(X_1,Y)}{Var(X_1)}
\end{aligned}
$$

So the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat! 


Let's calculate the simple regression coefficients for age predicted by vaccine beliefs

```{r,echo=T}
wrk<-na.omit(nes16[,c("age","vaccines")])
# Variance Covariance of age and vaccines
vcov1<-var(wrk)
vcov1
# Means
age_mn<-mean(wrk$age)
vac_mn<-mean(wrk$vaccines)
beta1<-vcov1[1,2]/vcov1[2,2]
beta0<-age_mn-beta1*vac_mn
```


And compare them to `R`'s `lm()` function

```{r,echo=T}
c(beta0,beta1)
coef(lm(age~vaccines,data=wrk))
```




---
## Models partition variance

$$\begin{aligned}
\textrm{Total Variation} &= \textrm{Explained} + \textrm{Unexplained} \\
\textrm{Observed Variation} &= \textrm{Predicted Value} + \textrm{Residual}\\
\textrm{Y} &= X\beta + \epsilon \\
\textrm{Y} &= \hat{Y} + \hat{\epsilon}
\end{aligned}$$

---
## Prediction Error

Prediction error is simply the difference between the actual (observed) outcome ($Y$) and the predicted outcome ($\hat{Y}$)

$$
\hat{\epsilon}=Y-\hat{Y}
$$

---
## Bias

Bias is the expected value of the prediction error ($E[\hat{\epsilon}]$). We could write a simple function to calculate bias as follows:

```{r}
bias_fn<-function(y,yhat){mean(y-yhat,na.rm=T)}
```


We say a prediction is unbiased if the expected value of the prediction error is 0 ($E[\hat{\epsilon}]=0$).

Intuitively, this means that, on average, our prediction is neither systematically too high, nor too low, relative to the typical value of our outcome. 

---
## Bias vs. variance 

```{r, echo=F, out.height="80%"}
knitr::include_graphics("https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png")
```


---
## Root Mean Squared Error (RMSE)

RMSE is a useful concept for quantifying the typical or average error of the prediction. 

1. calculate the error 
  - difference between observed and predicted for each observation
2. square the error 
  - treat positive and negative errors equally
2. the mean of the squared errors
  - get a typical value
4. take square root 
  - put the value back in the units of variable of interest.

---
## Root Mean Squared Error (RMSE)

Mathematically, RMSE is

$$
RMSE = \sqrt{\frac{1}{n} \sum \hat{\epsilon_i}^2 } = \sqrt{\frac{1}{n} \sum (Y_i-\hat{Y_i})^2 }
$$

In R code:

```{r}
rmse_fn <- function(y,yhat){sqrt(mean((y-yhat)^2,na.rm=T))}
```


To help illustrate these concepts, let's load a subset of data from the 2016 National Elections Study (NES) survey 


```{r}
load(url("https://raw.github.com/PTesta/POLS_1600/master/nes16.rda"))
```

And look at what we did

```{r}
head(nes16)
```

## Example: Predicting Age with different statistics:

Suppose we wanted to know the typical value of $age$ for respondents in the 2016 NES. Recall from previous classes we have several measures of centrality, or what a typical value of some variable might be:

- median
- mode
- mean

Let's calculate the bias and RMSE associated with each for our prediction of $age$ 

First, we'll calculate the predictions ($\hat{Y}$)

```{r}
# Median
age_med<-median(nes16$age,na.rm=T)
age_med
# Mode
age_mode<-as.numeric(names(sort(table(nes16$age),decreasing = T)[1]))
age_mode
# Mean
age_mn<-mean(nes16$age,na.rm=T)
age_mn


```


Now, let's calculate the bias of these functions using the function we created above

```{r}
# Calculate bias for each prediction
bias_med <-bias_fn(nes16$age,age_med)
bias_mode <- bias_fn(nes16$age,age_mode)
bias_mn <- bias_fn(nes16$age,age_mn)
# Put in object "biases" with labels and sort by size of bias
biases <- sort(c(Median = bias_med, Mode = bias_mode, Mean = bias_mn))
# Barplot
barplot(biases,main = "Bias of Mean, Median and Mode for\nPredicting Age in 2016 NES")
```

So the mode, looks a like a pretty bad, or biased predictor, the median has some bias, and the mean (by definition) is an unbiased. That is, if $\hat{Y}=\bar{Y}$ 

$$
E[\hat{\epsilon}] = E[Y - \hat{Y}]= E[Y]-E[\hat{Y}] = \bar{Y} - \bar{Y} = 0
$$

However, if we looked at the RMSE (the typical prediction error), we'd see that the mean and median are pretty close

```{r}
rmse_med <-rmse_fn(nes16$age,age_med)
rmse_mode <- rmse_fn(nes16$age,age_mode)
rmse_mn <- rmse_fn(nes16$age,age_mn)
# Put in object "rmsees" with labels and sort by size of rmse
rmses <- sort(c(Median = rmse_med, Mode = rmse_mode, Mean = rmse_mn))
# Barplot
barplot(rmses,main = "rmse of Mean, Median and Mode for\nPredicting Age in 2016 NES")
```


Differing by only `r round(rmse_mn - rmse_med,3)` years in these data. Most of the time, the models we fit will make predictions that approximate the conditional mean of some outcome (i.e. the mean of Y conditional on X). Sometimes, however, it's useful to make predictions for the conditional median (or some other quantile).



# Conditional Expecations and OLS Regression

Conditional expectations are our way of incorporating what we know about the world to improve our predictions. That is, how does our prediction of Y change conditional on the values of some additional predictor/information X.

Suppose in addition to knowing the age of our respondents, we also knew whether they believed the benefits of vaccines outweighed the potential risks

$$
Age \sim Vaccines
$$

Note: nothing about this relationship implies causation. We're simply interested in explaining variation in some outcome ($Y$, here $Age$) with some predictor(s) ($X$, here $Vaccines$)

## Age and belief in the benefit of vaccines

Just as we used the sample mean as an unbiased estimate of expected value of age, we can use separate sample means (sub-classification via logical indexing) to provide  unbiased estimates of the expected value of age conditional on vaccine beliefs

$$
E[Age|Vaccines=Good]=\frac{1}{n_{good}}\sum Age_{i,|Vac=good}
$$

In R:

```{r}
age_mn_vac_good<-round(mean(nes16$age[nes16$vaccines==1],na.rm=T),3)
age_mn_vac_good
```

$$
E[Age|Vaccines=Bad]=\frac{1}{n_{bad}}\sum Age_{i,|Vac=bad}
$$

```{r}

age_mn_vac_bad<-round(mean(nes16$age[nes16$vaccines==0],na.rm=T),3)
age_mn_vac_bad
```

So on average, people who believe the benefits of vaccines outweigh the costs, tend to be older by about `r age_mn_vac_good-age_mn_vac_bad` years.

Now consider the output of the following:

```{r}
m1 <- lm(age~vaccines, data = nes16,na.action = "na.exclude")
m1
# Extract and round  coefficients
beta0 <- round(coef(m1)[1],3)
beta1 <- round(coef(m1)[2],3)

```

Formally, we've asked R fit the following model:

$$
age = \beta_0 +\beta_1 \times vaccines +\epsilon_i
$$

Specifically, we've asked R to model Age as a function of some constant $\beta_0$ plus some coefficient $\beta_1$ times a variable capturing people's beliefs about vaccines ($vaccines=1$ if they believe benefits outweigh costs, 0 otherwise) and some error (reflecting the fact that there's a lot about people's ages unexplained by their belief in vaccines)

R produces estimates of these parameters:

$$
\hat{age} = \hat{\beta_0} +\hat{\beta_1} \times vaccines + \hat{\epsilon_i}
$$

Specifically:

$$
\hat{age_i} = `r beta0` + `r beta1` \times vaccines_i + \hat{\epsilon_i}
$$

That is, R has said modeling age conditional Vaccine beliefs, it's best prediction of person's age for people who are skeptical of the value of vaccines ($vaccinces=0$) is

$$
\begin{align*}
\hat{age_i} = `r beta0` + `r beta1` \times 0 \\
= `r beta0` = \frac{1}{n_{bad}}\sum Age_{i,|Vac=bad} =E[Age|Vaccines=Bad]
\end{align*}
$$

And for people who believe vaccines are good ($vaccinces=1$):

$$
\begin{align*}
\hat{age_i} = `r beta0` + `r beta1` \times 1 \\
= `r beta0 +beta1` = \frac{1}{n_{bad}}\sum Age_{i,|Vac=bad} =E[Age|Vaccines=Good]
\end{align*}
$$

So for this simple model, R's `lm()` function says the best prediction of person's age knowing their beliefs about vaccines are the means conditional on their beliefs about vaccines

```{r}
age_mn_vac_bad 
beta0
age_mn_vac_bad == beta0
age_mn_vac_good 
beta0 + beta1

```

We can visualize what's going on by plotting beliefs on the x axis and age on the y. The black line corresponds to the overall mean. The red diamond corresponds to the average age of people who are skeptical of vaccines, and the blue circle corresponds to the age among people who believe the benefits outweigh the costs. The grey line connects the two dots with the line defined by an intercept ($\beta_0$) and a slope ($\beta_1$) from `lm()`

```{r}
plot(age~vaccines,nes16,xlab="Vaccines")
points(0,age_mn_vac_bad,col="red",pch=18,cex=2)
points(1,age_mn_vac_good,col="blue",pch=19,cex=2)
abline(h=age_mn,col="black")
abline(lm(age~vaccines,nes16),col="grey")
```

Let's clean up the plot a little

```{r}
plot(age~jitter(vaccines),nes16,xaxt="n",xlab="Vaccines")
axis(1,at = c(0,1),labels=c("Gee, I don't know","Science!"))
points(0,age_mn_vac_bad,col="red",pch=18,cex=2)
points(1,age_mn_vac_good,col="blue",pch=19,cex=2)
abline(h=age_mn,col="black")
abline(lm(age~vaccines,nes16),col="grey")
```

Note that both the unconditional and conditional means provide "unbiased" estimates

```{r,echo=T}
round(bias_fn(nes16$age,age_mn),5)
round(bias_fn(nes16$age,predict(m1,na.action = "na.exclude")),5)
```

But the conditional estimate has a smaller RMSE. That is, knowing something about people's beliefs about vaccines, improves our prediction of their age.

```{r}
round(rmse_fn(nes16$age,age_mn),2)
round(rmse_fn(nes16$age,predict(m1,na.action = "na.exclude")),2)

```


---
# Mechanics of OLS Regression

## How did choose the coefficients that define the grey line?


We used a procedure, Ordinary Least Squares regression, that choose and intercept ($\beta_0$) and slope ($\beta_1$) to minimize the Sum of Squared Residuals ($\sum \hat{\epsilon}^2$). 


## Minimizing the sum of squared residuals

$$
\hat{Y_i}=\beta_0+\beta_1 X_{1,i} + \hat{\epsilon_i}
$$

$$
\hat{\epsilon_i}=Y_i-\hat{Y_i}=(Y_i-(\beta_0+\beta_1 X_{1,i}))
$$

OLS chooses $\beta_0$ and $\beta_1$ to minimize $\sum \epsilon^2$, the Sum of Squared Residuals (SSR)

$$
\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (Y-(\beta_0+\beta_1X))^2
$$

## Why minimize $\sum{\hat{\epsilon_i}^2}$


Substantively, by minimizing the sum of squared residuals, we're trying to minimize the distance between our models predictions and the observed data.

Suppose the true relationship between y and x is:

$$
y~1+2x + \epsilon
$$

If we simulated some data where this was true:

```{r}
set.seed(123)
x <- rnorm(20)
y <- 1 + 2*x + rnorm(20)
m0 <- lm(y~x)
m0
```

The least squares fitting criteria does a decent job recovering those true values^[And in expectation as $N\to \infty$ the estimates would converge on the true values)]. It does so by trying to minimize the distance (grey segments) between the model's predictions (black line) and the observed values (open dots)


```{r}
# Residual: Distance between predicted and observed
e <- resid(m0)
# Predicted (or fitted values)
yhat <- predict(m0)
# Same as plugging each value of x into equation
yhat1 <- coef(m0)[1] + coef(m0)[2]*x
all.equal(yhat,yhat)

plot(x,y)
abline(m0)
segments(x,y,x,yhat,col="grey",lwd=1)
```

Had we chosen some other line (intercept and slope) 

```{r}
yhat2 <- 1+1*x
e2 <- y - yhat2
plot(x,y)
abline(1,1,col="red")
segments(x,y,x,yhat2,col="red",lwd=2)
abline(m0)
segments(x,y,x,yhat,col="blue",lwd=1)
```


Our predictions would be worse (i.e. our residuals would be higher)

```{r}
mean(e)
mean(e2)
```


Furthermore, in addition to being unbiased (mean 0), the residuals from OLS are uncorrelated with the predictors and predictions in our model

```{r}
round(cor(e,x),2)
round(cor(e,yhat),2)
round(cor(e2,x),2)
round(cor(e2,yhat2),2)
```


Why not $\sum{\hat{\epsilon_i}}$

- Any line that passes through $\bar{Y}$ and $\bar{X}$ will satisfy this criteria

Why not $\sum{|\hat{\epsilon_i}|}$

- Sometimes we do! For example in quantile regression...


# The Math behind OLS Regression

To understand what's going on under the hood, you need a broad understanding of some basic calculus. The notes below provide a brief review of some basic calculus to show how to calculate by hand the coefficient for simple bivariate (Y modeled by X) model. We'll probably skip over this in class. The key takeaway is that if you do the math, you'll find that coefficient that minimizes the SSR is:

$$
\begin{aligned}
\beta_1 &= \frac{Cov(X_1,Y)}{Var(X_1)}
\end{aligned}
$$

## A Brief Review of Derivatives


A derivative of $f$ at $x$ is its rate of change at $x$

- For a line: the slope
- For a curve: the slope of a line tangent to the curve

You'll see too notations for derivatives:

1. Leibniz notation:

$$
\frac{df}{dx}(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

2. Lagrange: $f^{\prime}(x)$



## Derivatives

Derivative of a constant

$$
f^{\prime}(c)=0
$$

Derivative of a line f(x)=2x

$$
f^{\prime}(x)=2
$$

Chain rule: y= f(g(x)). The derivative of y with respect to x is

$$
\frac{d}{dx}(f(g(x)))=f^{\prime}(g(x))g^{\prime}(x)
$$

The derivative of the "outside" times the derivative of the "inside," remembering that the derivative of the outside function is evaluated at the value of the inside function.

## Finding Local Minimums

Local minimum:

$$
f^{\prime}(x)=0 \text{ and } f^{\prime\prime}(x)>0 
$$

## Partial Derivatives

Let $f$ be a function of the variables $(X_1, \dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is

$$\begin{align*}
\frac{\partial f(X_1, \dots, X_n)}{\partial X_i}=\lim_{h\to0}\frac{f(X_1, \dots X_i+h \dots, X_n)-f(X_1, \dots X_i \dots, X_n)}{h}
\end{align*}$$

## Minimizing the sum of squared errors

Our model

$$
Y_i =\beta_0+\beta_1X_{1,i}+\epsilon_i
$$

Finds coefficients $\beta_0$ and $\beta_1$ to to minimize the sum of squared residuals, $\hat{\epsilon}_i$:

$$\begin{alignat*}{1}
\sum \hat{\epsilon_i}^2 &= \sum (Y_i-\beta_0-\beta_1 X_{1,i})^2
\end{alignat*}$$

We solve for $\beta_0$ and $\beta_1$, by taking the partial derivatives with respect to $\beta_0$ and $\beta_1$, and setting them equal to zero

$$
\begin{alignat*}{1}
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial \beta_0} &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})=0\\
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial\beta_1} &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})X_{1,i}=0
\end{alignat*}
$$

First, we'll solve for $\beta_0$, by multiplying both sides by -1/2 and distributing the $\sum$:

$$
\begin{alignat*}{1}
0 &= -2\sum (Y_i-\beta_0-\beta_1 X_{1,i})\\
\sum \beta_0 &= \sum Y_i - \sum \beta_1 X_{1,i}\\
N \beta_0 &= \sum Y_i -\sum \beta_1 X_{1,i}\\
\beta_0 &= \frac{\sum Y_i}{N} - \frac{\beta_1 \sum X_{1,i}}{N}\\
\beta_0 &= \bar{Y} - \beta_1 \bar{X_{1}}
\end{alignat*}
$$


Now, we can solve for $\beta_1$ plugging in $\beta_0$.

$$
\begin{alignat*}{1}
0 &= -2\sum [(Y_i-\beta_0-\beta_1 X_{1,i})X_{1,i}]\\
0 &= \sum [Y_iX_i-(\bar{Y} - \beta_1 \bar{X_{1}})X_{1,i}-\beta_1 X_{1,i}^2]\\
0 &= \sum [Y_iX_i-\bar{Y}X_{1,i} + \beta_1 \bar{X_{1}}X_{1,i}-\beta_1 X_{1,i}^2]
\end{alignat*}
$$

Now we'll rearrange some terms and pull out an $X_{1,i}$ to get

$$
\begin{alignat*}{1}
0 &= \sum [(Y_i -\bar{Y} + \beta_1 \bar{X_{1}}-\beta_1 X_{1,i})X_{1,i}]
\end{alignat*}
$$

Dividing both sides by $X_{1,i}$ and distributing the summation, we can isolate $\beta_1$
$$
\begin{alignat*}{1}
\beta_1 \sum (X_{1,i}-\bar{X_1}) &= \sum (Y_i -\bar{Y})
\end{alignat*}
$$

Dividing by $\sum (X_{1,i}-\bar{X_1})$ to get

$$
\begin{alignat*}{1}
\beta_1  &= \frac{\sum (Y_i -\bar{Y})}{\sum (X_{1,i}-\bar{X_1})}
\end{alignat*}
$$

Finally, by multiplying by $\frac{(X_{1,i}-\bar{X_1})}{(X_{1,i}-\bar{X_1})}$ we get

$$
\begin{alignat*}{1}
\beta_1  &= \frac{\sum (Y_i -\bar{Y})(X_{1,i}-\bar{X_1})}{\sum (\bar{X_1}-X_{1,i})^2}
\end{alignat*}
$$

Which has a nice interpretation: 

$$
\begin{aligned}
\beta_1 &= \frac{Cov(X_1,Y)}{Var(X_1)}
\end{aligned}
$$

So the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat! 


Let's calculate the simple regression coefficients for age predicted by vaccine beliefs

```{r,echo=T}
wrk<-na.omit(nes16[,c("age","vaccines")])
# Variance Covariance of age and vaccines
vcov1<-var(wrk)
vcov1
# Means
age_mn<-mean(wrk$age)
vac_mn<-mean(wrk$vaccines)
beta1<-vcov1[1,2]/vcov1[2,2]
beta0<-age_mn-beta1*vac_mn
```


And compare them to `R`'s `lm()` function

```{r,echo=T}
c(beta0,beta1)
coef(lm(age~vaccines,data=wrk))
```


# Interpreting Regression Coefficients

So far, we've seen that for a simple case, with a dichotomous (0-1) predictor, our simple linear regression provided us with a model that returned the conditional means in each group. 

Suppose instead of vaccine beliefs, we were interested in how age varied with ideology, where ideology is a measured on a seven-point scale where 1=strong liberal and 7=strong conservative

We could again calculate the conditional expectation for each level of ideology, from

$$
E[Age|Ideology=1]=\frac{1}{N_{ideo=1}}\sum Age_{i|ideo=1}
$$

To

$$
E[Age|Ideology=7]=\frac{1}{N_{ideo=7}}\sum Age_{i|ideo=7}
$$

Or....

We could model age as a  linear function of ideology 

$$
Age_i=\beta_0+\beta_1\times Ideology_i+\epsilon_i
$$

First, let's compare the results of those two approaches, before getting into the details of estimation

```{r}
# Means of Age, conditional on ideolog (E[age |ideology])
age_mn_ideo <- with(nes16, tapply(age,ideology,mean,na.rm=T))

# Linear approximation to conditional mean
m2 <- lm(age ~ ideology, nes16,na.action = "na.exclude")

age_mn_ideo
m2

# Calculate by hand

wrk<-na.omit(nes16[,c("age","ideology")])
# Variance Covariance of age and ideology
vcov1<-var(wrk)
vcov1
# Means
age_mn<-mean(wrk$age)
ideo_mn<-mean(wrk$ideology)
beta1<-vcov1[1,2]/vcov1[2,2]
beta0<-age_mn-beta1*ideo_mn
beta0
beta1
coef(m2)
```

And let's visualize these predictions to understand what's going. The red symbols below correspond to the average age of respondents conditional on their ideology. The grey line corresponds to a **linear approximation of these conditional means** with coefficients obtained by minimizing the sum of squared residuals. Sometimes the predictions of this linear approximation are too high, other times they're too low, but on average the error is 0 (by design).

Substantively, it tells us that on average, people who identify more strongly as conservatives tend to be older. How much older? Well, for each unit increase in ideology, our model predicts average age will be higher by `r round(beta1,2)` years.


```{r}
with(nes16,plot(jitter(ideology),age,pch=18,cex=.5,xlim=c(0,7)))
# Condtional means
points(1:7,with(nes16,tapply(age,ideology,mean,na.rm=T)),col="red",pch=1:7,cex=1.5)
# Linear approximation to the conditional mean function
abline(lm(age~ideology,nes16),col="grey")
```

When is this linear approximation a good prediction? Well, generally when the underlying conditional mean function is linear. In the relationship above, a linear trend seems reasonable. The less linear the conditional expectation function is, the less suitable a linear approximation.

Note, that we can recover the conditional means, by fitting a "saturated" model, that is by giving R a model with a predictor for each level of variable of interest. 

```{r}
m3 <- lm(age ~ factor(ideology), nes16, na.action = "na.exclude")
coef(m3)
```


By default, R drops the first level of a factor, and uses it as a reference category. So the intercept in this model corresponds to the average age among strong liberals 

```{r}
# Intercept is mean in excluded category
coef(m3)[1]
# Here the mean among strong liberals
age_mn_ideo[1]
```

And the coefficients for each model tell you the difference in average ages between strong liberals and other ideological identifications, which when added to the average among strong liberals, gives you the mean in that group:


```{r}
round(coef(m3),2)
# Average among liberals (ideology = 2)
coef(m3)[1]+coef(m3)[2]
age_mn_ideo[2]

# Avearge age from coefficeints in m3
round(c(coef(m3)[1],coef(m3)[1]+coef(m3)[-1]),2)
# Same as conditional means
round(age_mn_ideo,2)

# Removing the intercept, and R returns means for each group
m4 <- lm(age ~ factor(ideology) - 1, nes16,na.action = "na.exclude")
m4
round(age_mn_ideo,2)

# Compare RMSE of linear trend
rmse_fn(nes16$age, predict(m2))
# To saturated models
rmse_fn(nes16$age, predict(m3))
rmse_fn(nes16$age, predict(m4))


```

Mathematically, the difference between treating ideology as a linear predictor of age and estimating the means of age conditional on ideology is the difference between fitting:

```{r}
# Ideology is a linear predictor of age
head(model.matrix(m2))

# Estimating means of age conditional on ideology
head(model.matrix(m3))
```


Which we'll talk more about next week when we get to multiple regression.

# WYNK {-}

- Prediction involves error (difference between what we observe and what we predict)
- One way of evaluating our predictions is to consider the bias of a predictor. 
- Often we want predictors that are unbiased, but as we will see, this is not the only criteria we care about
- Means are good (unbiased) estimators, but are not the only tool in our tool kit.
- OLS regression provides a linear approximation to a conditional mean function
- For binary predictors (0-1 variables) and saturate models the predictions from a simple OLS regression ARE the conditional means
- For continuous variables, the predictions are linear approximations to the means
    - The intercept corresponds to the model's prediction when the predictor is 0
    - The coefficient tells you how the prediction changes given a unit change in your predictor.
- OLS obtains these coefficients by minimizing the some of squared residuals
- The intercept in a simple regression $\beta_0$ corresponds to:

$$
\beta_0 = \bar{Y} - \beta_1 \bar{X}
$$
- The slope $\beta_1$ corresponds to:

$$
\beta_1 = \frac{Cov(X,Y)}{Var(X)}
$$

---
# üí°  
# Three Designs for Causal Inference in Observational Studies


---
## Credible Cauasal Inference in Observational Studies

Subclassification, matching, and regression all require an assumption of selection on observables:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

But how do we know if we've got the right model or we've controlled for the right variables?

--

Typically, we don't

---
## Credible Cauasal Inference in Observational Studies

Instead, social scientists  look for situations where the credibility of

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

depends less on how much data you have and much more on how your data were generated.



---
class: middle

.pull-left[
> Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact. ... [T]he primary engine driving improvement has been a focus on the **quality of empirical research designs.** (p. 4)

]

.pull-right[
```{r cred1, echo=F}
knitr::include_graphics("./images/04_cred.png")
```

[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)

]

---
class: middle

.pull-left[
> Design-based studies are distinguished by their prima facie credibility and by the attention investigators devote to making both an **institutional** and a **data-driven** case for causality (p. 5)

]

.pull-right[
```{r cred2, echo=F}
knitr::include_graphics("./images/04_cred.png")
```
[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)
]

---
class: middle

.pull-left[
> The econometric methods that feature most prominently in quasi-experimental studies are **instrumental variables**, **regression discontinuity** methods,and **differences-in-differences**-style policy analysis. ... The best of today‚Äôs design-based studies make a strong institutional case, backed up with empirical evidence, for the variation thought to generate a useful **natural experiment**.(p. 12)

]

.pull-right[
```{r cred3, echo=F}
knitr::include_graphics("./images/04_cred.png")
```
[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)
]

---
## Three Designs for Causal Inference in Observational Studies

- Difference in Differences

- Regression Discontinuity

- Instrumental Variables


---
class: inverse, center, middle
# üí° Difference in Differences


---
class: inverse, center, middle
background-image:url(https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc)
background-size: cover
## London in the Time of Cholera

---
## Motivating Example: What causes Cholera?

- In the 1800s, cholera was thought to be transmitted through the air.

- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.

- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. 

---
## Notation

Let's adopt a little notation to help us think about the logic of Snow's design:

- $D$: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)

- $T$: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).

- $Y_{di}(t)$ the potential outcome of unit $i$ 

  - $Y_{1i}(t)$ the potential outcome of unit $i$ when treated between the two periods 

  - $Y_{0i}(t)$ the potential outcome of unit $i$ when control between the two periods 

---
## Causal Effects

The individual causal effect for unit i at time t is:

$$\tau_{it} = Y_{1i}(t) ‚àí Y_{0i}(t)$$

What we observe is 

$$Y_i(t) = Y_{0i}(t)\cdot(1 ‚àí D_i(t)) + Y_{1i}(t)\cdot D_i(t)$$

$D$ only equals 1, when $T$ equals 1, so we never observe $Y_0i(1)$ for the treated units. 

In words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.

---
## Average Treatment on Treated

Our goal is to estimate the average effect of treatment on treated (ATT):


$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

That is, what would have happened in Lambeth, had their water company not moved their pipes

---
## Average Treatment on Treated

Our goal is to estimate the average effect of treatment on treated (ATT):

We we can observe is:

|               | Post-Period (T=1)  | Pre-Period (T=0)  |
|---------------|--------------------|-------------------|
| Treated $D_{i}=1$  | $E[Y_{1i}(1)\vert D_i = 1]$  | $E[Y_{0i}(0)\vert D_i = 1]$ |
| Control $D_i=0$  | $E[Y_{0i}(1)\vert D_i = 0]$  | $E[Y_{0i}(0)\vert D_i = 0]$ |

---
## Data

Because potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):

```{r choleradat,echo=F}
snow <- tibble(Company = c("Lambeth (D=1)", "Southwark and Vauxhall (D=0"),
               `1854 (T=1)` = c(19,147),
               `1849 (T=0)` = c(85,135)
               )

knitr::kable(snow)

```

---
## How can we estimate the effect of moving pumps upstream?

Recall, our goal is to estimate the effect of the the treatment on the treated:

$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

Let's conisder some strategies Snow could take to estimate this quantity:

---
## Before vs after comparisons:

- Snow could have compared Labmeth in 1854 $(E[Y_i(1)|D_i = 1] = 19)$ to Lambeth in 1849 $(E[Y_i(0)|D_i = 1]=85)$, and claimed that moving the pumps upstream led to 66 fewer cholera deaths. 

- This comparison assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved ($E[Y_{0i}(1)|D_i = 1]$).

- A skeptic might argue that Lambeth in 1849 $\neq$ Lambeth in 1854


---
## Treatment-Control comparisons in the Post Period.

- Snow could have compared outcomes between Lambeth and S&V in 1954  ($E[Yi(1)|Di = 1] ‚àí E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to 128 fewer deaths.

- Here the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved ($E[Y_{0i}(1)|D_i = 1]$)

- Again, our skeptic could argue  Lambeth $\neq$ S&V 

---
## Difference in Differences

To address these concerns, Snow employed what we now call a difference-in-differences design, 

There are two, equivalent ways to view this design. 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}‚àí \overbrace{\{E[Y_{i}(0)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$

- Difference 1: Average change between Treated and Control  in Post Period
- Difference 2: Average change between Treated and Control  in Pre Period

Which is equivalent to:

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i} = 1]\}}_{\text{Post - Pre |Treated }}‚àí \overbrace{\{E[Y_{i}(1)|D_{i} = 0] ‚àí E[Y_{i}(0)|D_{i}=0 ]}^{\text{Post-Pre|Control}}$$


- Difference 1: Average change between Treated over time
- Difference 2: Average change between Control over time

---
## Difference in Differences


You'll see the DiD design represented both ways, but they produce the same result:

$$
\tau_{ATT} = (19-147) - (85-135) = -78
$$

$$
\tau_{ATT} = (19-85) - (147-135) = -78
$$

---
## Identifying Assumption of a Difference in Differences Design

The key assumption in this design is what's known as the parallel trends assumption: $E[Y_{0i}(1) ‚àí Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) ‚àí Y_{0i}(0)|D_i = 0]$ 

- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&V

---
```{r paralleltrends, echo=F, fig.height=4}

snow_g <- tibble(
  Period = c(0,0,3,3,0,3),
  Treatment = c(0,1,0, 1,1,1),
  Line = c(1,1,1,1,2,2),
  Company = c("S&V","Lambeth","S&V","Lambeth","Lambeth (D=0)","Lambeth (D=0)"),
  Deaths = c(135,85,147,19,85,97)
)

snow_g %>%
  ggplot(aes(Period,Deaths,col = Company))+
  geom_point()+
  geom_line()+
  geom_segment(aes(x=3.1,xend=3.1,y=19,yend=147), linetype = 2, col= "gray")+
  annotate(geom="text",x = 3.3,y=125, label = "1",hjust=.5)+
  geom_segment(aes(x=3.2,xend=3.2,y=19,yend=97), linetype = 2,col="gray")+
  annotate(geom="text",x = 3.3,y=55, label = "3",hjust=-.5)+
  geom_segment(aes(x=-.1,xend=-.1,y=85,yend=135), linetype = 2,col="gray")+
  annotate(geom="text",x = -.1,y=120, label = "2",hjust=1.5)+
  xlim(-2,6)+
  scale_x_continuous(breaks = c(0,3),labels = c("Pre","Post"))+
  theme_bw() -> snow_p

snow_p


```


Where:

1. $E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(1)|D_{i} = 0]$
2. $E[Y_{i}(0)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i}\} = 0]$
3. $E[Y_{1i}(1) ‚àí Y_{0i}(1)|D_{i} = 1]$


---
## Summary

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
  - Taking the pre-post difference removes any fixed differences between the units
  
  - Then taking the difference between treated and control differences removes any common differences over time

- The key identifying assumption of a DiD design is the "assumption of parallel trends"
  - Absent treatment, treated and control groups
would see the same changes over time.
  - Hard to prove, possible to test


---
## Extensions and limitations

- DiD easy to estimate with linear regression
- Generalizes to multiple periods and treatment interventions
  - More pre-treatment periods allow you assess "parallel trends" assumption
- Alternative methods 
  - Synthetic control
  - Event Study Designs
- What if you have multiple treatments or treatments that come and go?
  - Panel Matching
  - Generalized Synthetic control

---
## Applications

- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment

- [Abadie, Diamond, & Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany

- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?

---
class: inverse, center, middle
# üí° Regression Discontinuity Design

---
## Motivating Example

.pull-left[
- Do Members of Parliament in the UK get richer from holding office (QSS Chapter 4.3.4)
]
.pull-right[
```{r eggers1, echo=F}
knitr::include_graphics("./images/04_eggers.png")
```
[Eggers and Hainmueller (2009)](https://www.cambridge.org/core/journals/american-political-science-review/article/abs/mps-for-sale-returns-to-office-in-postwar-british-politics/E4C2B102194AA1EA0D2F1F777EAE3C08)
]


---
## Logic of the Regression Discontinuity Design (RDD) 

- What's the effect of holding elected office in the UK on personal wealth?

- People who win elections differ in many ways from people who lose elections.

- Logic of an RDD: 

  - Just look at the wealth of individuals who either narrowly won or lost elections.

  - Candidates close to 50 percent cutoff (discontinuity) should be more comparable (better counterfactuals)

---
## Data from Eggers and Hainmueller (2009)

```{r mpdata}
library(qss)
data(MPs)
glimpse(MPs)

```

---
```{r rddtab,echo=F}
rdd_vars <- data.frame(
Variable = c("surname", "firstname", "party", "ln.gross", "ln.net", "yob", "yod", "margin.pre", "region", "margin"),
Description = c(
"surname of the candidate",
"first name of the candidate",
"party of the candidate (labour or tory)",
"log gross wealth at the time of death",
"log net wealth at the time of death",
"year of birth of the candidate",
"year of death of the candidate",
"margin of the candidate‚Äôs party in the previous election electoral", 
"region",
"margin of victory (vote share)")
)
DT::datatable(rdd_vars)
```



---
```{r rddfigcode, eval = F}
MPs %>%
  ggplot(aes(margin, ln.net))+
  geom_point(shape=1)+
  facet_grid(~party)+
  geom_smooth(data =MPs %>%
                filter(margin <0),
              method = "lm")+
  geom_smooth(data =MPs %>%
                filter(margin >0),
              method = "lm")+
  theme_bw() -> fig_rdd
fig_rdd
```

---
```{r rddfig, echo = F}
MPs %>%
  ggplot(aes(margin, ln.net))+
  geom_point(shape=1)+
  facet_grid(~party)+
  geom_smooth(data =MPs %>%
                filter(margin <0),
              method = "lm")+
  geom_smooth(data =MPs %>%
                filter(margin >0),
              method = "lm")+
  theme_bw() -> fig_rdd
fig_rdd
```
---
## RDD Notation

- $X$ is a **forcing** variable
- Treatment $D$ is a determined by $X$

$$
D_i = 1\{X_i > c\}
$$

- $X$ is the `margin` variable in the example data, and $D=1$ if `margin` is greater than 0 (i.e. the candidate won the election)

- Interested in the differences in the outcome at the threshold

$$\lim_{x \downarrow  c} E[Y_i|X=x] - \lim_{x \uparrow  c} E[Y_i|X=x]$$

---
## Causal Identification with an RDD

If we assume $E[Y_i(0)|X=x]$ and $E[Y_i(1)|X=x]$ are continuous in x, then we can estimate a (local) ATE at the threshold:

$$\begin{align}
ATE_{RDD} &= E[Y(1)-Y(0)|X_i=c] \\
&=  E[Y(1)|X_i=c] -  E[Y(0)|X_i=c]\\
&= \lim_{x \downarrow  c} E[Y_i|X=x] - \lim_{x \uparrow  c} E[Y_i|X=x] \\
\end{align}$$

---
## Continuity Assumption

```{r continuity, echo=F}
knitr::include_graphics("https://mixtape.scunning.com/graphics/rdd_simul_ex.jpg")
```

[Cunningham (2022)](https://mixtape.scunning.com/regression-discontinuity.html#continuity-assumption)

---
## Causal Identification with an RDD

- The continuity assumption is a formal way of saying that observations close to the threshold are good counterfactuals for each other

- We can't prove this assumption

- But if it holds, we should observe
 
  - no sorting around the cutoff (no self selection)
 
  - similar distributions of covariates around the cutoff (balance tests)
  
  - no effect of treatment on things measured pre-treatment (placebo tests)



---
class: inverse, center, middle
# üí° 
## Instrumental Variables


---
## Instrumental Variables

Instrumental variables are an economists favorite tool for dealing with **omitted variable bias**

- We have some non random treatment whose effects we'd like to assess
- We're worried that these effects are **confounded** by some unobserved, omitted variable, that influences both the treatment and the outcome
- We find an **instrumental variable** that satisfies the following:
  - Randomization
  - Excludability
  - First-stage relatioship
  - Monotonicity
- Allowing us estimate a Local Average Treatment Effect (LATE) using the only the variation in our treatment is **exogenous** (uncorrelated with ommited variables)

---
class: center
## IV Assumption: Randomization


.left-column[
- No path from $U$ to $Z$
]
.right-column[
```{r iv1, echo=F}
knitr::include_graphics("https://book.declaredesign.org/figures/figure_15.4.svg")
```

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption: Excludability


.left-column[
- No path from $Z$ to $Y$
]

.right-column[
```{r iv2, echo=F}
knitr::include_graphics("https://book.declaredesign.org/figures/figure_15.4.svg")
```

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption:  First Stage

.left-column[
- Path from $Z$ to $D$
]
.right-column[
```{r iv3, echo=F}
knitr::include_graphics("https://book.declaredesign.org/figures/figure_15.4.svg")
```

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption: Monotonicity


.left-column[

-  $D_i(Z=1)\geq D_i(Z=0)$
- "No Defiers"

]

.right-column[
```{r iv4, echo=F}
knitr::include_graphics("https://book.declaredesign.org/figures/figure_15.4.svg")
```
[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)
]

---
class: center
## Compliance

With a binary treatment, $D$ and binary instrument $Z$ there are four types of compliance

```{r compliance, echo=F}
comp_tab <- tibble(
  Type = c("Always Takers",
           "Never Takers",
           "Compliers",
           "Defiers"),
   '$D_i(Z=1)$' = c(1,0,1,0),
   '$D_i(Z=0)$' = c(1,0,0,1)
)
kable(comp_tab)
```

- Assuming Monotonicity means there are "No Defiers"

---
## Estimating the Local Average Treatment Effect

If we believe our assumptions of:

- Randomization
- Excludability
- First-stage relationship
- Monotonicity

Then we can estimate Local Average Treatment Effect (LATE) sometimes called the Complier Average Treatment Effect CATE) 

---
## Estimating the Local Average Treatment Effect

It can be [shown](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf) that the LATE:

$$LATE = \frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1]-E[D|Z=0]}= \frac{ATE_{Z\to Y}}{ATE_{Z\to D}}$$


---

## Example: Earnings and Military Service

Adapted from [Edward Rubin](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#42)

*Example:* If we want to estimate the effect of veteran status on earnings,
$$\begin{align}
  \text{Earnings}_i = \beta_0 + \beta_1 \text{Veteran}_i + u_i \tag{1}
\end{align}$$

--

We would love to calculate $\color{#e64173}{\text{Earnings}_{1i}} - \color{#6A5ACD}{\text{Earnings}_{0i}}$, but we can't.

--

And OLS will likely be biased for $(1)$ due to selection/omitted-variable bias.

---

## Introductory example

Imagine that we can split veteran status into an exogenous (as-if random, unbiased) part and an endogenous (non-random, biased) part...

--

$$\begin{align}
  \text{Earnings}_i
  &= \beta_0 + \beta_1 \text{Veteran}_i + u_i \tag{1} \\
  &= \beta_0 + \beta_1 \left(\text{Veteran}_i^{\text{Exog.}} + \text{Veteran}_i^{\text{Endog.}}\right) + u_i \\
  &= \beta_0 + \beta_1 \text{Veteran}_i^{\text{Exog.}} + \underbrace{\beta_1 \text{Veteran}_i^{\text{Endog.}} + u_i}_{w_i} \\
  &= \beta_0 + \beta_1 \text{Veteran}_i^{\text{Exog.}} + w_i
\end{align}$$

--

We could use this exogenous variation in veteran status to consistently estimate $\beta_1$.

--

**Q:** What would exogenous variation in veteran status mean?

---

## Introductory example

**Q:** What would exogenous variation in veteran status mean?

--

**A.sub[1]:** Choices to enlist in the military that are essentially random‚Äîor at least uncorrelated with omitted variables and the disturbance.

--

**A.sub[2]:** .No selection bias:
$$\begin{align}
  \color{#e64173}{\mathop{E}\left(\text{Earnings}_{0i}\mid\text{Veteran}_i = 1\right)} - \color{#6A5ACD}{\mathop{E}\left( \text{Earnings}_{0i} \mid \text{Veteran}_i = 0 \right)} = 0
\end{align}$$

---
## Instruments

**Q:** How do we isolate this *exogenous variation* in our explanatory variable?
--
<br>**A:** Find an instrument (an instrumental variable).

--

**Q:** What's an instrument?
--
<br>**A:** An **instrument** is a variable that is

1. **correlated** with the **explanatory variable** of interest (*relevant*),
2. **uncorrelated** with the **error** term (*exogenous*).

---

## Instruments


So if we want an instrument $z_i$ for endogenous veteran status in

$$\begin{align}
  \text{Earnings}_i = \beta_0 + \beta_1 \text{Veteran}_i + u_i
\end{align}$$

1. **Relevant:** $\mathop{\text{Cov}} \left( \text{Veteran}_i,\, z_i \right) \neq 0$
2. **Exogenous:** $\mathop{\text{Cov}} \left( z_i,\, u_i \right) = 0$

---
## Instruments: Relevance

**Relevance:** We need the instrument to cause a change in (correlate with) our endogenous explanatory variable.

We can actually test this requirement using regression and a *t* test.

--

***Example:*** For the veteran status, consider three potential instruments:

.pull-left[
1. Social security number

2. Physical fitness

3. Vietnam War draft
]

--

.pull-right[ 
- **Probably not relevant** uncorrelated with military service

- *Potentially relevant* service may correlate with fitness

- **Relevant** being drafted led to service
]

---


## Instruments: Exogeneity

.hi[Exogeneity:] The instrument to be independent of omitted factors that affect our outcome variable‚Äîas good as randomly assigned.

$z_i$ must be uncorrelated with our disturbance $u_i$. .hi[Not testable.]

--

***Example:*** For the .pink[veteran status], consider three potential instruments:

.pull-left[
1. Social security number

2. Physical fitness

3. Vietnam War draft
]

--

.pull-right[ 
- **Exogenous** SSN essentially random

- *Not Exogenous* fitness correlated with many things

- **Exogenous** draft via lottery
]

---
## Relevant and Exogenous


```{r venn_iv, echo = F, fig.height = 7.5}
# Colors (order: x1, x2, x3, y, z)
venn_colors <- c("purple", "red", "grey60", "orange", "blue")
# Line types (order: x1, x2, x3, y, z)
venn_lines <- c("solid", "dotted", "dotted", "solid", "solid")
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    1.5,   -1.0, -1.4),
  y  = c( 0.0,   -2.5,   -1.8,    2.0, -2.6),
  r  = c( 1.9,    1.5,    1.5,    1.3,  1.3),
  l  = c( "Y", "D", "X[2]", "X[3]",  "Z"),
  xl = c( 0.0,    0.7,    1.6,   -1.0, -2.9),
  yl = c( 0.0,   -3.8,   -1.9,    2.2, -2.6)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, parse = T) +
annotate(
  x = -5.5, y = 3.3,
  geom = "text", label = "Figure 1", size = 10, hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

---
## Relevant, Not Exogenous

```{r venn_iv_endog, echo = F, fig.height = 7.5}
# Change locations of circles
venn_df %>%
mutate(
  x = x +   c(0, 0, 0, 0, 0),
  xl = xl + c(0, 0, 0, 0, 0),
  y = y +   c(0, 0, 0, 0, 1),
  yl = yl + c(0, 0, 0, 0, 1)
) %>%
# Venn
ggplot(data = ., aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, parse = T) +
annotate(
  x = -5.5, y = 3.3,
  geom = "text", label = "Figure 2", size = 10, hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

---
## Not Relevant and Not Exogenous


```{r venn_iv_irrelevant, echo = F, fig.height = 7.5}
# Change locations of circles
venn_df %>%
mutate(
  x = x +   c(0, 0, 0, 0,-1),
  xl = xl + c(0, 0, 0, 0,-1),
  y = y +   c(0, 0, 0, 0, 2.3),
  yl = yl + c(0, 0, 0, 0, 2.3)
) %>%
# Venn
ggplot(data = ., aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, parse = T) +
annotate(
  x = -5.5, y = 3.3,
  geom = "text", label = "Figure 3", size = 10, hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

---
## Relevant, Not Exogenous
```{r venn_iv_endog2, echo = F, fig.height = 7.5}
# Change locations of circles
venn_df %>%
mutate(
  x = x +   c(0,    0,   0, 0,    2),
  xl = xl + c(0, -2.4, 0.8, 0,  4.6),
  y = y +   c(0,    0,   0, 0,    0),
  yl = yl + c(0,    0,   0, 0, -1.1)
) %>%
# Venn
ggplot(data = ., aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, parse = T) +
annotate(
  x = -5.5, y = 3.3,
  geom = "text", label = "Figure 4", size = 10, hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

---
# Venn diagram explanation

In these figures (Venn diagrams)

- Each circle illustrates a variable.
- Overlap gives the share of correlatation between two variables.
- Dotted borders denote *omitted* variables.

Take-aways

- Figure 1: .hi-pink[Valid instrument] (relevant; exogenous)
- Figure 2: .hi-slate[Invalid instrument] (relevant; not exogenous)
- Figure 3: .hi-slate[Invalid instrument] (not relevant; not exogenous)
- Figure 4: .hi-slate[Invalid instrument] (relevant; not exogenous)

---
## IV Applications

```{r, echo=F}
knitr::include_graphics("https://pbs.twimg.com/media/EJGyHnyUYAA-yhM?format=jpg&name=large")
```

[@AndrewHeiss](https://twitter.com/andrewheiss/status/1193931226865901569?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1193931226865901569%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2Fandrewheiss2Fstatus2F1193931226865901569widget%3DTweet)



---
## IV Summary

Instrumental variables require a number of assumptions to yield credible causal claims:

- Randomization
- Excludability
- First-stage relationship
- Monotonicity

Estimation and inference of IVs is beyond the scope of this course. 

  - See Edward Rubin's excellent [slides](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#85)
  - And Matt Blackwells [notes](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf)

- Understanding the identifying assumptions of IV can help you critique a study (even if the you don't fully understand the math)

---
class: inverse, middle, center
# üí° 
## Summary

---
## What you need to know

- Causal inference in observational and experimental studies is about counterfactual comparisons
- In observational studies, to make causal claims we generally make some  assumption of conditional independence:

$$
Y_i(1),Y_i(0), \perp D_i |X_i
$$

- The credibility of this assumption depends less on the data, and more on how the data were generated.
- **Selection on Observables** is rarely a credible assumption
- Observational designs that produce credible causal inference, leverage aspects of the world that create *natural experiments*
- You should be able to describe the logic and assumptions of common designs in social science 
  - **Difference-in-Differences:** *Parallel Trends* 
  - **Regression Discontiniuity:** *Continuity at the cutoff*
  - **Instrumental Variables:** Instruments need to be *Relevant and Exogenous*


