<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 05:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Paul Testa" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <script src="libs/jquery/jquery-3.6.0.min.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="css/brown.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Week 05:
## Prediction with Linear Regression
### Paul Testa

---


---
class: inverse, center, middle
background-image:url("https://img1.hscicdn.com/image/upload/f_auto/lsci/db/PICTURES/CMS/332100/332134.4.jpg")
background-size:cover

# Overview

















---
## General Plan

- Group Assignment 1: Research Questions
- Setup
  - Packages
  - Data
- Feedback
- Review
  - Casual inference
  - Covariate Adjustment
- Linear Models
  - The Basics
  - The Mechanics
  - The Intuitions
- Three Common Observational Designs
  - Difference in Difference
  - Regression Discontinuity
  - Instrumental Variable Designs

---
class: inverse, center, middle
# Research Questions

---
## Research Questions

- Template [here](https://pols1600.paultesta.org/files/assignments/A1_research_questions.docx)

- Upload to [Canvas](https://canvas.brown.edu/courses/1087979/assignments/7870538?module_item_id=10762418) this Sunday

- 3 Research questions:
  
  - One-sentence research question
  
  - Why do we care
  
  - Ideal experiment
  
  - Observational study
  
  - Feasibility



---
class:inverse, middle, center
# 💪
## Get set up to work

---
## New packages

Hopefully, you were all able to install the following packages 


```r
install.packages("dataverse")
install.packages("tidycensus")
install.packages("easystats")
install.packages("DeclareDesign")
```


---
## Census API:

Additionally, I hope you have all followed the steps [here](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package):

1. Install the `tidycensus` package
2. Load the installed package
3. Request an API key from the Census
4. Check your email
5. Activate your key
6. Install your API key in R
7. Check that everything worked

To install the an API key so we can download data directly from the US Census


---
## Packages for today



```r
the_packages &lt;- c(
  ## R Markdown
  "kableExtra","DT",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
* "COVID19","maps","mapdata","qss","tidycensus", "dataverse",
  # Analysis
* "DeclareDesign", "easystats", "zoo"
)
```

---
## Define a function to load (and if needed install) packages



```r
ipak &lt;- function(pkg){
    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}
```

---
## Load packages for today


```r
ipak(the_packages)
```

```
   kableExtra            DT     tidyverse     lubridate       forcats 
         TRUE          TRUE          TRUE          TRUE          TRUE 
        haven      labelled         ggmap       ggrepel      ggridges 
         TRUE          TRUE          TRUE          TRUE          TRUE 
     ggthemes        ggpubr        GGally        scales       dagitty 
         TRUE          TRUE          TRUE          TRUE          TRUE 
        ggdag       ggforce       COVID19          maps       mapdata 
         TRUE          TRUE          TRUE          TRUE          TRUE 
          qss    tidycensus     dataverse DeclareDesign     easystats 
         TRUE          TRUE          TRUE          TRUE          TRUE 
          zoo 
         TRUE 
```


---
class:inverse, center, middle
# 💪
## Load Data for today

---
## Load the Covid-19 Data


```r
covid &lt;- COVID19::covid19(
  country = "US",
  level = 2,
  verbose = F
)
```

---
## Filter Covid-19 Data to US States


```r
# Vector containing of US territories
territories &lt;- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )

# Filter out Territories and create state variable
covid_us &lt;- covid %&gt;%
  filter(!administrative_area_level_2 %in% territories)%&gt;%
  mutate(
    state = administrative_area_level_2
  )
```

---
## Mutate: Calculate New Cases



```r
covid_us %&gt;%
  dplyr::group_by(state) %&gt;%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000,
    new_cases_pc_7da = zoo::rollmean(new_cases_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA )
    ) -&gt; covid_us
```

---
## Calculating a Rolling Average New Cases


```r
covid_us %&gt;%
  filter(date &gt; "2020-03-05") %&gt;%
  select(date,new_cases_pc,new_cases_pc_7da)
```

```
# A tibble: 36,974 × 4
# Groups:   state [51]
   state     date       new_cases_pc new_cases_pc_7da
   &lt;chr&gt;     &lt;date&gt;            &lt;dbl&gt;            &lt;dbl&gt;
 1 Minnesota 2020-03-06      NA               NA     
 2 Minnesota 2020-03-07       0               NA     
 3 Minnesota 2020-03-08       0.0177          NA     
 4 Minnesota 2020-03-09       0               NA     
 5 Minnesota 2020-03-10       0.0177          NA     
 6 Minnesota 2020-03-11       0.0355          NA     
 7 Minnesota 2020-03-12       0.0709          NA     
 8 Minnesota 2020-03-13       0.0887           0.0329
 9 Minnesota 2020-03-14       0.124            0.0507
10 Minnesota 2020-03-15       0.248            0.0836
# … with 36,964 more rows
```

---
## New Case Per Capita

.pull-left[

```r
covid_us %&gt;%
  filter(date &gt; "2020-03-05", 
         state == "Minnesota") %&gt;%
  select(date,
         new_cases_pc,
         new_cases_pc_7da)%&gt;%
  ggplot(aes(date,new_cases_pc ))+
  geom_line(aes(col="Daily"))+
  theme(legend.position="bottom")+
    labs( col = "Measure",
    y = "New Cases Per 100k", x = "",
    title = "Minnesota"
  ) -&gt; fig_covid_mn 
```
]

.pull-right[
&lt;img src="05-slides_files/figure-html/plotmean2-1.png" width="80%" style="display: block; margin: auto;" /&gt;
]

---
## New Case Per Capita vs 7-day average

.pull-left[

```r
fig_covid_mn +
  geom_line(aes(y = new_cases_pc_7da,
                col = "7-day average")
            ) -&gt; fig_covid_mn
```
]

.pull-right[
&lt;img src="05-slides_files/figure-html/plotmean7d2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

]

---
## Facemask Policy


```r
covid_us %&gt;%
mutate(
  # Recode facial_coverings to create face_masks
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ),
    # Turn face_masks into a factor with ordered policy levels
    face_masks = factor(face_masks,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -&gt; covid_us
```

---
## Mutate: Dates and Vaccinations


```r
covid_us %&gt;%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(year, 
                       str_pad(month, width = 2, pad=0), 
                       sep = "-"),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -&gt; covid_us
```


---
## Load Data on Presidential Elections




```r
# This joyously stopped working last night...
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

pres_df &lt;- get_dataframe_by_name(
  "1976-2020-president.tab",
  "doi:10.7910/DVN/42MVDX"
)

# Just in case
# load(url("https://pols1600.paultesta.org/files/data/pres_df.rda"))
```

---
## HLO of Presidential Elections Data


```r
head(pres_df)
```

```
  year   state state_po state_fips state_cen state_ic       office
1 1976 ALABAMA       AL          1        63       41 US PRESIDENT
2 1976 ALABAMA       AL          1        63       41 US PRESIDENT
3 1976 ALABAMA       AL          1        63       41 US PRESIDENT
4 1976 ALABAMA       AL          1        63       41 US PRESIDENT
5 1976 ALABAMA       AL          1        63       41 US PRESIDENT
6 1976 ALABAMA       AL          1        63       41 US PRESIDENT
                        candidate             party_detailed writein
1                   CARTER, JIMMY                   DEMOCRAT   FALSE
2                    FORD, GERALD                 REPUBLICAN   FALSE
3                  MADDOX, LESTER AMERICAN INDEPENDENT PARTY   FALSE
4 BUBAR, BENJAMIN \\"\\"BEN\\"\\"                PROHIBITION   FALSE
5                       HALL, GUS        COMMUNIST PARTY USE   FALSE
6                 MACBRIDE, ROGER                LIBERTARIAN   FALSE
  candidatevotes totalvotes  version notes party_simplified
1         659170    1182850 20210113    NA         DEMOCRAT
2         504070    1182850 20210113    NA       REPUBLICAN
3           9198    1182850 20210113    NA            OTHER
4           6669    1182850 20210113    NA            OTHER
5           1954    1182850 20210113    NA            OTHER
6           1481    1182850 20210113    NA      LIBERTARIAN
```

---
## Transform Data to get just 2020 Election


```r
pres_df %&gt;%
  mutate(
    state = str_to_title(state)
  ) %&gt;%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%&gt;%
  filter(year == 2020) %&gt;%
  select(state, year, party_simplified, candidatevotes, totalvotes
         ) %&gt;%
  spread(party_simplified,candidatevotes) %&gt;%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes,
    rep_voteshare = REPUBLICAN/totalvotes,
    winner = ifelse(rep_voteshare &gt; dem_voteshare,"Trump","Biden")

  ) -&gt; pres2020_df
```

---
## Transform Data to get just 2020 Election


```r
head(pres2020_df)
```

```
       state year totalvotes DEMOCRAT REPUBLICAN dem_voteshare rep_voteshare
1    Alabama 2020    2323282   849624    1441170     0.3656999     0.6203164
2     Alaska 2020     359530   153778     189951     0.4277195     0.5283314
3    Arizona 2020    3387326  1672143    1661686     0.4936469     0.4905598
4   Arkansas 2020    1219069   423932     760647     0.3477506     0.6239573
5 California 2020   17500881 11110250    6006429     0.6348395     0.3432072
6   Colorado 2020    3279980  1804352    1364607     0.5501107     0.4160413
  winner
1  Trump
2  Trump
3  Biden
4  Trump
5  Biden
6  Biden
```


---
## Load Data on Median State Income from the Census


```r
acs_df &lt;- get_acs(geography = "state", 
              variables = c(med_income = "B19013_001",
                            med_age = "B01002_001"), 
              year = 2019)
```

---
## HLO: Census Data 


```r
head(acs_df)
```

```
# A tibble: 6 × 5
  GEOID NAME    variable   estimate    moe
  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;
1 01    Alabama med_age        39      0.2
2 01    Alabama med_income  50536    304  
3 02    Alaska  med_age        34.3    0.1
4 02    Alaska  med_income  77640   1015  
5 04    Arizona med_age        37.7    0.2
6 04    Arizona med_income  58945    266  
```


---
## Tidy Census Data


```r
acs_df %&gt;%
  mutate(
    state = NAME,
  ) %&gt;%
  select(state, variable, estimate) %&gt;%
  pivot_wider(names_from = variable,
              values_from = estimate) -&gt; acs_df
```

---
## Tidy Census Data


```r
head(acs_df)
```

```
# A tibble: 6 × 3
  state      med_age med_income
  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;
1 Alabama       39        50536
2 Alaska        34.3      77640
3 Arizona       37.7      58945
4 Arkansas      38.1      47597
5 California    36.5      75235
6 Colorado      36.7      72331
```


---
## Merge election data and covid data into single `df`

.pull-left[
- We're going to take our `covid_us` data and **merge** into this data on the 2020 election from `pres2020_df` using the common `state` variable in each data set for a `left_join()` 

- Always check the matches in your joining variable (i.e. `state`)

- Below we see that our recoding of state to title case in created a mismatch

]

.pull-right[

```r
# Should be 51
sum(pres2020_df$state %in% covid_us$state)
```

```
[1] 50
```

```r
# Find the mismatch:
pres2020_df$state[!pres2020_df$state %in% covid_us$state]
```

```
[1] "District Of Columbia"
```

```r
# Fix
pres2020_df$state[pres2020_df$state == "District Of Columbia"] &lt;- "District of Columbia"
# Problem Solved
sum(pres2020_df$state %in% covid_us$state)
```

```
[1] 51
```
]

---
## Merge election data into Covid data


```r
dim(covid_us)
```

```
[1] 38072    56
```

```r
dim(pres2020_df)
```

```
[1] 51  8
```

```r
df &lt;- covid_us %&gt;% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(df) # Same number of rows as covid_us w/ 7 additional columns
```

```
[1] 38072    63
```


---
## Merge Census data into Covid data


```r
dim(df)
```

```
[1] 38072    63
```

```r
dim(acs_df)
```

```
[1] 52  3
```

```r
df &lt;- df %&gt;% left_join(
  acs_df,
  by = c("state" = "state")
)
dim(df)  # Same number of rows as covid_us w/ 2 additional columns
```

```
[1] 38072    65
```

---

&lt;img src="./images/05_covid.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

[Red Covid, an Update](https://www.nytimes.com/2022/02/18/briefing/red-covid-partisan-deaths-vaccines.html) *New York Times*, 18 February, 2022


---
class: inverse, center, middle
background-image: url("https://i.pinimg.com/originals/a2/05/b6/a205b689caf19f3287b1544cbe0e6b7b.jpg")
background-size: contain
# 📢
## Feedback



---
## What we liked and learned

--

- Causal Inference!
- Data Viz
- Programming
- Review
- Dank Memes

---
## What we liked

<div id="htmlwidget-66c33c2f3c403c7ea829" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-66c33c2f3c403c7ea829">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9"],["I like when we review topics from before and the break down fo the slides","I find thinking about causal identification and observational designs really interesting. I'm looking forward to hopefully talking more about moral/ethical constraints there, especially since this is a class focused on social sciences and political research where the data can definitely be manipulated to target/influence divides.","The review at the start of class.","I liked how we reviewed things. The pace also seemed a bit less fast and more manageable","I liked the overview of means, and the introduction to regressions - it was interesting","Your memes :)\nUsing survey data to model our code","I like your cheerful energy!","I really like when you walk us through the visualizations you make step by step","I thought the general review was useful, and hope we can do a few more throughout the course just to make sure I'm keeping up"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Likes<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

```
[1] 4
```


---
## What we've learned

<div id="htmlwidget-334a6cda7251e23751c4" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-334a6cda7251e23751c4">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9"],["Programming for sure","I've been talking about the study we used in the lab last week since I first read it. I just found it so interesting and so promising that a 10 minute conversation with a stranger could build a substantial/measurable amount of empathy. Thinking about research design in that way, and the power statistics and the process of obtaining statistics has been cool.","Basics for setting up a R Markdown file and some syntax. Working with and creating vectors and modifying data.","The statistics aspect of class has brought back the useful things I learned in high school but forgot about, so it's nice to have the refresher.","I've started to really get comfortable with the R interface, and I understand basic descriptive statistics (mean, median, etc.) well.","That there are different \"code languages\"\nThat we can code to arrange data in cool ways as opposed to doing it manually","I think I've gotten better at coding, \"learning the language\" I also think all of the theoretical stuff makes sense.","I've learned a lot about the keyboard shortcuts in R and I think that's been really cool.","I've definitely gotten a lot more comfortable with R, I'm learning to look at data in a new and different way, I'm able to read data more critically, I can visualise it a lot more easily (or at least know more or less how to do so) and am semi-able to read code and have a basic idea of what it does"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Learn<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>


---
## What we disliked or struggled with


- Stats
  - DAGs
  - Regression
- Pacing
- Keeping track of various concepts and skills
- Lack of breaks
- Too much review

---
## What we disliked

<div id="htmlwidget-37f546f0e4c4e3719717" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-37f546f0e4c4e3719717">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9"],["The pace is a little fast. Its a lot of new info at once","I think review just generally makes it hard for me to engage fully, so I look forward to digging into new content.","In general I had some difficulty staying focused for the entire 1 hour and 20 minutes. I do not think it is reflective of the class itself, but rather of just how busy I am and how hectic everything is at the moment.","I feel pretty decent on a conceptual level with everything, but sometimes things are still a bit fuzzy. It is frustrating that there are so many different things to keep in mind, which are all necessary for a basic understanding of data sets and labs, but I think with time it will be easier.","I think we could use a bit of a longer break halfway through lecture","I just don't really understand the words you use during lecture","a little too much review, to be quite honest... but it was good to take a bit of a break this week...honestly a bit torn.","I wish there was a lab. I find that the hands on work is very helpful in learning","Maybe just because we haven't gotten too in depth with it, but regression I've found a little confusing, so if we could go over it not only in conceptual terms but also in practical terms I think that would be really useful"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Dislikes<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

```
[1] 4
```


---
## What we've struggled with

<div id="htmlwidget-ff3d251183942f4667fb" style="width:100%;height:90%;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ff3d251183942f4667fb">{"x":{"filter":"none","vertical":false,"fillContainer":false,"data":[["1","2","3","4","5","6","7","8","9"],["","I think some of the programming hasn't settled in, especially since copying and pasting means I don't have to the step-by-step typing (I still like copy and pasting though! it's definitely more streamlined and that doesn't need to change!), but more practice building code chains and understanding why they work will definitely be good","Basics for setting up a R Markdown file and some syntax. Working with and creating vectors and modifying data. Also, setting up diagrams/graphs. Some of the more complicated vectors and pipe functions?","Sometimes you bring up things that we don't have to remember or know about, but that can make me confused. Not always at the moment, but I can sometimes confuse the concept we don't have to understand with something we do.","I'm confused about acyclic graphs but I think we're going to learn about that more in the future","I understand what I wrote above, but still don't know how to go about coding","Sometimes I have difficulty understanding differences between the textbook and our version of R","Some of the statistics stuff has been over my head","I think mostly knowing when and how we're going to apply concepts in a more practical manner could be clarified (ie covariate adjustment) but that's a newer topic so hopefully it'll be covered soon!"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Struggle<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":3,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}],"lengthMenu":[3,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>


---
class:inverse, middle, center
# 🔍
## Review


---
## Review

- Casual Inference
  
- Covariate Adjustment

---
class:inverse, middle, center
# 🔍
## Causal Inference

---
## Review: Causal Inference 

- Causal inference is about counterfactual comparisons

--

- Some counterfactuals are easier to imagine or create than others

--

- Randomization solves the fundamental problem of causal inference allowing us to estimate average treatment effects free from selection bias

--

- Randomization is not always possible, desirable, or ethical

--

- Observational designs that try to estimate causal effects need to justify assumptions about conditional independence:

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

--

- This assumption goes by many, jargony names: Selection on Observables, Conditional Independence, No unmeasured confounders.

--

- Credibility of this assumption depends less on having a lot of data, and more on how your data were generated.

--

- Observational designs estimate the effect of `\(D\)` conditional on `\(X\)` using covariate adjustment.


---
class:inverse, middle, center
# 🔍
## Covariate Adjustment

---
## Review: Covariate Adjustment

- Covariate adjustment are a set of statistical procedures that allow us to estimate conditional values 
  - `\(e.g.(E[Y|X=x]), E[Happiness| leetCoder == T]\)`

- We adjust for covariates to improve our predictions and make credible comparisons
  - `\(E[\text{New Covid-19 cases}|\text{Face mask policy}]\)` vs `\(E[\text{New Covid-19 cases}|\text{Face mask policy |June 2020 },]\)`
  - `\(ATE = E[Y| D = 1] - E[Y | D=0]\)`

- Three approaches:
  - Subclassification
  - Matching
  - Regression

---
## Subclassification

- Subclassification is a simple way to adjust for a covariate
  
  - Subset the data to include only the values you want ($X=x$, `\(D=1\)`, `\(Interverntion = Treated\)`) and calculate the quantity of interest (e.g. a conditional mean, `mean(df$income[df$age &lt;30])`)

--

- But what if we want to control for more than one variable? 

- What if our variables aren't categorical like sex, but continuous like height?

--

- **The Curse of Dimensionality** as you attempt to adjust for more covariates (add more dimensions), the space of possible combinations grows exponentially
  
  - Assumption of Common Support likely to be violated `\(0 &lt; Pr(D_i = 1|X_i) &lt; 1\)` 

---
## Matching

- Matching refers to a broad set of procedures that try to recreate what randomization provides: **covariate balance**
  
  - **covariate balance** is a fancy term for saying that in the aggregate, two groups look similar accept on group received the *treatment* `\((D=1)\)` while another group did not `\((D=0)\)`

--

- There are many ways to try to match observations:
  - **Exact matching:** Find exact matches between treatment and control observations for all covariates `\(X\)`. Only works for a few covariates.
  - **Coarsened exact matching:** Find approximate matches within ranges of values for `\(X\)`
  - **Matching on summaries of the covariates** calculating  a single measure of the similarility of observations and matching on this summary to produce covariate balance.

--

- Matching:
  - Conceptually appealing (mirrors the logic of an experiment)
  - Technically complex (complicated algorhitms, finicky software) 
  - Only provides balance on **observed covariates**



---
class: inverse, center, middle
# 💡  
# Linear Regression
## The Basics

---
## Understanding Linear Regression

- **Conceptual**
  - Simple linear regression estimates "a line of best fit" that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function
  

```r
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses `\(\beta_0\)` and `\(\beta_1\)` to minimize the Sum of Squared Residuals (SSR): 

`$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$`

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): `\(E[Y|X]\)`

---
class: inverse, center, middle
# 💡  
# Conceptual: Linear Regression
## Linear Regression Provides an Estimate of the Line of Best Fit

---
## Conceptual: Linear Regression 

- Regression is a tool for describing relationships.
  
  - How does some outcome we're interested in tend to change as some predictor of that outcome changes?
  
  - How does economic development vary with democracy?
  
  - How does economic development vary with democracy, adjusting for natural resources like oil and gas

- Formally:

$$
y_i = f(x_i) + \epsilon
$$

  - Y is a function of X plus some error, `\(\epsilon\)`

- Linear regression assumes that relationship between an outcome and a predictor can be by a [linear](https://en.wikipedia.org/wiki/Linearity) function 

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$


---
##  Linear Regression and the Line of Best Fit


- The goal of linear regression is to choose coefficients `\(\beta_0\)` and `\(\beta_1\)`  to summarizes the relationship between `\(y\)` and `\(x\)`

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- To accomplish this we need some sort of criteria. 

- For linear regression, that criteria is minimizing the error between what our model predicts `\(\hat{y_i} = \beta_0 + \beta_1 x_i\)` and what we actually observed `\((y_i)\)` 

- More on this to come. But first...



---
## Regression Notation

- `\(y_i\)` an **outcome variable** or thing we're trying to explain
  
  - AKA: The dependent variable, The response Variable, The left hand side of the model

- `\(x_i\)` a **predictor variables** or things we think explain variation in our outcome
  
  - AKA: The independent variable, covariates, the right hand side of the model.
  
  - Cap or No Cap: I'll use  `\(X\)` (should be `\(\mathbf{X}\)`) to denote a set (matrix) of predictor variables. `\(y\)` vs `\(Y\)` can also have technical distinctions (Sample vs Population, observed value vs Random Variable, ...)

- `\(\beta\)` a set of **unknown parameters** that describe the relationship between our outcome `\(y_i\)` and our predictors `\(x_i\)`

- `\(\epsilon\)` the **error term** representing variation in  `\(y_i\)` not explained by our model.


---
## Linear Regression

Let's return to the simple (bivariate) linear regressions we introduced last week:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- We call this a bivariate regression, because there are only two variables.

- We call this a linear regression, because `\(y_i = \beta_0 + \beta_1 x_i\)` is the equation for a line, where:
 
  - `\(\beta_0\)` corresponds to the `\(y\)` intercept, or the model's prediction when `\(x = 0\)`.
 
  - `\(\beta_1\)` corresponds to the slope, or how `\(y\)` is predicted to change as `\(x\)` changes.

---
## Linear Regression 
  
- If you find this notation confusing, try plugging in substantive concepts for what `\(y\)` and `\(x\)` represent
- Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.

The generic linear model

`$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$`

Reflects:

`$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$`

---
## Practical: Estimating a Linear Regression

- We estimate linear regressions in `R` using the `lm()` function.
- `lm()` requires two arguments:
  - a `formula` argument of the general form `y ~ x` read as "Y modeled by X" or below "Transgender Feeling Thermometer (`y`) modeled by (`~`) Age (`x`)
  - a `data` argument telling R where to find the variables in the formula
  


```r
load(url("https://pols1600.paultesta.org/files/data/03_lab.rda"))
m1 &lt;- lm(therm_trans_t0 ~ vf_age, data = df)
m1
```

```

Call:
lm(formula = therm_trans_t0 ~ vf_age, data = df)

Coefficients:
(Intercept)       vf_age  
    62.8196      -0.2031  
```

---

The coefficients from `lm()` are saved in object called `m1` 
  

```r
m1
```

```

Call:
lm(formula = therm_trans_t0 ~ vf_age, data = df)

Coefficients:
(Intercept)       vf_age  
    62.8196      -0.2031  
```

`m1` actually contains a lot of information


```r
names(m1)
```

```
 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "na.action"     "xlevels"       "call"          "terms"        
[13] "model"        
```

```r
m1$coefficients
```

```
(Intercept)      vf_age 
 62.8195994  -0.2030711 
```

---
## Practical: Interpreting a Linear Regression

We can extract the intercept and slope from this simple bivariate model, using the `coef()` function


```r
# All the coefficients
coef(m1)
```

```
(Intercept)      vf_age 
 62.8195994  -0.2030711 
```

```r
# Just the intercept
coef(m1)[1]
```

```
(Intercept) 
    62.8196 
```

```r
# Just the slope
coef(m1)[2]
```

```
    vf_age 
-0.2030711 
```

---
## Practical: Interpreting a Linear Regression

The two coefficients from `m1` define a line of best fit, summarizing how feelings toward transgender individuals change with age


`$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$`

`$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$`

`$$\text{Transgender Feeling Thermometer}_i = 62.82 + -0.2 \text{Age}_i + \epsilon_i$$`
---
## Practical: Predicted values from a Linear Regression

- Often it's useful for interpretation to obtain predicted values from a regression. 


- To obtain predicted vales `\((\hat{y})\)`, we simply plug in a value for `\(x\)` (In this case, `\(Age\)`) and evaluate our equation.

- For example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?

`$$\hat{FT}_{x=18} = 62.82 + -0.2 \times 18  = 59.16$$`
`$$\hat{FT}_{x=65} = 62.82 + -0.2 \times 68  = 49.01$$`

---
## Practical: Predicted values from a Linear Regression

We could do this by hand


```r
coef(m1)[1] + coef(m1)[2] * 18
```

```
(Intercept) 
   59.16432 
```

```r
coef(m1)[1] + coef(m1)[2] * 68
```

```
(Intercept) 
   49.01076 
```

---
## Practical: Predicted values from a Linear Regression

More often we will:

- Make a prediction data frame (called `pred_df` below) with the values of interests
- Use the `predict()` function with our linear model (`m1`) and `pred_df`
- Save the predicted values to our new column in our prediction data frame



---
## Practical: Predicted values from a Linear Regression


```r
# Make prediction data frame
pred_df &lt;- data.frame(
  vf_age = c(18, 68)
)
# Predict FT for 18 and 68 year-olds
predict(m1, newdata = pred_df)
```

```
       1        2 
59.16432 49.01076 
```

```r
# Save predictions to data frame
pred_df$ft_trans_hat &lt;- predict(m1, newdata = pred_df)
pred_df
```

```
  vf_age ft_trans_hat
1     18     59.16432
2     68     49.01076
```



---
## Practical: Visualizing Linear Regression

We can visualize simple regression by:

- plotting a scatter plot of the outcome (y-axis) and predictors (x-axis)

- overlaying the line defined by `lm()`

---

```r
fig_lm &lt;- df %&gt;%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  geom_abline(intercept = coef(m1)[1],
              slope = coef(m1)[2],
              col = "blue"
              )+
  geom_vline(xintercept = 0,linetype = 2)+
  xlim(0,100)+
  annotate("point",
           x = 0, y = coef(m1)[1],
           col= "red",
           )+
  annotate("text",
           label = expression(paste(beta[0],"= 62.81" )),
           x = 1, y = coef(m1)[1]+5,
           hjust = "left",
           )+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People"
  )+
  theme_classic()
fig_lm
```

---
&lt;img src="05-slides_files/figure-html/fig_lm_plot-1.png" width="80%" style="display: block; margin: auto;" /&gt;



---
&lt;img src="05-slides_files/figure-html/figlm1code-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
&lt;img src="05-slides_files/figure-html/figlm1-1.png" width="80%" style="display: block; margin: auto;" /&gt;






---
background-image:url("https://i.imgflip.com/66mx9d.jpg")
background-size:contain

---
class: inverse, center, middle
# 💡  
# Technical: Linear Regression
## The Mechanics of Minimizing the Sum of Squared Errors

---
## How did `lm()` choose `\(\beta_0\)` and `\(\beta_1\)`

--

- P: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression

--

- Q: Ok, that's not really that helpful...

  - What's an error?
  - Why would we square and sum them
  - How do we minimize them.

P: Good questions!


---
## What's an error?

An error, `\(\epsilon_i\)` is simply the difference between the observed value of `\(y_i\)` and what our model would predict, `\(\hat{y_i}\)` given some value of `\(x_i\)`. So for a model:

`$$y_i=\beta_0+\beta_1 x_{i} + \epsilon_i$$`

We simply subtract our model's prediction `\(\beta_0+\beta_1 x_{i}\)` from the the observed value, `\(y_i\)`

`$$\hat{\epsilon_i}=y_i-\hat{y_i}=(Y_i-(\beta_0+\beta_1 x_{i}))$$`

To get `\(\epsilon_i\)`

---
## Why are we squaring and summing `\(\epsilon\)`

There are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)

  - Squaring `\(\epsilon\)` treats positive and negative residuals equally.
  
  - Summing produces single value summarizing our models overall performance.
  
There are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties


---
## How do we minimize `\(\sum \epsilon^2\)`

OLS chooses `\(\beta_0\)` and `\(\beta_1\)` to minimize `\(\sum \epsilon^2\)`, the Sum of Squared Residuals (SSR)

`$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$`

---
## How did `lm()` choose `\(\beta_0\)` and `\(\beta_1\)`

In an intro stats course, we would walk through the process of finding

`$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$`
Which involves a little bit of calculus. The big payoff is that 

`$$\beta_0 = \bar{y} - \beta_1 \bar{x}$$`
And 

$$ \beta_1 = \frac{Cov(x,y)}{Var(x)}$$
Which is never quite the epiphany, I think we think it is...

The following slides walk you through the mechanics of this exercise. We're gonna skip through them in class, but they're there for your reference




---
## How do we minimize `\(\sum \epsilon^2\)`

To understand what's going on under the hood, you need a broad understanding of some basic calculus.

The next few slides provide a brief review of derivatives and differential calculus.


---
## Derivatives


The derivative of `\(f\)` at `\(x\)` is its rate of change at `\(x\)`

- For a line: the slope
- For a curve: the slope of a line tangent to the curve

You'll see two notations for derivatives:

1. Leibniz notation:

$$
\frac{df}{dx}(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

2. Lagrange: `\(f^{\prime}(x)\)`


---
## Some useful Facts about Derivatives

Derivative of a constant

$$
f^{\prime}(c)=0
$$

Derivative of a line f(x)=2x

$$
f^{\prime}(2x)=2
$$

Derivative of `\(f(x)=x^2\)`

$$
f^{\prime}(x^2)=2x
$$

Chain rule: y= f(g(x)). The derivative of y with respect to x is

$$
\frac{d}{dx}(f(g(x)))=f^{\prime}(g(x))g^{\prime}(x)
$$

The derivative of the "outside" times the derivative of the "inside," remembering that the derivative of the outside function is evaluated at the value of the inside function.

---
## Finding a Local Minimums

.pull-left[
Local minimum:

$$
f^{\prime}(x)=0 \text{ and } f^{\prime\prime}(x)&gt;0 
$$
]

.pull-right[
&lt;img src="https://copingwithcalculus.com/SecondDeriv1.png" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://copingwithcalculus.com/SecondDerivativeTest.html)
]


---
## Partial Derivatives

Let `\(f\)` be a function of the variables `\((x, \dots, X_n)\)`. The partial derivative of `\(f\)` with respect to `\(X_i\)` is

`$$\begin{align*}
\frac{\partial f(x, \dots, X_n)}{\partial X_i}=\lim_{h\to0}\frac{f(x, \dots X_i+h \dots, X_n)-f(x, \dots X_i \dots, X_n)}{h}
\end{align*}$$`

&lt;img src="https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)

---
## Minimizing the sum of squared errors

Our model

`$$y_i =\beta_0+\beta_1x_{i}+\epsilon_i$$`

Finds coefficients `\(\beta_0\)` and `\(\beta_1\)` to to minimize the sum of squared residuals, `\(\hat{\epsilon}_i\)`:

`$$\begin{aligned}
\sum \hat{\epsilon_i}^2 &amp;= \sum (y_i-\beta_0-\beta_1 x_{i})^2
\end{aligned}$$`


---
## Minimizing the sum of squared errors

We solve for `\(\beta_0\)` and `\(\beta_1\)`, by taking the partial derivatives with respect to `\(\beta_0\)` and `\(\beta_1\)`, and setting them equal to zero

`$$\begin{aligned}
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial \beta_0} &amp;= -2\sum (y_i-\beta_0-\beta_1 x_{i})=0 &amp; f'(-x^2) = -2x\\
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial\beta_1} &amp;= -2\sum (y_i-\beta_0-\beta_1 x_{i})x_{i}=0 &amp; \text{chain rule}
\end{aligned}$$`


---
## Solving for `\(\beta_0\)`

First, we'll solve for `\(\beta_0\)`, by multiplying both sides by -1/2 and distributing the `\(\sum\)`:

`$$\begin{aligned}
0 &amp;= -2\sum (y_i-\beta_0-\beta_1 x_{i})\\
\sum \beta_0 &amp;= \sum y_i - \sum \beta_1 x_{i}\\
N \beta_0 &amp;= \sum y_i -\sum \beta_1 x_{i}\\
\beta_0 &amp;= \frac{\sum y_i}{N} - \frac{\beta_1 \sum x_{i}}{N}\\
\beta_0 &amp;= \bar{y} - \beta_1 \bar{x}
\end{aligned}$$`

---
## Solving for `\(\beta_1\)`

Now, we can solve for `\(\beta_1\)` plugging in `\(\beta_0\)`.

`$$\begin{aligned}
0 &amp;= -2\sum [(y_i-\beta_0-\beta_1 x_{i})x_{i}]\\
0 &amp;= \sum [y_ix_i-(\bar{y} - \beta_1 \bar{x})x_{i}-\beta_1 x_{i}^2]\\
0 &amp;= \sum [y_ix_i-\bar{y}x_{i} + \beta_1 \bar{x}x_{i}-\beta_1 x_{i}^2]
\end{aligned}$$`

---
## Solving for `\(\beta_1\)`

Now we'll rearrange some terms and pull out an `\(x_{i}\)` to get


`$$\begin{aligned}
0 &amp;= \sum [(y_i -\bar{y} + \beta_1 \bar{x}-\beta_1 x_{i})x_{i}]
\end{aligned}$$`

Dividing both sides by `\(x_{i}\)` and distributing the summation, we can isolate `\(\beta_1\)`

`$$\begin{aligned}
\beta_1 \sum (x_{i}-\bar{x}) &amp;= \sum (y_i -\bar{y})
\end{aligned}$$`

Dividing by `\(\sum (x_{i}-\bar{x})\)` to get

`$$\begin{aligned}
\beta_1  &amp;= \frac{\sum (y_i -\bar{y})}{\sum (x_{i}-\bar{x})}
\end{aligned}$$`

---
## Solving for `\(\beta_1\)`

Finally, by multiplying by `\(\frac{(x_{i}-\bar{x})}{(x_{i}-\bar{x})}\)` we get

`$$\begin{aligned}
\beta_1  &amp;= \frac{\sum (y_i -\bar{y})(x_{i}-\bar{x})}{\sum (\bar{x}-x_{i})^2}
\end{aligned}$$`

Which has a nice interpretation: 

`$$\begin{aligned}
\beta_1 &amp;= \frac{Cov(x,y)}{Var(x)}
\end{aligned}$$`

So the coefficient in a simple linear regression of `\(Y\)` on `\(X\)` is simply the ratio of the covariance between `\(X\)` and `\(Y\)` over the variance of `\(X\)`. Neat! 


---
class: inverse, center, middle
# 💡  
# Theoretical: Linear Regression
## OLS provides a linear estimate of CEF: E[Y|X]

---
## Linear Regression is a many splendored thing

[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of the various interpretations/motivations for linear regression.

- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)

- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of `\(y\)` on the subspace spanned by `\(X\beta\)`

- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)

- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)

- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)

- A [linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)


---
## Linear Regression is a many splendored thing

[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of various interpretations/motivations for linear regression.

- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)

- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of `\(y\)` on the subspace spanned by `\(X\beta\)`

- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)

- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)

- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)

- A **[linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)**


---
## The Conditional Expectation Function

Of all the functions we could choose to describe the relationship between `\(Y\)` and `\(X\)`, 

$$
Y_i = f(X_i) + \epsilon_i
$$

the conditional expectation of `\(Y\)` given `\(X\)` `\((E[Y|X])\)`, has some appealing properties

$$
Y_i = E[Y_i|X_i] + \epsilon
$$

The error, by definition, is uncorrelated with X and `\(E[\epsilon|X]=0\)` 

$$
E[\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0
$$

Of all the possible functions `\(g(X)\)`, we can show that $E[Y_i|X_i] $ is the best predictor in terms of minimizing **mean squared error**

$$
E[ (Y - g(Y))^2] \geq E[(Y - E[Y|X])^2] 
$$

---
## Linear Approximations to the  Conditional Expectation Function

- We can then show (in a different class) that linear regression provides the best linear predictor of the CEF
  - Chapter 3, of [Mostly Harmless Econometrics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991028523169706966)
  - Chapter 4 of [Foundations of Agnostic Statistics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991000736119706966)

- Furthermore, when the CEF is linear, it's equal exactly to OLS regression


---

```r
df %&gt;%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  stat_summary(geom="point", aes(col="CEF"))+
  stat_summary(geom="line", aes(col="CEF"))+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People",
    col = ""
  )+
  theme_classic() -&gt; plot_cef
```

---
&lt;img src="05-slides_files/figure-html/cef1-1.png" width="80%" style="display: block; margin: auto;" /&gt;



---

```r
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -&gt; plot_cef
plot_cef
```

---
&lt;img src="05-slides_files/figure-html/cef2-1.png" width="80%" style="display: block; margin: auto;" /&gt;



---

```r
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -&gt; plot_cef
plot_cef
```

---
&lt;img src="05-slides_files/figure-html/cef3-1.png" width="80%" style="display: block; margin: auto;" /&gt;



---
## What you need to know about Regression

- **Conceptual**
  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function
  

```r
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses `\(\beta_0\)` and `\(\beta_1\)` to minimize the Sum of Squared Residuals (SSR): 

`$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$`

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): `\(E[Y|X]\)`

---
class: center, middle
background-image:url(https://www.memecreator.org/static/images/memes/5312518.jpg)
background-size:contain

---
class: inverse, center, middle
# 💡  
# Three Designs for Causal Inference in Observational Studies


---
## Credible Cauasal Inference in Observational Studies

Subclassification, matching, and regression all require an assumption of selection on observables:

$$
Y_i(1),Y_i(0) \perp D_i |x_i
$$

But how do we know if we've got the right model or we've controlled for the right variables?

--

Typically, we don't

---
## Credible Cauasal Inference in Observational Studies

Instead, social scientists  look for situations where the credibility of

$$
Y_i(1),Y_i(0) \perp D_i |X_i
$$

depends less on how much data you have and much more on how your data were generated.



---
class: middle

.pull-left[
&gt; Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact. ... [T]he primary engine driving improvement has been a focus on the **quality of empirical research designs.** (p. 4)

]

.pull-right[
&lt;img src="./images/04_cred.png" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)

]

---
class: middle

.pull-left[
&gt; Design-based studies are distinguished by their prima facie credibility and by the attention investigators devote to making both an **institutional** and a **data-driven** case for causality (p. 5)

]

.pull-right[
&lt;img src="./images/04_cred.png" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)
]

---
class: middle

.pull-left[
&gt; The econometric methods that feature most prominently in quasi-experimental studies are **instrumental variables**, **regression discontinuity** methods,and **differences-in-differences**-style policy analysis. ... The best of today’s design-based studies make a strong institutional case, backed up with empirical evidence, for the variation thought to generate a useful **natural experiment**.(p. 12)

]

.pull-right[
&lt;img src="./images/04_cred.png" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)
]

---
## Three Designs for Causal Inference in Observational Studies

- Difference in Differences

- Regression Discontinuity

- Instrumental Variables


---
class: inverse, center, middle
# 💡 Difference in Differences


---
class: inverse, center, middle
background-image:url(https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&amp;itok=Mn-K5rQc)
background-size: cover
## London in the Time of Cholera

---
## Motivating Example: What causes Cholera?

- In the 1800s, cholera was thought to be transmitted through the air.

- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.

- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. 

---
## Notation

Let's adopt a little notation to help us think about the logic of Snow's design:

- `\(D\)`: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)

- `\(T\)`: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).

- `\(Y_{di}(t)\)` the potential outcome of unit `\(i\)` 

  - `\(Y_{1i}(t)\)` the potential outcome of unit `\(i\)` when treated between the two periods 

  - `\(Y_{0i}(t)\)` the potential outcome of unit `\(i\)` when control between the two periods 

---
## Causal Effects

The individual causal effect for unit i at time t is:

`$$\tau_{it} = Y_{1i}(t) − Y_{0i}(t)$$`

What we observe is 

`$$Y_i(t) = Y_{0i}(t)\cdot(1 − D_i(t)) + Y_{1i}(t)\cdot D_i(t)$$`

`\(D\)` only equals 1, when `\(T\)` equals 1, so we never observe `\(Y_0i(1)\)` for the treated units. 

In words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.

---
## Average Treatment on Treated

Our goal is to estimate the average effect of treatment on treated (ATT):


`$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$`

That is, what would have happened in Lambeth, had their water company not moved their pipes

---
## Average Treatment on Treated

Our goal is to estimate the average effect of treatment on treated (ATT):

We we can observe is:

|               | Post-Period (T=1)  | Pre-Period (T=0)  |
|---------------|--------------------|-------------------|
| Treated `\(D_{i}=1\)`  | `\(E[Y_{1i}(1)\vert D_i = 1]\)`  | `\(E[Y_{0i}(0)\vert D_i = 1]\)` |
| Control `\(D_i=0\)`  | `\(E[Y_{0i}(1)\vert D_i = 0]\)`  | `\(E[Y_{0i}(0)\vert D_i = 0]\)` |

---
## Data

Because potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Company &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1854 (T=1) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1849 (T=0) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Lambeth (D=1) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 85 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Southwark and Vauxhall (D=0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 147 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 135 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## How can we estimate the effect of moving pumps upstream?

Recall, our goal is to estimate the effect of the the treatment on the treated:

`$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$`

Let's conisder some strategies Snow could take to estimate this quantity:

---
## Before vs after comparisons:

- Snow could have compared Labmeth in 1854 `\((E[Y_i(1)|D_i = 1] = 19)\)` to Lambeth in 1849 `\((E[Y_i(0)|D_i = 1]=85)\)`, and claimed that moving the pumps upstream led to 66 fewer cholera deaths. 

- This comparison assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved ($E[Y_{0i}(1)|D_i = 1]$).

- A skeptic might argue that Lambeth in 1849 `\(\neq\)` Lambeth in 1854


---
## Treatment-Control comparisons in the post period

- Snow could have compared outcomes between Lambeth and S&amp;V in 1954  ($E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to 128 fewer deaths.

- Here the assumption is that the outcomes in S&amp;V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved ($E[Y_{0i}(1)|D_i = 1]$)

- Again, our skeptic could argue  Lambeth `\(\neq\)` S&amp;V 

---
## Difference in Differences

To address these concerns, Snow employed what we now call a difference-in-differences design, 

There are two, equivalent ways to view this design. 

`$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$`

- Difference 1: Average change between Treated and Control  in Post Period
- Difference 2: Average change between Treated and Control  in Pre Period

Which is equivalent to:

`$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\}}_{\text{Post - Pre |Treated }}− \overbrace{\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Post-Pre|Control}}$$`


- Difference 1: Average change between Treated over time
- Difference 2: Average change between Control over time

---
## Difference in Differences


You'll see the DiD design represented both ways, but they produce the same result:

$$
\tau_{ATT} = (19-147) - (85-135) = -78
$$

$$
\tau_{ATT} = (19-85) - (147-135) = -78
$$

---
## Identifying Assumption of a Difference in Differences Design

The key assumption in this design is what's known as the parallel trends assumption: `\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\)` 

- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&amp;V

---
&lt;img src="05-slides_files/figure-html/paralleltrends-1.png" width="80%" style="display: block; margin: auto;" /&gt;


Where:

1. `\(E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\)`
2. `\(E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}\} = 0]\)`
3. `\(E[Y_{1i}(1) − Y_{0i}(1)|D_{i} = 1]\)`


---
## Summary

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
  - Taking the pre-post difference removes any fixed differences between the units
  
  - Then taking the difference between treated and control differences removes any common differences over time

- The key identifying assumption of a DiD design is the "assumption of parallel trends"
  - Absent treatment, treated and control groups
would see the same changes over time.
  - Hard to prove, possible to test


---
## Extensions and limitations

- DiD easy to estimate with linear regression
- Generalizes to multiple periods and treatment interventions
  - More pre-treatment periods allow you assess "parallel trends" assumption
- Alternative methods 
  - Synthetic control
  - Event Study Designs
- What if you have multiple treatments or treatments that come and go?
  - Panel Matching
  - Generalized Synthetic control

---
## Applications

- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment

- [Abadie, Diamond, &amp; Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany

- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?

---
class: inverse, center, middle
# 💡 Regression Discontinuity Design

---
## Motivating Example

.pull-left[
- Do Members of Parliament in the UK get richer from holding office (QSS Chapter 4.3.4)
]
.pull-right[
&lt;img src="./images/04_eggers.png" width="80%" style="display: block; margin: auto;" /&gt;
[Eggers and Hainmueller (2009)](https://www.cambridge.org/core/journals/american-political-science-review/article/abs/mps-for-sale-returns-to-office-in-postwar-british-politics/E4C2B102194AA1EA0D2F1F777EAE3C08)
]


---
## Logic of the Regression Discontinuity Design (RDD) 

- What's the effect of holding elected office in the UK on personal wealth?

- People who win elections differ in many ways from people who lose elections.

- Logic of an RDD: 

  - Just look at the wealth of individuals who either narrowly won or lost elections.

  - Candidates close to 50 percent cutoff (discontinuity) should be more comparable (better counterfactuals)

---
## Data from Eggers and Hainmueller (2009)


```r
library(qss)
data(MPs)
glimpse(MPs)
```

```
Rows: 427
Columns: 10
$ surname    &lt;chr&gt; "Llewellyn", "Morris", "Walker", "Walker", "Waring", "Brown…
$ firstname  &lt;chr&gt; "David", "Claud", "George", "Harold", "John", "Ronald", "Le…
$ party      &lt;chr&gt; "tory", "labour", "tory", "labour", "tory", "labour", "tory…
$ ln.gross   &lt;dbl&gt; 12.13591, 12.44809, 12.42845, 11.91845, 13.52022, 12.46052,…
$ ln.net     &lt;dbl&gt; 12.135906, 12.448091, 10.349009, 12.395034, 13.520219, 9.63…
$ yob        &lt;int&gt; 1916, 1920, 1914, 1927, 1923, 1921, 1907, 1912, 1905, 1920,…
$ yod        &lt;int&gt; 1992, 2000, 1999, 2003, 1989, 2002, 1987, 1984, 1998, 2004,…
$ margin.pre &lt;dbl&gt; NA, NA, -0.057168204, -0.072508894, -0.269689620, 0.3409586…
$ region     &lt;chr&gt; "Wales", "South West England", "North East England", "Yorks…
$ margin     &lt;dbl&gt; 0.05690404, -0.04973833, -0.04158868, 0.02329524, -0.230005…
```

---
<div id="htmlwidget-b0d7f9738e3a5a456559" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-b0d7f9738e3a5a456559">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10"],["surname","firstname","party","ln.gross","ln.net","yob","yod","margin.pre","region","margin"],["surname of the candidate","first name of the candidate","party of the candidate (labour or tory)","log gross wealth at the time of death","log net wealth at the time of death","year of birth of the candidate","year of death of the candidate","margin of the candidate’s party in the previous election electoral","region","margin of victory (vote share)"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Variable<\/th>\n      <th>Description<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>



---

```r
MPs %&gt;%
  ggplot(aes(margin, ln.net))+
  geom_point(shape=1)+
  facet_grid(~party)+
  geom_smooth(data =MPs %&gt;%
                filter(margin &lt;0),
              method = "lm")+
  geom_smooth(data =MPs %&gt;%
                filter(margin &gt;0),
              method = "lm")+
  theme_bw() -&gt; fig_rdd
fig_rdd
```

---
&lt;img src="05-slides_files/figure-html/rddfig-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## RDD Notation

- `\(X\)` is a **forcing** variable
- Treatment `\(D\)` is a determined by `\(X\)`

$$
D_i = 1\{X_i &gt; c\}
$$

- `\(X\)` is the `margin` variable in the example data, and `\(D=1\)` if `margin` is greater than 0 (i.e. the candidate won the election)

- Interested in the differences in the outcome at the threshold

`$$\lim_{x \downarrow  c} E[Y_i|X=x] - \lim_{x \uparrow  c} E[Y_i|X=x]$$`

---
## Causal Identification with an RDD

If we assume `\(E[Y_i(0)|X=x]\)` and `\(E[Y_i(1)|X=x]\)` are continuous in x, then we can estimate a (local) ATE at the threshold:

`$$\begin{align}
ATE_{RDD} &amp;= E[Y(1)-Y(0)|X_i=c] \\
&amp;=  E[Y(1)|X_i=c] -  E[Y(0)|X_i=c]\\
&amp;= \lim_{x \downarrow  c} E[Y_i|X=x] - \lim_{x \uparrow  c} E[Y_i|X=x] \\
\end{align}$$`

---
## Continuity Assumption

&lt;img src="https://mixtape.scunning.com/graphics/rdd_simul_ex.jpg" width="80%" style="display: block; margin: auto;" /&gt;

[Cunningham (2022)](https://mixtape.scunning.com/regression-discontinuity.html#continuity-assumption)

---
## Causal Identification with an RDD

- The continuity assumption is a formal way of saying that observations close to the threshold are good counterfactuals for each other

- We can't prove this assumption

- But if it holds, we should observe
 
  - no sorting around the cutoff (no self selection)
 
  - similar distributions of covariates around the cutoff (balance tests)
  
  - no effect of treatment on things measured pre-treatment (placebo tests)


---
## Applications of RDD

- [Lee (2001)](https://www.nber.org/papers/w8441) Does incumbency create electoral advantage

- [Eggers (2014)](https://journals.sagepub.com/doi/full/10.1177/0010414014534199) Does proportional representatoin increase turnout?

- [Velez and Newman (2019)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12427) What's the effect of ethnic television on political participation

---
class: inverse, center, middle
# 💡 
## Instrumental Variables


---
## Instrumental Variables

Instrumental variables are an economists favorite tool for dealing with **omitted variable bias**

- We have some non random treatment whose effects we'd like to assess
- We're worried that these effects are **confounded** by some unobserved, omitted variable, that influences both the treatment and the outcome
- We find an **instrumental variable** that satisfies the following:
  - Randomization
  - Excludability
  - First-stage relatioship
  - Monotonicity
- Allowing us estimate a Local Average Treatment Effect (LATE) using the only the variation in our treatment is **exogenous** (uncorrelated with ommited variables)

---
class: center
## IV Assumption: Randomization


.left-column[
- No path from `\(U\)` to `\(Z\)`
]
.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption: Excludability


.left-column[
- No path from `\(Z\)` to `\(Y\)`
]

.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption:  First Stage

.left-column[
- Path from `\(Z\)` to `\(D\)`
]
.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;

[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)

]

---
class: center
## IV Assumption: Monotonicity


.left-column[

-  `\(D_i(Z=1)\geq D_i(Z=0)\)`
- "No Defiers"

]

.right-column[
&lt;img src="https://book.declaredesign.org/figures/figure_15.4.svg" width="80%" style="display: block; margin: auto;" /&gt;
[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)
]

---
class: center
## Compliance

With a binary treatment, `\(D\)` and binary instrument `\(Z\)` there are four types of compliance

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Type &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(D_i(Z=1)\)` &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; `\(D_i(Z=0)\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Always Takers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Never Takers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Compliers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Defiers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Assuming Monotonicity means there are "No Defiers"

---
## Estimating the Local Average Treatment Effect

If we believe our assumptions of:

- Randomization
- Excludability
- First-stage relationship
- Monotonicity

Then we can estimate Local Average Treatment Effect (LATE) sometimes called the Complier Average Treatment Effect CATE) 

---
## Estimating the Local Average Treatment Effect

It can be [shown](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf) that the LATE:

`$$LATE = \frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1]-E[D|Z=0]}= \frac{ATE_{Z\to Y}}{ATE_{Z\to D}}$$`


---

## Example: Earnings and Military Service

Adapted from [Edward Rubin](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#42)

*Example:* If we want to estimate the effect of veteran status on earnings,
`$$\begin{align}
  \text{Earnings}_i = \beta_0 + \beta_1 \text{Veteran}_i + u_i \tag{1}
\end{align}$$`

--

We would love to calculate `\(\color{#e64173}{\text{Earnings}_{1i}} - \color{#6A5ACD}{\text{Earnings}_{0i}}\)`, but we can't.

--

And OLS will likely be biased for `\((1)\)` due to selection/omitted-variable bias.

---

## Introductory example

Imagine that we can split veteran status into an exogenous (as-if random, unbiased) part and an endogenous (non-random, biased) part...

--

`$$\begin{align}
  \text{Earnings}_i
  &amp;= \beta_0 + \beta_1 \text{Veteran}_i + u_i \tag{1} \\
  &amp;= \beta_0 + \beta_1 \left(\text{Veteran}_i^{\text{Exog.}} + \text{Veteran}_i^{\text{Endog.}}\right) + u_i \\
  &amp;= \beta_0 + \beta_1 \text{Veteran}_i^{\text{Exog.}} + \underbrace{\beta_1 \text{Veteran}_i^{\text{Endog.}} + u_i}_{w_i} \\
  &amp;= \beta_0 + \beta_1 \text{Veteran}_i^{\text{Exog.}} + w_i
\end{align}$$`

--

We could use this exogenous variation in veteran status to consistently estimate `\(\beta_1\)`.

--

**Q:** What would exogenous variation in veteran status mean?

---

## Introductory example

**Q:** What would exogenous variation in veteran status mean?

--

**A.sub[1]:** Choices to enlist in the military that are essentially random—or at least uncorrelated with omitted variables and the disturbance.

--

**A.sub[2]:** .No selection bias:
`$$\begin{align}
  \color{#e64173}{\mathop{E}\left(\text{Earnings}_{0i}\mid\text{Veteran}_i = 1\right)} - \color{#6A5ACD}{\mathop{E}\left( \text{Earnings}_{0i} \mid \text{Veteran}_i = 0 \right)} = 0
\end{align}$$`

---
## Instruments

**Q:** How do we isolate this *exogenous variation* in our explanatory variable?
--
&lt;br&gt;**A:** Find an instrument (an instrumental variable).

--

**Q:** What's an instrument?
--
&lt;br&gt;**A:** An **instrument** is a variable that is

1. **correlated** with the **explanatory variable** of interest (*relevant*),
2. **uncorrelated** with the **error** term (*exogenous*).

---

## Instruments


So if we want an instrument `\(z_i\)` for endogenous veteran status in

`$$\begin{align}
  \text{Earnings}_i = \beta_0 + \beta_1 \text{Veteran}_i + u_i
\end{align}$$`

1. **Relevant:** `\(\mathop{\text{Cov}} \left( \text{Veteran}_i,\, z_i \right) \neq 0\)`
2. **Exogenous:** `\(\mathop{\text{Cov}} \left( z_i,\, u_i \right) = 0\)`

---
## Instruments: Relevance

**Relevance:** We need the instrument to cause a change in (correlate with) our endogenous explanatory variable.

We can actually test this requirement using regression and a *t* test.

--

***Example:*** For the veteran status, consider three potential instruments:

.pull-left[
1. Social security number

2. Physical fitness

3. Vietnam War draft
]

--

.pull-right[ 
- **Probably not relevant** uncorrelated with military service

- *Potentially relevant* service may correlate with fitness

- **Relevant** being drafted led to service
]

---


## Instruments: Exogeneity

.hi[Exogeneity:] The instrument to be independent of omitted factors that affect our outcome variable—as good as randomly assigned.

`\(z_i\)` must be uncorrelated with our disturbance `\(u_i\)`. .hi[Not testable.]

--

***Example:*** For the .pink[veteran status], consider three potential instruments:

.pull-left[
1. Social security number

2. Physical fitness

3. Vietnam War draft
]

--

.pull-right[ 
- **Exogenous** SSN essentially random

- *Not Exogenous* fitness correlated with many things

- **Exogenous** draft via lottery
]

---
## Relevant and Exogenous


&lt;img src="05-slides_files/figure-html/venn_iv-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Relevant, Not Exogenous

&lt;img src="05-slides_files/figure-html/venn_iv_endog-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Not Relevant and Not Exogenous


&lt;img src="05-slides_files/figure-html/venn_iv_irrelevant-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
## Relevant, Not Exogenous
&lt;img src="05-slides_files/figure-html/venn_iv_endog2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
# Venn diagram explanation

In these figures (Venn diagrams)

- Each circle illustrates a variable.
- Overlap gives the share of correlatation between two variables.
- Dotted borders denote *omitted* variables.

Take-aways

- Figure 1: .hi-pink[Valid instrument] (relevant; exogenous)
- Figure 2: .hi-slate[Invalid instrument] (relevant; not exogenous)
- Figure 3: .hi-slate[Invalid instrument] (not relevant; not exogenous)
- Figure 4: .hi-slate[Invalid instrument] (relevant; not exogenous)

---
## IV Applications

&lt;img src="https://pbs.twimg.com/media/EJGyHnyUYAA-yhM?format=jpg&amp;name=large" width="80%" style="display: block; margin: auto;" /&gt;

[@AndrewHeiss](https://twitter.com/andrewheiss/status/1193931226865901569?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1193931226865901569%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2Fandrewheiss2Fstatus2F1193931226865901569widget%3DTweet)



---
## IV Summary

Instrumental variables require a number of assumptions to yield credible causal claims:

- Randomization
- Excludability
- First-stage relationship
- Monotonicity

Estimation and inference of IVs is beyond the scope of this course. 

  - See Edward Rubin's excellent [slides](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#85)
  - And Matt Blackwells [notes](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf)

- Understanding the identifying assumptions of IV can help you critique a study (even if the you don't fully understand the math)

---
class: inverse, middle, center
# 💡 
## Summary

---
## What you need to know

- Causal inference in observational and experimental studies is about counterfactual comparisons
- In observational studies, to make causal claims we generally make some  assumption of conditional independence:

$$
Y_i(1),Y_i(0), \perp D_i |X_i
$$

- The credibility of this assumption depends less on the data, and more on how the data were generated.
- **Selection on Observables** is rarely a credible assumption
- Observational designs that produce credible causal inference, leverage aspects of the world that create *natural experiments*
- You should be able to describe the logic and assumptions of common designs in social science 
  - **Difference-in-Differences:** *Parallel Trends* 
  - **Regression Discontinuity:** *Continuity at the cutoff*
  - **Instrumental Variables:** Instruments need to be *Relevant and Exogenous*


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-lakeside-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
