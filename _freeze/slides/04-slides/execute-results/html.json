{
  "hash": "c5bd4d110bee1b414ee0ee5812b0f73f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"POLS 1600\"\nsubtitle: \"Casual Inference in<br>Observational Designs & <br> Simple Linear Regression\"\ndate: last-modified\ndate-format: \"[Updated ]MMM D, YYYY\"\nformat: \n  revealjs:\n    theme: brownslides.scss\n    logo: images/pols1600_hex.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    html-math-method: mathjax\n    incremental: true\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    chalkboard: true\n    # include-after-body: title-slide.html\n    title-slide-attributes:\n      align: left\n      data-background-image: images/pols1600_hex.png\n      data-background-position: 90% 50%\n      data-background-size: 40%\nfilters:\n  - openlinksinnewpage\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  message: false\n  cache: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# {{< fa map-location>}} Overview {.inverse}\n\n## Overview\n\n- Announcements\n- Setup\n- Feedback\n- Review\n- Class plan\n\n## Learing goals {.smaller}\n\n- Introduce the concept of [Directed Acyclic Graphs]{.blue} to describe causal relationships and illustrate potential bias from [confounders]{.blue} and [colliders]{.blue} \n\n- Discuss three approaches to [covariate adjustment]{.blue}\n\n  - Subclassification\n  - Matching\n  - [Linear Regression]{.blue}\n\n- Begin discussing three research designs to make causal claims with observational data\n\n  - [Differences-in-Differences]{.blue} (More likely next week)\n  - Regression Discontinuity Designs\n  - Instrumental Variables\n\n\n\n## Annoucements\n\n- Assignment 1 due October 9 on [Canvas](https://canvas.brown.edu/courses/1097425/assignments/8018240?module_item_id=11109320)\n\n- Do we want the opportunity to provide weekly feedback?\n\n# {{< fa magnifying-glass >}} Review {.inverse}\n\n## Review\n\n- Data wrangling \n\n- Descriptive Statistics\n\n- Levels of understanding\n\n- Data visualization\n\n# {{< fa magnifying-glass >}} Data Wrangling {.inverse}\n\n## Data wrangling {.smaller}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: You're learning how to map conceptual tasks to commands in R\n\n|Skill                      |Common Commands                           |\n|:--------------------------|:-----------------------------------------|\n|Setup R                    |install.packages(),library(), ipak()      |\n|Load data                  |read_csv(), load()                        |\n|Get HLO of data            |df$x, glimpse(), table(), summary()       |\n|Transform data             |<-, mutate(), ifelse(), case_when()       |\n|Reshape data               |pivot_longer(), left_join()               |\n|Summarize data numerically |mean(), median(), summarise(), group_by() |\n|Summarize data graphically |ggplot(), aes(), geom_                    |\n\n\n:::\n:::\n\n\n\n## Mapping Concepts to Code\n\n- Takes time and practice\n\n- Don't be afraid to [FAAFO]{.blue}\n\n- Break long blocks of code into [sequential parts]{.blue}\n\n- Don't worry about memorizing everything.\n\n- Statistical programming is necessary to actually [do]{.blue} empirical research\n\n- [Learning to code]{.blue} will help us [understand statistical concepts]{.blue}.\n\n- Learning to [think programmatically]{.blue} and algorithmically will help us [tackle complex problems]{.blue}\n\n# {{< fa magnifying-glass >}} Descriptive Statiscs{.inverse}\n\n## Descriptive statistics {.smaller}\n\n- Descriptive statistics help us describe what's typical of our data\n\n- [What's a typical value in our data]{.blue}\n\n  - [Mean](https://pols1600.paultesta.org/labs/01-lab-comments.html#mean)\n  - [Median](https://pols1600.paultesta.org/labs/01-lab-comments.html#median)\n  - [Mode](https://pols1600.paultesta.org/labs/01-lab-comments.html#modes)\n\n- [How much do our data vary?]{.blue}\n\n  - [Variance](https://pols1600.paultesta.org/labs/01-lab-comments.html#variance)\n  - [Standard deviation](https://pols1600.paultesta.org/labs/01-lab-comments.html#standard-deviations)\n\n- As one variable changes [how does another change]{.blue}?\n\n  - [Covariance](https://pols1600.paultesta.org/labs/01-lab-comments.html#covariance)\n  - [Correlation](https://pols1600.paultesta.org/labs/01-lab-comments.html#correlation)\n\n- Descriptive statistics are:\n\n  - Diagnostic\n  - Generative\n\n# {{< fa magnifying-glass>}} Levels of understanding{.inverse}\n\n## Levels of understanding in POLS 1600\n\n- Conceptual\n\n- Practical\n\n- Definitional\n\n- Theoretical\n\n. . .\n\nLet's illustrate these different levels of understanding about our old friend the [mean]{.blue}\n\n## Mean: Conceptual Understanding{.smaller}\n\nA mean is:\n\n- A common and important [measure of central tendency]{.blue} (what's typical)\n\n  - It's the [arithmetic average]{.blue} you learned in school\n  - It's an unbiased estiamte of $E[X]$,\n  - We can think of it as the [balancing point]{.blue} of a distribution\n\n- A [conditional mean]{.blue} is the average of one variable $X$, when some other variable, $Z$ takes a value $z$\n\n  - Think about the average height in our class ([unconditional mean]{.blue}) vs the average height among men and women (\\[conditional means\\].{blue})\n\n## Mean as a balancing point{.smaller}\n\n![](https://mathbitsnotebook.com/Algebra1/StatisticsData/balancepoint1.jpg)\n\n[Source](https://mathbitsnotebook.com/Algebra1/StatisticsData/STCenter.html)\n\n## Mean: Practical{.smaller}\n\nThere are lots of ways to calculate means in `R`\n\n- The simplest is to use the `mean()` function\n\n  - If our data have missing values, we need to to tell `R` to remove them\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(df$x, na.rm=T)\n```\n:::\n\n\n\n## Conditional Means: Practical{.smaller}\n\n- To calculate a conditional mean we could us a logical index `[df$z == 1]`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(df$x[df$z == 1], na.rm=T)\n```\n:::\n\n\n\n- If we wanted to a calculate a lot of conditional means we could use the `mean()` in combination with `group_by()` and `summarise()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% \n  group_by(z)%>%\n  summarise(\n    x = mean(x, na.rm=T)\n  )\n```\n:::\n\n\n\n## Mean: Definitional{.smaller}\n\nFormally, we define the arithmetic mean of $x$ as $\\bar{x}$:\n\n$$\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}\n$$\n\nIn words, this formula says, to calculate the average of x, we sum up all the values of $x_i$ from observation $i=1$ to $i=n$ and then divide by the total number of observations $n$\n\n## Mean: Definitional {.smaller}\n\n- In this class, I don't put a lot of weight on memorizing definitions (that's what Google's for).\n\n- But being comfortable with \"the math\" is important and useful\n\n- Definitional knowledge is a prerequisite for understanding more theoretical claims.\n\n## Mean: Theoretical{.smaller}\n\nSuppose I asked you to show that the sum of deviations from a mean equals 0?\n\n$$\n\\text{Claim:} \\sum_{i=1}^n (x_i -\\bar{x}) = 0\n$$\n\n## Mean: Theoretical {.smaller}\n\nKnowing the definition of an arithmetic mean, we could write:\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^n (x_i -\\bar{x}) &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n\\bar{x} & \\text{Distribute Summation}\\\\\n              &= \\sum_{i=1}^n x_i - n\\bar{x} & \\text{Summing a constant, } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - n\\times \\left ( \\frac{1}{n} \\sum_{i=1}^n{x_i}\\right ) & \\text{Definition of } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n{x_i} & n \\times \\frac{1}{n}=1\\\\\n              &= 0             \n\\end{aligned}\n$$\n\n## Mean: Theoretical {.smaller}\n\nWhy do we care?\n\n- Showing the deviations sum to 0 is another way of saying the mean is a [balancing point]{.blue}.\n\n- This turns out to be a useful property of means that will reappear throughout the course\n\n- If I asked you to make a prediction, $\\hat{x}$ of a random person's height in this class, the mean would have the lowest [mean squared error]{.blue} (MSE $=\\frac{1}{n}\\sum (x_i - \\hat{x_i})^2)$\n\n## Mean: Theoretical\n\nOccasionally, you'll read or here me say say things like:\n\n> The sample mean is an unbiased estimator of the population mean\n\nIn a statistics class, we would take time to prove this.\n\n## The sample mean is an unbiased estimator of the population mean{.smaller}\n\nClaim:\n\nLet $x_1, x_2, \\dots x_n$ from a random sample from a population with mean $\\mu$ and variance $\\sigma^2$\n\nThen:\n\n$$\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n x_i\\right )\n$$\n\nis an unbiased estimator of $\\mu$\n\n$$\nE[\\bar{x}] = \\mu\n$$\n\n## The sample mean is an unbiased estimator of the population mean {.smaller}\n\nProof:\n\n$$\n\\begin{aligned}\nE\\left [\\bar{x} \\right] &= E\\left [\\frac{1}{n}\\left (\\sum_{i=1}^n x_i \\right) \\right] & \\text{Definition of } \\bar{x} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^nE\\left [ x_i \\right]  & \\text{Linearity of Expectations} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu  & E[x_i] = \\mu \\\\\n&= \\frac{n}{n}  \\mu  & \\sum_{i=1}^n \\mu = n\\mu \\\\\n&= \\mu  & \\blacksquare \\\\\n\\end{aligned}\n$$\n\n## Theoretical: Moment Generating Functions{.smaller}\n\nThe moment generating function of a random variable $X$ is defined as:\n\n$$\nM_X(t) = E[e^{tX}]\n$$\n\nfor values of  $t$ where the expectation exists.\n\n- Why is it Called \"Moment Generating\"?\n\n- If we take derivatives of  $M_X(t)$ and evaluate at $t = 0$, we can extract the moments of $X$:\n\n:::{.fragment}\n\n$$\nM_X^{(n)}(0) = E[X^n]\n$$\n\n:::\n\n\n## Theoretical: Distributions can be described by their moments {.smaller}\n\n- **First Moment (Mean)**  \n$$\n  \\mu_1' = E[X]\n$$\n  - This is simply the **expected value** (mean) of \\( X \\).\n\n- **Second Central Moment (Variance)**  \n  $$\n  \\mu_2 = E[(X - \\mu)^2]\n  $$\n  \n  - Measures the spread or dispersion of \\( X \\) around the mean.\n\n- **Third Central Moment (Skewness)**  \n  $$\n  \\gamma_1 = \\frac{E[(X - \\mu)^3]}{\\sigma^3}\n  $$\n  - Measures the **asymmetry** of the distribution.\n\n- **Fourth Central Moment (Kurtosis)**  \n  $$\n  \\gamma_2 = \\frac{E[(X - \\mu)^4]}{\\sigma^4}\n  $$\n  - Measures the **tailedness** of the distribution (how extreme values occur compared to a normal distribution).\n\n\n\n\n## Levels of understanding {.smaller}\n\n::: nonincremental\nIn this course, we tend to emphasize the\n\n- **Conceptual**\n\n- **Practical**\n\nOver\n\n- Definitional\n\n- Theoretical\n\nIn an intro statistics class, the ordering might be reversed.\n\nTrade offs:\n:::\n\n- Pro: We actually get to *work with data* and *do empirical research* much sooner\n- Cons: We substitute intuitive understandings for more rigorous proofs\n\n# {{< fa magnifying-glass>}} Data Visualization {.inverse}\n\n## Data Visualization {.smaller}\n\n-   The grammar of graphics\n\n-   At minimum you need:\n\n    -   `data`\n    -   `aesthetic` mappings\n    -   `geometries`\n\n-  Take a sad plot and make it better by:\n\n    -   `labels`\n    -   `themes`\n    -   `statistics`\n    -   `cooridnates`\n    -   `facets`\n    -   transforming your data before plotting\n\n## You're about to be reincaranted...\n\nDo you want to come back as a...\n\n- Animal/Land dweller\n- Bird/Air dweller\n- Fish/Sea dweller\n- Insect\n- Plant\n- Single-celled organism\n\n## My undergrads are about to be reincarnated: HLO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\n\nload(url(\"https://pols2580.paultesta.org/files/data/wk04_df.rda\"))\n\ndf$reincarnation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<labelled<double>[30]>: You're about to be re-incarnated. Do you want to come back as a:\n [1]  6  1  1  6  1  9  6  6  7 10  7  1  6  1  1  6  1  6  6  1  6  6  6  6  1\n[26]  6  1  7  1  1\n\nLabels:\n value               label\n     1 Animal/land dweller\n     6    Bird/air dweller\n     7  Fish/water dweller\n     2              Insect\n     9               Plant\n    10    Single-celled or\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(df$reincarnation)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 1  6  7  9 10 \n12 13  3  1  1 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Basic Plot\n\n::: panel-tabset\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  ggplot(aes(x = reincarnation, \n             fill = reincarnation))+\n  geom_bar(\n    stat = \"count\"\n  ) \n```\n:::\n\n\n\n\n## Figure\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/plot0-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n##  Use a factor to label and order responses\n\n::: panel-tabset\n\n## Recode\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  mutate(\n    # Turn numeric values into factor labels \n    Reincarnation = forcats::as_factor(reincarnation),\n    # Order factor in decreasing frequency of levels\n    Reincarnation = forcats::fct_infreq(Reincarnation),\n    # Reverse order so levels are increasing in frequency\n    Reincarnation = forcats::fct_rev(Reincarnation),\n    # Rename explanations\n    Why = reincarnation_why\n  ) -> df\n```\n:::\n\n\n\n## Check recoding\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(recode= df$Reincarnation, \n      original = df$reincarnation)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     original\nrecode                 1  6  7  9 10\n  Insect               0  0  0  0  0\n  Single-celled or     0  0  0  0  1\n  Plant                0  0  0  1  0\n  Fish/water dweller   0  0  3  0  0\n  Animal/land dweller 12  0  0  0  0\n  Bird/air dweller     0 13  0  0  0\n```\n\n\n:::\n:::\n\n\n:::\n\n\n## Revised figure\n\n::: panel-tabset\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% # Data\n  # Aesthetics\n  ggplot(aes(x = Reincarnation, \n             fill = Reincarnation))+\n  # Geometry\n  geom_bar(stat = \"count\")+ # Statistic\n  ## Include levels of Reincarnation w/ no values\n  scale_x_discrete(drop=FALSE)+\n  # Don't include a legend\n  scale_fill_discrete(drop=FALSE, guide=\"none\")+\n  # Flip x and y\n  coord_flip()+\n  # Remove lines\n  theme_classic() -> fig1\n```\n:::\n\n\n\n## Revised Figure\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n\n## {.smaller}\n#### What creature and why?\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-44d79e415fad676596e4\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-44d79e415fad676596e4\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"],[\"Bird/air dweller\",\"Animal/land dweller\",\"Animal/land dweller\",\"Bird/air dweller\",\"Animal/land dweller\",\"Plant\",\"Bird/air dweller\",\"Bird/air dweller\",\"Fish/water dweller\",\"Single-celled or\",\"Fish/water dweller\",\"Animal/land dweller\",\"Bird/air dweller\",\"Animal/land dweller\",\"Animal/land dweller\",\"Bird/air dweller\",\"Animal/land dweller\",\"Bird/air dweller\",\"Bird/air dweller\",\"Animal/land dweller\",\"Bird/air dweller\",\"Bird/air dweller\",\"Bird/air dweller\",\"Bird/air dweller\",\"Animal/land dweller\",\"Bird/air dweller\",\"Animal/land dweller\",\"Fish/water dweller\",\"Animal/land dweller\",\"Animal/land dweller\"],[\"I want to fly\",\"That would be sick if I could return as a lion or a panther. Also wouldn't complain if I was a Baboon in my next life.\",\"animal means human so definitely that\",\"I would love to see what it feels like to fly and travel all over!\",\"I want to be a giraffe because they're tall and have minimal predators. They're also really cute and seem chill so its the best of all worlds\",\"sounds peaceful\",\"Flying would be cool\",\"I like flying\",\"So much to discover in the water!\",\"no thoughts\",\"I think being a dolphin would be pretty awesome because they don't have many predators and I would love to see the ocean.\",\"A dog in a family with kids because I feel like I would have a lot of fun and I would get to sleep a lot\",\"An eagle- I could live on the coast and watch everything below me\",\"Alligator because its my favorite animal and it seems like a pretty chill life\",\"I would come back as a dog because their life seems very relaxing \\n\",\"Flying seems fun!\",\"I think I'd want to be Tortoise. They live a long time, get to hang out in tropical weather. That doesn't sound so bad.\",\"Flying is cool\",\"Bird -- Part of the reason is that my childhood nickname is \\\"Kate-e-bird\\\" so it just seems like a natural fit. But also I think it would be great to fly.\",\"House cat: free food, free housing, free love, freedom to be a bitch.\",\"I would like to be a hawk, so I can see everything from above.\",\"I want to experience the air and wind under my wings as I fly across a large body of water.\",\"A Eagle - flying around would be beautiful!\",\"Being able to fly sounds amazing. Plus, months in the winter on vacation in the south!\",\"Grizzley bear so I can eat everyone and all the fishes and have it be tasty\",\"The ability to fly makes this question a no-brainer for me.\",\"Squirrel. They look like they have fun. Preferably flying squirrel\",\"I would go for a water dweller to be able to experience an entirely new environment, plus being in a school of fish not having to make decisions for my self would be nice.\",\"Probably a lemur because they're just kinda silly guys, seems like a fun time.\",\"flying squirrel. You can just fly around and also climb shit. best of both worlds.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Reincarnation<\\/th>\\n      <th>Why<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Reincarnation\",\"targets\":1},{\"name\":\"Why\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n\n\n\n## Adding labelled values\n\n::: panel-tabset\n\n\n## Recodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  mutate(\n    # Create numeric id\n    id = 1:n(),\n    # Create a label with 3 answers and NA elsewhere\n    Label = case_when(\n      id == 10 ~ str_wrap(reincarnation_why[10],30),\n      id == 20 ~ str_wrap(reincarnation_why[20],30),\n      id == 18 ~ str_wrap(reincarnation_why[18],30),\n      TRUE ~ NA_character_\n\n    )\n\n  ) -> df\n```\n:::\n\n\n\n## Recode Output\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-4632005b488b3f2c75fb\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-4632005b488b3f2c75fb\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"],[null,null,null,null,null,null,null,null,null,\"no thoughts\",null,null,null,null,null,null,null,\"Flying is cool\",null,\"House cat: free food, free\\nhousing, free love, freedom to\\nbe a bitch.\",null,null,null,null,null,null,null,null,null,null],[\"I want to fly\",\"That would be sick if I could return as a lion or a panther. Also wouldn't complain if I was a Baboon in my next life.\",\"animal means human so definitely that\",\"I would love to see what it feels like to fly and travel all over!\",\"I want to be a giraffe because they're tall and have minimal predators. They're also really cute and seem chill so its the best of all worlds\",\"sounds peaceful\",\"Flying would be cool\",\"I like flying\",\"So much to discover in the water!\",\"no thoughts\",\"I think being a dolphin would be pretty awesome because they don't have many predators and I would love to see the ocean.\",\"A dog in a family with kids because I feel like I would have a lot of fun and I would get to sleep a lot\",\"An eagle- I could live on the coast and watch everything below me\",\"Alligator because its my favorite animal and it seems like a pretty chill life\",\"I would come back as a dog because their life seems very relaxing \\n\",\"Flying seems fun!\",\"I think I'd want to be Tortoise. They live a long time, get to hang out in tropical weather. That doesn't sound so bad.\",\"Flying is cool\",\"Bird -- Part of the reason is that my childhood nickname is \\\"Kate-e-bird\\\" so it just seems like a natural fit. But also I think it would be great to fly.\",\"House cat: free food, free housing, free love, freedom to be a bitch.\",\"I would like to be a hawk, so I can see everything from above.\",\"I want to experience the air and wind under my wings as I fly across a large body of water.\",\"A Eagle - flying around would be beautiful!\",\"Being able to fly sounds amazing. Plus, months in the winter on vacation in the south!\",\"Grizzley bear so I can eat everyone and all the fishes and have it be tasty\",\"The ability to fly makes this question a no-brainer for me.\",\"Squirrel. They look like they have fun. Preferably flying squirrel\",\"I would go for a water dweller to be able to experience an entirely new environment, plus being in a school of fish not having to make decisions for my self would be nice.\",\"Probably a lemur because they're just kinda silly guys, seems like a fun time.\",\"flying squirrel. You can just fly around and also climb shit. best of both worlds.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Label<\\/th>\\n      <th>Why<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Label\",\"targets\":1},{\"name\":\"Why\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n## Aggregate Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate totals before calling ggplot\nplot_df <- df %>%\n  group_by(Reincarnation)%>%\n  summarise( \n    Count = n(), \n    Why = unique(Label)\n  ) %>% \n  ungroup() %>% \n  # Kludge to get rid of NA rows...\n  slice(c(1,2,3,5,7))\n```\n:::\n\n\n\n:::\n\n\n## You're about to be reincarnated:{.smaller}\n\n::: panel-tabset\n\n## Aggregate df\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-cc6a8d55d70741c65f0e\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-cc6a8d55d70741c65f0e\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\"],[\"Single-celled or\",\"Plant\",\"Fish/water dweller\",\"Animal/land dweller\",\"Bird/air dweller\"],[1,1,3,12,13],[\"no thoughts\",null,null,\"House cat: free food, free\\nhousing, free love, freedom to\\nbe a bitch.\",\"Flying is cool\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Reincarnation<\\/th>\\n      <th>Count<\\/th>\\n      <th>Why<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Reincarnation\",\"targets\":1},{\"name\":\"Count\",\"targets\":2},{\"name\":\"Why\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n## Revised Figure Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_df %>%\n  ggplot(aes(x = Reincarnation, \n             y = Count,\n             fill = Reincarnation, \n             label=Why))+\n  geom_bar(stat = \"identity\")+ #<<\n  ## Include levels of Reincarnation w/ no values\n  scale_x_discrete(drop=FALSE)+\n  # Don't include a legend\n  scale_fill_discrete(drop=FALSE, guide=\"none\")+\n  coord_flip()+\n  labs(x = \"\",y=\"\",title=\"You're about to be reincarnated.\\nWhat do you want to come back as?\")+\n  theme_classic()+\n  ggrepel::geom_label_repel(\n    fill=\"white\",\n    nudge_y = 1, \n    hjust = \"left\",\n    size=3,\n    arrow = arrow(length = unit(0.015, \"npc\"))\n    )+ \n  scale_y_continuous(\n    breaks = c(0,2,4,6,8,10,12),\n    expand = expansion(add =c(0,6))\n    ) -> fig1\n```\n:::\n\n\n\n## Labelled Figure\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/fig1labelled-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Data visualization is an iterative process\n\n- Data visualization is an iterative process\n\n- Good data viz requires lots of data transformations\n\n- Start with a minimum working example and build from there\n\n- Don't let the perfect be the enemy of the good enough.\n\n\n# {{< fa code >}} Setup {.inverse}\n\n## New packages\n\nThis week's lab we'll be using the `dataverse` package to download data on presidential elections\n\nNext week's lab, we'll be using the `tidycensus` package to download census data. \n\nWe'll also need to [install a census API]{.blue} to get the data.\n\nHere's a [detailed guide](https://pols1600.paultesta.org/resources/04-packages) of what we'll do in class right now.\n\n## Install new packages\n\nThese packages are easier to install live:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"dataverse\")\ninstall.packages(\"tidycensus\")\ninstall.packages(\"easystats\")\ninstall.packages(\"DeclareDesign\")\n```\n:::\n\n\n\n## Census API {.smaller}\n\nPlease follow these steps so you can download data directly from the U.S. Census [here](https://pols1600.paultesta.org/resources/04-packages.html#3_Install_a_Census_API_tidycensus_package):\n\n1.  Install the `tidycensus` package\n2.  Load the installed package\n3.  Request an API key from the Census\n4.  Check your email\n5.  Activate your key\n6.  Install your API key in R\n7.  Check that everything worked\n\n\n## Packages for today\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Pacakges for today\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE \n```\n\n\n:::\n:::\n\n\n\n\n\n# {{< fa code >}} Previewing the Lab {.inverse}\n\n## Red Covid{.smaller}\n\n\n::::{.columns}\n\n:::{.column width=\"45%\"}\n\n![](images/05_redcovid_orig.png)\n\n\n[Red Covid](https://www.nytimes.com/2021/09/27/briefing/covid-red-states-vaccinations.html) *New York Times*, 27 September, 2021\n:::\n\n:::{.column width=\"45%\"}\n![](images/05_covid.png)\n[Red Covid, an Update](https://www.nytimes.com/2022/02/18/briefing/red-covid-partisan-deaths-vaccines.html) *New York Times*, 18 February, 2022\n:::\n::::\n\n## Preview of the Lab{.smaller}\n\n[Please download]{.blue} the lab [here](https://pols1600.paultesta.org/labs/05-lab.qmd)\n\n- Conceptually, this lab is designed to help reinforce the relationship between linear models like $y=\\beta_0 + \\beta_1x$ and the conditional expectation function $E[Y|X]$.\n\n- Substantively, we will explore whether David Leonhardt's claims about [Red Covid](https://www.nytimes.com/2021/09/27/briefing/covid-red-states-vaccinations.html) the political polarization of vaccines and its consequences\n\n## Lab: Questions 1-5: Review{.smaller}\n\nQuestions 1-5 are designed to [reinforce]{.blue} your [data wrangling]{.blue} skills.  In particular, you will get practice:\n\n  - Creating and recoding variables using `mutate()`\n  - Calculating a [moving average](https://en.wikipedia.org/wiki/Moving_average) or rolling mean using the `rollmean()` function from the `zoo` package\n  - Transforming the data on presidential elections so that it can be merged with the data on Covid-19 using the `pivot_wider()` function.\n  - [Merging data](https://r4ds.had.co.nz/relational-data.html) together using the `left_join()` function.\n\n\n\n## Lab: Questions 6-10: Simple Linear Regression{.smaller}\n\n- In question 6, you will see how calculating conditional means provides a simple test of \"Red Covid\" claim.\n\n- In question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)\n\n- In question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)\n\n- In question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.\n\n- Question 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week's lab.\n\n## Guidance\n\nThe following slides provide detailed explanations of all the code you'll need for each question. \n\n\n## Q1: Setup your workspace\n\n::: panel-tabset\n\n## Task\n\nQ1 asks you to setup your workspace\n\nThis means [loading]{.blue} and, if needed, [installing]{.blue} the packages you will use.\n\n\n\n## Code for Q1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Pacakges for today\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n```\n:::\n\n\n\n\n:::\n\n\n## Q2 Load the data\n\nTo explore Leonhardt's claims about *Red Covid*, we'll need data on:\n\n- Covid-19\n- The 2020 Presidential Election\n\n\n## Q2.1 Load the Covid-19 Data{.smaller}\n\nTo load data on Covid-19 just run this\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n```\n:::\n\n\n\n## Q2.2 Load Election Data {.smaller}\n\n::: panel-tabset\n\n## Task\n\nQ2.2. asks you to write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab's dataverse\n\n- Once you've installed the `dataverse` package you should be able to do this:\n\n## Code for Q2.2\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Try this code first\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df <- dataverse::get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# If the code above fails, comment out and uncomment the code below:\n\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n```\n:::\n\n\n:::\n\n## Q3 Describe the structure of each dataset{.smaller}\n\nQuestion 3 asks you to [describe the structure]{.blue} of each dataset.\n\n- Specifically, it asks you to get a high level overview of `covid` and `pres_df` and describe the [unit of analysis]{.blue} in each dataset:\n  - Describe substantively what specific, observation each [row]{.blue} in the dataset corresponds to\n  - In covid `covid` dataset, the [unit of analysis]{.blue} is a [state-date]{.blue}\n\n## Q3 Describe the structure of each dataset{.smaller}\n\nHere's some possible code you could use to get a quick [HLO]{.blue} of each dataset:\n\n::: panel-tabset\n\n\n## HLO `covid`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check names in `covid`\nnames(covid)\n\n# take a quick look values of each variable\n\nglimpse(covid)\n\n# Look at first few observations for:\n# date, administrative_area_level_2, \n\ncovid %>% \n  select(date, administrative_area_level_2) %>%\n  head()\n\n# Summarize data to get a better sense of the unit of observastion\n\ncovid %>% \n  group_by(administrative_area_level_2) %>%\n  summarise(\n    n = n(), # Number of observations for each state\n    start_date = min(date, na.rm = T),\n    end_date = max(date, na.rm=T)\n  ) -> hlo_covid_df\n\nhlo_covid_df\n\n\n# How many unique values of date and state are their:\n\nn_dates <- length(unique(covid$date))\nn_states <- length(unique(covid$administrative_area_level_2))\nn_dates\nn_states\n\n# If we had observations for every state on every date then the number of rows \n# in the data \ndim(covid)[1]\n# Should equal\ndim(covid)[1] == n_dates * n_states\n\n# This is what economists would call an unbalanced panel\n```\n:::\n\n\n\n## HLO `pres_df`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check names in `pres_df`\nnames(pres_df)\n\n# take a quick look values of each variable\n\nglimpse(pres_df)\n\n# Unit of analysis is a year-state-candidate\npres_df %>% \n  select(year, state_po, candidate) %>%\n  head()\n\n# How many states?\nlength(unique(pres_df$state_po))\n\n\n\n# How many candidates and parties on the ballot in a given election year\npres_df %>% \n  group_by(year) %>%\n  summarise(\n    n_candidates = length(unique(candidate)),\n    # Look at both party_detailed and party_simplified\n    n_parties_detailed = length(unique(party_detailed)),\n    n_parties_simplified = length(unique(party_simplified))\n  ) -> hlo_pres_df\nhlo_pres_df\n\n# Look at 2020\n# pres_df$candidate[pres_df$year == \"2020\"]\n```\n:::\n\n\n:::\n \n \n## Q4 Recode the data for analysis\n\nUsing our understanding of the structure of the data, Q4 asks you to:\n\n\n- Recode the Covid-19 data like we've done before [plus]{.blue}\n- Calculate [rolling means]{.blue},  7 and 14 day averages\n- Reshape, recode, and filter the presidential election data\n\n\n\n## Q4.1 Recode the Covid-19{.smaller}\n\n::: panel-tabset\n\n## Task\nThis is the same code we've used before to create `covid_us` from `covid` with the addition of code to calculate a [rolling mean]{.blue} or [moving average]{.blue} of the number of new cases\n\n## Code for Q4.1\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a vector containing of US territories\nterritories <- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us <- covid %>%\n  filter(!administrative_area_level_2 %in% territories)%>%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %>%\n  dplyr::group_by(state) %>%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -> covid_us\n\n# Recode facemask policy\n\ncovid_us %>%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -> covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %>%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -> covid_us\n```\n:::\n\n\n\n## Template Code for Q4.2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %>%\n  dplyr::group_by(state) %>%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -> covid_us\n```\n:::\n\n\n\n:::\n\n\n## Q4.2 Calculate Rolling Means of Covid Deaths{.smaller}\n\n::: panel-tabset\n\n## Task\n\nQ4.2 asks you to create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\n\nIt encourages you to use the code `new_cases_pc_7da` as a template\n\nTo build your coding skills, try writing this yourself, then comparing it to the code in the next tab:\n\n## Code for Q4.2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_us %>%\n  dplyr::group_by(state) %>%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -> covid_us\n```\n:::\n\n\n\n:::\n\n\n\n## Rolling Averages{.smaller} \n\nThe next slides [aren't necessary for the lab]{.blue} but are designed to illustrate:\n\n- the [concept of a rolling mean]{.blue}\n- [what]{.blue} the code does\n- [why]{.blue} might prefer rolling averages over daily values\n\n\n## Look at the output of `zoo::rollmean()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_us %>%\n  filter(date > \"2020-03-05\") %>%\n  select(date,new_cases_pc,new_cases_pc_7day)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 52,580 × 4\n# Groups:   state [51]\n   state     date       new_cases_pc new_cases_pc_7day\n   <chr>     <date>            <dbl>             <dbl>\n 1 Minnesota 2020-03-06      NA                NA     \n 2 Minnesota 2020-03-07       0                NA     \n 3 Minnesota 2020-03-08       0.0177           NA     \n 4 Minnesota 2020-03-09       0                NA     \n 5 Minnesota 2020-03-10       0.0177           NA     \n 6 Minnesota 2020-03-11       0.0355           NA     \n 7 Minnesota 2020-03-12       0.0709           NA     \n 8 Minnesota 2020-03-13       0.0887            0.0329\n 9 Minnesota 2020-03-14       0.124             0.0507\n10 Minnesota 2020-03-15       0.248             0.0836\n# ℹ 52,570 more rows\n```\n\n\n:::\n:::\n\n\n\n## Comparing Daily Cases to Rolling Average{.smaller}\n\nThe following code illustrates how a 7-day rolling mean smooths (`new_cases_pc_7da`) over the [*noisiness*]{.blue} of the daily measure\n\n::: panel-tabset\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_us %>%\n  filter(date > \"2020-03-05\", \n         state == \"Minnesota\") %>%\n  select(date,\n         new_cases_pc,\n         new_cases_pc_7day)%>%\n  ggplot(aes(date,new_cases_pc ))+\n  geom_line(aes(col=\"Daily\"))+\n  # set y aesthetic for second line of rolling average\n  geom_line(aes(y = new_cases_pc_7day,\n                col = \"7-day average\")\n            ) +\n  theme(legend.position=\"bottom\")+\n    labs( col = \"Measure\",\n    y = \"New Cases Per 100k\", x = \"\",\n    title = \"Minnesota\"\n  ) -> fig_covid_mn \n```\n:::\n\n\n\n## Figure\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/plotmean2-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Q4.3 Recode Presidential data{.smaller}\n\n::: panel-tabset\n\n## Task\n\nQ4.3 Gives you a long list of steps to recode, reshape, and filter `pres_df` to produce `pres_df2020`\n\nMost of this is review but it can seem like a lot.\n\nWalk through the provided code and see if you can [map each conceptual step]{.blue} in Q4.3 to its [implementation in the code]{.blue}\n\n## Code for Q4.3\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npres_df %>%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %>%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%>%\n  filter(year == 2020) %>%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %>%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %>%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes *100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,\"Trump\",\"Biden\")))\n  ) -> pres2020_df\n\n# Check Output:\n\nglimpse(pres2020_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 51\nColumns: 9\n$ state         <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\"…\n$ state_po      <chr> \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"F…\n$ year_election <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 20…\n$ totalvotes    <dbl> 2323282, 359530, 3387326, 1219069, 17500881, 3279980, 18…\n$ DEMOCRAT      <dbl> 849624, 153778, 1672143, 423932, 11110250, 1804352, 1080…\n$ REPUBLICAN    <dbl> 1441170, 189951, 1661686, 760647, 6006429, 1364607, 7147…\n$ dem_voteshare <dbl> 36.56999, 42.77195, 49.36469, 34.77506, 63.48395, 55.011…\n$ rep_voteshare <dbl> 62.031643, 52.833143, 49.055981, 62.395730, 34.320724, 4…\n$ winner        <fct> Trump, Trump, Biden, Trump, Biden, Biden, Biden, Biden, …\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Q5 merging data{.smaller}\n\n::: panel-tabset\n\n## Task\n\nQ5 asks you to merge the 2020 election data from `pres2020_df` into `covid_us` using the common `state` variable in each dataset using the function `left_join()`\n\n\n## Merge election data into Covid data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(covid_us)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    61\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(pres2020_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51  9\n```\n\n\n:::\n\n```{.r .cell-code}\ncovid_df <- covid_us %>% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    61\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(covid_df) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    69\n```\n\n\n:::\n:::\n\n\n:::\n\n## Advice for merging {.smaller}\n\n::: panel-tabset\n\n## Advice\n\nWhen merging datasets:\n\n- Check the matches in your joining variables\n  - Make sure the values `state` are the same in each dataset \n  - Check for differences in spelling, punctuation, etc.\n- Check the [dimensions]{.blue} of output of your `left_join()`\n  - If there is a 1-1 match the number of rows should be the same before after\n  \n:::{.callout-tip}\nIn general (although not in my sample code), you should save the merged data to a [new]{.blue} object in R. Saving it back into an existing object can cause issues if you run the merge code multiple times by mistake.\n:::\n\n## Illustration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Should be 51 states and DC in each\nsum(unique(pres_df$state) %in% covid_df$state)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Look at each state variable\n## With [] index\npres_df$state[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ALABAMA\" \"ALABAMA\" \"ALABAMA\" \"ALABAMA\" \"ALABAMA\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncovid_df$state[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Matching is case sensitive \n\n# make pres_df$state title case\n\n## Base R:\npres_df$state <- str_to_title(pres_df$state )\n## Tidy R:\npres_df %>% \n  mutate(\n    state = str_to_title(state )\n  ) -> pres_df\n\n# Should be 51\nsum(unique(pres_df$state) %in% covid_us$state)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 50\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find the mismatch:\nunique(pres_df$state[!pres_df$state %in% covid_us$state])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"District Of Columbia\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Two equivalent ways to fix this mismatch\n## Base R: Quick fix to change spelling of DC\npres_df$state[pres2020_df$state == \"District Of Columbia\"] <- \"District of Columbia\"\n\n## Tidy R: Quick fix to change spelling of DC\n\npres_df %>% \n  mutate(\n    state = ifelse(test = state == \"District Of Columbia\",\n                   yes = \"District of Columbia\",\n                   no = state\n                   )\n  ) -> pres_df\n\n\n# Problem Solved\nsum(unique(pres2020_df$state) %in% covid_df$state)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n# {{< fa lightbulb >}} Causal Inference {.inverse}\n\n## Causal inference is about counterfactual comparisons\n\n- Causal inference is about counterfactual comparisons\n\n  - What would have happened if some aspect of the world either had or had not been present\n\n## Causal Identification{.smaller}\n\n- [Casual Identification]{.blue} refers to \"the assumptions needed for statistical estimates to be given a causal interpretation\" [Keele (2015)](http://lukekeele.com/wp-content/uploads/2016/03/causal.pdf)\\]\n\n  - What do we need to assume to make our claims about cause and effect credible\n\n- [Experimental Designs]{.blue} rely on [randomization]{.blue} of treatment to justify their causal claims\n\n- [Observational Designs]{.blue} require [additional assumptions]{.blue} and [knowledge]{.blue} to make causal claims \n\n## Experimental Designs {.smaller}\n\n- [Experimental designs]{.blue} are studies in which a causal variable of interest, the *treatement*, is [manipulated by the researcher]{.blue} to examine its causal effects on some *outcome* of interest\n\n- [Random assignment]{.blue} is the key to causal identification in experiments because it creates [statistical independence]{.blue} between [treatment]{.blue} and [potential outcomes]{.blue} any potential [confounding factors]{.blue}\n\n:::{.fragment}\n\n$$\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\n$$\\\n\n:::\n\n## Randomization creates credible counterfactual comparisons{.smaller}\n\nIf treatment has been randomly assigned, then:\n\n- The only thing that differs between treatment and control is that one group got the treatment, and another did not.\n- We can estimate the Average Treatment Effect (ATE) using the difference of sample means\n\n:::{.fragment}\n\n$$\n\\begin{aligned}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{aligned}\n$$\n\n:::\n\n## Observational Designs {.smaller}\n\n- [Observational designs]{.blue} are studies in which a causal variable of interest is determined by someone/thing [other than the researcher]{.blue} (nature, governments, people, etc.)\n\n- Since treatment has not been randomly assigned, observational studies typically require [stronger assumptions]{.blue} to make causal claims.\n\n- Generally speaking, these assumptions amount to a claim about conditional independence\n\n:::{.fragment}\n\n\n$$\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i | K_i\n$$\n\n:::\n\n- Where after conditioning on $K_i$, some [knowledge about the world]{.blue} and how the [data were generated]{.blue}, our [treatment]{.blue} is as good as (as-if) randomly assigned (hence [conditionally independent]{.blue})\n  - Economists often call this assumption of [selection on observables]{.blue}\n\n## Causal Inference in Observational Studies {.smaller}\n\nTo understand how to make causal claims in observational studies we will:\n\n- Introduce the concept of [Directed Acyclic Graphs]{.blue} to describe causal relationships\n\n- Discuss three approaches to [covariate adjustment]{.blue}\n\n  - Subclassification\n  - Matching\n  - [Linear Regression]{.blue}\n\n- Three research designs for observational data\n\n  - [Differences-in-Differences]{.blue}\n  - Regression Discontinuity Designs\n  - Instrumental Variables\n  \n  \n\n\n# {{< fa lightbulb >}} Directed Acyclic Graphs {.inverse}\n\n\n## Two Ways to Describe Causal Claims\n\nIn this course, we will use two forms of notation to describe our causal claims.\n\n- *Potential Outcomes Notation* (last lecture)\n\n  - Illustrates the [fundamental problem of causal inference]{.blue}\n\n- [**Directed Acyclic Graphs** (DAGs)]{.blue}\n\n  - Illustrates potential bias from [confounders]{.blue} and [colliders]{.blue}\n\n## Directed Acyclic Graphs\n\n- Directed Acyclic Graphs provide a way of encoding assumptions about casual relationships\n\n  - **Directed** Arrows $\\to$ describe a direct causal effect\n\n  - Arrow from $D\\to Y$ means $Y_i(d) \\neq Y_i(d^\\prime)$ \"The outcome ( $Y$) for person $i$ when D happens ( $Y_i(d)$ ) is different than the the outcome when $D$ doesn't happen ( $Y_i(d^\\prime)$ )\n\n  - No arrow = no effect ( $Y_i(d) = Y_i(d^\\prime)$ )\n\n  - **Acyclic:** No cycles. A variable can't cause itself\n\n## Types of variables in a DAG{.smaller}\n\n\n:::: panel-tabset\n\n## DAG\n![](https://book.declaredesign.org/figures/figure-6-2.svg)\n\n@Blair2023-yg [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)\n\n## Variables\n\n:::{.nonincremental}\n\nCausal Explanations Involve:\n\n- `Y` our outcome\n- `D` A possible cause of Y\n- `M` A [mediator]{.blue} or mechanism through which `D` effects `Y`\n- `Z` An [instrument]{.blue} that can help us isolate the effects of D on  `Y`\n- `X2` a [covariate]{.blue} that may [moderate]{.blue} the effect of `D` on `Y`\n\nThreats to causal claims/Sources of bias:\n\n- `X1` an [observed confounder]{.blue} that is a common cause of both `D` & `Y` \n- `U` an [unobserved confounder]{.blue} a common cause of both `D` & `Y` \n- `K` a [collider]{.blue} that is a common consequence of both `D` & `Y`\n\n:::\n\n::::\n\n## DAGs illustrate two sources of bias:\n\n:::{.nonincremental}\n- **Confounder bias:** Failing to control for a common cause of `D` **and** `Y` (aka Omitted Variable Bias)\n\n- **Collider bias:** Controlling for a common consequence (aka Selection Bias^[Note in practice there's some slippage/debate/disagreement around this nomenclature])\n\n:::\n\n## {.smaller}\n#### Confounding Bias: The Coffee Example\n\n:::: panel-tabset\n\n## Confounding Bias\n:::{.nonincremental}\n- Drinking coffee doesn't cause lung cancer we might find correlation between them because they share a [common cause:]{.blue} smoking.\n\n- Smoking is a [confounding]{.blue} variable, that if [omitted]{.blue} will [bias our results]{.blue} producing a [spurious]{.blue} relationship \n\n- [Adjusting]{.blue} for [confounders]{.blue} removes this source of bias\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Coffee and Cancer\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/confounded_fig1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Adjusting for Smoking\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/confounded_fig2-1.png){width=960}\n:::\n:::\n\n\n\n\n\n::::\n\n## {.smaller} \n#### Collider Bias: The Dating Example\n\n:::: panel-tabset\n\n## Collider bias\n\n:::{.nonincremental}\n\n\n\n- Why are attractive people such jerks?\n\n\n- Suppose [dating]{.blue} is a function of [looks]{.blue} and [personality]{.blue}\n\n- Dating is a [common consequences]{.blue} of [looks]{.blue} and [personality]{.blue}\n\n- Basing our claim off of who we date is an example of [selection bias]{.blue} created by [controlling for collider]{.blue}\n\n \n\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Selection bias \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n\n\n## No relationship in population\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n\n::::\n\n\n## When to control for a variable:\n\n![](https://book.declaredesign.org/figures/figure-16-3.svg)\n\n[@Blair2023-yg] [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)\n\n\n# {{< fa lightbulb >}} Covariate Adjustment {.inverse}\n\n##  Covariate Adjustment\n\nCovariate adjustment refers a broad class of procedures that try to make a comparison more credible or meaningful by adjusting for some other potentially confounding factor.\n\n\n##  Covariate Adjustment\n\nWhen you hear people talk about\n\n- Controlling for age\n- Conditional on income\n- Holding age and income constant\n- Ceteris paribus (All else equal)\n\nThey are typically talking about some sort of covariate adjustment.\n\n##  Three approaches to covariate adjustment\n\n- Subclassification\n\n- Matching \n\n- Regression \n\n\n\n##  Causal Identification through Subclassification\n\nMotivation: Treatment, $D$ is not randomly assigned\n\n$$\nY_i(1),Y_i(0) \\text{ is not} \\perp D_i\n$$\n\nIdentifying assumptions\n\n$$\\begin{aligned}\nY_i(1),Y_i(0) \\perp D_i |X_i && \\text{Selection on Observables}\\\\\n0 < Pr(D_i = 1|X_i) < 1  && \\text{Common Support}\n\\end{aligned}$$\n\n\n##  Causal Identification through Subclassification {.smaller}\n\nIf these assumptions hold, we can estimate the average treatment effect:\n\n$$\\begin{aligned}\nATE = E[Y_i(1)- Y_i(0)|X_i] &= E[Y_i(1)- Y_i(0)|X_i, D_i=1]\\\\\n& = E[Y_i |X_i, D_i=1] - E[Y_i |X_i, D_i=0]\n\\end{aligned}$$\n\nThe average treatment effect is identified by the observed difference of means between treatment and control conditional on the values of $X$\n\n\n##  Causal Identification through Subclassification {.smaller}\n\n- Economists call $Y_i(1),Y_i(0) \\perp D_i |X_i$ an assumption of **Selection on Observables**\n  \n  - Controlling for what we can observe, $X$, $D$ is conditionally independent of Potential Outcomes\n  \n  - Violated if there were some other factor, $U$ that influenced both $D$ and $Y$ (i.e. $U$ is a confounder)\n\n- $0 < Pr(D = 1|X) < 1$ is called an assumption of **Common Support**\n  \n  - There is a non-zero probability of receiving the treatment for all values of X\n  \n  - Violated if only one subgroup had access to the treatment (e.g. Vaccine by age group comparisons)\n\n\n##  Example of Subclassification\n\n- We used subclassification when we compared the unconditional rates of new Covid-19 cases by face mask policy to the conditional rates new cases by policy regime in each month of our data.\n  - Overall rates are misleading. \n  - Lots of things differ between January 2020 and January 2022\n  - Subclassification by month provides a \"fairer\" comparison\n  - But is it \"causal\"\n\n## Limits of Subclassification {.smaller}\n\n- Even controlling for \"month\" there are other omitted variables:\n  - Other policies in place?\n  - Socio-economic differences between states\n  - Others?\n\n\n- Trying to subclassify (stratify) comparisons on more than one or two variables gets hard\n  - **The Curse of Dimensionality**\n\n\n## The Curse of Dimensionality {.smaller}\n\n- As we try to control for more factors, the number of observations per dimension declines rapidly\n  - Men vs Women\n  - Men, ages 20-30 vs Men ages 30-40\n  - Men, ages 20-30 with college degrees and blue eyes vs Men  ages 20-30 with college degrees and green eyes\n\n- Subclassification with more than a few variables, will often produce a lack of common support: \n  - Not enough observations to make credible counterfactual comparisons\n\n## Matching {.smaller}\n\n- Matching refers to a broad set of procedures that essentially try to generalize subclassification to \n  - address to **curse of dimensionality**\n  - achieve balance on a range of observable covariates between **treated** and **control** groups\n\n## Matching {.smaller}\n\nDifferent types of matching procedures:\n\n- **Exact matching:** Find exact matches between treatment and control observations for all covariates $X$\n\n- **Coarsened exact matching:** Find approximate matches within ranges of values for $X$\n\n- **Distance-metric matching:** Calculate a distance metric between observations based on their values of $X$, and match treated and control to minimize that distance\n\n- **Propensity score matching:** Calculate the propsenity to receive treatment using $X$ to predict $D$ and treated and control based on their *propensity scores*\n\n## \n\n![](https://cdn2.hubspot.net/hubfs/355318/images/Propensity-Score-Graphic.jpg)\n\n[Source](https://www.summitllc.us/propensity-score-matching)\n\n\n## Causal Identification with Matching (ICYI) {.smaller}\n\nMatching again requires an assumption of **selection on observables**\n\n$$\n\\begin{aligned}\nY_i(1),Y_i(0) \\perp D_i |X_i && \\text{Selection on Observables}\\\\\n0 < Pr(D_i = 1|X_i) < 1  && \\text{Common Support}\n\\end{aligned}\n$$\n\n\nMatching procedures like propensity score matching, allow us to match treated and control observations based on a *propensity score*, a predicted value of receiving the treatment, $D$ based on observed variables, $X$.\n\n$$\np(X_i) = Pr(D=1|X_i) = \\pi_i\n$$\n\nAllowing us to estimate an ATE conditional on $\\pi_i$\n\n$$\n\\begin{aligned}\nATE &= E[Y_i(1)- Y_i(0)|p(X_i) = \\pi_i] \\\\\n&= E[Y_i(1)- Y_i(0)|p(X_i) = \\pi_i, D_i=1]\\\\\n& = E[Y_i |p(X_i) = \\pi_i, D_i=1] - E[Y_i |p(X_i) = \\pi_i, D_i=0]\n\\end{aligned}\n$$\n\n\n## What to Know about Matching {.smaller}\n\n- The mechanics of matching are beyond the scope of this course\n\n- Just think of it as a generalization of subclassification when we want to condition on multiple variables\n\n- \"Solves\" the curse of dimensionality, creating Treatment-Control comparisons between groups that are similar on **observed covariates**\n\n- But **no guarantee** that matching produces balance on **unobserved covariates**.\n\n## Regression {.smaller}\n\n- We will spend the next two weeks talking in detail about regression, in general and linear regression in particular.\n\n- Today we'll introduce some basic notation and simple examples\n\n- Conceptually, think of regression as \n  - a tool to make predictions\n  - by fitting lines to data\n\n- Theoretically, we will build towards an understanding of linear regression as a \"linear estimate of the conditional expectation function $(CEF = E[Y|X])$\n\n##  Three approaches to covariate adjustment{.smaller}\n\n- [Subclassification]{.blue}\n  - 👍: Easy to implement and interpret\n  - 👎: Curse of dimensionality, Selection on observables, \n\n- [Matching ]{.blue}\n  - 👍: Balance on multiple covariates, Mirrors logic of experimental design, Fewer functional form assumptions\n  - 👎: Selection on observables, Only provides balance on observed variables, Lot's of technical details...\n\n- [Regression]{.blue}\n  - 👍: Easy to implement, control for many factors (good and bad)\n  - 👎: Selection on observables, Assumes a linear functional form, easy to fit \"bad\" models\n\n\n# {{< fa lightbulb >}} Simple Linear Regression {.inverse}\n\n## Understanding Linear Regression{.smaller}\n\n\n:::{.nonincremental}\n- **Conceptual**\n  - Simple linear regression estimates \"a line of best fit\" that summarizes relationships between two variables\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\n- **Practical**\n  - We estimate linear models in R using the `lm()` function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x, data = df)\n```\n:::\n\n\n\n:::\n\n## Understanding Linear Regression{.smaller}\n\n:::{.nonincremental}\n\n- *Technical/Definitional*\n  - Linear regression chooses $\\beta_0$ and $\\beta_1$ to minimize the Sum of Squared Residuals (SSR):\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n- *Theoretical*\n  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$\n\n:::\n\n\n# {{< fa lightbulb >}} Conceptual: Linear Regression {.inverse}\n\n## Conceptual: Linear Regression\n\n- Regression is a tool for describing relationships.\n\n  - How does some outcome we're interested in tend to change as some predictor of that outcome changes?\n\n  - How does economic development vary with democracy?\n\n  - How does economic development vary with democracy, adjusting for natural resources like oil and gas\n\n\n## Conceptual: Linear Regression {.smaller}\n\n:::{.nonincremental}\n\nMore formally:\n\n$$\ny_i = f(x_i) + \\epsilon\n$$\n\n- Y is a function of X plus some error, $\\epsilon$\n\n- Linear regression assumes that relationship between an outcome and a predictor can be by a [linear](https://en.wikipedia.org/wiki/Linearity) function\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n:::\n\n## Linear Regression and the Line of Best Fit {.smaller}\n\n:::{.nonincremental}\n\n\n- The goal of linear regression is to choose coefficients $\\beta_0$ and $\\beta_1$ to summarizes the relationship between $y$ and $x$\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n- To accomplish this we need some sort of criteria.\n\n- For linear regression, that criteria is minimizing the error between what our model predicts $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$ and what we actually observed $(y_i)$\n\n- More on this to come. But first...\n\n:::\n\n## Regression Notation{.smaller}\n\n- $y_i$ an **outcome variable** or thing we're trying to explain\n\n  - AKA: The dependent variable, The response Variable, The left hand side of the model\n\n- $x_i$ a **predictor variables** or things we think explain variation in our outcome\n\n  - AKA: The independent variable, covariates, the right hand side of the model.\n\n  - Cap or No Cap: I'll use $X$ (should be $\\mathbf{X}$) to denote a set (matrix) of predictor variables. $y$ vs $Y$ can also have technical distinctions (Sample vs Population, observed value vs Random Variable, ...)\n\n- $\\beta$ a set of **unknown parameters** that describe the relationship between our outcome $y_i$ and our predictors $x_i$\n\n- $\\epsilon$ the **error term** representing variation in $y_i$ not explained by our model.\n\n  - Technically $\\epsilon$ refers to theoretical error inherent to the data generating process, while $\\hat{\\epsilon}_i$ or $u_i$ is used to refer to [residuals]{.blue}, an estimated error that reflects the difference between the observed ($y_i$) and predicted ($\\hat{y}_i$) values.\n\n## Linear Regression {.smaller}\n\n:::{.nonincremental}\n\n- We call this a bivariate regression, because there are only two variables\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n- We call this a linear regression, because $y_i = \\beta_0 + \\beta_1 x_i$ is the equation for a line, where:\n\n  - $\\beta_0$ corresponds to the $y$ intercept, or the model's prediction when $x = 0$.\n\n  - $\\beta_1$ corresponds to the slope, or how $y$ is predicted to change as $x$ changes.\n:::\n\n## Linear Regression {.smaller}\n\n:::{.nonincremental}\n\n- If you find this notation confusing, try plugging in substantive concepts for what $y$ and $x$ represent\n- Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.\n\nThe generic linear model\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon$$\n\nReflects:\n\n$$\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i$$\n:::\n\n# {{< fa lightbulb >}} Practical: Estimating a Linear Regression {.inverse}\n\n## Practical: Estimating a Linear Regression{.smaller}\n\n:::{.nonincremental}\n\n- We estimate linear regressions in `R` using the `lm()` function.\n- `lm()` requires two arguments:\n  - a `formula` argument of the general form `y ~ x` read as \"Y modeled by X\" or below \"Transgender Feeling Thermometer (`y`) modeled by (`~`) Age (`x`)\n  - a `data` argument telling R where to find the variables in the formula\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))\nm1 <- lm(therm_trans_t0 ~ vf_age, data = df)\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n```\n\n\n:::\n:::\n\n\n\n:::\n\n## The `lm()` function{.smaller}\n\n:::{.nonincremental}\n\nThe coefficients from `lm()` are saved in object called `m1`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n```\n\n\n:::\n:::\n\n\n\n`m1` actually contains a lot of information\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n```\n\n\n:::\n\n```{.r .cell-code}\nm1$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Practical: Interpreting a Linear Regression{.smaller}\n\nWe can extract the intercept and slope from this simple bivariate model, using the `coef()` function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# All the coefficients\ncoef(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Just the intercept\ncoef(m1)[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n    62.8196 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Just the slope\ncoef(m1)[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    vf_age \n-0.2030711 \n```\n\n\n:::\n:::\n\n\n\n## Practical: Interpreting a Linear Regression{.smaller}\n\nThe two coefficients from `m1` define a line of best fit, summarizing how feelings toward transgender individuals change with age\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon$$\n\n$$\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i$$\n\n$$\\text{Transgender Feeling Thermometer}_i = 62.82 + -0.2 \\text{Age}_i + \\epsilon_i$$\n\n## Practical: Predicted values from a Linear Regression{.smaller}\n\n:::{.nonincremental}\n\n- Often it's useful for interpretation to obtain predicted values from a regression.\n\n- To obtain predicted vales $(\\hat{y})$, we simply plug in a value for $x$ (In this case, $Age$) and evaluate our equation.\n\n- For example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?\n\n$$\\hat{FT}_{x=18} = 62.82 + -0.2 \\times 18  = 59.16$$ $$\\hat{FT}_{x=65} = 62.82 + -0.2 \\times 68  = 49.01$$\n:::\n\n## Practical: Predicted values from a Linear Regression\n\nWe could do this by hand\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(m1)[1] + coef(m1)[2] * 18\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   59.16432 \n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(m1)[1] + coef(m1)[2] * 68\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   49.01076 \n```\n\n\n:::\n:::\n\n\n\n## Practical: Predicted values from a Linear Regression{.smaller}\n\nMore often we will:\n\n- Make a [prediction data frame]{.blue} (called `pred_df` below) with the values of interests\n- Use the `predict()` function with our linear model (`m1`) and `pred_df`\n- Save the predicted values to our new column in our prediction data frame\n\n## Practical: Predicted values from a Linear Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make prediction data frame\npred_df <- data.frame(\n  vf_age = c(18, 68)\n)\n# Predict FT for 18 and 68 year-olds\npredict(m1, newdata = pred_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2 \n59.16432 49.01076 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Save predictions to data frame\npred_df$ft_trans_hat <- predict(m1, newdata = pred_df)\npred_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  vf_age ft_trans_hat\n1     18     59.16432\n2     68     49.01076\n```\n\n\n:::\n:::\n\n\n\n##\n#### Practical: Visualizing Linear Regression\n\n::: panel-tabset\n\n## Concept\nWe can visualize simple regression by:\n\n- plotting a scatter plot of the outcome (y-axis) and predictors (x-axis)\n\n- overlaying the line defined by `lm()`\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfig_lm <- df %>%\n  ggplot(aes(vf_age,therm_trans_t0))+\n  geom_point(size=.5, alpha=.5)+\n  geom_abline(intercept = coef(m1)[1],\n              slope = coef(m1)[2],\n              col = \"blue\"\n              )+\n  geom_vline(xintercept = 0,linetype = 2)+\n  xlim(0,100)+\n  annotate(\"point\",\n           x = 0, y = coef(m1)[1],\n           col= \"red\",\n           )+\n  annotate(\"text\",\n           label = expression(paste(beta[0],\"= 62.81\" )),\n           x = 1, y = coef(m1)[1]+5,\n           hjust = \"left\",\n           )+\n  labs(\n    x = \"Age\",\n    y = \"Feeling Thermometer toward\\nTransgender People\"\n  )+\n  theme_classic() -> fig_lm\n```\n:::\n\n\n\n\n## Intercept\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/fig_lm_plot-1.png){width=960}\n:::\n:::\n\n\n\n## Slope\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/figlm1-1.png){width=960}\n:::\n:::\n\n\n\n## Errors\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/figlm1code-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n# {{< fa lightbulb >}} Technichal: Mechanics of Linear Regression {background-color=\"lightgrey\"}\n\n## How did `lm()` choose $\\beta_0$ and $\\beta_1${.smaller}\n\n:::{.nonincremental}\n\n- P: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression\n\n\n\n- Q: Ok, that's not really that helpful...\n\n  - What's an error?\n  - Why would we square and sum them\n  - How do we minimize them.\n\nP: Good questions!\n\n:::\n\n## What's an error? {.smaller}\n\nAn error, $\\epsilon_i$ is simply the difference between the observed value of $y_i$ and what our model would predict, $\\hat{y_i}$ given some value of $x_i$. So for a model:\n\n$$y_i=\\beta_0+\\beta_1 x_{i} + \\epsilon_i$$\n\nWe simply subtract our model's prediction $\\beta_0+\\beta_1 x_{i}$ from the the observed value, $y_i$\n\n$$\\hat{\\epsilon_i}=y_i-\\hat{y_i}=(Y_i-(\\beta_0+\\beta_1 x_{i}))$$\n\nTo get $\\epsilon_i$\n\n## Why are we squaring and summing $\\epsilon${.smaller}\n\n:::{.nonincremental}\n\nThere are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)\n\n- Squaring $\\epsilon$ treats positive and negative residuals equally.\n\n- Summing produces single value summarizing our models overall performance.\n\nThere are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties\n:::\n\n## How do we minimize $\\sum \\epsilon^2$ {.smaller}\n\nOLS chooses $\\beta_0$ and $\\beta_1$ to minimize $\\sum \\epsilon^2$, the Sum of Squared Residuals (SSR)\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n## How did `lm()` choose $\\beta_0$ and $\\beta_1$ {.smaller}\n\nIn an intro stats course, we would walk through the process of finding\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$ Which involves a little bit of calculus. The big payoff is that\n\n$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$ And\n\n$$ \\beta_1 = \\frac{Cov(x,y)}{Var(x)}$$ Which is never quite the epiphany, I think we think it is...\n\nThe following slides walk you through the mechanics of this exercise. We're gonna skip through them in class, but they're there for your reference\n\n## How do we minimize $\\sum \\epsilon^2${.smaller}\n\nTo understand what's going on under the hood, you need a broad understanding of some basic calculus.\n\nThe next few slides provide a brief review of derivatives and differential calculus.\n\n## Derivatives{.smaller}\n\n:::{.nonincremental}\n\nThe derivative of $f$ at $x$ is its rate of change at $x$\n\n- For a line: the slope\n- For a curve: the slope of a line tangent to the curve\n\nYou'll see two notations for derivatives:\n\n1.  Leibniz notation:\n\n$$\n\\frac{df}{dx}(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{(x+h)-x}\n$$\n\n2.  Lagrange: $f^{\\prime}(x)$\n\n:::\n\n\n## Some useful facts about derivatives{.smaller}\n\nDerivative of a constant\n\n$$\nf^{\\prime}(c)=0\n$$\n\nDerivative of a line f(x)=2x\n\n$$\nf^{\\prime}(2x)=2\n$$\n\nDerivative of $f(x)=x^2$\n\n$$\nf^{\\prime}(x^2)=2x\n$$\n\nChain rule: y= f(g(x)). The derivative of y with respect to x is\n\n$$\n\\frac{d}{dx}(f(g(x)))=f^{\\prime}(g(x))g^{\\prime}(x)\n$$\n\nThe derivative of the \"outside\" times the derivative of the \"inside,\" remembering that the derivative of the outside function is evaluated at the value of the inside function.\n\n## Finding a Local Minimums{.smaller}\n\nLocal minimum:\n\n$$\nf^{\\prime}(x)=0 \\text{ and } f^{\\prime\\prime}(x)>0 \n$$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://copingwithcalculus.com/SecondDeriv1.png)\n:::\n:::\n\n\n\n[Source](https://copingwithcalculus.com/SecondDerivativeTest.html)\n\n## Partial Derivatives{.smaller}\n\nLet $f$ be a function of the variables $(x, \\dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is\n\n$$\\begin{align*}\n\\frac{\\partial f(x, \\dots, X_n)}{\\partial X_i}=\\lim_{h\\to0}\\frac{f(x, \\dots X_i+h \\dots, X_n)-f(x, \\dots X_i \\dots, X_n)}{h}\n\\end{align*}$$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png)\n:::\n:::\n\n\n\n[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)\n\n## Minimizing the sum of squared errors{.smaller}\n\nOur model\n\n$$y_i =\\beta_0+\\beta_1x_{i}+\\epsilon_i$$\n\nFinds coefficients $\\beta_0$ and $\\beta_1$ to to minimize the sum of squared residuals, $\\hat{\\epsilon}_i$:\n\n$$\\begin{aligned}\n\\sum \\hat{\\epsilon_i}^2 &= \\sum (y_i-\\beta_0-\\beta_1 x_{i})^2\n\\end{aligned}$$\n\n## Minimizing the sum of squared errors{.smaller}\n\nWe solve for $\\beta_0$ and $\\beta_1$, by taking the partial derivatives with respect to $\\beta_0$ and $\\beta_1$, and setting them equal to zero\n\n$$\\begin{aligned}\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial \\beta_0} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})=0 & f'(-x^2) = -2x\\\\\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial\\beta_1} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})x_{i}=0 & \\text{chain rule}\n\\end{aligned}$$\n\n## Solving for $\\beta_0${.smaller}\n\nFirst, we'll solve for $\\beta_0$, by multiplying both sides by -1/2 and distributing the $\\sum$:\n\n$$\\begin{aligned}\n0 &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})\\\\\n\\sum \\beta_0 &= \\sum y_i - \\sum \\beta_1 x_{i}\\\\\nN \\beta_0 &= \\sum y_i -\\sum \\beta_1 x_{i}\\\\\n\\beta_0 &= \\frac{\\sum y_i}{N} - \\frac{\\beta_1 \\sum x_{i}}{N}\\\\\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}\n\\end{aligned}$$\n\n## Solving for $\\beta_1${.smaller}\n\nNow, we can solve for $\\beta_1$ plugging in $\\beta_0$.\n\n$$\\begin{aligned}\n0 &= -2\\sum [(y_i-\\beta_0-\\beta_1 x_{i})x_{i}]\\\\\n0 &= \\sum [y_ix_i-(\\bar{y} - \\beta_1 \\bar{x})x_{i}-\\beta_1 x_{i}^2]\\\\\n0 &= \\sum [y_ix_i-\\bar{y}x_{i} + \\beta_1 \\bar{x}x_{i}-\\beta_1 x_{i}^2]\n\\end{aligned}$$\n\n## Solving for $\\beta_1${.smaller}\n\nNow we'll rearrange some terms and pull out an $x_{i}$ to get\n\n$$\\begin{aligned}\n0 &= \\sum [(y_i -\\bar{y} + \\beta_1 \\bar{x}-\\beta_1 x_{i})x_{i}]\n\\end{aligned}$$\n\nDividing both sides by $x_{i}$ and distributing the summation, we can isolate $\\beta_1$\n\n$$\\begin{aligned}\n\\beta_1 \\sum (x_{i}-\\bar{x}) &= \\sum (y_i -\\bar{y})\n\\end{aligned}$$\n\nDividing by $\\sum (x_{i}-\\bar{x})$ to get\n\n$$\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})}{\\sum (x_{i}-\\bar{x})}\n\\end{aligned}$$\n\n## Solving for $\\beta_1${.smaller}\n\nFinally, by multiplying by $\\frac{(x_{i}-\\bar{x})}{(x_{i}-\\bar{x})}$ we get\n\n$$\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})(x_{i}-\\bar{x})}{\\sum (\\bar{x}-x_{i})^2}\n\\end{aligned}$$\n\nWhich has a nice interpretation:\n\n$$\\begin{aligned}\n\\beta_1 &= \\frac{Cov(x,y)}{Var(x)}\n\\end{aligned}$$\n\nSo the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat!\n\n# {{< fa lightbulb >}} Theoretical:  OLS provides a linear estimate of CEF: E\\[Y|X\\]{.inverse}\n\n## Linear Regression is a many splendored thing{.smaller}\n\n[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of the various interpretations/motivations for linear regression.\n\n- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)\n\n- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\\beta$\n\n- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)\n\n- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)\n\n- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)\n\n- A [linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)\n\n## Linear Regression is a many splendored thing{.smaller}\n\n:::{.nonincremental}\n\n[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of various interpretations/motivations for linear regression.\n\n- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)\n\n- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\\beta$\n\n- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)\n\n- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)\n\n- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)\n\n- A [**linear approximation of the conditional expectation function**](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)\n\n:::\n\n\n\n## The Conditional Expectation Function{.smaller}\n\nOf all the functions we could choose to describe the relationship between $Y$ and $X$,\n\n$$\nY_i = f(X_i) + \\epsilon_i\n$$\n\nthe conditional expectation of $Y$ given $X$ $(E[Y|X])$, has some appealing properties\n\n$$\nY_i = E[Y_i|X_i] + \\epsilon\n$$\n\nThe error, by definition, is uncorrelated with X and $E[\\epsilon|X]=0$\n\n$$\nE[\\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0\n$$\n\nOf all the possible functions $g(X)$, we can show that $E[Y_i|X_i]$ is the best predictor in terms of minimizing **mean squared error**\n\n$$\nE[ (Y - g(Y))^2] \\geq E[(Y - E[Y|X])^2] \n$$\n\n## {.smaller}\n#### Linear Approximations to the Conditional Expectation Function\n\n::: panel-tabset\n\n## Concept\n\n:::{.nonincremental}\n\n- We can then show (in a different class) that linear regression provides the best linear predictor of the CEF\n  - Chapter 3, of [Mostly Harmless Econometrics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991028523169706966)\n  - Chapter 4 of [Foundations of Agnostic Statistics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991000736119706966)\n- Furthermore, when the CEF is linear, it's equal exactly to OLS regression\n:::\n\n## CEF\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/cef1code-1.png){width=960}\n:::\n:::\n\n\n\n## OLS\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/cef2code-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n## What you need to know about Regression {.smaller}\n\n:::{.nonincremental}\n\n- **Conceptual**\n  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\n- **Practical**\n  - We estimate linear models in R using the `lm()` function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x, data = df)\n```\n:::\n\n\n\n:::\n\n## What you need to know about Regression {.smaller}\n\n:::{.nonincremental}\n\n- *Technical/Definitional*\n  - Linear regression chooses $\\beta_0$ and $\\beta_1$ to minimize the Sum of Squared Residuals (SSR):\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n- *Theoretical*\n  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$\n\n:::\n\n# {{< fa lightbulb >}} Difference-in-Differences {.inverse}\n\n## Motivating Example: What causes Cholera? {.smaller background-image=https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc, background-opacity=.3}\n\n- In the 1800s, cholera was thought to be transmitted through the air.\n\n- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\n\n- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. \n\n\n## Notation {.smaller}\n\nLet's adopt a little notation to help us think about the logic of Snow's design:\n\n- $D$: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\n- $T$: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\n- $Y_{di}(t)$ the potential outcome of unit $i$ \n\n  - $Y_{1i}(t)$ the potential outcome of unit $i$ when treated between the two periods \n\n  - $Y_{0i}(t)$ the potential outcome of unit $i$ when control between the two periods \n\n\n## Causal Effects {.smaller}\n\nThe individual causal effect for unit i at time t is:\n\n$$\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)$$\n\nWhat we observe is \n\n$$Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)$$\n\n$D$ only equals 1, when $T$ equals 1, so we never observe $Y_0i(1)$ for the treated units. \n\nIn words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.\n\n\n## Average Treatment on Treated {.smaller}\n\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\n\n$$\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$\n\nThat is, what would have happened in Lambeth, had their water company not moved their pipes\n\n\n## Average Treatment on Treated {.smaller}\n\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\nWe we can observe is:\n\n|               | Pre-Period (T=0)  | Post-Period (T=1)  |\n|-|--|-|\n| Treated $D_{i}=1$  |  $E[Y_{0i}(0)\\vert D_i = 1]$ | $E[Y_{1i}(1)\\vert D_i = 1]$  |\n| Control $D_i=0$  |  $E[Y_{0i}(0)\\vert D_i = 0]$ | $E[Y_{0i}(1)\\vert D_i = 0]$  |\n\n\n## Data {.smaller}\n\nBecause potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Company                      | 1849 (T=0)| 1854 (T=1)|\n|:----------------------------|----------:|----------:|\n|Lambeth (D=1)                |         85|         19|\n|Southwark and Vauxhall (D=0) |        135|        147|\n\n\n:::\n:::\n\n\n\n\n## How can we estimate the effect of moving pumps upstream? {.smaller}\n\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\n$$\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$\n\nLet's conisder some strategies Snow could take to estimate this quantity:\n\n\n## Before vs after comparisons:{.smaller}\n\n:::{.nonincremental}\n- Snow could have compared Labmeth in 1854 $(E[Y_i(1)|D_i = 1] = 19)$ to Lambeth in 1849 $(E[Y_i(0)|D_i = 1]=85)$, and claimed that moving the pumps upstream led to **66 fewer cholera deaths.** \n\n- Assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved $(E[Y_{0i}(1)|D_i = 1])$.\n\n- A skeptic might argue that Lambeth in 1849 $\\neq$ Lambeth in 1854\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Company </th>\n   <th style=\"text-align:right;\"> 1849 (T=0) </th>\n   <th style=\"text-align:right;\"> 1854 (T=1) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;color: blue !important;\"> Lambeth (D=1) </td>\n   <td style=\"text-align:right;font-weight: bold;color: blue !important;\"> 85 </td>\n   <td style=\"text-align:right;font-weight: bold;color: blue !important;\"> 19 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Southwark and Vauxhall (D=0) </td>\n   <td style=\"text-align:right;\"> 135 </td>\n   <td style=\"text-align:right;\"> 147 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n:::\n\n\n## Treatment-Control comparisons in the Post Period. {.smaller}\n\n:::{.nonincremental}\n\n- Snow could have compared outcomes between Lambeth and S&V in 1954  ($E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to **128 fewer deaths.**\n\n- Here the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved $(E[Y_{0i}(1)|D_i = 1])$\n\n- Again, our skeptic could argue  Lambeth $\\neq$ S&V \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Company </th>\n   <th style=\"text-align:right;\"> 1849 (T=0) </th>\n   <th style=\"text-align:right;\"> 1854 (T=1) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Lambeth (D=1) </td>\n   <td style=\"text-align:right;\"> 85 </td>\n   <td style=\"text-align:right;font-weight: bold;color: red !important;\"> 19 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Southwark and Vauxhall (D=0) </td>\n   <td style=\"text-align:right;\"> 135 </td>\n   <td style=\"text-align:right;font-weight: bold;color: red !important;\"> 147 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n:::\n\n## Difference in Differences {.smaller}\n\n:::{.nonincremental}\nTo address these concerns, Snow employed what we now call a [difference-in-differences]{.blue} design, \n\nThere are two, equivalent ways to view this design. \n\n$$\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}$$\n\n- Difference 1: Average change between Treated and Control  in Post Period\n\n- Difference 2: Average change between Treated and Control  in Pre Period\n\n:::\n\n## Difference in Differences {.smaller}\n\n:::{.nonincremental}\n\n$$\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}$$\nIs equivalent to: \n\n$$\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}$$\n\n\n- Difference 1: Average change between Treated over time\n- Difference 2: Average change between Control over time\n\n:::\n\n## Difference in Differences {.smaller}\n\n\nYou'll see the DiD design represented both ways, but they produce the same result:\n\n$$\n\\tau_{ATT} = (19-147) - (85-135) = -78\n$$\n\n$$\n\\tau_{ATT} = (19-85) - (147-135) = -78\n$$\n\n\n## Identifying Assumption of a Difference in Differences Design {.smaller}\n\nThe key assumption in this design is what's known as the parallel trends assumption: $E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]$ \n\n- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&V\n\n## Parralel Trends\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-slides_files/figure-revealjs/paralleltrends-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n## Summary {.smaller}\n\n- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\n  \n  - Taking the pre-post difference removes any fixed differences between the units\n  \n  - Then taking the difference between treated and control differences removes any common differences over time\n\n- The key identifying assumption of a DiD design is the \"assumption of parallel trends\"\n  - Absent treatment, treated and control groups\nwould see the same changes over time.\n  - Hard to prove, possible to test\n\n\n\n## Extensions and limitations {.smaller}\n\n- Diff-in-Diff easy to estimate with linear regression\n- Generalizes to multiple periods and treatment interventions\n  - More pre-treatment periods allow you assess \"parallel trends\" assumption\n- Alternative methods \n  - Synthetic control\n  - Event Study Designs\n- What if you have multiple treatments or treatments that come and go?\n  - Panel Matching\n  - Generalized Synthetic control\n\n\n## Applications{.smaller}\n\n- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment\n\n- [Abadie, Diamond, & Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany\n\n- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?\n\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}