{
  "hash": "73241e78325a4b7bb5b3f0845a6209f2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 05:\"\nsubtitle: \"Prediction with Linear Regression\"\nauthor: \"Paul Testa\"\nformat: \n  revealjs:\n    theme: [default, brownslides.scss]\n    logo: images/pols1600_hex.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: true\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    # include-after-body: title-slide.html\n    title-slide-attributes:\n      align: left\n      data-background-image: images/pols1600_hex.png\n      data-background-position: 90% 50%\n      data-background-size: 40%\nexecute: \n  echo: true\nfilters:\n  - openlinksinnewpage\n---\n\n\n# Overview\n\n\n\n\n\n\n\n\n\n\n\n## General Plan\n\n-   Group Assignment 1: Research Questions\n-   Setup\n    -   Packages\n    -   Data\n-   Feedback\n-   Review\n    -   Casual inference\n    -   Covariate Adjustment\n-   Linear Models\n    -   Conceptual\n    -   Practical\n    -   Technical\n    -   Theoretical\n-   Three Common Observational Designs\n    -   Difference in Difference\n    -   Regression Discontinuity\n    -   Instrumental Variable Designs\n\nclass: inverse, center, middle \\# Research Questions\n\n## Research Questions\n\n-   Template [here](https://pols1600.paultesta.org/files/assignments/A1_research_questions.docx)\n\n-   Upload to [Canvas](https://canvas.brown.edu/courses/1087979/assignments/7870538?module_item_id=10762418) this Sunday\n\n-   3 Research questions:\n\n    -   One-sentence research question\n\n    -   Why do we care\n\n    -   Ideal experiment\n\n    -   Observational study\n\n    -   Feasibility\n\nclass:inverse, middle, center \\# üí™ \\## Get set up to work\n\n## New packages\n\nHopefully, you were all able to install the following packages\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstall.packages(\"dataverse\")\ninstall.packages(\"tidycensus\")\ninstall.packages(\"easystats\")\ninstall.packages(\"DeclareDesign\")\n```\n:::\n\n\n## Census API:\n\nAdditionally, I hope you have all followed the steps [here](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package):\n\n1.  Install the `tidycensus` package\n2.  Load the installed package\n3.  Request an API key from the Census\n4.  Check your email\n5.  Activate your key\n6.  Install your API key in R\n7.  Check that everything worked\n\nTo install the an API key so we can download data directly from the US Census\n\n## Packages for today\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", #<<\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"#<<\n)\n```\n:::\n\n\n## Define a function to load (and if needed install) packages\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n```\n:::\n\n\n## Load packages for today\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE \n```\n\n\n:::\n:::\n\n\nclass:inverse, center, middle \\# üí™ \\## Load Data for today\n\n## Load the Covid-19 Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# covid <- COVID19::covid19(\n#   country = \"US\",\n#   level = 2,\n#   verbose = F\n# )\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n```\n:::\n\n\n## Filter Covid-19 Data to US States\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Vector containing of US territories\nterritories <- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us <- covid %>%\n  filter(!administrative_area_level_2 %in% territories)%>%\n  mutate(\n    state = administrative_area_level_2\n  )\n```\n:::\n\n\n## Mutate: Calculate New Cases\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncovid_us %>%\n  dplyr::group_by(state) %>%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -> covid_us\n```\n:::\n\n\n## Calculating a Rolling Average New Cases\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncovid_us %>%\n  filter(date > \"2020-03-05\") %>%\n  select(date,new_cases_pc,new_cases_pc_7da)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 52,580 √ó 4\n# Groups:   state [51]\n   state     date       new_cases_pc new_cases_pc_7da\n   <chr>     <date>            <dbl>            <dbl>\n 1 Minnesota 2020-03-06      NA               NA     \n 2 Minnesota 2020-03-07       0               NA     \n 3 Minnesota 2020-03-08       0.0177          NA     \n 4 Minnesota 2020-03-09       0               NA     \n 5 Minnesota 2020-03-10       0.0177          NA     \n 6 Minnesota 2020-03-11       0.0355          NA     \n 7 Minnesota 2020-03-12       0.0709          NA     \n 8 Minnesota 2020-03-13       0.0887           0.0329\n 9 Minnesota 2020-03-14       0.124            0.0507\n10 Minnesota 2020-03-15       0.248            0.0836\n# ‚Ñπ 52,570 more rows\n```\n\n\n:::\n:::\n\n\n## New Case Per Capita\n\n.pull-left\\[\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncovid_us %>%\n  filter(date > \"2020-03-05\", \n         state == \"Minnesota\") %>%\n  select(date,\n         new_cases_pc,\n         new_cases_pc_7da)%>%\n  ggplot(aes(date,new_cases_pc ))+\n  geom_line(aes(col=\"Daily\"))+\n  theme(legend.position=\"bottom\")+\n    labs( col = \"Measure\",\n    y = \"New Cases Per 100k\", x = \"\",\n    title = \"Minnesota\"\n  ) -> fig_covid_mn \n```\n:::\n\n\n\\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/plotmean2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\\]\n\n## New Case Per Capita vs 7-day average\n\n.pull-left\\[\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfig_covid_mn +\n  geom_line(aes(y = new_cases_pc_7da,\n                col = \"7-day average\")\n            ) -> fig_covid_mn\n```\n:::\n\n\n\\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/plotmean7d2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\\]\n\n## Facemask Policy\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncovid_us %>%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -> covid_us\n```\n:::\n\n\n## Mutate: Dates and Vaccinations\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncovid_us %>%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -> covid_us\n```\n:::\n\n\n## Load Data on Presidential Elections\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Try this code first\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df <- get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# If the code above fails, comment out and uncomment the code below:\n\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n```\n:::\n\n\n## HLO of Presidential Elections Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(pres_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 15\n   year state   state_po state_fips state_cen state_ic office       candidate   \n  <dbl> <chr>   <chr>         <dbl>     <dbl>    <dbl> <chr>        <chr>       \n1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, JI‚Ä¶\n2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GERA‚Ä¶\n3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, LE‚Ä¶\n4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BEN‚Ä¶\n5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\" \n6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE, ‚Ä¶\n# ‚Ñπ 7 more variables: party_detailed <chr>, writein <lgl>,\n#   candidatevotes <dbl>, totalvotes <dbl>, version <dbl>, notes <lgl>,\n#   party_simplified <chr>\n```\n\n\n:::\n:::\n\n\n## Transform Data to get just 2020 Election\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npres_df %>%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %>%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%>%\n  filter(year == 2020) %>%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %>%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %>%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes *100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,\"Trump\",\"Biden\")))\n  ) -> pres2020_df\n```\n:::\n\n\n## Transform Data to get just 2020 Election\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(pres2020_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 9\n  state      state_po year_election totalvotes DEMOCRAT REPUBLICAN dem_voteshare\n  <chr>      <chr>            <dbl>      <dbl>    <dbl>      <dbl>         <dbl>\n1 Alabama    AL                2020    2323282   849624    1441170          36.6\n2 Alaska     AK                2020     359530   153778     189951          42.8\n3 Arizona    AZ                2020    3387326  1672143    1661686          49.4\n4 Arkansas   AR                2020    1219069   423932     760647          34.8\n5 California CA                2020   17500881 11110250    6006429          63.5\n6 Colorado   CO                2020    3279980  1804352    1364607          55.0\n# ‚Ñπ 2 more variables: rep_voteshare <dbl>, winner <fct>\n```\n\n\n:::\n:::\n\n\n## Load Data on Median State Income from the Census\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nacs_df <- get_acs(geography = \"state\", \n              variables = c(med_income = \"B19013_001\",\n                            med_age = \"B01002_001\"), \n              year = 2019)\n```\n:::\n\n\n## HLO: Census Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(acs_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 5\n  GEOID NAME    variable   estimate    moe\n  <chr> <chr>   <chr>         <dbl>  <dbl>\n1 01    Alabama med_age        39      0.2\n2 01    Alabama med_income  50536    304  \n3 02    Alaska  med_age        34.3    0.1\n4 02    Alaska  med_income  77640   1015  \n5 04    Arizona med_age        37.7    0.2\n6 04    Arizona med_income  58945    266  \n```\n\n\n:::\n:::\n\n\n## Tidy Census Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nacs_df %>%\n  mutate(\n    state = NAME,\n  ) %>%\n  select(state, variable, estimate) %>%\n  pivot_wider(names_from = variable,\n              values_from = estimate) -> acs_df\n```\n:::\n\n\n## Tidy Census Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(acs_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 3\n  state      med_age med_income\n  <chr>        <dbl>      <dbl>\n1 Alabama       39        50536\n2 Alaska        34.3      77640\n3 Arizona       37.7      58945\n4 Arkansas      38.1      47597\n5 California    36.5      75235\n6 Colorado      36.7      72331\n```\n\n\n:::\n:::\n\n\n## Merge election data and covid data into single `df`\n\n.pull-left\\[ - We're going to take our `covid_us` data and **merge** into this data on the 2020 election from `pres2020_df` using the common `state` variable in each data set for a `left_join()`\n\n-   Always check the matches in your joining variable (i.e. `state`)\n\n-   Below we see that our recoding of state to title case in created a mismatch\n\n\\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Should be 51\nsum(pres2020_df$state %in% covid_us$state)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find the mismatch:\npres2020_df$state[!pres2020_df$state %in% covid_us$state]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncharacter(0)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fix\npres2020_df$state[pres2020_df$state == \"District Of Columbia\"] <- \"District of Columbia\"\n# Problem Solved\nsum(pres2020_df$state %in% covid_us$state)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51\n```\n\n\n:::\n:::\n\n\n\\]\n\n## Merge election data into Covid data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(covid_us)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    56\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(pres2020_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51  9\n```\n\n\n:::\n\n```{.r .cell-code}\ncovid_us <- covid_us %>% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    64\n```\n\n\n:::\n:::\n\n\n## Merge Census data into Covid data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(covid_us)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    64\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(acs_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 52  3\n```\n\n\n:::\n\n```{.r .cell-code}\ncovid_us <- covid_us %>% left_join(\n  acs_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us)  # Same number of rows as covid_us w/ 2 additional columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    66\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/05_covid.png){fig-align='center' width=80% height=80%}\n:::\n:::\n\n\n[Red Covid, an Update](https://www.nytimes.com/2022/02/18/briefing/red-covid-partisan-deaths-vaccines.html) *New York Times*, 18 February, 2022\n\n## Preview of the Lab\n\nConceptually, this lab is designed to help reinforce the relationship between linear models like $y=\\beta_0 + \\beta_1x$ and the conditional expectation function $E[Y|X]$.\n\n-   Questions 1-5 are designed to reinforce your **data wrangling** skills. In particular, you will get practice:\n\n    -   Creating and recoding variables using `mutate()`\n    -   Calculating a [moving average](https://en.wikipedia.org/wiki/Moving_average) or rolling mean using the `rollmean()` function from the `zoo` package\n    -   Transforming the data on presidential elections so that it can be merged with the data on Covid-19 using the `pivot_wider()` function.\n    -   [Merging data](https://r4ds.had.co.nz/relational-data.html) together using the `left_join()` function.\n\n-   In question 6, you will see how calculating conditional means provides a simple test of \"Red Covid\" claim.\n\n-   In question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)\n\n-   In question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)\n\n-   In question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.\n\n-   Question 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week's lab.\n\nclass: inverse, center, middle background-image: url(\"https://i.pinimg.com/originals/a2/05/b6/a205b689caf19f3287b1544cbe0e6b7b.jpg\") background-size: contain \\# üì¢ \\## Feedback\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 √ó 29\n   StartDate           EndDate             Status         IPAddress     Progress\n   <dttm>              <dttm>              <dbl+lbl>      <chr>            <dbl>\n 1 2023-02-23 13:47:36 2023-02-23 13:49:09 0 [IP Address] 128.148.206.‚Ä¶      100\n 2 2023-02-23 13:47:34 2023-02-23 13:49:13 0 [IP Address] 128.148.207.‚Ä¶      100\n 3 2023-02-23 13:47:30 2023-02-23 13:49:17 0 [IP Address] 128.148.204.‚Ä¶      100\n 4 2023-02-23 13:47:32 2023-02-23 13:49:26 0 [IP Address] 128.148.204.‚Ä¶      100\n 5 2023-02-23 13:47:50 2023-02-23 13:49:35 0 [IP Address] 128.148.205.‚Ä¶      100\n 6 2023-02-23 13:47:17 2023-02-23 13:49:38 0 [IP Address] 192.91.235.1‚Ä¶      100\n 7 2023-02-23 13:48:03 2023-02-23 13:49:42 0 [IP Address] 128.148.205.‚Ä¶      100\n 8 2023-02-23 13:47:14 2023-02-23 13:49:48 0 [IP Address] 128.148.206.‚Ä¶      100\n 9 2023-02-23 13:47:33 2023-02-23 13:49:54 0 [IP Address] 128.148.207.‚Ä¶      100\n10 2023-02-23 13:47:49 2023-02-23 13:49:54 0 [IP Address] 128.148.207.‚Ä¶      100\n# ‚Ñπ 11 more rows\n# ‚Ñπ 24 more variables: Duration__in_seconds_ <dbl>, Finished <dbl+lbl>,\n#   RecordedDate <dttm>, ResponseId <chr>, RecipientLastName <chr>,\n#   RecipientFirstName <chr>, RecipientEmail <chr>, ExternalReference <chr>,\n#   LocationLatitude <chr>, LocationLongitude <chr>, DistributionChannel <chr>,\n#   UserLanguage <chr>, Likes <chr>, Dislikes <chr>, code_skills <chr>,\n#   code_challenge <chr>, eval_1 <dbl>, eval_2 <dbl>, eval_3 <dbl>, ‚Ä¶\n```\n\n\n:::\n:::\n\n\n## What we liked and learned\n\n--\n\n-   Causal Inference!\n-   Data Viz\n-   Programming\n-   Review\n-   Dank Memes\n\n## What we liked\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-b5a840e22738d802b72d\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b5a840e22738d802b72d\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\"],[\"\",\"Getting to work/meet lab partners I have not communicated with yet in this class!\",\"I found our lab quite interesting last week. It was quite rewarding to be able to analyze the data and apply the skills we already learnt\",\"\",\"The Clueless meme\\n\",\"The comparisons and examples with real-life studies (like the sophomore problem).\",\"I appreciated the content of today's lecture\\n\",\"I found interesting the different ways to interept data and how useful that information is when coding.\",\"lab breakdown of working with our groups but still coming back to working as one large group to make sure we're all on the same page\",\"I've been enjoying the tutorials a lot!\",\"I enjoyed the lab! I appreciated having the comments guide me along the way whenever I needed it. It is especially useful now.\",\"I liked working in groups! It was very interesting to see the way other people work and the troubleshoot with each other. I am looking forward to continuing to work with and learn from my peers.\",\"The mapping concepts to code made the entire process seem a bit more straightforward.\",\"I liked having the time to work with my lab group to just play around with the code.\",\"This is silly but copy and paste is fun lol\",\"We didn't get to it in lab, but did my own experimenting to plot the results of the study. Felt very fulfilling.\",\"I liked the causal questions lecture, especially the section where we got to brainstorm a question with our peers. I think showing us how to understand what's going on in chunks of code was useful. Copy pasting large chunks of code still feels confusing but I think we're getting the skills to get the general picture.\",\"I enjoyed working on replicating published figures from data sets. I think having an end goal/figure helped in understanding the steps we were taking and how to get there.\",\"I liked the lab. I think that having to work through it is the most useful for learning and getting better at coding.\\n\",\"I liked during the lab last week when I understood what I was doing and was able to correct my code independently- that felt good.\\nI found that taking my time on the tutorials and following in my textbook was useful.\",\"I have enjoyed going over many of the terms that I have heard used when discussing social science research in my previous courses. I also enjoyed going through the different lines of code so we can get a better understanding of what each function and operation is actually doing.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Likes<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":3,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Likes\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[3,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## What we've learned\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"\"                                                                                                               \n [2] \"Separate my code effectively into useful code chunks!\\n\"                                                        \n [3] \"I'm not entirely sure\"                                                                                          \n [4] \"putting data$ something in the tutorials\"                                                                       \n [5] \"the basics I have down pretty well, setting up loading and the basic adjustments\"                               \n [6] \"I think I am learning pretty well to discern commands and explain what each line does.\"                         \n [7] \"organizing my code into chunks that make sense for me\\n\"                                                        \n [8] \"Creating different code chunks and labelling them.\"                                                             \n [9] \"setting up workspace and staying organized while coding\"                                                        \n[10] \"It's really satisfying making graphs!\"                                                                          \n[11] \"Loading them packages.\"                                                                                         \n[12] \"follow instructions\"                                                                                            \n[13] \"I feel like I am good at understanding what things do\"                                                          \n[14] \"Set up data/loading libraries, simple filtering of data and making graphs\"                                      \n[15] \"Making things organized\"                                                                                        \n[16] \"Simple arithmetic functions, basic plotting\"                                                                    \n[17] \"Make chunks, load packages, paste things\"                                                                       \n[18] \"using summary on data tables\"                                                                                   \n[19] \"I think I've gotten comfortable with getting the workspace set up and looking over and cleaning data.\"          \n[20] \"I am good at FAAFing O\"                                                                                         \n[21] \"I feel confident using the commands to look at the data! But not quite yet confident on how to mutate the data.\"\nattr(,\"label\")\n[1] \"What's one thing you do really well in R?\"\nattr(,\"format.spss\")\n[1] \"A2000\"\nattr(,\"display_width\")\n[1] 15\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-494fcca211bfa60ce667\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-494fcca211bfa60ce667\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\"],[\"\",\"Separate my code effectively into useful code chunks!\\n\",\"I'm not entirely sure\",\"putting data$ something in the tutorials\",\"the basics I have down pretty well, setting up loading and the basic adjustments\",\"I think I am learning pretty well to discern commands and explain what each line does.\",\"organizing my code into chunks that make sense for me\\n\",\"Creating different code chunks and labelling them.\",\"setting up workspace and staying organized while coding\",\"It's really satisfying making graphs!\",\"Loading them packages.\",\"follow instructions\",\"I feel like I am good at understanding what things do\",\"Set up data/loading libraries, simple filtering of data and making graphs\",\"Making things organized\",\"Simple arithmetic functions, basic plotting\",\"Make chunks, load packages, paste things\",\"using summary on data tables\",\"I think I've gotten comfortable with getting the workspace set up and looking over and cleaning data.\",\"I am good at FAAFing O\",\"I feel confident using the commands to look at the data! But not quite yet confident on how to mutate the data.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>code_skills<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":4,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"code_skills\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[4,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## What we disliked or struggled with\n\n-   Stats\n    -   DAGs\n    -   Regression\n-   Pacing\n-   Keeping track of various concepts and skills\n-   Lack of breaks\n-   Too much review\n\n## What we disliked\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-b96047b10f7b5ce9b27d\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b96047b10f7b5ce9b27d\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\"],[\"\",\"My computer malfunctioning during lab lol\",\"The concepts can be quite hard. The tutorials are important, but sometime it requires a lot of depth\",\"\",\"Needed slightly more time in group work for the lab\",\"\",\"n/a\\n\",\"I am still confused about counterfacturals. I also am unsure how all the math equations are relevant, like what/how are we suppose to think about them.\",\"the tutorials are not my favorite, I don't find them super helpful (yet at least) and they're tedious \\n\",\"Maybe before labs, we could go over the experiment we are working on a bit more in-depth? I got a bit lost during the lab not because of the coding, but because I wasn't sure what information I was trying to find and how that fit into the experiment we were looking at\",\"There are a lot of concepts and mathematical jargon that can kinda get confusing and clunky and makes me get lost at times.\",\"Although I liked working in groups, I found the lab difficult and was thankful for help from my peers and Prof Testa.\",\"The concepts are a bit technical, and my code has been difficult to load recently because of oftentimes granular and difficult-to-spot errors.\",\"\",\"It is hard to keep up with the group when we are using one person's computer and just doing a few things on our own to see if we are following (basically the speed of info recall)\",\"\",\"The pace is still a bit fast.\",\"I think my brain turns off for some of the more theoretical interpretations of descriptive statistics, but that's not really the focus of the class.\",\"I find the amount of slides confusing because if I need to go back to the slides to remember something it is difficult to find what I am looking for because there are so many and we don't usually go over all of them.\",\"I had trouble understanding the mathematical notations that were used last week (really just in reference to the slides and explanations). But it is just frustrating when I semi-grasp the subject and can see the potential\",\"I still feel confused working with R. I don't feel like I have a strong enough understanding of the language to be able to do labs independently/in a group setting.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Dislikes<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":4,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Dislikes\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[4,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n:::\n\n\n## What we've struggled with\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-f9233d388f0e17076163\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-f9233d388f0e17076163\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\"],[\"transforming data\",\"Understanding how to read / comprehend code more effectively!\\n\",\"How to design a graph, and choose the type of graph\",\"I wish I had a better understanding of what types of functions I should use when I want to do something\",\"Wish I understood the long chunks more deeply\",\"More time to digest the topics and methods.\",\"Learning how to write out multiple lines of code at once\",\"I just wish I understood the functions more. I feel like there is always so many functions to keep track up that it gets confusing. I would like a list or something that could maybe explain them better.\",\"I just don't think i understand a lot of the functions super well yet in terms of how to write them myself. like I understand broadly what they're doing, but not how to format them to get them to do those things\",\"Data wrangling is still confusing to me\",\"In the course, I wish to understand the theories and mathematical concepts better with practice or examples.\",\"I often have a hard time locating my mistakes when I have errors.\",\"I struggle to properly set up R and understand the language from scratch.\",\"I would like to understand visually how the columns transform so I can see what the code does.\",\"How we are going to do our own research and find credible data\",\"the less intuitive function names\",\"Mutate function, ggplot\",\"pivot_wider and pivot_longer often confuse me in how to best transpose data.\",\"Knowing how to make graphs has been a bit confusing because we use ggplot in class and something different in the tutorials.\",\"The individual panels and their functions/relations to one another\",\"The syntax behind mutating data, as well as the math behind descriptive statistics.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>code_challenge<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":4,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"code_challenge\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[4,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0 NA NA NA NA NA  0 NA  0 NA  0 NA NA NA NA NA NA  9 NA NA 10\nattr(,\"label\")\n[1] \"How do you think we're doing? Please provide a rough assessment of the performance of: - President Donald Trump\"\nattr(,\"format.spss\")\n[1] \"F40.2\"\nattr(,\"display_width\")\n[1] 5\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/unnamed-chunk-36-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nclass:inverse, middle, center \\# üîç \\## Review\n\n## Review\n\n-   Casual Inference\n\n-   Covariate Adjustment\n\nclass:inverse, middle, center \\# üîç \\## Causal Inference\n\n## Review: Causal Inference\n\n-   Causal inference is about counterfactual comparisons\n\n--\n\n-   Some counterfactuals are easier to imagine or create than others\n\n--\n\n-   Randomization solves the fundamental problem of causal inference allowing us to estimate average treatment effects free from selection bias\n\n--\n\n-   Randomization is not always possible, desirable, or ethical\n\n--\n\n-   Observational designs that try to estimate causal effects need to justify assumptions about conditional independence:\n\n$$\nY_i(1),Y_i(0) \\perp D_i |X_i\n$$\n\n--\n\n-   This assumption goes by many, jargony names: Selection on Observables, Conditional Independence, No unmeasured confounders.\n\n--\n\n-   Credibility of this assumption depends less on having a lot of data, and more on how your data were generated.\n\n--\n\n-   Observational designs estimate the effect of $D$ conditional on $X$ using covariate adjustment.\n\nclass:inverse, middle, center \\# üîç \\## Covariate Adjustment\n\n## Review: Covariate Adjustment\n\n-   Covariate adjustment are a set of statistical procedures that allow us to estimate conditional values\n    -   $e.g.(E[Y|X=x]), E[Happiness| leetCoder == T]$\n-   We adjust for covariates to improve our predictions and make credible comparisons\n    -   $E[\\text{New Covid-19 cases}|\\text{Face mask policy}]$ vs $E[\\text{New Covid-19 cases}|\\text{Face mask policy |June 2020 },]$\n    -   $ATE = E[Y| D = 1] - E[Y | D=0]$\n-   Three approaches:\n    -   Subclassification\n    -   Matching\n    -   Regression\n\n## Subclassification\n\n-   Subclassification is a simple way to adjust for a covariate\n\n    -   Subset the data to include only the values you want ($X=x$, $D=1$, $Interverntion = Treated$) and calculate the quantity of interest (e.g. a conditional mean, `mean(df$income[df$age <30])`)\n\n--\n\n-   But what if we want to control for more than one variable?\n\n-   What if our variables aren't categorical like sex, but continuous like height?\n\n--\n\n-   **The Curse of Dimensionality** as you attempt to adjust for more covariates (add more dimensions), the space of possible combinations grows exponentially\n\n    -   Assumption of Common Support likely to be violated $0 < Pr(D_i = 1|X_i) < 1$\n\n## Matching\n\n-   Matching refers to a broad set of procedures that try to recreate what randomization provides: **covariate balance**\n\n    -   **covariate balance** is a fancy term for saying that in the aggregate, two groups look similar accept on group received the *treatment* $(D=1)$ while another group did not $(D=0)$\n\n--\n\n-   There are many ways to try to match observations:\n    -   **Exact matching:** Find exact matches between treatment and control observations for all covariates $X$. Only works for a few covariates.\n    -   **Coarsened exact matching:** Find approximate matches within ranges of values for $X$\n    -   **Matching on summaries of the covariates** calculating a single measure of the similarility of observations and matching on this summary to produce covariate balance.\n\n--\n\n-   Matching:\n    -   Conceptually appealing (mirrors the logic of an experiment)\n    -   Technically complex (complicated algorithms, finicky software)\n    -   Only provides balance on **observed covariates**\n\nclass: inverse, center, middle \\# üí°\\\n\\# Linear Regression \\## The Basics\n\n## Understanding Linear Regression\n\n-   **Conceptual**\n    -   Simple linear regression estimates \"a line of best fit\" that summarizes relationships between two variables\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\n-   **Practical**\n    -   We estimate linear models in R using the `lm()` function\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(y ~ x, data = df)\n```\n:::\n\n\n-   *Technical/Definitional*\n    -   Linear regression chooses $\\beta_0$ and $\\beta_1$ to minimize the Sum of Squared Residuals (SSR):\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n-   *Theoretical*\n    -   Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$\n\nclass: inverse, center, middle \\# üí°\\\n\\# Conceptual: Linear Regression \\## Linear Regression Provides an Estimate of the Line of Best Fit\n\n## Conceptual: Linear Regression\n\n-   Regression is a tool for describing relationships.\n\n    -   How does some outcome we're interested in tend to change as some predictor of that outcome changes?\n\n    -   How does economic development vary with democracy?\n\n    -   How does economic development vary with democracy, adjusting for natural resources like oil and gas\n\n-   Formally:\n\n$$\ny_i = f(x_i) + \\epsilon\n$$\n\n-   Y is a function of X plus some error, $\\epsilon$\n\n-   Linear regression assumes that relationship between an outcome and a predictor can be by a [linear](https://en.wikipedia.org/wiki/Linearity) function\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n## Linear Regression and the Line of Best Fit\n\n-   The goal of linear regression is to choose coefficients $\\beta_0$ and $\\beta_1$ to summarizes the relationship between $y$ and $x$\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n-   To accomplish this we need some sort of criteria.\n\n-   For linear regression, that criteria is minimizing the error between what our model predicts $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$ and what we actually observed $(y_i)$\n\n-   More on this to come. But first...\n\n## Regression Notation\n\n-   $y_i$ an **outcome variable** or thing we're trying to explain\n\n    -   AKA: The dependent variable, The response Variable, The left hand side of the model\n\n-   $x_i$ a **predictor variables** or things we think explain variation in our outcome\n\n    -   AKA: The independent variable, covariates, the right hand side of the model.\n\n    -   Cap or No Cap: I'll use $X$ (should be $\\mathbf{X}$) to denote a set (matrix) of predictor variables. $y$ vs $Y$ can also have technical distinctions (Sample vs Population, observed value vs Random Variable, ...)\n\n-   $\\beta$ a set of **unknown parameters** that describe the relationship between our outcome $y_i$ and our predictors $x_i$\n\n-   $\\epsilon$ the **error term** representing variation in $y_i$ not explained by our model.\n\n## Linear Regression\n\nLet's return to the simple (bivariate) linear regressions we introduced last week:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n-   We call this a bivariate regression, because there are only two variables.\n\n-   We call this a linear regression, because $y_i = \\beta_0 + \\beta_1 x_i$ is the equation for a line, where:\n\n    -   $\\beta_0$ corresponds to the $y$ intercept, or the model's prediction when $x = 0$.\n\n    -   $\\beta_1$ corresponds to the slope, or how $y$ is predicted to change as $x$ changes.\n\n## Linear Regression\n\n-   If you find this notation confusing, try plugging in substantive concepts for what $y$ and $x$ represent\n-   Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.\n\nThe generic linear model\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon$$\n\nReflects:\n\n$$\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i$$\n\n## Practical: Estimating a Linear Regression\n\n-   We estimate linear regressions in `R` using the `lm()` function.\n-   `lm()` requires two arguments:\n    -   a `formula` argument of the general form `y ~ x` read as \"Y modeled by X\" or below \"Transgender Feeling Thermometer (`y`) modeled by (`~`) Age (`x`)\n    -   a `data` argument telling R where to find the variables in the formula\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))\nm1 <- lm(therm_trans_t0 ~ vf_age, data = df)\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n```\n\n\n:::\n:::\n\n\nThe coefficients from `lm()` are saved in object called `m1`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n```\n\n\n:::\n:::\n\n\n`m1` actually contains a lot of information\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnames(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n```\n\n\n:::\n\n```{.r .cell-code}\nm1$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n```\n\n\n:::\n:::\n\n\n## Practical: Interpreting a Linear Regression\n\nWe can extract the intercept and slope from this simple bivariate model, using the `coef()` function\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# All the coefficients\ncoef(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Just the intercept\ncoef(m1)[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n    62.8196 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Just the slope\ncoef(m1)[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    vf_age \n-0.2030711 \n```\n\n\n:::\n:::\n\n\n## Practical: Interpreting a Linear Regression\n\nThe two coefficients from `m1` define a line of best fit, summarizing how feelings toward transgender individuals change with age\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon$$\n\n$$\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i$$\n\n$$\\text{Transgender Feeling Thermometer}_i = 62.82 + -0.2 \\text{Age}_i + \\epsilon_i$$\n\n## Practical: Predicted values from a Linear Regression\n\n-   Often it's useful for interpretation to obtain predicted values from a regression.\n\n-   To obtain predicted vales $(\\hat{y})$, we simply plug in a value for $x$ (In this case, $Age$) and evaluate our equation.\n\n-   For example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?\n\n$$\\hat{FT}_{x=18} = 62.82 + -0.2 \\times 18  = 59.16$$ $$\\hat{FT}_{x=65} = 62.82 + -0.2 \\times 68  = 49.01$$\n\n## Practical: Predicted values from a Linear Regression\n\nWe could do this by hand\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(m1)[1] + coef(m1)[2] * 18\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   59.16432 \n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(m1)[1] + coef(m1)[2] * 68\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   49.01076 \n```\n\n\n:::\n:::\n\n\n## Practical: Predicted values from a Linear Regression\n\nMore often we will:\n\n-   Make a prediction data frame (called `pred_df` below) with the values of interests\n-   Use the `predict()` function with our linear model (`m1`) and `pred_df`\n-   Save the predicted values to our new column in our prediction data frame\n\n## Practical: Predicted values from a Linear Regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make prediction data frame\npred_df <- data.frame(\n  vf_age = c(18, 68)\n)\n# Predict FT for 18 and 68 year-olds\npredict(m1, newdata = pred_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2 \n59.16432 49.01076 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Save predictions to data frame\npred_df$ft_trans_hat <- predict(m1, newdata = pred_df)\npred_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  vf_age ft_trans_hat\n1     18     59.16432\n2     68     49.01076\n```\n\n\n:::\n:::\n\n\n## Practical: Visualizing Linear Regression\n\nWe can visualize simple regression by:\n\n-   plotting a scatter plot of the outcome (y-axis) and predictors (x-axis)\n\n-   overlaying the line defined by `lm()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfig_lm <- df %>%\n  ggplot(aes(vf_age,therm_trans_t0))+\n  geom_point(size=.5, alpha=.5)+\n  geom_abline(intercept = coef(m1)[1],\n              slope = coef(m1)[2],\n              col = \"blue\"\n              )+\n  geom_vline(xintercept = 0,linetype = 2)+\n  xlim(0,100)+\n  annotate(\"point\",\n           x = 0, y = coef(m1)[1],\n           col= \"red\",\n           )+\n  annotate(\"text\",\n           label = expression(paste(beta[0],\"= 62.81\" )),\n           x = 1, y = coef(m1)[1]+5,\n           hjust = \"left\",\n           )+\n  labs(\n    x = \"Age\",\n    y = \"Feeling Thermometer toward\\nTransgender People\"\n  )+\n  theme_classic()\nfig_lm\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/fig_lm_plot-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/figlm1code-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/figlm1-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nbackground-image:url(\"https://i.imgflip.com/66mx9d.jpg\") background-size:contain\n\nclass: inverse, center, middle \\# üí°\\\n\\# Technical: Linear Regression \\## The Mechanics of Minimizing the Sum of Squared Errors\n\n## How did `lm()` choose $\\beta_0$ and $\\beta_1$\n\n--\n\n-   P: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression\n\n--\n\n-   Q: Ok, that's not really that helpful...\n\n    -   What's an error?\n    -   Why would we square and sum them\n    -   How do we minimize them.\n\nP: Good questions!\n\n## What's an error?\n\nAn error, $\\epsilon_i$ is simply the difference between the observed value of $y_i$ and what our model would predict, $\\hat{y_i}$ given some value of $x_i$. So for a model:\n\n$$y_i=\\beta_0+\\beta_1 x_{i} + \\epsilon_i$$\n\nWe simply subtract our model's prediction $\\beta_0+\\beta_1 x_{i}$ from the the observed value, $y_i$\n\n$$\\hat{\\epsilon_i}=y_i-\\hat{y_i}=(Y_i-(\\beta_0+\\beta_1 x_{i}))$$\n\nTo get $\\epsilon_i$\n\n## Why are we squaring and summing $\\epsilon$\n\nThere are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)\n\n-   Squaring $\\epsilon$ treats positive and negative residuals equally.\n\n-   Summing produces single value summarizing our models overall performance.\n\nThere are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties\n\n## How do we minimize $\\sum \\epsilon^2$\n\nOLS chooses $\\beta_0$ and $\\beta_1$ to minimize $\\sum \\epsilon^2$, the Sum of Squared Residuals (SSR)\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n## How did `lm()` choose $\\beta_0$ and $\\beta_1$\n\nIn an intro stats course, we would walk through the process of finding\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$ Which involves a little bit of calculus. The big payoff is that\n\n$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$ And\n\n$$ \\beta_1 = \\frac{Cov(x,y)}{Var(x)}$$ Which is never quite the epiphany, I think we think it is...\n\nThe following slides walk you through the mechanics of this exercise. We're gonna skip through them in class, but they're there for your reference\n\n## How do we minimize $\\sum \\epsilon^2$\n\nTo understand what's going on under the hood, you need a broad understanding of some basic calculus.\n\nThe next few slides provide a brief review of derivatives and differential calculus.\n\n## Derivatives\n\nThe derivative of $f$ at $x$ is its rate of change at $x$\n\n-   For a line: the slope\n-   For a curve: the slope of a line tangent to the curve\n\nYou'll see two notations for derivatives:\n\n1.  Leibniz notation:\n\n$$\n\\frac{df}{dx}(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{(x+h)-x}\n$$\n\n2.  Lagrange: $f^{\\prime}(x)$\n\n## Some useful Facts about Derivatives\n\nDerivative of a constant\n\n$$\nf^{\\prime}(c)=0\n$$\n\nDerivative of a line f(x)=2x\n\n$$\nf^{\\prime}(2x)=2\n$$\n\nDerivative of $f(x)=x^2$\n\n$$\nf^{\\prime}(x^2)=2x\n$$\n\nChain rule: y= f(g(x)). The derivative of y with respect to x is\n\n$$\n\\frac{d}{dx}(f(g(x)))=f^{\\prime}(g(x))g^{\\prime}(x)\n$$\n\nThe derivative of the \"outside\" times the derivative of the \"inside,\" remembering that the derivative of the outside function is evaluated at the value of the inside function.\n\n## Finding a Local Minimums\n\n.pull-left\\[ Local minimum:\n\n$$\nf^{\\prime}(x)=0 \\text{ and } f^{\\prime\\prime}(x)>0 \n$$ \\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://copingwithcalculus.com/SecondDeriv1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://copingwithcalculus.com/SecondDerivativeTest.html) \\]\n\n## Partial Derivatives\n\nLet $f$ be a function of the variables $(x, \\dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is\n\n$$\\begin{align*}\n\\frac{\\partial f(x, \\dots, X_n)}{\\partial X_i}=\\lim_{h\\to0}\\frac{f(x, \\dots X_i+h \\dots, X_n)-f(x, \\dots X_i \\dots, X_n)}{h}\n\\end{align*}$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)\n\n## Minimizing the sum of squared errors\n\nOur model\n\n$$y_i =\\beta_0+\\beta_1x_{i}+\\epsilon_i$$\n\nFinds coefficients $\\beta_0$ and $\\beta_1$ to to minimize the sum of squared residuals, $\\hat{\\epsilon}_i$:\n\n$$\\begin{aligned}\n\\sum \\hat{\\epsilon_i}^2 &= \\sum (y_i-\\beta_0-\\beta_1 x_{i})^2\n\\end{aligned}$$\n\n## Minimizing the sum of squared errors\n\nWe solve for $\\beta_0$ and $\\beta_1$, by taking the partial derivatives with respect to $\\beta_0$ and $\\beta_1$, and setting them equal to zero\n\n$$\\begin{aligned}\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial \\beta_0} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})=0 & f'(-x^2) = -2x\\\\\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial\\beta_1} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})x_{i}=0 & \\text{chain rule}\n\\end{aligned}$$\n\n## Solving for $\\beta_0$\n\nFirst, we'll solve for $\\beta_0$, by multiplying both sides by -1/2 and distributing the $\\sum$:\n\n$$\\begin{aligned}\n0 &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})\\\\\n\\sum \\beta_0 &= \\sum y_i - \\sum \\beta_1 x_{i}\\\\\nN \\beta_0 &= \\sum y_i -\\sum \\beta_1 x_{i}\\\\\n\\beta_0 &= \\frac{\\sum y_i}{N} - \\frac{\\beta_1 \\sum x_{i}}{N}\\\\\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}\n\\end{aligned}$$\n\n## Solving for $\\beta_1$\n\nNow, we can solve for $\\beta_1$ plugging in $\\beta_0$.\n\n$$\\begin{aligned}\n0 &= -2\\sum [(y_i-\\beta_0-\\beta_1 x_{i})x_{i}]\\\\\n0 &= \\sum [y_ix_i-(\\bar{y} - \\beta_1 \\bar{x})x_{i}-\\beta_1 x_{i}^2]\\\\\n0 &= \\sum [y_ix_i-\\bar{y}x_{i} + \\beta_1 \\bar{x}x_{i}-\\beta_1 x_{i}^2]\n\\end{aligned}$$\n\n## Solving for $\\beta_1$\n\nNow we'll rearrange some terms and pull out an $x_{i}$ to get\n\n$$\\begin{aligned}\n0 &= \\sum [(y_i -\\bar{y} + \\beta_1 \\bar{x}-\\beta_1 x_{i})x_{i}]\n\\end{aligned}$$\n\nDividing both sides by $x_{i}$ and distributing the summation, we can isolate $\\beta_1$\n\n$$\\begin{aligned}\n\\beta_1 \\sum (x_{i}-\\bar{x}) &= \\sum (y_i -\\bar{y})\n\\end{aligned}$$\n\nDividing by $\\sum (x_{i}-\\bar{x})$ to get\n\n$$\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})}{\\sum (x_{i}-\\bar{x})}\n\\end{aligned}$$\n\n## Solving for $\\beta_1$\n\nFinally, by multiplying by $\\frac{(x_{i}-\\bar{x})}{(x_{i}-\\bar{x})}$ we get\n\n$$\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})(x_{i}-\\bar{x})}{\\sum (\\bar{x}-x_{i})^2}\n\\end{aligned}$$\n\nWhich has a nice interpretation:\n\n$$\\begin{aligned}\n\\beta_1 &= \\frac{Cov(x,y)}{Var(x)}\n\\end{aligned}$$\n\nSo the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat!\n\nclass: inverse, center, middle \\# üí°\\\n\\# Theoretical: Linear Regression \\## OLS provides a linear estimate of CEF: E\\[Y\\|X\\]\n\n## Linear Regression is a many splendored thing\n\n[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of the various interpretations/motivations for linear regression.\n\n-   A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)\n\n-   A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\\beta$\n\n-   A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)\n\n-   A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)\n\n-   A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)\n\n-   A [linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)\n\n## Linear Regression is a many splendored thing\n\n[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of various interpretations/motivations for linear regression.\n\n-   A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)\n\n-   A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\\beta$\n\n-   A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)\n\n-   A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)\n\n-   A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)\n\n-   A [**linear approximation of the conditional expectation function**](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)\n\n## The Conditional Expectation Function\n\nOf all the functions we could choose to describe the relationship between $Y$ and $X$,\n\n$$\nY_i = f(X_i) + \\epsilon_i\n$$\n\nthe conditional expectation of $Y$ given $X$ $(E[Y|X])$, has some appealing properties\n\n$$\nY_i = E[Y_i|X_i] + \\epsilon\n$$\n\nThe error, by definition, is uncorrelated with X and $E[\\epsilon|X]=0$\n\n$$\nE[\\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0\n$$\n\nOf all the possible functions $g(X)$, we can show that \\$E\\[Y_i\\|X_i\\] \\$ is the best predictor in terms of minimizing **mean squared error**\n\n$$\nE[ (Y - g(Y))^2] \\geq E[(Y - E[Y|X])^2] \n$$\n\n## Linear Approximations to the Conditional Expectation Function\n\n-   We can then show (in a different class) that linear regression provides the best linear predictor of the CEF\n    -   Chapter 3, of [Mostly Harmless Econometrics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991028523169706966)\n    -   Chapter 4 of [Foundations of Agnostic Statistics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991000736119706966)\n-   Furthermore, when the CEF is linear, it's equal exactly to OLS regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf %>%\n  ggplot(aes(vf_age,therm_trans_t0))+\n  geom_point(size=.5, alpha=.5)+\n  stat_summary(geom=\"point\", aes(col=\"CEF\"))+\n  stat_summary(geom=\"line\", aes(col=\"CEF\"))+\n  labs(\n    x = \"Age\",\n    y = \"Feeling Thermometer toward\\nTransgender People\",\n    col = \"\"\n  )+\n  theme_classic() -> plot_cef\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/cef1-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_cef+\n  geom_smooth(method = \"lm\", se=F, aes(col = \"OLS\")) -> plot_cef\nplot_cef\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/cef2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_cef+\n  geom_smooth(method = \"lm\", se=F, aes(col = \"OLS\")) -> plot_cef\nplot_cef\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/cef3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## What you need to know about Regression\n\n-   **Conceptual**\n    -   Simple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\n-   **Practical**\n    -   We estimate linear models in R using the `lm()` function\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(y ~ x, data = df)\n```\n:::\n\n\n-   *Technical/Definitional*\n    -   Linear regression chooses $\\beta_0$ and $\\beta_1$ to minimize the Sum of Squared Residuals (SSR):\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n-   *Theoretical*\n    -   Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$\n\nclass: center, middle background-image:url(https://www.memecreator.org/static/images/memes/5312518.jpg) background-size:contain\n\nclass: inverse, center, middle \\# üí°\\\n\\# Three Designs for Causal Inference in Observational Studies\n\n## Credible Cauasal Inference in Observational Studies\n\nSubclassification, matching, and regression all require an assumption of selection on observables:\n\n$$\nY_i(1),Y_i(0) \\perp D_i |x_i\n$$\n\nBut how do we know if we've got the right model or we've controlled for the right variables?\n\n--\n\nTypically, we don't\n\n## Credible Cauasal Inference in Observational Studies\n\nInstead, social scientists look for situations where the credibility of\n\n$$\nY_i(1),Y_i(0) \\perp D_i |X_i\n$$\n\ndepends less on how much data you have and much more on how your data were generated.\n\nclass: middle\n\n.pull-left\\[ \\> Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact. ... \\[T\\]he primary engine driving improvement has been a focus on the **quality of empirical research designs.** (p. 4)\n\n\\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/04_cred.png){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3)\n\n\\]\n\nclass: middle\n\n.pull-left\\[ \\> Design-based studies are distinguished by their prima facie credibility and by the attention investigators devote to making both an **institutional** and a **data-driven** case for causality (p. 5)\n\n\\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/04_cred.png){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3) \\]\n\nclass: middle\n\n.pull-left\\[ \\> The econometric methods that feature most prominently in quasi-experimental studies are **instrumental variables**, **regression discontinuity** methods,and **differences-in-differences**-style policy analysis. ... The best of today‚Äôs design-based studies make a strong institutional case, backed up with empirical evidence, for the variation thought to generate a useful **natural experiment**.(p. 12)\n\n\\]\n\n.pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/04_cred.png){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.24.2.3) \\]\n\n## Three Designs for Causal Inference in Observational Studies\n\n-   Difference in Differences\n\n-   Regression Discontinuity\n\n-   Instrumental Variables\n\nclass: inverse, center, middle \\# üí° Difference in Differences\n\nclass: inverse, center, middle background-image:url(https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc) background-size: cover \\## London in the Time of Cholera\n\n## Motivating Example: What causes Cholera?\n\n-   In the 1800s, cholera was thought to be transmitted through the air.\n\n-   John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\n\n-   Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location.\n\n## Notation\n\nLet's adopt a little notation to help us think about the logic of Snow's design:\n\n-   $D$: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\n-   $T$: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\n-   $Y_{di}(t)$ the potential outcome of unit $i$\n\n    -   $Y_{1i}(t)$ the potential outcome of unit $i$ when treated between the two periods\n\n    -   $Y_{0i}(t)$ the potential outcome of unit $i$ when control between the two periods\n\n## Causal Effects\n\nThe individual causal effect for unit i at time t is:\n\n$$\\tau_{it} = Y_{1i}(t) ‚àí Y_{0i}(t)$$\n\nWhat we observe is\n\n$$Y_i(t) = Y_{0i}(t)\\cdot(1 ‚àí D_i(t)) + Y_{1i}(t)\\cdot D_i(t)$$\n\n$D$ only equals 1, when $T$ equals 1, so we never observe $Y_0i(1)$ for the treated units.\n\nIn words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.\n\n## Average Treatment on Treated\n\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\n$$\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$\n\nThat is, what would have happened in Lambeth, had their water company not moved their pipes\n\n## Average Treatment on Treated\n\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\nWe we can observe is:\n\n|               \\| Post-Period (T=1) \\| Pre-Period (T=0) \\|\n\n\\|\\|--\\|-\\| \\| Treated $D_{i}=1$ \\| $E[Y_{1i}(1)\\vert D_i = 1]$ \\| $E[Y_{0i}(0)\\vert D_i = 1]$ \\| \\| Control $D_i=0$ \\| $E[Y_{0i}(1)\\vert D_i = 0]$ \\| $E[Y_{0i}(0)\\vert D_i = 0]$ \\|\n\n## Data\n\nBecause potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Company </th>\n   <th style=\"text-align:right;\"> 1854 (T=1) </th>\n   <th style=\"text-align:right;\"> 1849 (T=0) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Lambeth (D=1) </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 85 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Southwark and Vauxhall (D=0 </td>\n   <td style=\"text-align:right;\"> 147 </td>\n   <td style=\"text-align:right;\"> 135 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## How can we estimate the effect of moving pumps upstream?\n\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\n$$\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$\n\nLet's conisder some strategies Snow could take to estimate this quantity:\n\n## Before vs after comparisons:\n\n-   Snow could have compared Labmeth in 1854 $(E[Y_i(1)|D_i = 1] = 19)$ to Lambeth in 1849 $(E[Y_i(0)|D_i = 1]=85)$, and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\n\n-   This comparison assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved ($E[Y_{0i}(1)|D_i = 1]$).\n\n-   A skeptic might argue that Lambeth in 1849 $\\neq$ Lambeth in 1854\n\n## Treatment-Control comparisons in the post period\n\n-   Snow could have compared outcomes between Lambeth and S&V in 1954 ($E[Yi(1)|Di = 1] ‚àí E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to 128 fewer deaths.\n\n-   Here the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved ($E[Y_{0i}(1)|D_i = 1]$)\n\n-   Again, our skeptic could argue Lambeth $\\neq$ S&V\n\n## Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\n\nThere are two, equivalent ways to view this design.\n\n$$\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}‚àí \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}$$\n\n-   Difference 1: Average change between Treated and Control in Post Period\n-   Difference 2: Average change between Treated and Control in Pre Period\n\nWhich is equivalent to:\n\n$$\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}‚àí \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] ‚àí E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}$$\n\n-   Difference 1: Average change between Treated over time\n-   Difference 2: Average change between Control over time\n\n## Difference in Differences\n\nYou'll see the DiD design represented both ways, but they produce the same result:\n\n$$\n\\tau_{ATT} = (19-147) - (85-135) = -78\n$$\n\n$$\n\\tau_{ATT} = (19-85) - (147-135) = -78\n$$\n\n## Identifying Assumption of a Difference in Differences Design\n\nThe key assumption in this design is what's known as the parallel trends assumption: $E[Y_{0i}(1) ‚àí Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) ‚àí Y_{0i}(0)|D_i = 0]$\n\n-   In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&V\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/paralleltrends-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nWhere:\n\n1.  $E[Y_{i}(1)|D_{i} = 1] ‚àí E[Y_{i}(1)|D_{i} = 0]$\n2.  $E[Y_{i}(0)|D_{i} = 1] ‚àí E[Y_{i}(0)|D_{i}\\} = 0]$\n3.  $E[Y_{1i}(1) ‚àí Y_{0i}(1)|D_{i} = 1]$\n\n## Summary\n\n-   A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\n\n    -   Taking the pre-post difference removes any fixed differences between the units\n\n    -   Then taking the difference between treated and control differences removes any common differences over time\n\n-   The key identifying assumption of a DiD design is the \"assumption of parallel trends\"\n\n    -   Absent treatment, treated and control groups would see the same changes over time.\n    -   Hard to prove, possible to test\n\n## Extensions and limitations\n\n-   DiD easy to estimate with linear regression\n-   Generalizes to multiple periods and treatment interventions\n    -   More pre-treatment periods allow you assess \"parallel trends\" assumption\n-   Alternative methods\n    -   Synthetic control\n    -   Event Study Designs\n-   What if you have multiple treatments or treatments that come and go?\n    -   Panel Matching\n    -   Generalized Synthetic control\n\n## Applications\n\n-   [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment\n\n-   [Abadie, Diamond, & Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany\n\n-   [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?\n\nclass: inverse, center, middle \\# üí° Regression Discontinuity Design\n\n## Motivating Example\n\n.pull-left\\[ - Do Members of Parliament in the UK get richer from holding office (QSS Chapter 4.3.4)\\] .pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/04_eggers.png){fig-align='center' width=80%}\n:::\n:::\n\n\n[Eggers and Hainmueller (2009)](https://www.cambridge.org/core/journals/american-political-science-review/article/abs/mps-for-sale-returns-to-office-in-postwar-british-politics/E4C2B102194AA1EA0D2F1F777EAE3C08) \\]\n\n## Logic of the Regression Discontinuity Design (RDD)\n\n-   What's the effect of holding elected office in the UK on personal wealth?\n\n-   People who win elections differ in many ways from people who lose elections.\n\n-   Logic of an RDD:\n\n    -   Just look at the wealth of individuals who either narrowly won or lost elections.\n\n    -   Candidates close to 50 percent cutoff (discontinuity) should be more comparable (better counterfactuals)\n\n## Data from Eggers and Hainmueller (2009)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(qss)\ndata(MPs)\nglimpse(MPs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 427\nColumns: 10\n$ surname    <chr> \"Llewellyn\", \"Morris\", \"Walker\", \"Walker\", \"Waring\", \"Brown‚Ä¶\n$ firstname  <chr> \"David\", \"Claud\", \"George\", \"Harold\", \"John\", \"Ronald\", \"Le‚Ä¶\n$ party      <chr> \"tory\", \"labour\", \"tory\", \"labour\", \"tory\", \"labour\", \"tory‚Ä¶\n$ ln.gross   <dbl> 12.13591, 12.44809, 12.42845, 11.91845, 13.52022, 12.46052,‚Ä¶\n$ ln.net     <dbl> 12.135906, 12.448091, 10.349009, 12.395034, 13.520219, 9.63‚Ä¶\n$ yob        <int> 1916, 1920, 1914, 1927, 1923, 1921, 1907, 1912, 1905, 1920,‚Ä¶\n$ yod        <int> 1992, 2000, 1999, 2003, 1989, 2002, 1987, 1984, 1998, 2004,‚Ä¶\n$ margin.pre <dbl> NA, NA, -0.057168204, -0.072508894, -0.269689620, 0.3409586‚Ä¶\n$ region     <chr> \"Wales\", \"South West England\", \"North East England\", \"Yorks‚Ä¶\n$ margin     <dbl> 0.05690404, -0.04973833, -0.04158868, 0.02329524, -0.230005‚Ä¶\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-0bd698fa55adaa1e52a6\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-0bd698fa55adaa1e52a6\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],[\"surname\",\"firstname\",\"party\",\"ln.gross\",\"ln.net\",\"yob\",\"yod\",\"margin.pre\",\"region\",\"margin\"],[\"surname of the candidate\",\"first name of the candidate\",\"party of the candidate (labour or tory)\",\"log gross wealth at the time of death\",\"log net wealth at the time of death\",\"year of birth of the candidate\",\"year of death of the candidate\",\"margin of the candidate‚Äôs party in the previous election electoral\",\"region\",\"margin of victory (vote share)\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Variable<\\/th>\\n      <th>Description<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Variable\",\"targets\":1},{\"name\":\"Description\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMPs %>%\n  ggplot(aes(margin, ln.net))+\n  geom_point(shape=1)+\n  facet_grid(~party)+\n  geom_smooth(data =MPs %>%\n                filter(margin <0),\n              method = \"lm\")+\n  geom_smooth(data =MPs %>%\n                filter(margin >0),\n              method = \"lm\")+\n  theme_bw() -> fig_rdd\nfig_rdd\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/rddfig-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## RDD Notation\n\n-   $X$ is a **forcing** variable\n-   Treatment $D$ is a determined by $X$\n\n$$\nD_i = 1\\{X_i > c\\}\n$$\n\n-   $X$ is the `margin` variable in the example data, and $D=1$ if `margin` is greater than 0 (i.e. the candidate won the election)\n\n-   Interested in the differences in the outcome at the threshold\n\n$$\\lim_{x \\downarrow  c} E[Y_i|X=x] - \\lim_{x \\uparrow  c} E[Y_i|X=x]$$\n\n## Causal Identification with an RDD\n\nIf we assume $E[Y_i(0)|X=x]$ and $E[Y_i(1)|X=x]$ are continuous in x, then we can estimate a (local) ATE at the threshold:\n\n$$\\begin{align}\nATE_{RDD} &= E[Y(1)-Y(0)|X_i=c] \\\\\n&=  E[Y(1)|X_i=c] -  E[Y(0)|X_i=c]\\\\\n&= \\lim_{x \\downarrow  c} E[Y_i|X=x] - \\lim_{x \\uparrow  c} E[Y_i|X=x] \\\\\n\\end{align}$$\n\n## Continuity Assumption\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://mixtape.scunning.com/graphics/rdd_simul_ex.jpg){fig-align='center' width=80%}\n:::\n:::\n\n\n[Cunningham (2022)](https://mixtape.scunning.com/regression-discontinuity.html#continuity-assumption)\n\n## Causal Identification with an RDD\n\n-   The continuity assumption is a formal way of saying that observations close to the threshold are good counterfactuals for each other\n\n-   We can't prove this assumption\n\n-   But if it holds, we should observe\n\n    -   no sorting around the cutoff (no self selection)\n\n    -   similar distributions of covariates around the cutoff (balance tests)\n\n    -   no effect of treatment on things measured pre-treatment (placebo tests)\n\n## Applications of RDD\n\n-   [Lee (2001)](https://www.nber.org/papers/w8441) Does incumbency create electoral advantage\n\n-   [Eggers (2014)](https://journals.sagepub.com/doi/full/10.1177/0010414014534199) Does proportional representatoin increase turnout?\n\n-   [Velez and Newman (2019)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12427) What's the effect of ethnic television on political participation\n\nclass: inverse, center, middle \\# üí° \\## Instrumental Variables\n\n## Instrumental Variables\n\nInstrumental variables are an economists favorite tool for dealing with **omitted variable bias**\n\n-   We have some non random treatment whose effects we'd like to assess\n-   We're worried that these effects are **confounded** by some unobserved, omitted variable, that influences both the treatment and the outcome\n-   We find an **instrumental variable** that satisfies the following:\n    -   Randomization\n    -   Excludability\n    -   First-stage relationship\n    -   Monotonicity\n-   Allowing us estimate a Local Average Treatment Effect (LATE) using the only the variation in our treatment is **exogenous** (uncorrelated with ommitted variables)\n\n## IV Assumption: Randomization\n\n.left-column\\[ - No path from $U$ to $Z$\\] .right-column\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://book.declaredesign.org/figures/figure_16.7.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)\n\n\\]\n\nclass: center \\## IV Assumption: Excludability\n\n.left-column\\[ - No path from $Z$ to $Y$\\]\n\n.right-column\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://book.declaredesign.org/figures/figure_16.7.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)\n\n\\]\n\nclass: center \\## IV Assumption: First Stage\n\n.left-column\\[ - Path from $Z$ to $D$\\] .right-column\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://book.declaredesign.org/figures/figure_16.7.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables)\n\n\\]\n\nclass: center \\## IV Assumption: Monotonicity\n\n.left-column\\[\n\n-   $D_i(Z=1)\\geq D_i(Z=0)$\n-   \"No Defiers\"\n\n\\]\n\n.right-column\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://book.declaredesign.org/figures/figure_16.7.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n[Source](https://book.declaredesign.org/observational-causal.html?q=instrumental#instrumental-variables) \\]\n\nclass: center \\## Compliance\n\nWith a binary treatment, $D$ and binary instrument $Z$ there are four types of compliance\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Type </th>\n   <th style=\"text-align:right;\"> $D_i(Z=1)$ </th>\n   <th style=\"text-align:right;\"> $D_i(Z=0)$ </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Always Takers </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Never Takers </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Compliers </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Defiers </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n-   Assuming Monotonicity means there are \"No Defiers\"\n\n## Estimating the Local Average Treatment Effect\n\nIf we believe our assumptions of:\n\n-   Randomization\n-   Excludability\n-   First-stage relationship\n-   Monotonicity\n\nThen we can estimate Local Average Treatment Effect (LATE) sometimes called the Complier Average Treatment Effect CATE)\n\n## Estimating the Local Average Treatment Effect\n\nIt can be [shown](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf) that the LATE:\n\n$$LATE = \\frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1]-E[D|Z=0]}= \\frac{ATE_{Z\\to Y}}{ATE_{Z\\to D}}$$\n\n## Example: Earnings and Military Service\n\nAdapted from [Edward Rubin](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#42)\n\n*Example:* If we want to estimate the effect of veteran status on earnings, $$\\begin{align}\n  \\text{Earnings}_i = \\beta_0 + \\beta_1 \\text{Veteran}_i + u_i \\tag{1}\n\\end{align}$$\n\n--\n\nWe would love to calculate $\\color{#e64173}{\\text{Earnings}_{1i}} - \\color{#6A5ACD}{\\text{Earnings}_{0i}}$, but we can't.\n\n--\n\nAnd OLS will likely be biased for $(1)$ due to selection/omitted-variable bias.\n\n## Introductory example\n\nImagine that we can split veteran status into an exogenous (as-if random, unbiased) part and an endogenous (non-random, biased) part...\n\n--\n\n$$\\begin{align}\n  \\text{Earnings}_i\n  &= \\beta_0 + \\beta_1 \\text{Veteran}_i + u_i \\tag{1} \\\\\n  &= \\beta_0 + \\beta_1 \\left(\\text{Veteran}_i^{\\text{Exog.}} + \\text{Veteran}_i^{\\text{Endog.}}\\right) + u_i \\\\\n  &= \\beta_0 + \\beta_1 \\text{Veteran}_i^{\\text{Exog.}} + \\underbrace{\\beta_1 \\text{Veteran}_i^{\\text{Endog.}} + u_i}_{w_i} \\\\\n  &= \\beta_0 + \\beta_1 \\text{Veteran}_i^{\\text{Exog.}} + w_i\n\\end{align}$$\n\n--\n\nWe could use this exogenous variation in veteran status to consistently estimate $\\beta_1$.\n\n--\n\n**Q:** What would exogenous variation in veteran status mean?\n\n## Introductory example\n\n**Q:** What would exogenous variation in veteran status mean?\n\n--\n\n**A.sub\\[1\\]:** Choices to enlist in the military that are essentially random‚Äîor at least uncorrelated with omitted variables and the disturbance.\n\n--\n\n**A.sub\\[2\\]:** .No selection bias: $$\\begin{align}\n  \\color{#e64173}{\\mathop{E}\\left(\\text{Earnings}_{0i}\\mid\\text{Veteran}_i = 1\\right)} - \\color{#6A5ACD}{\\mathop{E}\\left( \\text{Earnings}_{0i} \\mid \\text{Veteran}_i = 0 \\right)} = 0\n\\end{align}$$\n\n## Instruments\n\n## **Q:** How do we isolate this *exogenous variation* in our explanatory variable?\n\n<br>**A:** Find an instrument (an instrumental variable).\n\n--\n\n## **Q:** What's an instrument?\n\n<br>**A:** An **instrument** is a variable that is\n\n1.  **correlated** with the **explanatory variable** of interest (*relevant*),\n2.  **uncorrelated** with the **error** term (*exogenous*).\n\n## Instruments\n\nSo if we want an instrument $z_i$ for endogenous veteran status in\n\n$$\\begin{align}\n  \\text{Earnings}_i = \\beta_0 + \\beta_1 \\text{Veteran}_i + u_i\n\\end{align}$$\n\n1.  **Relevant:** $\\mathop{\\text{Cov}} \\left( \\text{Veteran}_i,\\, z_i \\right) \\neq 0$\n2.  **Exogenous:** $\\mathop{\\text{Cov}} \\left( z_i,\\, u_i \\right) = 0$\n\n## Instruments: Relevance\n\n**Relevance:** We need the instrument to cause a change in (correlate with) our endogenous explanatory variable.\n\nWe can actually test this requirement using regression and a *t* test.\n\n--\n\n***Example:*** For the veteran status, consider three potential instruments:\n\n.pull-left\\[ 1. Social security number\n\n2.  Physical fitness\n\n3.  Vietnam War draft \\]\n\n--\n\n.pull-right\\[ - **Probably not relevant** uncorrelated with military service\n\n-   *Potentially relevant* service may correlate with fitness\n\n-   **Relevant** being drafted led to service \\]\n\n## Instruments: Exogeneity\n\n.hi\\[Exogeneity:\\] The instrument to be independent of omitted factors that affect our outcome variable‚Äîas good as randomly assigned.\n\n$z_i$ must be uncorrelated with our disturbance $u_i$. .hi\\[Not testable.\\]\n\n--\n\n***Example:*** For the .pink\\[veteran status\\], consider three potential instruments:\n\n.pull-left\\[ 1. Social security number\n\n2.  Physical fitness\n\n3.  Vietnam War draft \\]\n\n--\n\n.pull-right\\[ - **Exogenous** SSN essentially random\n\n-   *Not Exogenous* fitness correlated with many things\n\n-   **Exogenous** draft via lottery \\]\n\n## Relevant and Exogenous\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/venn_iv-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Relevant, Not Exogenous\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/venn_iv_endog-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Not Relevant and Not Exogenous\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/venn_iv_irrelevant-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Relevant, Not Exogenous\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/venn_iv_endog2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n# Venn diagram explanation\n\nIn these figures (Venn diagrams)\n\n-   Each circle illustrates a variable.\n-   Overlap gives the share of correlatation between two variables.\n-   Dotted borders denote *omitted* variables.\n\nTake-aways\n\n-   Figure 1: .hi-pink\\[Valid instrument\\] (relevant; exogenous)\n-   Figure 2: .hi-slate\\[Invalid instrument\\] (relevant; not exogenous)\n-   Figure 3: .hi-slate\\[Invalid instrument\\] (not relevant; not exogenous)\n-   Figure 4: .hi-slate\\[Invalid instrument\\] (relevant; not exogenous)\n\n## IV Applications\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://pbs.twimg.com/media/EJGyHnyUYAA-yhM?format=jpg&name=large){fig-align='center' width=80%}\n:::\n:::\n\n\n[\\@AndrewHeiss](https://twitter.com/andrewheiss/status/1193931226865901569?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1193931226865901569%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2Fandrewheiss2Fstatus2F1193931226865901569widget%3DTweet)\n\n## IV Summary\n\nInstrumental variables require a number of assumptions to yield credible causal claims:\n\n-   Randomization\n-   Excludability\n-   First-stage relationship\n-   Monotonicity\n\nEstimation and inference of IVs is beyond the scope of this course.\n\n-   See Edward Rubin's excellent [slides](https://raw.githack.com/edrubin/EC421W19/master/LectureNotes/11InstrumentalVariables/11_instrumental_variables.html#85)\n\n-   And Matt Blackwells [notes](https://www.mattblackwell.org/files/teaching/s10-iv-handout.pdf)\n\n-   Understanding the identifying assumptions of IV can help you critique a study (even if the you don't fully understand the math)\n\nclass: inverse, middle, center \\# üí° \\## Summary\n\n## What you need to know\n\n-   Causal inference in observational and experimental studies is about counterfactual comparisons\n-   In observational studies, to make causal claims we generally make some assumption of conditional independence:\n\n$$\nY_i(1),Y_i(0), \\perp D_i |X_i\n$$\n\n-   The credibility of this assumption depends less on the data, and more on how the data were generated.\n-   **Selection on Observables** is rarely a credible assumption\n-   Observational designs that produce credible causal inference, leverage aspects of the world that create *natural experiments*\n-   You should be able to describe the logic and assumptions of common designs in social science\n    -   **Difference-in-Differences:** *Parallel Trends*\n    -   **Regression Discontinuity:** *Continuity at the cutoff*\n    -   **Instrumental Variables:** Instruments need to be *Relevant and Exogenous*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding/datatables.js\"></script>\n<script src=\"../site_libs/jquery/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk/js/crosstalk.min.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}