{
  "hash": "63247d8419b41ab073875ec8840df17f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"POLS 1600\"\nsubtitle: \"Casual Inference in<br>Observational Designs & <br> Simple Linear Regression\"\ndate: last-modified\ndate-format: \"[Updated ]MMM D, YYYY\"\nformat: \n  revealjs:\n    theme: brownslides.scss\n    logo: images/pols1600_hex.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: true\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    # include-after-body: title-slide.html\n    title-slide-attributes:\n      align: left\n      data-background-image: images/pols1600_hex.png\n      data-background-position: 90% 50%\n      data-background-size: 40%\nfilters:\n    - openlinksinnewpage\n\n    # title-slide-attributes:\n    #   data-background-image: ../../assets/stat20-hex-bg.png\n    #   data-background-size = contain\n---\n\n::: {.cell}\n\n:::\n\n\n# {{< fa map-location>}} Overview {.inverse}\n\n## Overview\n\n-   Announcements\n-   Setup\n-   Feedback\n-   Review\n-   Class plan\n\n## Annoucements\n\n-   Sit with your groups (for now)\n\n## Group Assignments {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-b3ace9200692ecc018ab\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b3ace9200692ecc018ab\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"],[\"Group 1\",\"Group 2\",\"Group 3\",\"Group 4\",\"Group 5\",\"Group 6\",\"Group 7\",\"Group 8\"],[\"Maia Eng\",\"Andrew Rovinsky\",\"Serafym Rybachkivskyi\",\"Tiffany Eddy\",\"Christopher Maron\",\"Mia Hamilton\",\"Mariana Melzer\",\"Logan Szittai\"],[\"Guadalupe Herrera\",\"Spencer Lorin\",\"Rachel Kim\",\"Daniel Solomon\",\"Daniel Baker\",\"Emily Colon\",\"Kahrie Langham\",\"Keiley Thompson\"],[\"Stephen Robinson\",\"Lucinda Anderson\",\"Kai Blades\",\"Zoe Smith\",\"Neve Diaz-Carr\",\"Davis Kelly\",\"Shannon Feerick-Hillenbrand\",\"Lydell Dyer\"],[\"Jeremiah Harrington\",\"Serenity Hamilton\",\"Emma Coleman\",\"Lorena Calderon\",\"Olivia Hanley\",\"Talia Levine\",\"Jarret Fernandes\",\"Mahir Arora\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Group<\\/th>\\n      <th>1<\\/th>\\n      <th>2<\\/th>\\n      <th>3<\\/th>\\n      <th>4<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Group\",\"targets\":1},{\"name\":\"1\",\"targets\":2},{\"name\":\"2\",\"targets\":3},{\"name\":\"3\",\"targets\":4},{\"name\":\"4\",\"targets\":5}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## {{< fa bullhorn >}} Feedback {top=\"50%\" background-image=\"https://jplilley.com/images/easyblog_articles/145/holding-ears-300x196.jpg\"}\n\n\n::: {.cell}\n\n:::\n\n\n## What did we like {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-4c89101f8551124b85ac\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-4c89101f8551124b85ac\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[\"Data mapping is very interesting\",\"I like making pretty graphs!\",\"I like that we were able to create a plot and visualize our date, that was pretty cool.\",\"It was fun to get to experiment with code ourselves, and figure out how we can change plots and their visuals\",\"turning data into visual\",\"This week has been the best week so far and learning about graphs has been really fun. The most useful thing so far was outlining and implementing the steps to complete today's lab-- I felt like I was keeping up and able to ask a few questions as well.\",\"I thought that the lab was much easier to follow\",\"I liked cleaning the data and mutating it to make it more understandable\",\"I liked that you went over the process of HLOs because I was not sure how to approach them. Now i feel better on how to start the HLO process.\",\"I enjoyed learning about how to employ certain functions to labels we created. It was much clearer in class than it was in the textbook and made me feel comfortable during the lab.\",\"\",\"I liked working in groups and also having the opportunity to ask lots of questions. I also liked being able to talk through the code as a class since I don't have a lot of (any) coding experience. I feel a little more confident after this class.\",\"\",\"I enjoyed making plots and graphs. It's fun to watch something be created from the messy code.\",\"Being able to visualize the data\",\"I liked creating the plot.\",\"I thought it was interesting to create the data plot.\",\"This week I enjoyed messing around with different characteristics of the graphics.\",\"I liked being able to keep up for the most part. I think it a lot more fun when I am able to keep up!\",\"\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Likes<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Likes\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## What did we dislike {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-c6d729a559c3c8b6aebd\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-c6d729a559c3c8b6aebd\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"],[\"Data mapping also happens to be quite confusing and can become rather difficult\",\"I'm still getting the hang of things, so sometimes it's hard to follow along.\",\"That I'm still confused about each of the functions and I don't know exactly what I'm coding.\",\"I feel like we haven't done a lot of coding ourselves, but have mostly just copied existing code. I am also not certain, but I felt like the tutorials were in base r, and not tidyverse r, and that made completing the tutorial harder. \\n\",\"NOTHING. i thought this week's lab was super duper helpful\\n\",\"The tutorial this week was a little confusing/frustrating.\",\"\",\"I had a hard time reloading the packages before every time we used the code\",\"I wish we could go over a bit more in depth of what each code is doing for each part so that I have a better understanding of whats going on.\",\"I didn't fully understand where some of the complex code chunks were coming from or why we were using them.\",\"\",\"I think you sometimes go through instructions too fast and me and my group would miss sections of code because you would switch tabs away from the code. If possible, it would be great to leave the code up for an extra minute or to repeat the instructions so everyone catches them.\",\"\",\"Labs still move quite quickly sometimes.\",\"Getting error messages and not knowing how to solve them on my own\",\"I felt like I was playing catch-up sometimes on Tuesday.\",\"I kept getting errors on Tuesday when we were walking through steps which made it difficult to follow.\",\"I found it difficult when parts of my code weren't working and I had no idea where to start in finding the problem.\",\"One thing I didn't like is when I get an error code, it is so frustrating and I have no idea what to do.\",\"\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Dislikes<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":4,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Dislikes\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[4,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n# Setup\n\n## Packages for the lab\n\nHopefully, you were all able to install the following packages\n\n\n::: {.cell}\n\n:::\n\n\n## Census API\n\nAdditionally, I hope you have all followed the steps [here](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package):\n\n1.  Install the `tidycensus` package\n2.  Load the installed package\n3.  Request an API key from the Census\n4.  Check your email\n5.  Activate your key\n6.  Install your API key in R\n7.  Check that everything worked\n\nTo install the an API key so we can download data directly from the US Census\n\n## Packages for today\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Pacakges for today\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", #<<\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"#<<\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE \n```\n\n\n:::\n:::\n\n\n## Packages for the Lab\n\n# {{< fa magnifying-glass>}} Review {.inverse}\n\n## Review\n\n-   Data wrangling and visualization\n\n-   Descriptive Statistics\n\n-   Levels of understanding\n\n## Data wrangling and visualization {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: You're learning how to map conceptual tasks to commands in R\n\n|Skill                      |Common Commands                           |\n|:--------------------------|:-----------------------------------------|\n|Setup R                    |library(), ipak()                         |\n|Load data                  |read_csv(), load()                        |\n|Get HLO of data            |df$x, glimpse(), table(), summary()       |\n|Transform data             |<-, mutate(), ifelse(), case_when()       |\n|Reshape data               |pivot_longer(), left_join()               |\n|Summarize data numerically |mean(), median(), summarise(), group_by() |\n|Summarize data graphically |ggplot(), aes(), geom_                    |\n\n\n:::\n:::\n\n\n## Mapping Concepts to Code\n\n-   Takes time and practice\n\n-   Don't be afraid to FAAFO\n\n-   Don't worry about memorizing everything.\n\n-   Statistical programming is necessary to actually **do** empirical research\n\n-   Learning to code will help us understand statistical concepts.\n\n-   Learning to think programmatically and algorithmically will help us tackle complex problems\n\n## Descriptive statistics {.smaller}\n\n-   Descriptive statistics help us describe what's typical of our data\n\n-   [What's a typical value in our data]{.blue}\n\n    -   [Mean](https://pols1600.paultesta.org/labs/01-lab-comments.html#mean)\n    -   [Median](https://pols1600.paultesta.org/labs/01-lab-comments.html#median)\n    -   [Mode](https://pols1600.paultesta.org/labs/01-lab-comments.html#modes)\n\n-   [How much do our data vary?]{.blue}\n\n    -   [Variance](https://pols1600.paultesta.org/labs/01-lab-comments.html#variance)\n    -   [Standard deviation](https://pols1600.paultesta.org/labs/01-lab-comments.html#standard-deviations)\n\n-   As one variable changes [how does another change]{.blue}?\n\n    -   [Covariance](https://pols1600.paultesta.org/labs/01-lab-comments.html#covariance)\n    -   [Correlation](https://pols1600.paultesta.org/labs/01-lab-comments.html#correlation)\n\n-   Descriptive statistics are:\n\n    -   Diagnostic\n    -   Generative\n\n## Levels of understanding in POLS 1600\n\n-   Conceptual\n\n-   Practical\n\n-   Definitional\n\n-   Theoretical\n\n## Descriptive statistics: Levels of understanding\n\n-   **Conceptual**\n\n-   **Practical**\n\n-   Definitional\n\n-   Theoretical\n\n## Mean: Conceptual Understanding\n\nA mean is:\n\n-   A common and important [measure of central tendency]{.blue} (what's typical)\n\n-   It's the [arithmetic average]{.blue} you learned in school\n\n-   We can think of it as the [balancing point]{.blue} of a distribution\n\n-   A conditional mean is the average of one variable $X$, when some other variable, $Z$ takes a value $z$\n\n    -   Think about the average height in our class ([unconditional mean]{.blue}) vs the average height among men and women (\\[conditional means\\].{blue})\n\n## Mean as a balancing point\n\n![](https://mathbitsnotebook.com/Algebra1/StatisticsData/balancepoint1.jpg)\n\n[Source](https://mathbitsnotebook.com/Algebra1/StatisticsData/STCenter.html)\n\n## Mean: Practical\n\nThere are lots of ways to calculate means in `R`\n\n-   The simplest is to use the `mean()` function\n\n    -   If our data have missing values, we need to to tell `R` to remove them\n\n\n::: {.cell}\n\n:::\n\n\n## Conditional Means: Practical\n\n-   To calculate a conditional mean we could us a logical index `[df$z == 1]`\n\n\n::: {.cell}\n\n:::\n\n\n-   If we wanted to a calculate a lot of conditional means we could use the `mean()` in combination with `group_by()` and `summarise()`\n\n\n::: {.cell}\n\n:::\n\n\n## Mean: Definitional\n\nFormally, we define the arithmetic mean of $x$ as $\\bar{x}$:\n\n$$\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}\n$$\n\nIn words, this formula says, to calculate the average of x, we sum up all the values of $x_i$ from observation $i=1$ to $i=n$ and then divide by the total number of observations $n$\n\n## Mean: Definitional\n\n-   In this class, I don't put a lot of weight on memorizing definitions (that's what Google's for).\n\n-   But being comfortable with \"the math\" is important and useful\n\n-   Definitional knowledge is a prerequisite for understanding more theoretical claims.\n\n## Mean: Theoretical\n\nSuppose I asked you to show that the sum of deviations from a mean equals 0?\n\n$$\n\\text{Claim:} \\sum_{i=1}^n (x_i -\\bar{x}) = 0\n$$\n\n## Mean: Theoretical {.smaller}\n\nKnowing the definition of an arithmetic mean, we could write:\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^n (x_i -\\bar{x}) &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n\\bar{x} & \\text{Distribute Summation}\\\\\n              &= \\sum_{i=1}^n x_i - n\\bar{x} & \\text{Summing a constant, } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - n\\times \\left ( \\frac{1}{n} \\sum_{i=1}^n{x_i}\\right ) & \\text{Definition of } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n{x_i} & n \\times \\frac{1}{n}=1\\\\\n              &= 0             \n\\end{aligned}\n$$\n\n## Mean: Theoretical\n\nWhy do we care?\n\n-   Showing the deviations sum to 0 is another way of saying the mean is a [balancing point]{.blue}.\n\n-   This turns out to be a useful property of means that will reappear throughout the course\n\n-   If I asked you to make a prediction, $\\hat{x}$ of a random person's height in this class, the mean would have the lowest [mean squared error]{.blue} (MSE $=\\frac{1}{n}\\sum (x_i - \\hat{x_i})^2)$\n\n## Mean: Theoretical\n\nOccasionally, you'll read or here me say say things like:\n\n> The sample mean is an unbiased estimator of the population mean\n\nIn a statistics class, we would take time to prove this.\n\n## The sample mean is an unbiased estimator of the population mean\n\nClaim:\n\nLet $x_1, x_2, \\dots x_n$ from a random sample from a population with mean $\\mu$ and variance $\\sigma^2$\n\nThen:\n\n$$\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n x_i\\right )\n$$\n\nis an unbiased estimator of $\\mu$\n\n$$\nE[\\bar{x}] = \\mu\n$$\n\n## The sample mean is an unbiased estimator of the population mean {.smaller}\n\nProof:\n\n$$\n\\begin{aligned}\nE\\left [\\bar{x} \\right] &= E\\left [\\frac{1}{n}\\left (\\sum_{i=1}^n x_i \\right) \\right] & \\text{Definition of } \\bar{x} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^nE\\left [ x_i \\right]  & \\text{Linearity of Expectations} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu  & E[x_i] = \\mu \\\\\n&= \\frac{n}{n}  \\mu  & \\sum_{i=1}^n \\mu = n\\mu \\\\\n&= \\mu  & \\blacksquare \\\\\n\\end{aligned}\n$$\n\n## Levels of understanding {.smaller}\n\n::: nonincremental\nIn this course, we tend to emphasize the\n\n-   **Conceptual**\n\n-   **Practical**\n\nOver\n\n-   Definitional\n\n-   Theoretical\n\nIn an intro statistics class, the ordering might be reversed.\n\nTrade offs:\n:::\n\n-   Pro: We actually get to *work with data* and *do empirical research* much sooner\n-   Cons: We substitute intuitive understandings for more rigorous proofs\n\n# {{< fa eye >}}Previewing the Lab {.inverse}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](./images/05_covid.png){width=854}\n:::\n:::\n\n\n[Red Covid, an Update](https://www.nytimes.com/2022/02/18/briefing/red-covid-partisan-deaths-vaccines.html) *New York Times*, 18 February, 2022\n\n## Preview of the Lab\n\nConceptually, this lab is designed to help reinforce the relationship between linear models like $y=\\beta_0 + \\beta_1x$ and the conditional expectation function $E[Y|X]$.\n\n-   Questions 1-5 are designed to reinforce your **data wrangling** skills. In particular, you will get practice:\n\n    -   Creating and recoding variables using `mutate()`\n    -   Calculating a [moving average](https://en.wikipedia.org/wiki/Moving_average) or rolling mean using the `rollmean()` function from the `zoo` package\n    -   Transforming the data on presidential elections so that it can be merged with the data on Covid-19 using the `pivot_wider()` function.\n    -   [Merging data](https://r4ds.had.co.nz/relational-data.html) together using the `left_join()` function.\n\n## Preview of the Lab\n\n-   In question 6, you will see how calculating conditional means provides a simple test of \"Red Covid\" claim.\n\n-   In question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)\n\n-   In question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)\n\n-   In question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.\n\n-   Question 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week's lab.\n\n## Load the Covid-19 Data\n\n\n::: {.cell}\n\n:::\n\n\n## Filter Covid-19 Data to US States\n\n\n::: {.cell}\n\n:::\n\n\n## Mutate: Calculate New Cases\n\n\n::: {.cell}\n\n:::\n\n\n## Calculating a Rolling Average New Cases\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 52,580 × 4\n# Groups:   state [51]\n   state     date       new_cases_pc new_cases_pc_7da\n   <chr>     <date>            <dbl>            <dbl>\n 1 Minnesota 2020-03-06      NA               NA     \n 2 Minnesota 2020-03-07       0               NA     \n 3 Minnesota 2020-03-08       0.0177          NA     \n 4 Minnesota 2020-03-09       0               NA     \n 5 Minnesota 2020-03-10       0.0177          NA     \n 6 Minnesota 2020-03-11       0.0355          NA     \n 7 Minnesota 2020-03-12       0.0709          NA     \n 8 Minnesota 2020-03-13       0.0887           0.0329\n 9 Minnesota 2020-03-14       0.124            0.0507\n10 Minnesota 2020-03-15       0.248            0.0836\n# ℹ 52,570 more rows\n```\n\n\n:::\n:::\n\n\n## New Case Per Capita\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/plotmean2-1.png){width=960}\n:::\n:::\n\n\n## New Case Per Capita vs 7-day average\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/plotmean7d2-1.png){width=960}\n:::\n:::\n\n\n## Facemask Policy\n\n\n::: {.cell}\n\n:::\n\n\n## Mutate: Dates and Vaccinations\n\n\n::: {.cell}\n\n:::\n\n\n## Load Data on Presidential Elections\n\n\n::: {.cell}\n\n:::\n\n\n## HLO of Presidential Elections Data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 15\n   year state   state_po state_fips state_cen state_ic office       candidate   \n  <dbl> <chr>   <chr>         <dbl>     <dbl>    <dbl> <chr>        <chr>       \n1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, JI…\n2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GERA…\n3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, LE…\n4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BEN…\n5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\" \n6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE, …\n# ℹ 7 more variables: party_detailed <chr>, writein <lgl>,\n#   candidatevotes <dbl>, totalvotes <dbl>, version <dbl>, notes <lgl>,\n#   party_simplified <chr>\n```\n\n\n:::\n:::\n\n\n## Transform Data to get just 2020 Election\n\n\n::: {.cell}\n\n:::\n\n\n## Transform Data to get just 2020 Election\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  state      state_po year_election totalvotes DEMOCRAT REPUBLICAN dem_voteshare\n  <chr>      <chr>            <dbl>      <dbl>    <dbl>      <dbl>         <dbl>\n1 Alabama    AL                2020    2323282   849624    1441170          36.6\n2 Alaska     AK                2020     359530   153778     189951          42.8\n3 Arizona    AZ                2020    3387326  1672143    1661686          49.4\n4 Arkansas   AR                2020    1219069   423932     760647          34.8\n5 California CA                2020   17500881 11110250    6006429          63.5\n6 Colorado   CO                2020    3279980  1804352    1364607          55.0\n# ℹ 2 more variables: rep_voteshare <dbl>, winner <fct>\n```\n\n\n:::\n:::\n\n\n## Load Data on Median State Income from the Census\n\n\n::: {.cell}\n\n:::\n\n\n## HLO: Census Data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  GEOID NAME    variable   estimate    moe\n  <chr> <chr>   <chr>         <dbl>  <dbl>\n1 01    Alabama med_age        39      0.2\n2 01    Alabama med_income  50536    304  \n3 02    Alaska  med_age        34.3    0.1\n4 02    Alaska  med_income  77640   1015  \n5 04    Arizona med_age        37.7    0.2\n6 04    Arizona med_income  58945    266  \n```\n\n\n:::\n:::\n\n\n## Tidy Census Data\n\n\n::: {.cell}\n\n:::\n\n\n## Tidy Census Data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  state      med_age med_income\n  <chr>        <dbl>      <dbl>\n1 Alabama       39        50536\n2 Alaska        34.3      77640\n3 Arizona       37.7      58945\n4 Arkansas      38.1      47597\n5 California    36.5      75235\n6 Colorado      36.7      72331\n```\n\n\n:::\n:::\n\n\n## Merge election data and covid data into single `df`\n\n-   We're going to take our `covid_us` data and **merge** into this data on the 2020 election from `pres2020_df` using the common `state` variable in each data set for a `left_join()`\n\n-   Always check the matches in your joining variable (i.e. `state`)\n\n-   Below we see that our recoding of state to title case in created a mismatch\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncharacter(0)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51\n```\n\n\n:::\n:::\n\n\n## Merge election data into Covid data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    56\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51  9\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    64\n```\n\n\n:::\n:::\n\n\n## Merge Census data into Covid data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    64\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 52  3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53678    66\n```\n\n\n:::\n:::\n\n\n# {{< fa lightbulb >}} Causal Identification with Observational Designs {.inverse}\n\n## Causal inference is about counterfactual comparisons\n\n-   Causal inference is about counterfactual comparisons\n\n    -   What would have happened if some aspect of the world either had or had not been present\n\n-   [Casual Identification]{.blue} refers to \"the assumptions needed for statistical estimates to be given a causal interpretation\" [Keele (2015)](http://lukekeele.com/wp-content/uploads/2016/03/causal.pdf)\\]\n\n    -   What do we need to assume to make our claims about cause and effect credible\n\n## Experimental Designs\n\n-   [Experimental designs]{.blue} are studies in which a causal variable of interest, the *treatement*, is [manipulated by the researcher]{.blue} to examine its causal effects on some *outcome* of interest\n\n-   Random assignment is the key to causal identification in experiments because it creates [statistical independence]{.blue} between [treatment]{.blue} and [potential outcomes]{.blue} any potential [confounding factors]{.blue}\n\n$$\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\n$$\\\n\\## Randomization creates credible counterfactual comparisons\n\nIf treatment has been randomly assigned, then:\n\n-   The only thing that differs between treatment and control is that one group got the treatment, and another did not.\n-   We can estimate the Average Treatment Effect (ATE) using the difference of sample means\n\n$$\n\\begin{aligned}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{aligned}\n$$\n\n## Observational Designs {.smaller}\n\n-   [Observational designs]{.blue} are studies in which a causal variable of interest is determined by someone/thing [other than the researcher]{.blue} (nature, governments, people, etc.)\n\n-   Since treatment has not been randomly assigned, observational studies typically require [stronger assumptions]{.blue} to make causal claims.\n\n-   Generally speaking, these assumptions amount to a claim about conditional independence\n\n$$\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i | K_i\n$$\n\n-   Where after conditioning on $K_i$, some [knowledge about the world]{.blue} and how the [data were generated]{.blue}, our [treatment]{.blue} is as good as (as-if) randomly assigned (hence [conditionally independent]{.blue})\n\n## Causal Inference in Observational Studies {.smaller}\n\nTo understand how to make causal claims in observational studies we will:\n\n-   Introduce the concept of Directed Acyclic Graphs to describe causal relationships\n\n-   Discuss three approaches to [covariate adjustment]{.blue}\n\n    -   Subclassification\n    -   Matching\n    -   [Linear Regression]{.blue}\n\n-   Three research designs for observational data\n\n    -   [Differences-in-Differences]{.blue}\n    -   Regression Discontinuity Designs\n    -   Instrumental Variables\n\n# Directed Acyclic Graphs {.inverse}\n\n## Describing Casual Claims\n\nTwo ways to represent causal claims:\n\n-   Potential Outcomes Notation helped illustrate the **Fundamental Problem of Causal Inference**\n\n-   [Directed Acyclic Graphs]{.blue} provide a way of encoding assumptions about casual relationships and are useful for illustrating [types of bias]{.blue}\n\n    -   **Confounder bias:** Failing to control for a common cause (aka Selection Bias, Omitted Variable Bias)\n    -   **Collider bias:** Controlling for a common consequence\n\n## Directed Acyclic Graphs\n\n-   Directed Acyclic Graphs provide a way of encoding assumptions about casual relationships\n\n    -   **Directed** Arrows $\\to$ describe a direct causal effect\n\n    -   Arrow from $D\\to Y$ means $Y_i(d) \\neq Y_i(d^\\prime)$ \"The outcome ( $Y$) for person $i$ when D happens ( $Y_i(d)$ ) is different than the the outcome when $D$ doesn't happen ( $Y_i(d^\\prime)$ )\n\n    -   No arrow = no effect ( $Y_i(d) = Y_i(d^\\prime)$ )\n\n    -   **Acyclic:** No cycles. A variable can't cause itself\n\n## Types of variables in a DAG\n\n![](https://book.declaredesign.org/figures/figure-6-2.svg)\n\n@Blair2023-yg [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)\n\n## When to control for a variable:\n\n![](https://book.declaredesign.org/figures/figure-16-3.svg)\n\n[@Blair2023-yg] [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)\n\n# {{< fa lightbulb >}} Covariate Adjustment {.inverse}\n\n# {{< fa lightbulb >}} Simple Linear Regression {.inverse}\n\n## Understanding Linear Regression\n\n-   **Conceptual**\n    -   Simple linear regression estimates \"a line of best fit\" that summarizes relationships between two variables\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\n-   **Practical**\n    -   We estimate linear models in R using the `lm()` function\n\n\n::: {.cell}\n\n:::\n\n\n-   *Technical/Definitional*\n    -   Linear regression chooses $\\beta_0$ and $\\beta_1$ to minimize the Sum of Squared Residuals (SSR):\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n-   *Theoretical*\n    -   Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$\n\nclass: inverse, center, middle \\# 💡\\\n\\# Conceptual: Linear Regression \\## Linear Regression Provides an Estimate of the Line of Best Fit\n\n## Conceptual: Linear Regression\n\n-   Regression is a tool for describing relationships.\n\n    -   How does some outcome we're interested in tend to change as some predictor of that outcome changes?\n\n    -   How does economic development vary with democracy?\n\n    -   How does economic development vary with democracy, adjusting for natural resources like oil and gas\n\n-   Formally:\n\n$$\ny_i = f(x_i) + \\epsilon\n$$\n\n-   Y is a function of X plus some error, $\\epsilon$\n\n-   Linear regression assumes that relationship between an outcome and a predictor can be by a [linear](https://en.wikipedia.org/wiki/Linearity) function\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n## Linear Regression and the Line of Best Fit\n\n-   The goal of linear regression is to choose coefficients $\\beta_0$ and $\\beta_1$ to summarizes the relationship between $y$ and $x$\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n-   To accomplish this we need some sort of criteria.\n\n-   For linear regression, that criteria is minimizing the error between what our model predicts $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$ and what we actually observed $(y_i)$\n\n-   More on this to come. But first...\n\n## Regression Notation\n\n-   $y_i$ an **outcome variable** or thing we're trying to explain\n\n    -   AKA: The dependent variable, The response Variable, The left hand side of the model\n\n-   $x_i$ a **predictor variables** or things we think explain variation in our outcome\n\n    -   AKA: The independent variable, covariates, the right hand side of the model.\n\n    -   Cap or No Cap: I'll use $X$ (should be $\\mathbf{X}$) to denote a set (matrix) of predictor variables. $y$ vs $Y$ can also have technical distinctions (Sample vs Population, observed value vs Random Variable, ...)\n\n-   $\\beta$ a set of **unknown parameters** that describe the relationship between our outcome $y_i$ and our predictors $x_i$\n\n-   $\\epsilon$ the **error term** representing variation in $y_i$ not explained by our model.\n\n## Linear Regression\n\nLet's return to the simple (bivariate) linear regressions we introduced last week:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n$$\n\n-   We call this a bivariate regression, because there are only two variables.\n\n-   We call this a linear regression, because $y_i = \\beta_0 + \\beta_1 x_i$ is the equation for a line, where:\n\n    -   $\\beta_0$ corresponds to the $y$ intercept, or the model's prediction when $x = 0$.\n\n    -   $\\beta_1$ corresponds to the slope, or how $y$ is predicted to change as $x$ changes.\n\n## Linear Regression\n\n-   If you find this notation confusing, try plugging in substantive concepts for what $y$ and $x$ represent\n-   Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.\n\nThe generic linear model\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon$$\n\nReflects:\n\n$$\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i$$\n\n## Practical: Estimating a Linear Regression\n\n-   We estimate linear regressions in `R` using the `lm()` function.\n-   `lm()` requires two arguments:\n    -   a `formula` argument of the general form `y ~ x` read as \"Y modeled by X\" or below \"Transgender Feeling Thermometer (`y`) modeled by (`~`) Age (`x`)\n    -   a `data` argument telling R where to find the variables in the formula\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n```\n\n\n:::\n:::\n\n\nThe coefficients from `lm()` are saved in object called `m1`\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n```\n\n\n:::\n:::\n\n\n`m1` actually contains a lot of information\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n```\n\n\n:::\n:::\n\n\n## Practical: Interpreting a Linear Regression\n\nWe can extract the intercept and slope from this simple bivariate model, using the `coef()` function\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n    62.8196 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    vf_age \n-0.2030711 \n```\n\n\n:::\n:::\n\n\n## Practical: Interpreting a Linear Regression\n\nThe two coefficients from `m1` define a line of best fit, summarizing how feelings toward transgender individuals change with age\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon$$\n\n$$\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i$$\n\n$$\\text{Transgender Feeling Thermometer}_i = 62.82 + -0.2 \\text{Age}_i + \\epsilon_i$$\n\n## Practical: Predicted values from a Linear Regression\n\n-   Often it's useful for interpretation to obtain predicted values from a regression.\n\n-   To obtain predicted vales $(\\hat{y})$, we simply plug in a value for $x$ (In this case, $Age$) and evaluate our equation.\n\n-   For example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?\n\n$$\\hat{FT}_{x=18} = 62.82 + -0.2 \\times 18  = 59.16$$ $$\\hat{FT}_{x=65} = 62.82 + -0.2 \\times 68  = 49.01$$\n\n## Practical: Predicted values from a Linear Regression\n\nWe could do this by hand\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   59.16432 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   49.01076 \n```\n\n\n:::\n:::\n\n\n## Practical: Predicted values from a Linear Regression\n\nMore often we will:\n\n-   Make a prediction data frame (called `pred_df` below) with the values of interests\n-   Use the `predict()` function with our linear model (`m1`) and `pred_df`\n-   Save the predicted values to our new column in our prediction data frame\n\n## Practical: Predicted values from a Linear Regression\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2 \n59.16432 49.01076 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  vf_age ft_trans_hat\n1     18     59.16432\n2     68     49.01076\n```\n\n\n:::\n:::\n\n\n## Practical: Visualizing Linear Regression\n\nWe can visualize simple regression by:\n\n-   plotting a scatter plot of the outcome (y-axis) and predictors (x-axis)\n\n-   overlaying the line defined by `lm()`\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/fig_lm_plot-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/figlm1code-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/figlm1-1.png){width=960}\n:::\n:::\n\n\n## How did `lm()` choose $\\beta_0$ and $\\beta_1$\n\n--\n\n-   P: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression\n\n--\n\n-   Q: Ok, that's not really that helpful...\n\n    -   What's an error?\n    -   Why would we square and sum them\n    -   How do we minimize them.\n\nP: Good questions!\n\n## What's an error?\n\nAn error, $\\epsilon_i$ is simply the difference between the observed value of $y_i$ and what our model would predict, $\\hat{y_i}$ given some value of $x_i$. So for a model:\n\n$$y_i=\\beta_0+\\beta_1 x_{i} + \\epsilon_i$$\n\nWe simply subtract our model's prediction $\\beta_0+\\beta_1 x_{i}$ from the the observed value, $y_i$\n\n$$\\hat{\\epsilon_i}=y_i-\\hat{y_i}=(Y_i-(\\beta_0+\\beta_1 x_{i}))$$\n\nTo get $\\epsilon_i$\n\n## Why are we squaring and summing $\\epsilon$\n\nThere are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)\n\n-   Squaring $\\epsilon$ treats positive and negative residuals equally.\n\n-   Summing produces single value summarizing our models overall performance.\n\nThere are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties\n\n## How do we minimize $\\sum \\epsilon^2$\n\nOLS chooses $\\beta_0$ and $\\beta_1$ to minimize $\\sum \\epsilon^2$, the Sum of Squared Residuals (SSR)\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n## How did `lm()` choose $\\beta_0$ and $\\beta_1$\n\nIn an intro stats course, we would walk through the process of finding\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$ Which involves a little bit of calculus. The big payoff is that\n\n$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$ And\n\n$$ \\beta_1 = \\frac{Cov(x,y)}{Var(x)}$$ Which is never quite the epiphany, I think we think it is...\n\nThe following slides walk you through the mechanics of this exercise. We're gonna skip through them in class, but they're there for your reference\n\n## How do we minimize $\\sum \\epsilon^2$\n\nTo understand what's going on under the hood, you need a broad understanding of some basic calculus.\n\nThe next few slides provide a brief review of derivatives and differential calculus.\n\n## Derivatives\n\nThe derivative of $f$ at $x$ is its rate of change at $x$\n\n-   For a line: the slope\n-   For a curve: the slope of a line tangent to the curve\n\nYou'll see two notations for derivatives:\n\n1.  Leibniz notation:\n\n$$\n\\frac{df}{dx}(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{(x+h)-x}\n$$\n\n2.  Lagrange: $f^{\\prime}(x)$\n\n## Some useful Facts about Derivatives\n\nDerivative of a constant\n\n$$\nf^{\\prime}(c)=0\n$$\n\nDerivative of a line f(x)=2x\n\n$$\nf^{\\prime}(2x)=2\n$$\n\nDerivative of $f(x)=x^2$\n\n$$\nf^{\\prime}(x^2)=2x\n$$\n\nChain rule: y= f(g(x)). The derivative of y with respect to x is\n\n$$\n\\frac{d}{dx}(f(g(x)))=f^{\\prime}(g(x))g^{\\prime}(x)\n$$\n\nThe derivative of the \"outside\" times the derivative of the \"inside,\" remembering that the derivative of the outside function is evaluated at the value of the inside function.\n\n## Finding a Local Minimums\n\nLocal minimum:\n\n$$\nf^{\\prime}(x)=0 \\text{ and } f^{\\prime\\prime}(x)>0 \n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://copingwithcalculus.com/SecondDeriv1.png)\n:::\n:::\n\n\n[Source](https://copingwithcalculus.com/SecondDerivativeTest.html)\n\n## Partial Derivatives\n\nLet $f$ be a function of the variables $(x, \\dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is\n\n$$\\begin{align*}\n\\frac{\\partial f(x, \\dots, X_n)}{\\partial X_i}=\\lim_{h\\to0}\\frac{f(x, \\dots X_i+h \\dots, X_n)-f(x, \\dots X_i \\dots, X_n)}{h}\n\\end{align*}$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png)\n:::\n:::\n\n\n[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)\n\n## Minimizing the sum of squared errors\n\nOur model\n\n$$y_i =\\beta_0+\\beta_1x_{i}+\\epsilon_i$$\n\nFinds coefficients $\\beta_0$ and $\\beta_1$ to to minimize the sum of squared residuals, $\\hat{\\epsilon}_i$:\n\n$$\\begin{aligned}\n\\sum \\hat{\\epsilon_i}^2 &= \\sum (y_i-\\beta_0-\\beta_1 x_{i})^2\n\\end{aligned}$$\n\n## Minimizing the sum of squared errors\n\nWe solve for $\\beta_0$ and $\\beta_1$, by taking the partial derivatives with respect to $\\beta_0$ and $\\beta_1$, and setting them equal to zero\n\n$$\\begin{aligned}\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial \\beta_0} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})=0 & f'(-x^2) = -2x\\\\\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial\\beta_1} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})x_{i}=0 & \\text{chain rule}\n\\end{aligned}$$\n\n## Solving for $\\beta_0$\n\nFirst, we'll solve for $\\beta_0$, by multiplying both sides by -1/2 and distributing the $\\sum$:\n\n$$\\begin{aligned}\n0 &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})\\\\\n\\sum \\beta_0 &= \\sum y_i - \\sum \\beta_1 x_{i}\\\\\nN \\beta_0 &= \\sum y_i -\\sum \\beta_1 x_{i}\\\\\n\\beta_0 &= \\frac{\\sum y_i}{N} - \\frac{\\beta_1 \\sum x_{i}}{N}\\\\\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}\n\\end{aligned}$$\n\n## Solving for $\\beta_1$\n\nNow, we can solve for $\\beta_1$ plugging in $\\beta_0$.\n\n$$\\begin{aligned}\n0 &= -2\\sum [(y_i-\\beta_0-\\beta_1 x_{i})x_{i}]\\\\\n0 &= \\sum [y_ix_i-(\\bar{y} - \\beta_1 \\bar{x})x_{i}-\\beta_1 x_{i}^2]\\\\\n0 &= \\sum [y_ix_i-\\bar{y}x_{i} + \\beta_1 \\bar{x}x_{i}-\\beta_1 x_{i}^2]\n\\end{aligned}$$\n\n## Solving for $\\beta_1$\n\nNow we'll rearrange some terms and pull out an $x_{i}$ to get\n\n$$\\begin{aligned}\n0 &= \\sum [(y_i -\\bar{y} + \\beta_1 \\bar{x}-\\beta_1 x_{i})x_{i}]\n\\end{aligned}$$\n\nDividing both sides by $x_{i}$ and distributing the summation, we can isolate $\\beta_1$\n\n$$\\begin{aligned}\n\\beta_1 \\sum (x_{i}-\\bar{x}) &= \\sum (y_i -\\bar{y})\n\\end{aligned}$$\n\nDividing by $\\sum (x_{i}-\\bar{x})$ to get\n\n$$\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})}{\\sum (x_{i}-\\bar{x})}\n\\end{aligned}$$\n\n## Solving for $\\beta_1$\n\nFinally, by multiplying by $\\frac{(x_{i}-\\bar{x})}{(x_{i}-\\bar{x})}$ we get\n\n$$\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})(x_{i}-\\bar{x})}{\\sum (\\bar{x}-x_{i})^2}\n\\end{aligned}$$\n\nWhich has a nice interpretation:\n\n$$\\begin{aligned}\n\\beta_1 &= \\frac{Cov(x,y)}{Var(x)}\n\\end{aligned}$$\n\nSo the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat!\n\nclass: inverse, center, middle \\# 💡\\\n\\# Theoretical: Linear Regression \\## OLS provides a linear estimate of CEF: E\\[Y\\|X\\]\n\n## Linear Regression is a many splendored thing\n\n[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of the various interpretations/motivations for linear regression.\n\n-   A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)\n\n-   A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\\beta$\n\n-   A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)\n\n-   A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)\n\n-   A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)\n\n-   A [linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)\n\n## Linear Regression is a many splendored thing\n\n[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of various interpretations/motivations for linear regression.\n\n-   A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)\n\n-   A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\\beta$\n\n-   A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)\n\n-   A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)\n\n-   A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)\n\n-   A [**linear approximation of the conditional expectation function**](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)\n\n## The Conditional Expectation Function\n\nOf all the functions we could choose to describe the relationship between $Y$ and $X$,\n\n$$\nY_i = f(X_i) + \\epsilon_i\n$$\n\nthe conditional expectation of $Y$ given $X$ $(E[Y|X])$, has some appealing properties\n\n$$\nY_i = E[Y_i|X_i] + \\epsilon\n$$\n\nThe error, by definition, is uncorrelated with X and $E[\\epsilon|X]=0$\n\n$$\nE[\\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0\n$$\n\nOf all the possible functions $g(X)$, we can show that \\$E\\[Y_i\\|X_i\\] \\$ is the best predictor in terms of minimizing **mean squared error**\n\n$$\nE[ (Y - g(Y))^2] \\geq E[(Y - E[Y|X])^2] \n$$\n\n## Linear Approximations to the Conditional Expectation Function\n\n-   We can then show (in a different class) that linear regression provides the best linear predictor of the CEF\n    -   Chapter 3, of [Mostly Harmless Econometrics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991028523169706966)\n    -   Chapter 4 of [Foundations of Agnostic Statistics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991000736119706966)\n-   Furthermore, when the CEF is linear, it's equal exactly to OLS regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n  ggplot(aes(vf_age,therm_trans_t0))+\n  geom_point(size=.5, alpha=.5)+\n  stat_summary(geom=\"point\", aes(col=\"CEF\"))+\n  stat_summary(geom=\"line\", aes(col=\"CEF\"))+\n  labs(\n    x = \"Age\",\n    y = \"Feeling Thermometer toward\\nTransgender People\",\n    col = \"\"\n  )+\n  theme_classic() -> plot_cef\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/cef1-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_cef+\n  geom_smooth(method = \"lm\", se=F, aes(col = \"OLS\")) -> plot_cef\nplot_cef\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/cef2-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_cef+\n  geom_smooth(method = \"lm\", se=F, aes(col = \"OLS\")) -> plot_cef\nplot_cef\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-slides_files/figure-revealjs/cef3-1.png){width=960}\n:::\n:::\n\n\n## What you need to know about Regression\n\n-   **Conceptual**\n    -   Simple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n$$\n\n-   **Practical**\n    -   We estimate linear models in R using the `lm()` function\n\n\n::: {.cell}\n\n:::\n\n\n-   *Technical/Definitional*\n    -   Linear regression chooses $\\beta_0$ and $\\beta_1$ to minimize the Sum of Squared Residuals (SSR):\n\n$$\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2$$\n\n-   *Theoretical*\n    -   Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$\n\n# {{< fa lightbulb >}} Difference-in-Differences {.inverse}\n\n## Summary\n\n## References\n",
    "supporting": [
      "05-slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.31/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}