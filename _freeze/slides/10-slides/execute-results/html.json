{
  "hash": "b293985289586047aa90937ec68e4bb1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 10:\"\nsubtitle: \"Quantifying uncertainty with confidence intervals\"\nauthor: \"Paul Testa\"\nformat: \n  revealjs:\n    theme: [default, brownslides.scss]\n    logo: images/pols1600_hex.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: true\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    # include-after-body: title-slide.html\n    title-slide-attributes:\n      align: left\n      data-background-image: images/pols1600_hex.png\n      data-background-position: 90% 50%\n      data-background-size: 40%\nexecute: \n  echo: true\nfilters:\n  - openlinksinnewpage\n---\n\n\n\n\n\n\n\n\n\n\nclass: inverse, center, middle \\# Overview\n\n## General Plan\n\n-   Course Plan\n-   Setup\n-   Feedback\n-   Review\n-   Confidence Intervals\n\nbackground-image: url(\"https://i.kym-cdn.com/entries/icons/original/000/037/873/We're_All_Trying_To_Find_The_Guy_Who_Did_This_banner_1.jpg\") background-size:contain\n\n## Two Options:\n\n-   Proceed with group projects with condensed schedule/assignments\n\n--\n\n-   Replace group projects with a take home (open book/notes) final exam\n    -   Posted April 30\n    -   Due May 7\n    -   Mix of theory, concepts, and coding.\n\n## Course Plan: Option 1\n\n-   April 13: No Class, Review Feedback to A2\n\n-   April 18: Lecture -- Hypothesis Testing\n\n-   April 20 Workshop on Paper -- Inference About Models: Counts as Assignment 3\n\n-   April 25: Lecture -- Course Review\n\n-   April 27: Workshop: Paper drafts and Presentations\n\n-   April 30: Upload Presentations\n\n-   May 2: Class Presentations Part 1\n\n-   May 4: Class Presentations Part 2\n\n-   May ?: Tacos?\n\n## Course Plan: Option 2\n\n-   April 13: No Class,\n\n-   April 18: Lecture -- Hypothesis Testing\n\n-   April 20 Lab -- Hypothesis Testing and Interval Estimation\n\n-   April 25: Lecture -- Course Review\n\n-   April 27: Workshop:\n\n-   April 30: Take Home Final Exam\n\n-   May ?: Tacos or Pizza with POLS 1140?\n\n-   May 7: Take Home Final Exam due\n\n## What do we want to do?\n\nclass:inverse, middle, center \\# 💪 \\## Get set up to work\n\n## Packages for today\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\",# <<\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\", \"boot\",\"purrr\"\n)\n```\n:::\n\n\n## Define a function to load (and if needed install) packages\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n```\n:::\n\n\n## Load packages for today\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled        modelr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign           zoo          boot         purrr \n         TRUE          TRUE          TRUE          TRUE \n```\n\n\n:::\n:::\n\n\nclass:inverse, center, middle \\# 💪 \\## Load Data for today\n\nWe'll use data from last week's lab to\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n```\n:::\n\n\nclass:inverse, middle, center \\# 🔍 \\## Review: Generalized Linear Models\n\n## Generalized Linear Models\n\nIn last week's lab we fit two models\n\n-   OLS\n-   Logistic regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# OLS\nm1 <- lm(support_war01 ~ age + sex + education_n, df_drww)\n\n# Logisitic \nm2 <- glm(support_war01 ~ age + sex + education_n, df_drww,\n          family = binomial)\n```\n:::\n\n\n## Generalized Linear Model\n\n-   Logisitic regression is a type of **generalized linear model** used to model **binary outcomes**\n\n-   We estimate logistic regression using Maximum Likelihood, which allows us to model outcomes using different probability distributions\n\n-   Other common generalized linear models\n\n    -   Probit regression (binary outcomes)\n    -   Poisson regression (count data)\n    -   Negative binomial regression (count data)\n\n-   It's still \"regression\", but interpretation typically requires transforming predictions (inverting the link function)\n\n\n::: {.cell layout-align=\"center\"}\n<table class=\"texreg\" style=\"margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;\">\n<caption>Statistical models</caption>\n<thead>\n<tr>\n<th style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</th>\n<th style=\"padding-left: 5px;padding-right: 5px;\">Model 1</th>\n<th style=\"padding-left: 5px;padding-right: 5px;\">Model 2</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"border-top: 1px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">(Intercept)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.28<sup>&#42;&#42;&#42;</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">-1.36<sup>&#42;&#42;&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.05)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.29)</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">age</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.01<sup>&#42;&#42;&#42;</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.05<sup>&#42;&#42;&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.00)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.00)</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">sexMale</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.09<sup>&#42;&#42;&#42;</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.50<sup>&#42;&#42;&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.02)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.13)</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">education_n</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">-0.02</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">-0.10</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.01)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.06)</td>\n</tr>\n<tr style=\"border-top: 1px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">R<sup>2</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.11</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Adj. R<sup>2</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.11</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Num. obs.</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">1463</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">1463</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">AIC</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">1575.54</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">BIC</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">1596.69</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Log Likelihood</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">-783.77</td>\n</tr>\n<tr style=\"border-bottom: 2px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">Deviance</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">1567.54</td>\n</tr>\n</tbody>\n<tfoot>\n<tr>\n<td style=\"font-size: 0.8em;\" colspan=\"3\"><sup>&#42;&#42;&#42;</sup>p &lt; 0.001; <sup>&#42;&#42;</sup>p &lt; 0.01; <sup>&#42;</sup>p &lt; 0.05</td>\n</tr>\n</tfoot>\n</table>\n:::\n\n\n## Prediction Data Frame\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_df <- expand_grid(\n  age = 18 : 99,\n  sex = \"Female\",\n  education_n = mean(df_drww$education_n, na.rm = T)\n)\n```\n:::\n\n\n## Predicted Values\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# #Predicted values for m1\npred_df$pred_ols <- predict(m1,\n                            newdata = pred_df)\n# Predicted values for m2\n# Remember to add type = \"response\"\npred_df$pred_logit <- predict(m2,\n                            newdata = pred_df,\n                            type = \"response\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# data\npred_df%>%\n  # aesthetics\n  ggplot(aes(age, pred_ols, col = \"OLS\"))+\n  # geometries\n  geom_line()+\n  geom_line(aes(y = pred_logit, col = \"Logistic\"))+\n  geom_jitter(data=df_drww, aes(age, support_war01),\n              col = \"black\",\n              height = .05,\n              size = .5,\n              alpha = .5)+\n  labs(\n    col = \"Model\",\n    x = \"Age\",\n    y = \"Predicted Values\"\n  )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/fig1-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nclass: inverse, center, middle \\# 💡 \\# Confidence Intervals \\## The Basics\n\n## Overview:\n\n-   Confidence intervals provide a way of quantifying uncertainty about **estimates**\n\n-   Confidence intervals describe a range of plausible values\n\n-   That range is a function of the **standard error** of the estimate, and the a **critical value** determined $\\alpha$, which describes the degree of confidence we want\n\n    -   A 95% confidence interval corresponds to an $\\alpha$ of 0.05\n\n-   A **standard error** is the standard deviation of the sampling distribution of our estimate\n\n-   We can obtain the sampling distribution via:\n\n    -   simulation (bootstrapping)\n    -   asymptotic theory (the CLT)\n\n-   Our confidence is about the interval, not the estimate.\n\n## Defintions: Populations and Samples\n\n-   **Population**: All the cases from which you could have sampled\n\n-   **Parameter:** A quantity or quantities of interest often generically called $\\theta$ (\"theta\"). Something we'd like to know about our population\n\n-   **Sample:** A (random) draw from that population\n\n-   **Sample Size:** The number of observations in your draw (without replacement)\n\n## Defintions: Estimators, Estimates, and Statistics\n\n-   **Estimator:** A rule for calculating an *estimate* of our parameter of interest.\n\n-   **Estimate:** The value produced by some estimator for some parameter from some data. Often called $\\hat{\\theta}$\n\n-   **Unbiased estimators:** $E(\\hat{\\theta})=E(\\theta)$ On average, the estimates produced by some estimator will be centered around the truth\n\n-   **Consistent estimates:** $\\lim_{n\\to \\infty} \\hat{\\theta_N} = \\theta$ As the sample size increases, the estimates from an estimator converge in probability to the parameter value\n\n-   **Statistic:** A summary of the data (mean, regression coefficient, $R^2$). An estimator without a specified target of inference\n\n## Definitions: Distrubtions and Standard Errors\n\n-   **Sampling Distribution:** How some estimate would vary if you took repeated samples from the population\n\n-   **Standard Error:** The standard deviation of the sampling distribution\n\n-   **Resampling Distribution:** How some estimate would vary if you took repeated samples **from your sample WITH REPLACEMENT**\n\n    -   \"Sampling from our sample, as the sample was sampled from the population.\"\n\n## Confidence Intervals: Interpretation\n\n-   Confidence intervals give a range of values that are likely to include the true value of the parameter $\\theta$ with probability $(1-\\alpha) \\times 100\\%$\n\n    -   $\\alpha = 0.05$ corresponds to a \"95-percent confidence interval\"\n\n-   Our \"confidence\" is about the interval\n\n-   In repeated sampling, we expect that $(1-\\alpha) \\times 100\\%$ of the intervals we construct would contain the truth.\n\n-   For any one interval, the truth, $\\theta$, either falls within in the lower and upper bounds of the interval or it does not.\n\n## Two Approaches to Calculating Confidence Intervals:\n\nIn general, there are two ways to calculate confidence intervals:\n\n-   **Simulation:** Use our computers to simulate the idea of repeated sampling (e.g. bootstrapping)\n\n    -   Flexible, but more computationally intensive\n\n-   **Asymptotic Theory:** Use math to derive the properties of the distributions that would arise under repeated sampling\n\n    -   Faster, but requires more assumptions that may not hold\n\nWe will consider both.\n\n-   The theory of CIs is easier to illustrate via simulation\n\n-   The practice of calculating CIs is (generally) easier using asymptotic theory\n\n## Steps to Calculating a Confidence Interval\n\nFrom QSS (p. 330)\n\n1.  Choose the desired level of confidence $(1-\\alpha)\\times 100%$ by specifying a value of α between 0 and 1: the most common choice is= $\\alpha = 0.05$, which gives a 95% confidence level.\n\n2.  Derive the sampling distribution of the estimator by computing its mean and variance.\n\n3.  Compute the standard error based on this sampling distribution. (square root of the variance)\n\n4.  Compute the critical value $z_{\\alpha/2}$ as the $(1-\\alpha)\\times 100$ percentile value of the standard normal distribution\n\n5.  Compute the lower and upper confidence limits as $\\hat{\\theta} - z_{\\alpha/2}\\times SE$ and $\\hat{\\theta} + z_{\\alpha/2}\\times SE$ standard error, respectively.\n\nclass: inverse, center, middle \\# 💡 \\# Confidence Intervals \\## Simulating the Sampling Distribution through Bootstrapping\n\n## Populations\n\nLet's load the data from the *Do Russians Want War* survey\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n```\n:::\n\n\nTo understand the logic of confidence intervals, let's treat this data as our **population** from which we we could draw repeated samples.\n\n## Population Age\n\nIn our population, there are **parameters**, true values of things we want to know.\n\nSuppose we're interested in the average age of our population.\n\nIn our population, the true value of $\\mu_{age} = E[Age]$ is\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_age <- mean(df_drww$age)\nmu_age\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 46.64693\n```\n\n\n:::\n:::\n\n\nSimilarly, the true $\\sigma_{age} = \\sqrt{E[Age^2] - E[Age]^2}$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsd_age <- sqrt(mean((df_drww$age-mean(df_drww$age))^2))\nsd_age\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.81829\n```\n\n\n:::\n:::\n\n\n## Distribution Population Age\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_pop <- df_drww %>%\n  ggplot(aes(age))+\n  geom_density(col=\"grey\")+\n  geom_rug()+\n  geom_vline(\n    aes(xintercept = mu_age, \n             col = \"Population Mean\"),\n    linetype=2)+\n  theme_bw()+\n  labs(color = \"Age\")\n\np_pop\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Sample Estimates of Average Age (N = 25)\n\nSuppose we took three samples, **without replacement** of size 25, and calculated the average age in each sample:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nmean_age1 <- mean(sample(df_drww$age, 25, replace = F))\nmean_age2 <- mean(sample(df_drww$age, 25, replace = F))\nmean_age3 <- mean(sample(df_drww$age, 25, replace = F))\n\nmean_age1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 43.36\n```\n\n\n:::\n\n```{.r .cell-code}\nmean_age2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39.36\n```\n\n\n:::\n\n```{.r .cell-code}\nmean_age3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 49.72\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Repeated Sampling\n\n-   Imagine we could draw a 1,000 or 10,000 or an infinite number of samples of size N=25 from our population.\n\n-   How much would our estimate of the average of age of the population vary?\n\n-   Let's use our computers to simulate this process and find out!\n\n## Simualting Repeated Sampling\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_sims <- 1000\nsamp_size <- 25\nset.seed(123)\n\nmu_age_samp_dist_n25 <- tibble(\n  sim = 1:n_sims,\n  distribution = \"Sampling\",\n  sample = \"Population\"\n) %>%\n  mutate(\n    samp = purrr::map(sim, ~ slice_sample(df_drww, n = samp_size, replace = F)),\n    estimate = purrr::map_dbl(samp, ~ mean(.$age))\n  )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_age_samp_dist_n25\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,000 × 5\n     sim distribution sample     samp           estimate\n   <int> <chr>        <chr>      <list>            <dbl>\n 1     1 Sampling     Population <df [25 × 42]>     43.4\n 2     2 Sampling     Population <df [25 × 42]>     39.4\n 3     3 Sampling     Population <df [25 × 42]>     49.7\n 4     4 Sampling     Population <df [25 × 42]>     46.2\n 5     5 Sampling     Population <df [25 × 42]>     50.6\n 6     6 Sampling     Population <df [25 × 42]>     47.8\n 7     7 Sampling     Population <df [25 × 42]>     46.3\n 8     8 Sampling     Population <df [25 × 42]>     41.4\n 9     9 Sampling     Population <df [25 × 42]>     45.4\n10    10 Sampling     Population <df [25 × 42]>     48.8\n# ℹ 990 more rows\n```\n\n\n:::\n:::\n\n\n# One Sample\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_age_samp_dist_n25$samp[[1]]$age\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 35 31 58 26 56 25 53 26 36 35 83 52 41 61 38 29 30 36 60 70 40 32 28 58 45\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(mu_age_samp_dist_n25$samp[[1]]$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 43.36\n```\n\n\n:::\n\n```{.r .cell-code}\nmu_age_samp_dist_n25$estimate[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 43.36\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_dist <- mu_age_samp_dist_n25 %>%\n  ggplot(aes(estimate))+\n  geom_density()+\n  geom_rug()+\n  geom_density(\n    data = df_drww,\n    aes(x=age),col = \"grey\"\n  )+\n   geom_vline(\n    aes(xintercept = mu_age, \n             col = \"Population Mean\"),\n    linetype=2)+\n  theme_bw()+\n  labs(title = \"Distribution of Sample Means (N=25)\")\np_dist\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-27-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Standard Errors\n\n-   A **standard error** is simply the standard deviation of the sampling distribution.\n\n-   The standard error for our simulation above:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nse_age_n25 <- sd(mu_age_samp_dist_n25$estimate)\nse_age_n25\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.146543\n```\n\n\n:::\n:::\n\n\n## Coverage Intervals\n\n-   From the Central Limit Theorem, we know that the distribution of sample means will converge to a normal distribution.\n\n-   From probability theory, we know that we that roughly 95 percent of the values in a normal distribution fall between *Two* Standard Deviations of the mean.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nci_age_ul_n25 <- mu_age + 2*se_age_n25\nci_age_ll_n25 <- mu_age - 2*se_age_n25\n\nmean(mu_age_samp_dist_n25$estimate >ci_age_ll_n25 & \n       mu_age_samp_dist_n25$estimate <ci_age_ul_n25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.954\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_age_samp_dist_n25 %>%\n  ggplot(aes(estimate))+\n  geom_density()+\n  geom_rug(\n    aes(col = estimate >ci_age_ll_n25 & \n          estimate <ci_age_ul_n25)\n  )+\n  geom_vline(xintercept = mu_age,\n             col = \"red\",\n             linetype=2)+\n  guides(col=\"none\")+\n  geom_segment(aes(x=ci_age_ll_n25,\n                   xend = ci_age_ul_n25,\n                   y = .15,yend = .15 ),\n               col = \"#00BFC4\")+\n  theme_bw()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Boostrapped Standard Errors\n\n-   A standard error is the standard deviation of a hypothetical sampling distribution\n\n-   How do we calculate a standard error from a single sample?\n\n-   It turns out that a random sample provides unbiased estimates of both the population mean **and** the standard deviation of the of the sampling distribution (i.e. the standard error).\n\n-   We can estimate this this standard error, by sampling with replacement from our sample to generate a **bootstrapped** sampling distribution\n\n## Boostrapped Standard Errors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nbs_resamp_1 <- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 1\",\n) %>%\n  mutate(\n    samp = purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[1]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )\n```\n:::\n\n\n## Boostrapped Standard Errors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbs_resamp_2 <- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 2\",\n) %>%\n  mutate(\n    samp =  purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[2]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )\n```\n:::\n\n\n## Boostrapped Standard Errors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbs_resamp_3 <- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 3\",\n) %>%\n  mutate(\n    samp =  purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[3]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )\n```\n:::\n\n\n## Boostrapped Standard Errors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbs_example <- rbind(\n  mu_age_samp_dist_n25,\n  bs_resamp_1,\n  bs_resamp_2,\n  bs_resamp_3\n)\n\ndf_se <- bs_example %>%\n  select(sample, estimate)%>%\n  dplyr::group_by(sample)%>%\n  dplyr::summarise(\n    se = sd(estimate)\n  )\n\n\ndf_ci <- df_mn %>%\n  left_join(df_se)\n\ndf_ci%>%\n  mutate(\n    ll = xint - qt(df=25,.975)*se,\n    ul = xint + qt(df=25,.975)*se,\n    y = .15,\n    xint_pop = xint[1]\n  ) -> df_ci\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbs_example %>%\n  ggplot(aes(estimate,col = sample))+\n  geom_density(aes(linetype=distribution))+\n  facet_wrap(~sample, ncol=1)+\n   geom_vline(\n    data = df_ci,\n    aes(xintercept = xint, \n             col = sample,\n        linetype=distribution)\n    )+\n    geom_vline(\n    data = df_ci,\n    aes(xintercept = xint_pop), \n             col = \"black\",\n        linetype=2)+\n  geom_segment(\n    data = df_ci,\n    aes(x =ll, xend =ul, y=y, yend=y)\n  )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-37-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim_ci_fn<-function(x,\n                    samp_size=100,\n                    n_sims=1000,\n                    level=.95,\n                    bs=F){\n    # Take a sample of size \"nsamp\"\n    y<-sample(x=na.omit(x),size=samp_size,replace=F)\n    # Calculate the mean\n    mu<-mean(y,na.rm=T)\n    # If bs=TRUE do bootstrapped SEs \n    if(bs==T){\n    mu_dist<-rerun(\n      n_sims,\n      mean(sample(y, samp_size, replace = T)))%>%\n      unlist()\n    se<-sd(mu_dist)}else{\n    # Otherwise, just use assymptotic result (Quicker)\n    se<-sd(y,na.rm=T)/sqrt(samp_size-1)\n    }\n    # Significance level\n    the.p<-1-(1-level)/2\n    # Calculate lower and upper limits of interval\n    ll<-mu-qt(p=the.p,df=samp_size-1)*se\n    ul<-mu+qt(p=the.p,df=samp_size-1)*se\n    results<-tibble(mu=mu,ll=ll,ul=ul,se=se)\n    return(results)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(12345)\nsamp25 <-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 25)) %>%dplyr::mutate(sample = 1:n() )\nsamp50 <-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 50))%>%dplyr::mutate(sample = 1:n() )\nsamp100 <-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 100))%>%dplyr::mutate(sample = 1:n() )\nsamp200 <-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 200))%>%dplyr::mutate(sample = 1:n() )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-40-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-slides_files/figure-revealjs/unnamed-chunk-41-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Standard Errors and Sample Size\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Standard errors decrease as sample size increases\nc(mean(samp25$se),\nmean(samp50$se),\nmean(samp100$se),\nmean(samp200$se))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.193029 2.247602 1.588096 1.121755\n```\n\n\n:::\n\n```{.r .cell-code}\n# Specifically, by the square root of the sample size\nc(sd_age/sqrt(25),\nsd_age/sqrt(50),\nsd_age/sqrt(100),\nsd_age/sqrt(200))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.163659 2.237045 1.581829 1.118522\n```\n\n\n:::\n:::\n\n\n## Next Week: Standard Errors for Linear Models\n\n-   As you saw in your lab, we can apply the same principles to calculate standard errors for other quantities like the coefficients from a regression\n\n-   Next week, we'll compare these quantities to those obtained from asymptotic theory, and then turn to an alternative approach to quantifying uncertainty: Hypothesis testing.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}