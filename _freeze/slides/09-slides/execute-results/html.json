{
  "hash": "78cc5af61fafc58dae24fe83d034a4de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 09:\"\nsubtitle: \"Probability: Limit Theorems and Maximum Likelihood Estimation \"\nauthor: \"Paul Testa\"\nformat: \n  revealjs:\n    author: \"Paul Testa\"\n    height: 900\n    width: 1600\n    # css: css/brown.css\n    theme: ../files/slides.scss\n    logo: brown.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: false\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    # title-slide-attributes:\n    #   data-background-image: ../../assets/stat20-hex-bg.png\n    #   data-background-size = contain\n---\n\n\n\n\n\n\n\n\n\n\n\n\nclass: inverse, center, middle\n# Overview\n\n\n## General Plan\n\n- Setup\n- Feedback\n- Review\n  - Probability Distributions\n- Lecture \n  - The Law of Large Numbers\n  - The Central Limit Theorem\n  - Generalized Linear Models (Maybe...)\n\n\n## Goals\n\n- The Law of Large Number's says that as our sample size increases, our sample mean will converge to the population value\n\n--\n\n- The Central Limit Theorem says that the distribution of those sample means will follow a normal distribution \n\n--\n\n- Generalized Linear Models allow us to more accurately model different types of data-generating processes using Maximum Likelihood Estimation.\n\n\n## Emoji Slide notation\n\n\n- üí™: Exercises\n\n- üì¢: Feedback\n\n- üîç: Review\n\n- üí°: Core concept\n\n- ü¶â: In case you're interested\n\n\n\nclass:inverse, middle, center\n# üí™\n## Get set up to work\n\n\n## New packages\n\nNone!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n## Packages for today\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\"\n)\n```\n:::\n\n\n\n\n## Define a function to load (and if needed install) packages\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n```\n:::\n\n\n\n\n## Load packages for today\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n          zoo \n         TRUE \n```\n\n\n:::\n:::\n\n\n\n\n\nclass:inverse, center, middle\n# üí™\n## Load Data for today\n\n\n\nclass:inverse, middle, center\n# üîç\n# Review\n## Random Variables and Probability Distributions\n\n\n\n## Probability \n\n- Probability describes the likelihood of an event happening.\n\n- Statistics uses probability to quantify uncertainty about estimates and hypotheses.\n\n- Three *rules* of probability (**Kolmogorov axioms**)\n  - Positivity: $$Pr(A) \\geq 0 $$\n  - Certainty: $$Pr(\\Omega) = 1 $$\n  - Additivity: $$Pr(A \\text{ or } B) = Pr(A) + Pr(B)$$ iff A and B are mutually exclusive\n\n\n## Probability \n  \n- Two interpretations interpreting probabilities (**Frequentist** and **Bayesian**)\n    \n- Conditional Probability and Bayes Rule:\n\n$$Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}$$\n\n\n## Random Variables\n\n- Random variables assign numeric values to each event in an experiment.\n\n  - Mutually exclusive and exhaustive, together cover the entire sample space.\n\n- Discrete random variables take on finite, or [countably infinite](http://mathworld.wolfram.com/CountablyInfinite.html) distinct values.\n\n- Continuous variables can take on an uncountably infinite number of values.\n\n\n## Example: Toss Two Coins\n\n- $S={TT,TH,HT,HH}$\n\n- Let $X$ be the number of heads\n    - $X(TT)=0$\n    - $X(TH)=1$\n    - $X(HT)=1$\n    - $X(HH)=2$\n\n    \n\n## Probability Distributions\n\n- Broadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events. \n\n$$\\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities}$$\n\nThe can be represented in terms of:\n\n- Probability Mass/Density Functions\n  \n  - Discrete variables have probability mass functions (PMF)\n  \n  - Continuous variables have probability density functions (PDF)\n\n- Cumulative Density Functions\n  \n  - Discrete: Summation of discrete probabilities\n    \n  - Continuous: Integration over a range of values\n\n\n## Discrete distributions\n\n- **Probability Mass Function (pmf):** $f(x)=p(X=x)$\n\n  - Assigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply \n\n- **Cumulative Distribution Function (cdf)** $F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)$\n\n  - Sum of the probability mass for events less than or equal to $x_j$\n\n\n## Example: Toss Two coins\n\n- $S={TT,TH,HT,HH}$\n\n- Let $X$ be the number of heads\n    - $X(TT)=0$\n    - $X(TH)=1$\n    - $X(HT)=1$\n    - $X(HH)=2$\n\n- $f(X=0)=p(X=0)=1/4$\n- $f(X=1)=p(X=1)=1/2$\n- $F(X\\leq 1) = p(X \\leq 1)= 3/4$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/coin-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X<=2) = 1/6 + 1/6 = 1/3\n\n\n\n## Continuous distributions\n\n\n- **Probability Density Functions (PDF):** $f(x)$\n    - Assigns probabilities to events in the sample space such that Kolmogorov Axioms still apply \n    - But... since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n\n- **Cumulative Distribution Function (CDF)** $F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx$\n    - Instead of summing up to a specific value (discrete) we integrate over all possible values up to $x$\n    - Probability of having a value less than x\n\n\n\n## ü¶â Integrals\n\nFirst, a brief aside  on integral calculus:\n\nWhat's the area of the rectangle? $base\\times height$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n## ü¶â Integrals\n\nHow would we find the area under a curve?\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## ü¶â Integrals\n\nWell suppose we added up the areas of a bunch of rectangles roughly whose height's approximated the height of the curve?\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nCan we do any better? \n\n\n## ü¶â Integrals\n\nLet's make the rectangles smaller\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get's even better:\n\n\n\n## ü¶â Link between PDF and CDF\n\nIf \n$$F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx $$\n\nThen by the [fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus)\n\n$$\\frac{d}{dx}F(x)=f(x)$$\n\nIn words\n\n- the PDF ($f(x)$) is the derivative (rate of change) of the CDF ($F(X)$)\n\n- the CDF describes the area under the curve defined by f(x) up to x \n\n\n\n## Properties of the CDF\n\n- $0\\leq F(x) \\leq 1$\n\n- $F$ is non-decreasing and right continuous\n\n- $\\lim_{x\\to-\\infty}F(x)=0$\n\n- $\\lim_{x\\to\\infty}F(x)=1$\n\n- For all $a,b \\in \\mathbb{R}$ s.t. $a<b$\n\n$$p(a < X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx $$\n\n\n## Recall the PMF and CDF of a die\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## What's the probability \n\n- $p(X=1)...p(X=6) = 1/6$\n\n- $p( 2 < X \\leq 5) = F(5)-F(2)=5/6-2/6=3/6=1/2$\n\n\n\n## Common Probablity Distirbutions\n\nIn this course, we'll use probability distributions to \n\n- model the data generating process as a function of parameters we can estimate (using Generalized Linear Models)\n\n- perform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)\n\n\nThere are a lot of probability distributions:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.math.wm.edu/~leemis/chart/UDR/BaseImage.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\n\nFortunately, the distributions you need to know to really master data science, are probably more something like\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/4854/1*szMCjXuMDfKu6L9T9c34wg.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nAnd the distributions we'll work with the most in this class are an even smaller subset.\n\n- **Bernoulli**: Coinflips with probability of heads, $p$\n\n- **Uniform**: Coinflip with more than two outcomes\n\n- **Binomial**: Adding up coinflips\n\n- **Poisson**: Counting the total number of events\n\n- **Geometric**: Counting till a specific event occurs \n\n- **Exponential**: Counting till a specific event occurs in continous time\n\n- **Normal**: \n  - The limit of a Binomial distribution as $n\\to \\infty$\n  - The [maximum entropy](https://naokishibuya.medium.com/normal-distribution-demystified-933cf72185d2) when we only know the mean and variance\n\n- **t**: A finite sample approximation of the normal\n\n- $\\chi^2$: Distribution of sums of squared variables from Normal distribution\n\n\n## Bernoulli Random Variables\n\nLet's start with our old friend the coin flip\n\n\nA coin flip is an example of a **Bernoulli random variable** defined by 1 parameter $p$, the probability of success. It has a pmf of\n\n$$f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array} \n    \\right.$$\n\n\nAnd a CDF of \n\n$$F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x<1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x<1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array} \n    \\right.$$\n\nNote that in our coin flip example $p=0.5$ but it need not. Just imagine a weighted coin like the Patriots use at Foxborough\n\n\n## Uniform Distribution\n\nOur fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range $[a,b]$, the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of\n\n$$f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                \\frac{1}{b-a}& \\mathrm{if\\ } a \\leq x\\leq b \\\\\n                0 & \\text{otherwise} \\\\\n        \\end{array} \n    \\right.$$\n\nAnd a CDF:\n\n$$F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                        0 & x <a \\\\\n                \\frac{x-a}{b-a}& \\mathrm{if\\ } a \\leq x < b \\\\\n                1 & x \\geq b \\\\\n        \\end{array} \n    \\right.$$\n\nWe won't run into uniform distributions all that often except in examples like rolling a fair sided die, but often they're used in Bayesian analysis as a form of uninformative prior.\n\n\n## Binomial Distributions\n\nThe binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.\n\nThe key parameters are the number of trials $n$ and the probability of success for each trial $p$ and the pdf of a binomial distribution is:\n\n$$f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n$$\nSo if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nThe binomial distribution will come in handy when trying to model binary outcomes. \n\n\n## Poisson Distributions\n\nWhat would happen if you let the $n$ in a binomial distribution go to infinity and $p$ go to 0 so that $np$ stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter $\\lambda$ which represents rate at which events occur.\n\n\n$$f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}$$\n\nWe use these distributions to try and predict to predict the [probability of a given number of events occurring in a fixed interval of time.](https://towardsdatascience.com/poisson-distribution-intuition-and-derivation-1059aeab90d) Things like how many acts of political participation would a voter engage in over a year. \n\n\n  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\n## Geometric Distributions\n\nWhat if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:\n\n$$f(x)=p(1-p) ^{x}$$\n\nGeometric and related distributions are useful for describing the time until an event occurs\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## Exponential Distributions\n\nTaking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a $\\lambda = \\frac{1}{\\beta}$ rate parameter\n\n$$f(x)=\\frac{1}{\\beta}\\exp\\left[-x/\\beta\\right]$$\n\n[Cioffa-Revilla (1984)](https://www.jstor.org/stable/1963367) uses an exponential distribution to model the stability of Italian governments.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n## Normal Distribution\n\nFinally, there's the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter $\\mu$ that determines the center of a distribution and a scale parameter $\\sigma^2$ that determines the spread of a distribution\n\n$$f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]$$\n\n\nStandard normal: $X \\sim N(\\mu =0,\\sigma^2=1)$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n- As we'll see normal distributions tend to arise when ever you're summing variables. \n\n- That is sum together a bunch of values from almost any distribution and the **distribution of their sums** tends to follow a normal distribution. \n\n- Since lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)\n\n\n\nConsider a binomial distribution with N=100 and p=.5. \n\nThe pmf of this variable (black lollipops) follows a distribution that's closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.\n\nA relationship explained more generally by the Central Limit Theorem, which we'll cover next week.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\n### What's the $p(X \\leq 0)$ for a normal distirbution with mean 0 and sd 1\n\nSince the normal distribution is so common, it's useful to get practice working with it's pdf and cdf.\n\nConsider the following question: If X is normally distributed variable with $\\mu=0$ and $\\sigma=1$, what's the probability that X is less than 0  $p(X\\leq0)=?$ We could solve:\n\n$$\\int_{-\\infty}^{0}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.5$$\n\n\n\n\nBut R's `pnorm()` function will quickly tell us\n\n- $p(X\\leq0)=$ 0.5\n\nAnd we can visualize this as follows:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nConsider some other questions?\n\n- $p(X=0)=0$\n  - The probability that a continuous variable is exactly some value is always 0.\n- $p(X<0)=0.5$\n- $p(-1< X< 1)$\n- $p(-2< X< 2)$\n\n\n### p(-1 < X <  1)\n\n\n- $p(-1< X< 1)=pr(X<1)-pr(X<-1)$ \n\n$$\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.841-0.158=0.682$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/norm1sd-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n### p(-2 < X < 2)\n\n- $p(-2< X\\leq 2)=$ 0.9544997\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/norm2sd-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nWe'll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0.\n\n\n\n\n## Expected Value\n\nA (probability) weighted average of the possible outcomes of a random variable, often labeled $\\mu$ \n\nDiscrete:\n\n$$\\mu_X=E(X)=\\sum xp(x)$$\n\nContinuous\n\n$$\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx$$\n\n\n## What's the expected value of a 1 roll of fair die?\n\n$$\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}$$\n\n\n## Properties of Expected Values\n\n- $E(c)=c$\n\n- $E(a+bX)=a+bE[X]$\n\n- $E[E[X]]=X$\n\n- $E[E[Y|X]]=E[Y]$\n\n- $E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx$\n\n- $E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)$\n\n- $E[XY]=E[X]E[Y]$ if $X$ and $Y$ are independent\n\n\n## Variance\n\nIf $X$ has a finite mean $E[X]=\\mu$, the $E[(X-\\mu)^2]$ is finite and called the variance of $X$ which we write as $\\sigma^2$ or $Var[X]$.\n\nNote:\n\n$$\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}$$\n\n- \"The variance of X is equal to the expected value of X-squared, minus the square of X's expected value.\"\n- $\\sigma^2=E[X^2]-E[X]^2$ is a useful identity in proofs and derivations\n\n\n## Variance and Standard Deviations\n\nWe often think of variances $Var[X]$ as describing the spread of a distribution\n\n$$\\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2$$\n\nA standard deviation is just the square root of the variance\n\n$$\\sigma=\\sqrt{Var[X]}$$\n\n\n## Covariance\n\nCovariance measures the degree to which two random variables vary together. \n\n- $Cov[X,Y] \\to +$ An increase in $X$ tends to be larger than its mean when $Y$ is larger than its mean\n\n$$Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]$$\n\n\n## Properties of Variance and Covariance\n\n- $Cov[X,Y]=E[XY]-E[X]E[Y]$\n\n- $Var[X]=E[X^2]-(E[X])^2$\n\n- $Var[X|Y]=E[X^2|Y]-(E[X|Y])^2$\n\n- $Cov[X,Y]=Cov[X,E[Y|X]]$\n\n- $Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]$\n\n- $Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]$\n\n\n\n## Correlation\n\n- The correlation between $X$ and $Y$ is simply the covariance of $X$ and $Y$ divided by the standard deviation of each.\n\n$$\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}$$\n\n- Normalize covariance to a scale that runs between [-1,1]\n\n\nclass:inverse, center, middle\n# üí°\n# The Law of Large Numbers\n\n\n## The Law of Large Numbers (Intuitive)\n\nSuppose we wanted to know the average height of our class. \n\nWe could pick someone at random, measure their height and get an estimate. It would be a pretty bad estimate (it would vary a lot from person to person), but it would be an unbiased estimate\n\nHow would we improve our estimate?\n\n\n\n\n\n\n\n## The Law of Large Numbers (Intuitive)\n\nSuppose we increased our sample size from N=1 to N = 5. \n\nNow our estimate reflects the average of 5 people's heights as opposed to just 1. Both are are unbiased estimates of the truth, but the N=5 sample has a lower variance. \n\n--\n\nNow suppose we took a sample of size N = N-1. That is we measured everyone except one person. Our estimate will be quite close to the truth, varying slightly based on the height of the person left out.\n\n--\n\nFinally we took a sample of size N = 24 (e.g. the class size). Since our sample is the population, our estimate will be exactly equal to to the population. Each sample will give us the same \"true\" value. That is, it wil not vary at all.\n\n--\n\nThe idea that as the sample size increases, the distance of a sample mean from the population mean $\\mu$ goes to 0 is called the **Law of Large Numbers**\n\n\n\n## The (Weak) Law of Large Numbers (Formally)\n\nLet $X_1, X_2, \\dots$ be independent and identically distributed (i.i.d.) random variables with mean $\\mu$ and variance $\\sigma^2$. \n\nThen for every $\\epsilon>0$, as the sample size increases (1), the distance of a sample mean from the population mean $\\mu$ (2) goes to 0 (3).\n\n\n$$\\overbrace{Pr(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| > \\epsilon)}^{\\text{2. The distance of the sample mean from the truth}} \\overbrace{\\to 0}^{\\text{3. Goes to 0}} \\underbrace{\\text{ as }n \\to \\infty}_{\\text{1. As the sample size increases}}$$\n\nEquivalently:\n\n$$\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| < \\epsilon) = 1$$\n\n\n\n## üí™ Simulating the LLN\n\nRhe expected value of rolling a die  3.5.\n\n$$ E[X] = \\Sigma x_ip(X=x_i) = 1/6 * (1+2+3+4+5+6)$$\n\nIn terms of the LLN, think of our sample size as the number of times we roll a die. \n\nIf we rolled the die just once and took the average of our role, we could get a 1, 2, 3, 4, 5, or 6. which would be pretty far from our expected value of 3.5\n\nIf we rolled the die two times and took an average, we could still get an value of 1 or 6 for average, but values closer to our expected value of 3.5, happen more often\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the average from 2 rows\ntable(rowMeans(expand.grid(1:6, 1:6)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  1 1.5   2 2.5   3 3.5   4 4.5   5 5.5   6 \n  1   2   3   4   5   6   5   4   3   2   1 \n```\n\n\n:::\n:::\n\n\n\n\nAs we increase our sample size (roll the die more times), the LLN says the chance that our sample average is far from the truth $(p(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| > \\epsilon))$, gets vanishingly small.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndie <- 1:6\nroll_fn <- function(n) {\n  rolls <- data.frame(rolls = sample(die, size = n, replace = TRUE))\n  # summarize rolls \n  df <- rolls %>%\n    summarise(\n    # number of rolls\n      n_rolls = n(),\n    # number of times 1 was rolled\n      ones = sum(rolls == 1),\n    # number of times 2 was rolled, etc..\n      twos = sum(rolls == 2),\n      threes = sum(rolls == 3),\n      fours = sum(rolls == 4),\n      fives = sum(rolls == 5),\n      sixes = sum(rolls == 6),\n      # Average of all our rolls\n      average =  mean(rolls),\n      # Absolute difference between averages and rolls\n      abs_error = abs(3.5-average)\n    )\n  # Return summary df\n  df\n}\n```\n:::\n\n\n\n\nThen we could use a for-loop to simulate rolling our die once and calculating the average all the way up to rolling our die a 1000 times.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Holder\nsim_df <- NULL\n\n# Set seed\nset.seed(123)\n\nfor(i in 1:1000){\n  sim_df <- rbind(sim_df,\n                  roll_fn(i)\n  )\n}\n```\n:::\n\n\n\n\nWith only a few rolls, our average bounces around a lot\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(sim_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  n_rolls ones twos threes fours fives sixes  average abs_error\n1       1    0    0      1     0     0     0 3.000000 0.5000000\n2       2    0    0      1     0     0     1 4.500000 1.0000000\n3       3    0    2      0     0     0     1 3.333333 0.1666667\n4       4    0    0      1     1     1     1 4.500000 1.0000000\n5       5    1    1      1     0     1     1 3.400000 0.1000000\n6       6    3    0      2     1     0     0 2.166667 1.3333333\n```\n\n\n:::\n:::\n\n\n\nWith a lot of rolls, our average is very close to 3.5\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntail(sim_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     n_rolls ones twos threes fours fives sixes  average  abs_error\n995      995  197  160    151   154   171   162 3.430151 0.06984925\n996      996  184  164    176   149   175   148 3.412651 0.08734940\n997      997  163  159    170   163   171   171 3.534604 0.03460381\n998      998  162  163    142   173   185   173 3.576152 0.07615230\n999      999  209  154    151   154   163   168 3.412412 0.08758759\n1000    1000  181  189    147   179   146   158 3.394000 0.10600000\n```\n\n\n:::\n:::\n\n\n\n\n\nLet's visualize see how our average changes with the number of rolls, using `ggplot()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_die_lln <- ggplot(sim_df, aes(n_rolls, average))+\n  geom_line()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/p_die_lln-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nYour turn! Plot how the absolute value of the error changes as the number of rolls increases. Does it increase or decrease? How does the rate at which it goes up or down seem to change?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Write your code here:\n```\n:::\n\n\n\n\n\nclass: inverse, center, middle\n#ü¶â\n## ICYI: Proving the Weak LLN\n\n\n##  Proving the Weak LLN\n\nA proof of the LLN is as follows:\n\nFirst define $U$ such that its a sample mean for sample of size $n$\n\n$$U=\\frac{X_1+\\dots +X_n}{n}$$\n\n\n## Proving the Weak LLN\n\nThen show that the sample mean, $U$ is an unbiased estimator of the population mean $\\mu$\n\n$$\\begin{align*}\nE[U]&=E[\\frac{X_1+\\dots +X_n}{n}]=\\frac{1}{n}E[X_1+\\dots +X_n]\\\\\n&=\\frac{n\\mu}{n}=\\mu\n\\end{align*}$$\n\nWith a variance\n\n$$\\begin{align*}\nVar[U]&=Var[\\frac{X_1+\\dots +X_n}{n}]=\\\\\n    &=Var[\\frac{X_1}{n}]\\dots Var[\\frac{+X_n}{n}]\\\\\n    &\\frac{\\sigma^2}{n^2}\\dots \\frac{\\sigma^2}{n^2}\\\\\n    &\\frac{n \\sigma^2}{n^2}\\\\\n    &\\frac{\\sigma^2}{n}\\\\\n\\end{align*}$$\n\nThat decreases with N.\n\n\n\n## Proving the Weak LLN\n\nThen, by [Chebyshev's inequality](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality), a theorem specifying, for a given distribution, the maximum fraction of values that can be some distance from that distribution's mean:\n\n$$Pr(\\left|U-\\mu\\right| > \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2}$$\n\nWhich $\\to 0$ as $n \\to \\infty$\n\n\n## The Strong Law of Large Numbers\n\nAs you may have inferred, there is a weak law of large numbers and a strong law of large numbers.\n\nThe weak law of large numbers states that as the sample size increases, the sample mean [converges in probability](https://en.wikipedia.org/wiki/Convergence_in_probability) to the population value $\\mu$\n\n$$\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| < \\epsilon) = 1$$\n\nThe strong law of large numbers states that as the sample size increases, the sample mean [converges almost surely](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Almost_sure_convergence) to the population value $\\mu$\n\n$$\\lim_{n \\to \\infty} Pr(|\\bar{X}_n = \\mu|) = 1$$\nThe [differences in types of convergence](https://en.wikipedia.org/wiki/Law_of_large_numbers#Differences_between_the_weak_law_and_the_strong_law) won't matter much for us in this course\n\n\nclass:inverse, center, middle\n# Break\n\n\n\n\nclass:inverse, center, middle\n# üí°\n## The Central Limit Theorem\n\n\n\nSo the LLN tells us that as our sample size grows, an unbiased estimator like the sample average, will get increasingly close to the to the \"true\" value of the population of mean. \n\nIif we took a bunch of samples of the same size and calculated the mean of each sample:\n\n- the distribution of those sample means (*the sampling distribution*) would be centered around the truth (because the estimator is unbiased).\n- the width of the distribution (its variance) would decrease as we increased the size of each sample (by the LLN)\n\nThe Central Limit Theorem tells us about the shape of that distribution. \n\n\n\n## Review: Z-scores and Standardization\n\nGiven a R.V. $X$ with mean $\\mu$ and standard deviation $\\sigma$, we can define a new R.V. $Z$ as the *standardization* of $X$:\n\n$$Z=\\frac{X-\\mu}{\\sigma}$$\n\nWhere Z has $\\mu=0$ and $\\sigma=1$.\n\n\n\n## Notation for the CLT\n\nNext let's define some variables $S$ and $\\bar{X}$ that are the sum $(S)$ and sample mean $(\\bar{X})$ of $n$ iid draws of $X$\n\nLet $X_1,X_2,\\dots,X_n$ be independent and identically distributed RVs with mean $\\mu$ and standard deviation $\\sigma$. \n\nDefine $S_n$ and $\\bar{X}_n$ as follows:\n\n$$S_n= X_1,X_2,\\dots,X_n= \\sum_{i=1}^n X_i$$\n\n$$\\bar{X}=\\frac{X_1,X_2,\\dots,X_n}{n}= \\frac{S_n}{n}$$\n\n\n\n## Additional facts for the CLT\n\nWe can show that:\n\n$$\\begin{alignat*}{3}\nE[S_n]&=n\\mu \\hspace{2em}Var[S_n]&=n\\sigma^2 \\hspace{2em} \\sigma_S&=\\sqrt{n}\\sigma\\\\\nE[\\bar{X}_n]&=\\mu \\hspace{2em}Var[\\bar{X}_n]&=\\frac{\\sigma^2}{n} \\hspace{2em}\\sigma_{\\bar{X}}&=\\frac{\\sigma}{\\sqrt{n}}\\\\\n\\end{alignat*}$$\n\nBasically: the expected value and variance of the sum is just $n$ times the population parameters (the true values for the distribution). \n\nSince the mean is just the sum divided by the sample size, the expected value of the mean is equal to the population value and the variance and standard deviations of the mean are decreasing in $n$.\n\nFinally, we can define $Z$ to be a function of either $S$ or $\\bar{X}$\n\n$$Z_n=\\frac{S_n-n\\mu}{\\sqrt{n}\\sigma}=\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}$$\n\n\n## Central Limit Theorem\n\nFor a *sufficiently large* $n$\n\n$$\\begin{align*}\n\\bar{X_n}&\\approx N(\\mu,\\sigma^2/n) \\\\ \n\\bar{S_n} &\\approx N(n\\mu,n\\sigma^2) \\\\\n\\bar{Z_n}&\\approx N(0,1) \n\\end{align*}$$\n\n- The distribution of means $(\\bar{X_n})$ from almost any distribution $X$ is approximately normal (converges in distribution), but with a smaller variance than ($\\sigma^2/n$)\n\n\n- Proof: [Several ways](https://towardsdatascience.com/central-limit-theorem-proofs-actually-working-through-the-math-a994cd582b33), but requires a little more math than is required for this course\n\n\n## CLT: Why it matters\n\nWhy is this result so important? \n\nWell lots of our questions come of the form, how does a typical value of Y vary with X. \n\nWe may not know the true underlying distribution of Y, but we can often approximate the distribution of a typical value of Y $(E[Y])$ using a normal distribution. \n\n\n## Simulating the CLT\n\nFor almost any distribution, the distribution of means from a sample of that distribution will converge to some Normal distribution. \n\nLet's consider a decidedly *non-Normal* [Binomial distribution:](https://en.wikipedia.org/wiki/Binomial_distribution) with p = 0.2.\n\nThe expected value of Binomial Distribution $X \\sim B(n,p)$ is $E[X] = n*p$.\n\nIf we were to flip a coin 20 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 4.\n\nIf we were to flip a coin 100 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 20.\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/binom20-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n### Simulating 10,000 draws from Binomial Distributions of Different Sizes \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Probability of success\np <- .2\n# Sample sizes\nsamp_sizes <- c(20, 50, 100,1000)\n# Number of simulations\nnsims <- 10000\n# Holder for simulations\ndf_sim <- tibble(\n  expand_grid(\n    samp_size = samp_sizes,\n    sim = 1:nsims,\n    sample_mean = NA\n  )\n)\n```\n:::\n\n\n\n\n\n### Simulating 1,000 draws from Binomial Distributions of Different Sizes \n\nBelow we loop through each sample size in `samp_sizes`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfor(i in samp_sizes){\n  df_sim$sample_mean[df_sim$samp_size == i] <- replicate(nsims, i*mean(rbinom(i, 1, p)))\n  \n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/binomsim20-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/binomsim50-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/binomsim100-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/binomsim1000-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n\nFinally, let's consider a decided non normal distribution:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndist <- sample(18:80,size=10000, replace = T, prob = runif(length(18:80)))\n\nsamp_mean25 <- replicate(10000,mean(sample(dist,25, replace=F)))\nsamp_mean100 <- replicate(10000,mean(sample(dist,100, replace=F)))\nsamp_mean500 <- replicate(10000,mean(sample(dist,500, replace=F)))\n\nex_df <- tibble(\n  distribution = dist,\n  samp_mean25 = samp_mean25,\n  samp_mean100 = samp_mean100,\n  samp_mean500 = samp_mean500\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/clt1 -1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/clt2 -1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/clt3 -1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/clt4 -1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## Summary\n\n\n\n- So we see that our sampling distributions are centered on the truth, and as the sample size increases, the width of the distribution decreases (Law of Large Numbers)\n\n- The shapes of distributions of sample means can be approximated by a Normal Distribution $\\bar{X} \\sim N(\\mu, \\sigma^2/n)$\n\n\nclass: inverse, center, middle\n#ü¶â\n## ICYI: Maximum Likelihood Estimation\n\n\n# Maximum Likelihood Estimation\n\n\nThe LLN and CLT lie behind many important proofs and theorems in statistics such as **maximum likelihood estimation (MLE)**\n\nBroadly, MLE seeks to find parameters $\\theta$ for model of some data generating process (i.e. a probability distribution), that are most probable (i.e. maximize the likelihood) given some data.\n\n\n\n## ü¶â Maximum Likelihood Estimation\n\nFormally, consider $n$ iid random variables $X_1, X_2, \\ldots X_n$. We can then write their **likelihood** as\n\n$$\\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta)$$\n\nwhere $f(x_i; \\theta)$ is the density (or mass) function of random variable $X_i$ evaluated at $x_i$ with parameter $\\theta$.\n\nMLE tries to find $\\hat{\\theta}_{MLE}$ that maximizes $\\mathcal{L}(\\theta \\mid X)$\n\n\n\n## ü¶â Properties of Maximum Likelihood Estimators\n\nMLE Estimators are \n\n- **Functionally Invariant** (The \"Plug in Principle\")\n    - If $\\hat{\\theta}$ is the MLE of $\\theta$ than then the MLE of some function of $\\theta$, $f(\\theta)$ is $f(\\hat\\theta_{MLE})$\n    - If we have the MLE of the variance, the square root of this will give us the MLE of the standard deviation\n- **Consistent** (by the LLN)\n  - $\\hat\\theta_{MLE}$ collapses to a spike over $\\theta$ as $n \\to \\infty$\n- **Asympotically Normal** (by the CLT)\n  - A $n \\to \\infty$ the sampling distribution of $\\hat\\theta_{MLE}$ becomes Normally distributed\n  - Makes calculating quantities for inference easy\n- **Asympotically Efficient**\n  - As $n \\to \\infty$, $\\hat\\theta_{MLE}$ tends to be the estimator with the lowest error\n\n\n\nclass: inverse, center, middle\n# üí°\n# Generalized Linear Models\n\n\n## Generalized Linear Models\n\n- OLS provides a linear estimate to the conditional mean function\n\n--\n\n- If the conditional mean function is linear and the errors are normally distributed, OLS is the MLE.\n\n--\n\n- What if the conditional mean function is non-linear?\n\n--\n\n- Sometimes we can transform the mean function so that it is linear, and estimate a generalized linear model (GLM) using MLE\n\n--\n\n- Using a GLM often produces more \"reasonable\" estimates, and can make more efficient use of the data, although there are many cases where a linear estimate to conditional mean function works just fine (or better)\n\n\n## MLE and Generalized Linear Models\n\nWe can think some variable $y$ as having a distribution $f$ that contains both a stochastic (random) and systematic components\n\n$$\\begin{aligned}\n\\text{Stochastic:    }&& y \\sim f(\\mu,\\alpha)\\\\\n\\text{Systematic:    }&&\\mu = g(X\\beta)\n\\end{aligned}$$\n\n\n## MLE and Generalized Linear Models\n\nIn the past we've described the process of modeling $y$ using a linear regression:\n\n$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n\n\nand with multiple predictors:\n\n$$y = X\\beta + \\epsilon$$\n\n\n## MLE and Generalized Linear Models\n\nWe haven't really talked about the distribution of $\\epsilon$, in part because OLS doesn't require any distributional assumptions to be unbiased.\n\nBut if we assumed $\\epsilon$ are normally distributed, with mean 0 and variance $\\sigma^2$\n\n$$\\epsilon \\sim f_\\mathcal{N}(0,\\sigma^2)$$ \n\nThen we could write our model for $y$ as follows:\n\n$$\\begin{aligned} y &\\sim f_{\\mathcal{N}}(\\mu,\\sigma^2)\\\\\n\\mu &= X\\beta\\end{aligned}$$\n\nWhere the systematic component of why is modeled by $X\\beta$ (i.e. g() is the identity function), with errors that are Normally distributed. \n\nThe $\\beta$s that OLS estimates turn out to be the same values that would get by maximizing the likelihood of this function, given our data, $X$, assuming normally distributed errors.\n\n\n## Generalized Linear Models\n\n\n**But what if our outcome doesn't follow a normal distribution?**\n\n\nSay for example, we have a binary outcome,that we think follows a Bernoulli distribution with $\\pi$ probability of success. \n\nWe could model the *systematic* portion of this using the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), $g()$\n\n$$\\begin{aligned}y &\\sim f_{Bern}(\\pi)\\\\\n\\pi &= \\frac{1}{1+\\exp(-{X\\beta})}\\end{aligned}$$\n\nAgain, we could estimate $\\beta$ using the MLE to fit a logistic regression.\n\n\n\n## MLE and Generalized Linear Models\n\nOr if we had a count variable, we might use a Poisson distribution:\n\n$$\\begin{aligned}y &\\sim f_{Pois}(\\lambda)\\\\\n\\lambda &= \\exp(X\\beta)\\end{aligned}$$\n\nAgain estimating $\\beta$ using MLE.\n\nIn this class, we'll let R handle mechanics of actually fitting these models,\nand instead focus on interpreting their substantive differences\n\n\n\n## OLS vs Logistic Regression\n\n\nOne situation where we'd use MLE is the case of binary responses variable coded using $0$ and $1$. \n\nIn practice, these $0$ and $1$s will code for two classes such as yes/no, non-voter/voter,, etc.\n\nHow should we model this relationship?\n\nWe could use OLS to produce a linear estimate of the conditional mean function $(\\text{E}[Y \\mid {\\bf X} = {\\bf x}])$, by finding $\\beta$s that minimize the sum of squared errors\n\nOr\n\nWe could use a logistic regression, to produce a linear estimate of the \"log-odds\" of the conditional mean function of our binary variable by finding $\\beta$s that maximize the likelihood of this function.\n\n\n\nLet's simulate data from the following model:\n\n$$\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -2 + 3 x$$\n\nWe'll codify this into a function:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {\n  x = rnorm(n = sample_size)\n  eta = beta_0 + beta_1 * x\n  p = 1 / (1 + exp(-eta))\n  y = rbinom(n = sample_size, size = 1, prob = p)\n  data.frame(y, x)\n}\n```\n:::\n\n\n\n\nAnd use it to generate some data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nexample_data = sim_logistic_data()\nhead(example_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  y          x\n1 0 -0.6264538\n2 1  0.1836433\n3 0 -0.8356286\n4 1  1.5952808\n5 0  0.3295078\n6 0 -0.8204684\n```\n\n\n:::\n:::\n\n\n\n\nAfter simulating a dataset, we'll then fit both ordinary linear regression and logistic regression.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ordinary linear regression\nfit_lm  = lm(y ~ x, data = example_data)\n# logistic regression\nfit_glm = glm(y ~ x, data = example_data, family = binomial)\n```\n:::\n\n\n\nNotice that the syntax is extremely similar. What's changed?\n\n- `lm()` has become `glm()`\n- We've added `family = binomial` argument\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n<table class=\"texreg\" style=\"margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;\">\n<caption>Statistical models</caption>\n<thead>\n<tr>\n<th style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</th>\n<th style=\"padding-left: 5px;padding-right: 5px;\">Model 1</th>\n<th style=\"padding-left: 5px;padding-right: 5px;\">Model 2</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"border-top: 1px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">(Intercept)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.31<sup>&#42;&#42;</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">-2.31<sup>&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.08)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(1.13)</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">x</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.30<sup>&#42;&#42;</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">3.66<sup>&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.09)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(1.65)</td>\n</tr>\n<tr style=\"border-top: 1px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">R<sup>2</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.34</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Adj. R<sup>2</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.31</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Num. obs.</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">25</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">25</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">AIC</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">22.74</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">BIC</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">25.18</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Log Likelihood</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">-9.37</td>\n</tr>\n<tr style=\"border-bottom: 2px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">Deviance</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">18.74</td>\n</tr>\n</tbody>\n<tfoot>\n<tr>\n<td style=\"font-size: 0.8em;\" colspan=\"3\"><sup>&#42;&#42;&#42;</sup>p &lt; 0.001; <sup>&#42;&#42;</sup>p &lt; 0.01; <sup>&#42;</sup>p &lt; 0.05</td>\n</tr>\n</tfoot>\n</table>\n:::\n\n\n\n\n\n\nMaking predictions with an object of type  `glm` is slightly different than making predictions after fitting with `lm()`. \n\nIn the case of logistic regression, with `family = binomial`, we have:\n\n| `type`             | Returned |\n|--|-|\n| `\"link\"` [default] | $\\hat{\\eta}({\\bf x}) = \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right)$ |\n| `\"response\"`       | $\\hat{p}({\\bf x})$                                                                     |\n\nThat is, `type = \"link\"` will get you the **log odds**, while `type = \"response\"` will return $P[Y = 1 \\mid {\\bf X} = {\\bf x}]$ for each observation.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(y ~ x, data = example_data, \n     pch = 20, ylab = \"Estimated Probability\", \n     main = \"Ordinary vs Logistic Regression\")\nabline(fit_lm, col = \"darkorange\")\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \n      add = TRUE, col = \"dodgerblue\", lty = 2)\nlegend(\"topleft\", c(\"Ordinary\", \"Logistic\", \"Data\"), lty = c(1, 2, 0), \n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-slides_files/figure-html/unnamed-chunk-53-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## OLS vs Logistic Regression\n\n- OLS produces impossible predictions\n\n- The coefficients from logistic regression aren't directly interpertable $\\to$ need predicted values.\n  - Can also calculate things like [odds-ratios](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/) but I find this convoluted.\n\n- The marginal effect of $X$ varies in a logistic regression\n\n\n## Interpreting Logistic Regression Coefficients\n\nOur estimated model is then:\n\n$$\\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right) = -2.3 + 3.7 x$$\n\nBecause we're not directly estimating the mean, but instead a function of the mean, we need to be careful with our interpretation of $\\hat{\\beta}_1 = 3.7$. \n\nThis means that, for a one unit increase in $x$, the log odds change (in this case increase) by $3.7$. Also, since $\\hat{\\beta}_1$ is positive, as we increase $x$ we also increase $p({\\bf x})$. \n\n\n\nFor example, we have:\n\n$$\\hat{P}[Y = 1 \\mid X = -0.5] = \\frac{e^{-2.3 + 3.7 \\cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \\cdot (-0.5)}} \\approx 0.016$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(fit_glm, newdata = data.frame(x=-0.5), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1 \n0.01567416 \n```\n\n\n:::\n:::\n\n\n\n\n$$\\hat{P}[Y = 1 \\mid X = 0] = \\frac{e^{-2.3 + 3.7 \\cdot (0)}}{1 + e^{-2.3 + 3.7 \\cdot (0)}} \\approx 0.09$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(fit_glm, newdata = data.frame(x=0), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1 \n0.09016056 \n```\n\n\n:::\n:::\n\n\n\n$$\\hat{P}[Y = 1 \\mid X = 1] = \\frac{e^{-2.3 + 3.7 \\cdot (1)}}{1 + e^{-2.3 + 3.7 \\cdot (1)}} \\approx 0.38$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(fit_glm, newdata = data.frame(x=.5), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1 \n0.3814476 \n```\n\n\n:::\n:::\n\n\n\n\n\nbackground-image:url(\"https://resourcemoon.com/wp-content/uploads/2018/09/summery.png\")\nbackground-size:cover\n\n\n## Summary\n\n- The Law of Large Number's says that as our sample size increases, our sample mean will converge to the  population value\n\n- The Central Limit Theorem says that the distribution of those sample means will follow a normal distribution \n\n- Generalized Linear Models allow us to more accurately model different types of data-generating processes using Maxium Likelihood Estimation.\n\n",
    "supporting": [
      "09-slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}