{
  "hash": "de342efcb248f5d52a6980f4bb708334",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"POLS 1600\"\nsubtitle: \"Probably too much Probability\"\ndate: last-modified\ndate-format: \"[Updated ]MMM D, YYYY\"\nformat: \n  revealjs:\n    theme: brownslides.scss\n    logo: images/pols1600_hex.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: true\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    chalkboard: true\n    # include-after-body: title-slide.html\n    title-slide-attributes:\n      align: left\n      data-background-image: images/pols1600_hex.png\n      data-background-position: 90% 50%\n      data-background-size: 40%\nfilters:\n  - openlinksinnewpage\n  - webr\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  message: false\n  cache: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n# {{< fa map-location>}} Overview {.inverse}\n\n## Class Plan {.smaller}\n\n- Announcements (5 min)\n- Feedback (5 min)\n- Class plan\n  - Probability (10 min)\n  - Conditional Probability (10 min)\n  - Probability Distributions (10 min)\n  - Expected Values and Variances (10 min)\n  - Standard Errors (10 min)\n  - Previewing Lab 8 (10 min)\n\n\n## Annoucements: Assignment 2 {.smaller}\n\nFull prompt [here](https://pols1600.paultesta.org/assignments/a2)\n\n1. A revised description of your group's research project\n2. A description of a linear model implied by your question\n3. R code that loads some potentially relevant data to your question and at least one descriptive summary of that data.\n4. Some information about your group such as:\n    - A group name^[If you're Group 01 don't change your name to Group 4]\n    - A group color or color scheme\n    - A group motto, mascot, crest, etc.\n    - Your group's theme song\n    - Your group's astrological sign\n    - Anything else that you think well help you form strong ingroup bounds that facilitate collaboration\n\n## Setup: Packages for today\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Pacakges for today\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE \n```\n\n\n:::\n:::\n\n\n\n## Feedback\n\n![](https://media.tenor.com/rAdl6RX2ci4AAAAe/spongebob-oh-brother-this-guy-stinks.png)\n\n\n::: {.cell}\n\n:::\n\n\n\n## What did we like {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-7764880f9457323167e1\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-7764880f9457323167e1\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"],[\"I really liked the reading for this lab I thought it was interesting.\",\"I have experience using online written research, but this was the first time I have used online datasets. I found that to be a really cool \\\"portal of discovery.\\\"\",\"I feel like I'm really starting to get the hang of coding, especially when it comes to \\\"wrangling\\\" our data and using things like mutate and case_when. Linear regressions are interesting (but a little confusing).\",\"Learning diff-in-diff and unique variation was surprisingly understandable from a conceptual angle\",\"I found the instructions on the lab to be clear and useful in order to do the lab\",\"How the lab was concise enough to fully understand each step rather than rapidly copying and pasting code.\",\"learning how to download the data\",\"I like how in the lab you gave us time to work on our own but also gave us guidance of kinda where to look.\",\"I liked the shorter lab. It really enabled us to focus on a smaller amount of code in more detail. I also liked that you gave us feedback on assignment 1! \\n\",\"I liked the assignment- it was fun to think about statistics in a way that didn't involve math\",\"I liked that the lab was shorter this week as I didn't feel as overwhelmed by it\",\"\",\"\",\"Linear regression is such a cool tool\",\"\",\"I liked being able to have some independence by working through code with our small groups\\n\",\"I thought the lab was followable and I appreciate when we walk through things together.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Likes<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Likes\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## What did we dislike {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-e6d33e9427d9f67bc8ce\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e6d33e9427d9f67bc8ce\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"],[\"Downloading the data myself was an interesting \\\"portal of discovery\\\".\",\"My own lack of understanding, which is completely within my control.\",\"Nothing really to improve, I think it would be helpful to talk a little bit more about linear regressions and difference in difference analyses.\",\"Lots of math! Without explaining the one-to-one translations of equation elements to what happens in the code. Or maybe we did and I just didn't pay attention\",\"I think one of my group members is not participating in the labs, but I do not want to ask them directly to \\\"please work on it\\\" since it would probably not create a little hostile environment where we won't be able to do the project\",\"No major complaints. Wasn't a huge fan of the Metallica suggestion though.\",\"\",\"Again, it gets really stressful toward the end of class when we haven’t finished the lab and we are rushing to do so. I think more independent work time could be good.\",\"I wish there were a more outlined syllabus with required readings for every week, so that we could start doing homework over the weekend, and not only after Tuesday's class.\",\"i did not like the math. i tried that damn tutorial like 8 times and it didnt work and i have never been so annoyed. i even asked my dad and he could give me an answer but had no idea how he got it. how is math stumping dads now??? please.\",\"I was frustrated that we spent so long going over the survey results instead of getting to the material during Tuesday's class\",\"How quickly things move\",\"\",\"Loading the data was difficult\",\"\",\"I am still having trouble understanding the longer code chunks especially when it comes to manipulating and adjusting data\",\"I had several errors and even when I thought they were fixed they weren’t which put me really far behind. I realize there isn’t really anything that can be done about it but I find it really frustrating when I have copied the code exactly.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Dislikes<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Dislikes\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## Our advice for POLS 1600 {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"htmlwidget-16b1ee2f517568e6901d\" style=\"width:100%;height:90%;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-16b1ee2f517568e6901d\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"fillContainer\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"],[\"Be patient with yourself, ask questions, and ask for help.\",\"To a prospective student:\\n\\nI would say that this is an exciting class with an even more exciting professor, which makes all the difference. I would say that what you take from this class is completely within your control and that you can leave POLS 1600 with varying levels of R competency depending on what you are willing to put in.\",\"I think I'm starting to learn is the worst that can happen is your code doesn't work, so when in doubt, try it out! It's much easier to get to the right answer if you know where you're going wrong in your code (and thinking), so don't be afraid of getting things wrong.\",\"It helps to go back, practice code (even if it's like 1 chunk only) and really let the info soak for a while\",\"- make sure to do the reading/data downloads beforehand\\n- please work on the lab and not something else while in lab\",\"Don't let initial frustration deter you from taking the class\",\"\",\"I think reading the lab ahead of time is quite helpful. Especially this week, reading the lab and reading the paper made it so I pretty good understanding of what was going on.\",\"This class, more than others, really comes down to how much effort you put into it. If you want to learn to code, you need to be proactive about it\",\"GO TO OFFICE HOURS truly Prof. Testa is the only way i am surviving and the man has the patience of a saint because i feel stupid as heck and i leave office hours feeling a solid 50% less dumb. i swear I'm not just saying this for brownie points! but if i get some extra credit for this ill take it bc you know i need it\",\"I think its really useful to refer to the textbook and the code examples they provide\",\"It is harder than it seems so requires more effort and time. Asking questions.\",\"Keep trying, even if it’s not exactly right you did something.\",\"Read the book\",\"\",\"the most important part is showing up to class ready to pay attention and lock in.\",\"I would say just to be patient and realize sometimes the computers run things or have them setup differently.\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Advice<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Advice\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## Our Fashion Advice\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Option </th>\n   <th style=\"text-align:left;\"> Choice </th>\n   <th style=\"text-align:right;\"> Votes </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Jacket </td>\n   <td style=\"text-align:left;\"> Tuxedo </td>\n   <td style=\"text-align:right;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Palette </td>\n   <td style=\"text-align:left;\"> Spring (Warm + Light) </td>\n   <td style=\"text-align:right;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pant </td>\n   <td style=\"text-align:left;\"> Khakis </td>\n   <td style=\"text-align:right;\"> 6 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Patterns </td>\n   <td style=\"text-align:left;\"> How 'bout a fun graphic or print </td>\n   <td style=\"text-align:right;\"> 7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shoe </td>\n   <td style=\"text-align:left;\"> Crocs </td>\n   <td style=\"text-align:right;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Tie </td>\n   <td style=\"text-align:left;\"> Bow tie </td>\n   <td style=\"text-align:right;\"> 12 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Top </td>\n   <td style=\"text-align:left;\"> Sports jersey </td>\n   <td style=\"text-align:right;\"> 8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n##\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Naive professor in class survey: &quot;Help me with my fit&quot;<br>Students: &quot;Ok, bet. Tuxedo, crocs, and a jersey&quot; <a href=\"https://t.co/NPmBZhsfen\">https://t.co/NPmBZhsfen</a> <a href=\"https://t.co/cWlytcZ12h\">pic.twitter.com/cWlytcZ12h</a></p>&mdash; Paul Testa (@ProfPaulTesta) <a href=\"https://twitter.com/ProfPaulTesta/status/1636820286950522894?ref_src=twsrc%5Etfw\">March 17, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\n# {{< fa lightbulb >}} Probability {.inverse}\n\n## Probability\n\n- Probability describes the [likelihood of an event]{.blue} happening.\n\n- Statistics uses probability to [quantify uncertainty]{.blue} about estimates and hypotheses.\n\n- To do this, we will need to understand:\n\n  - Definitions ([**experiment**, **sample space**, **events**]{.blue})\n\n  - Three *rules* of probability ([**Kolmogorov axioms**]{.blue})\n\n  - Two interpretations interpreting probabilities ([**Frequentist**]{.blue} and [**Bayesian**]{.blue})\n\n\n\n## Experiments, sample spaces, sets, and events {.smaller}\n\n- In probability theory, an [**experiment**]{.blue} describes a repeatable process where the outcome is uncertain\n\n  - Processes where the outcomes are uncertain are called [*non-deterministic*]{.blue} or [*stochastic*]{.blue}\n\n- The [**sample space**]{.blue} of an experiment is the **set** $(\\Omega$ \"omega\", or $S$) of [all the possible outcomes]{.blue} of an experiment\n\n## Experiments, sample spaces, sets, and events {.smaller}\n\n::::{.columns}\n\n:::{.column width=60%}\n- Sets can be:\n\n  - empty $( A: \\{\\emptyset\\})$\n  - a single event $( Coin: \\{\\text{Heads}\\})$\n  - multiple events $( Odd\\, \\#s: \\{\\text{1,3,5}\\})$\n  - infinite $(\\mathbb{R}: \\text{ The set of real numbers}\\{ -\\infty \\dots +\\infty\\}$)\n\n- An **event**, $(E$ or $A)$ is a **subset** of outcomes in the sample space\n\n  - The sample space for a coin flip is $\\Omega = \\{\\text{Heads, Tails}\\}$\n  - The event Heads is a subset of $\\Omega$\n\n:::\n\n:::{.column width=40%}\n\n![](https://www.playmonster.com/wp-content/uploads/2019/09/1000_set_pkgcontents-1.png)\n:::\n::::\n\n## Subsets {.smaller}\n\n- Subset: Let $D$ be the set outcomes for a 6-side die: $D=\\{1,2,3,4,5,6\\}$\n  - $Primes=\\{2,3,5\\}$\n  - $Primes \\subset D \\iff \\forall X \\in Primes, X \\in D$\n\n## Unions, Intersections, and Complements {.smaller}\n\n- Unions\n  - $A \\cup B = \\{X:X \\in A \\lor X \\in B \\}$\n  - Either $A$, $B$ or both $A$ and $B$ occur\n- Intersections\n  - $A \\cap B = \\{X:X \\in A \\land X \\in B \\}$\n  - Both $A$ and $B$ occur\n- Complements\n  - $A'=A^\\complement = \\{X:X\\notin A\\}$\n  - $A'=A^\\complement$ means $A$ does not occur\n  - $\\emptyset^\\complement=S$ and $S^\\complement=\\emptyset$\n\n##\n\n![](https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png) \n[Source](https://www.onlinemathlearning.com/union-set.html)\n\n## Three Rules of Probability{.smaller}\n\n:::{.nonincremental}\n\n- Probability is defined by three *rules* or assumptions called the [**Kolmogorov Axioms**](https://win-vector.com/2020/09/19/kolmogorovs-axioms-of-probability-even-smarter-than-you-have-been-told/)\n\n1. [Positivity:]{.blue} The probability of any event $A$ is [nonnegative]{.blue}\n\n$$Pr(A) \\geq 0 $$\n\n2. [Certainty:]{.blue} The probability that [one of the outcomes]{.blue} in the sample space [occurs]{.blue} is 1\n\n$$Pr(\\Omega) = 1 $$\n\n3.  [Additivity:]{.blue} If events $A$ and $B$ are [mutually exclusive]{.blue}, then:\n\n$$Pr(A \\text{ or } B) = Pr(A) + Pr(B)$$ \n\n:::\n\n## The Addition Rule\n\nFor events, $A$ and $B$, the **addition** rule says we can find the probability of either $A$ or $B$ occurring:\n\n$$Pr(A \\cup B) = Pr(A \\text{ or } B) = Pr(A) + Pr(B) - \\underbrace{Pr(A \\text{ and } B)}_{\\text{aka } Pr(A \\cap B)}$$ \n\nIn words: The probability of either A or B occurring is the probability that A occurs plus the probability that B occurs - minus the probability that both occur (so that we're not double counting...)\n\n##\n\n![](https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png) \n\n[Source](https://www.onlinemathlearning.com/union-set.html)\n\n## The Law of Total Probability (Part 1)\n\nFor any event two events, $A$ and $B$, the probability of $A$ $(Pr(A))$ can be **decomposed** into the sum of the probabilities of two **mutually exclusive** events:\n\n$$Pr(A) = Pr(A \\text{ and } B) + Pr(A \\text{ and } B^{\\complement})$$\n\n##\n\n![](https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png)\n\n[Source](https://www.onlinemathlearning.com/union-set.html)\n\n## Two interpretations of probablity\n\n- Probabilities are defined by these three axioms\n\n- The are two broad ways of interpreting what probabilities mean:\n\n  - [Frequentist]{.blue}\n\n  - [Bayesian]{.blue}\n\n## Frequentist interpretations of probability {.smaller}\n\n:::{.nonincremental}\n\n- Probability describes how likely it is that some event happens.\n\n  - Flip a fair coin, the probability of heads is Pr(Heads) = 0.5\n\n\n- **Frequentist:** view this probability as the [limit of the relative frequency]{.blue} of an event [over repeated trials]{.blue}.\n\n$$Pr(E) = \\lim_{n \\to \\infty} \\frac{n_{E}}{n} \\approx \\frac{ \\text{# of Times E happened}}{\\text{Total # of Trials}}$$\n\n- Thinking about probability as a relative frequency, requires us to [know how to count](https://bookdown.org/probability/beta/counting.html) the number of times an event occurred ([see also](https://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf))\n\n:::\n\n## Frequentist interpretations of probability\n\n- Probabilities from a Frequentist perspective are defined by *fixed* and *unknown* **parameters**\n\n- The goal of statistics for a frequentist is to learn about these parameters from data.\n\n- Frequentist statistics often ask questions like \"What is the probability of observing some data $Y$, given a hypothesis about the true value of parameter(s), $\\theta$, that generated it.\n\n## Frequentist interpretations of probability {.smaller}\n\nFor example, suppose we wanted to test whether a coin is \"fair\" $(p = Pr(Heads) = .5; q = Pr(Tails) = 1-p = .5).$ We could:\n\n- Flip a fair coin 10 times. Our estimate of the $Pr(H)$ is the number of heads divided by 10. It could be 0.5, but also 0 or 1, or some number in between.\n\n- Flip a coin 100 times and our estimate will be closer to the true $paramter$.\n\n- Flip a coin an $\\infty$ amount of times and the relative frequency will converge to the true parameter $(Pr(H) = \\lim_{n \\to \\infty} \\frac{n_{H}}{n} = p = 0.5 \\text{ for a fair coin})$\n\n## Bayesian interpretations of probability {.smaller}\n\n\n\n- Frequentist interpretations make sense for describing processes that we could easily repeat (e.g. Coin flips, Surveys, Experiments)\n\n\n- But feel more convoluted when trying to describe events like \"the probability of that Biden wins reelection.\"\n\n\n- Bayesian interpretations of probability view probabilities as [subjective beliefs]{.blue}.\n\n\n- The task for a Bayesian statistics is to update these [*prior* beliefs ]{.blue} () based on a model of the [*likelihood*]{.blue} of observing some data to form new beliefs after observing the data (called the [*posterior beliefs*]{.blue}).\n\n\n\n## Bayesian Updating\n\n:::{.nonincremental}\n\n- Bayesians update their beliefs according to  *Bayes Rule*, which says:\n\n$$\\text{posterior} \\propto \\text{likelihood} \\times \\text{prior}$$ More formally:\n\n$$\\underbrace{Pr(\\theta|Y)}_{\\text{Posterior}} \\propto \\underbrace{Pr(Y|\\theta)}_{\\text{Likelihood}}) \\times \\underbrace{Pr(\\theta)}_{\\text{Prior}}$$\n:::\n\n## \n\n![](https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png)\n\n\n\n## Bayesian vs Frequentists\n\n:::{.nonincremental}\n\nOur two main tools for doing statistical inference in this course\n\n- Hypothesis Testing\n- Interval Estimation\n\nFollow largely from frequentist interpretations of probability\n\n:::\n\n## Bayesian vs Frequentists {.smaller}\n\n:::{.nonincremental}\n\nThe differences between Bayesian and Frequentist frameworks, are both philosophical and technical in nature\n\n- Is probability a relative frequency or subjective belief? How do we form and use prior beliefs\n\n- Bayesian statistics relies heavily on algorhithms for [Markov Chain Monte-Carlo](http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf) simulations made possible by advances in computing.\n\n\nFor most of the questions in this course, these two frameworks will yield similar (even identical) conclusions.\n\n- Sometimes it's helpful to think like a Bayesian, others, like a frequentist\n:::\n\n## Summary: Probability{.smaller}\n\n- Probability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur\n\n- Probabilities are:\n\n  - Non-negative\n  - Unitary\n  - Additive\n\n- Two different interpretations of probability:\n\n  - Frequentists: Probability is a long run relative frequency\n  - Bayesians: Probability reflect subjective beliefs which we update upon observing data\n\n\n\n# {{<fa lighbulb>}} Conditional Probability{.inverse}\n\n## Conditional Probability: Definition{.smaller}\n\nThe conditional probability that event **A** occurred, given that event **B** occurred is written as $Pr(A|B)$ (*\"The probability of A given B\"*) and defined as:\n\n$$Pr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)} = \\frac{\\text{Probability of Both A and B}}{\\text{Probability of B}}$$\n\n- $Pr(A \\cap B)$ is the same as $Pr(A \\text{ and } B)$ is the **joint probability** of both events occurring\n\n- $Pr(B)$ is the **marginal probability** of B occuring\n\n## Conditional Probability: Multiplication Rule {.smaller}\n\nJoint probabilities are **symmetrical**. $Pr(A \\cap B) = Pr(B \\cap A)$.\n\nBy rearranging terms:\n\n$$Pr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)}$$ We get the **multiplication rule**:\n\n$$Pr(A \\cap B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A)$$ \n\n## The Law of Total Probability (Part 2)\n\nWe can use multiplication rule to derive an alternative form of the law of total probability:\n\n$$Pr(A) = Pr(A|B)Pr(B) + Pr(A|B^\\complement)Pr(B^\\complement)$$\n\n## Independence\n\nEvents $A$ and $B$ are independent if\n\n$$Pr(A|B) = Pr(A) \\text{ and } Pr(B|A) = Pr(B)$$\n\nConceputally, If $A$ and $B$ are **independent** knowing whether $B$ occurred, tells us nothing about $A$, and so the conditional probability of $A$ given $B$, $Pr(A|B)$ is equal to the unconditional, or marginal probability, $Pr(A)$\n\n## Independence\n\nFormally, two events are statistically independent if and only if the joint probability is equal to product of the marginal probabilities\n\n$$Pr(A\\text{ and }B) = Pr(A)Pr(B)$$\n\n## Conditional Independence\n\nWe can extend the concept of independence to situations with more than two events:\n\nIf events $A$, $B$, and $C$ are jointly independent then:\n\n$$Pr(A \\cap B \\cap C) = Pr(A)Pr(B)Pr(C)$$\n\nJoint independence implies pairwise independence and conditional independence:\n\n$$Pr(A \\cap B | C) = Pr(A|C)Pr(B|C)$$ But not the reverse.\n\n## Bayes Rule {.smaller}\n\n:::{.nonincremental}\nBayes rule is theorem for how we should update our beliefs about $A$ given that $B$ occurred:\n\n$$Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}$$ Where\n\n- $Pr(A)$ is called the prior probability of A (our initial belief)\n\n- $Pr(A|B)$ is called the posterior probability of A given B (our updated belief after observing B)\n:::\n\n## What's the probability you have Covid-19 given a positive test\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$\n\n## Possible Outcomes\n\nFour possible outcomes\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Test </th>\n   <th style=\"text-align:left;\"> Have Covid </th>\n   <th style=\"text-align:left;\"> Don't Have Covid </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Positive </td>\n   <td style=\"text-align:left;\"> True Positive </td>\n   <td style=\"text-align:left;\"> False Positive </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Negative </td>\n   <td style=\"text-align:left;\"> False Negative </td>\n   <td style=\"text-align:left;\"> True Negative </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n## What's the probability you have Covid-19 given a positive test {.smaller}\n\n:::{.nonincremental}\n\nLet's assume:\n\n- 1 out 100 people have Covid-19\n\n- Our test correctly identifies true positives 95 percent of the time ([sensitivity = True Positive Rate]{.blue})\n\n- Our test correctly identifies true negatives 95 percent of the time ([specificity = True Negative Rate]{.blue})\n\nIn a sample of 100,000 people then:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Test </th>\n   <th style=\"text-align:right;\"> Have Covid </th>\n   <th style=\"text-align:right;\"> Don't Have Covid </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Positive </td>\n   <td style=\"text-align:right;\"> 950 </td>\n   <td style=\"text-align:right;\"> 4950 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Negative </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 94050 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n:::\n\n## What's the probability you have Covid-19 given a positive test {.smaller}\n\n:::{.nonincremental}\n\nNow we can calculate the relevant quantities for:\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$\n\n- $Pr(+|Covid) = 950/(1000) \\approx 0.95$\\\n- $Pr(Covid) = 1000/100000 \\approx 0.01$\\\n- $Pr(+) = Pr(+|Covid) + Pr(+|Covid)= .95*.01 + .05*.99 \\approx 0.059$\n\nWhich yields:\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)} = \\frac{0.95 \\times 0.01}{0.059} \\approx 0.16$$ \n:::\n\n## What if you took a second test?{.smaller}\n\nWe could use our [updated posterior belief]{.blue} as our [new prior]{.blue}:\n\n$$Pr(Covid|2nd +) = \\frac{0.95 \\times 0.16}{0.16\\times0.95 + (1-0.16)\\times 0.95 } \\approx 0.783$$ \n\nNow we're much more confident that we have Covid-19\n\n\n\n# {{<fa lightbulb>}} Random Variables and Probability Distributions {.inverse}\n\n## Random Variables \n\n- [Random variables]{.blue} assign [numeric values]{.blue} to each event in an experiment.\n\n  - Mutually exclusive and exhaustive, together cover the entire sample space.\n\n- [Discrete random]{.blue} variables take on finite, or [countably infinite](http://mathworld.wolfram.com/CountablyInfinite.html) distinct values.\n\n- [Continuous variables]{.blue} can take on an uncountably infinite number of values.\n\n## Example: Toss Two Coins\n\n- $S={TT,TH,HT,HH}$\n- Let $X$ be the number of heads\n  - $X(TT)=0$\n  - $X(TH)=1$\n  - $X(HT)=1$\n  - $X(HH)=2$\n\n## Probability Distributions{.smaller}\n\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\n\nThe can be represented in terms of:\n\n- [Probability Mass/Density Functions]{.blue}\n  - [Discrete variables]{.blue} have probability mass functions (PMF)\n  - [Continuous variables]{.blue} have probability density functions (PDF)\n- [Cumulative Density Functions]{.blue}\n  - Discrete: Summation of discrete probabilities\n  - Continuous: Integration over a range of values\n\n## Discrete distributions\n\n- **Probability Mass Function (pmf):** $f(x)=p(X=x)$\n\n- Assigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\n\n- **Cumulative Distribution Function (cdf)** $F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)$\n\n  - Sum of the probability mass for events less than or equal to $x_j$\n\n## Example: Toss Two coins{.smaller}\n\n- $S={TT,TH,HT,HH}$\n- Let $X$ be the number of heads\n  - $X(TT)=0$\n  - $X(TH)=1$\n  - $X(HT)=1$\n  - $X(HH)=2$\n- $f(X=0)=p(X=0)=1/4$\n- $f(X=1)=p(X=1)=1/2$\n- $F(X\\leq 1) = p(X \\leq 1)= 3/4$\n\n## Rolling a die\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X\\<=2) = 1/6 + 1/6 = 1/3\n\n## Continuous distributions {.smaller}\n\n- **Probability Density Functions (PDF):** $f(x)$\n  - Assigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\n  - But... since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n- **Cumulative Distribution Function (CDF)** $F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx$\n  - Instead of summing up to a specific value (discrete) we integrate over all possible values up to $x$\n  - Probability of having a value less than x\n\n## Integrals{.smaller}\n\nWhat's the area of the rectangle? \n\n:::{.fragment}\n$base\\times height$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Integrals\n\nHow would we find the area under a curve?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## Integrals\n\nWell suppose we added up the areas of a bunch of rectangles roughly whose height's approximated the height of the curve?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\nCan we do any better?\n\n## Integrals\n\nLet's make the rectangles smaller\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get's even better\n\n## Link between PDF and CDF{.smaller}\n\nIf $$F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx $$\n\nThen by the [fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus)\n\n$$\\frac{d}{dx}F(x)=f(x)$$\n\nIn words\n\n- the PDF $(f(x))$is the derivative (rate of change) of the CDF $(F(X))$\n\n- the CDF describes the area under the curve defined by f(x) up to x\n\n## Properties of the CDF\n\n- $0\\leq F(x) \\leq 1$\n\n- $F$ is non-decreasing and right continuous\n\n- $\\lim_{x\\to-\\infty}F(x)=0$\n\n- $\\lim_{x\\to\\infty}F(x)=1$\n\n- For all $a,b \\in \\mathbb{R}$ s.t. $a<b$\n\n:::{.fragment}\n\n$$p(a < X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx $$\n\n:::\n\n## Recall the PMF and CDF of a die\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## What's the probability\n\n- $p(X=1)...p(X=6) = 1/6$\n\n- $p( 2 < X \\leq 5) = F(5)-F(2)=5/6-1/6=4/6=2/3$\n\n## What we'll use proability distributions for:\n\nIn this course, we'll use probability distributions to\n\n- Model the [data generating process]{.blue} as a function of parameters we can estimate\n\n- To perform [statitical inference]{.blue} based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)\n\n##\n\nPlease memorize these over Spring Break:\n\n![](http://www.math.wm.edu/~leemis/chart/UDR/BaseImage.png)\n\n![Source](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html)\n\n## \n\n![](https://miro.medium.com/max/4854/1*szMCjXuMDfKu6L9T9c34wg.png)\n\n[Source](https://www.datasciencecentral.com/common-probability-distributions-the-data-scientist-s-crib-sheet/)\n\n\n# {{<fa lightbulb >}} Expected Values and Variances{.inverse}\n\n## Expected Value\n\nA (probability) weighted average of the possible outcomes of a random variable, often labeled $\\mu$\n\nDiscrete:\n\n$$\\mu_X=E(X)=\\sum xp(x)$$\n\nContinuous\n\n$$\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx$$\n\n## Condtional Expectations:\n\nFor a continuous variable:\n\n$$\nE(X|Y=y) = \\int_{-\\infty}^{\\infty}xf_{x|y}(x|y) dx\n$$\n\nWhere:\n\n$$\nf_{x|y}(x|y) = \\frac{f_{x,y}(x,y)}{f_y(y)} = \\frac{\\text{Joint distribution of X and Y}}{\\text{Marginal distribution of Y}}\n$$\n\nWhich follows from the law of total probability\n\n## What's the expected value of a 1 roll of fair die?\n\n$$\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}$$\n\n## Properties of Expected Values\n\n- $E(c)=c$\n\n- $E(a+bX)=a+bE[X]$\n\n- $E[E[X]]=X$\n\n- $E[E[Y|X]]=E[Y]$\n\n- $E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx$\n\n- $E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)$\n\n- $E[XY]=E[X]E[Y]$ if $X$ and $Y$ are independent\n\n## {.smaller}\n#### How many times would you have to roll a fair die to get all six sides? \n\n- We can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success, $p$. The expected value of a geometric variable is:\n\n:::{.fragment}\n\n$$\\begin{align*}E(X)&=\\sum_{k=1}^{\\infty}kp(1-p)^{k-1} \\\\\n&=p\\sum_{k=1}^{\\infty}k(1-p)^{k-1} \\\\\n&=p\\left(-\\frac{d}{dp}\\sum_{k=1}^{\\infty}(1-p)^k\\right) \\text{(Chain rule)} \\\\\n&=p\\left(-\\frac{d}{dp}\\frac{1-p}{p}\\right) \\text{(Geometric Series)} \\\\\n&=p\\left(\\frac{d}{dp}\\left(1-\\frac{1}{p}\\right)\\right)=p\\left(\\frac{1}{p^2}\\right)=\\frac1p\\end{align*}$$\n:::\n\n## Rolling a fair die to get all six sides{.smaller}\n\nFor this question, we need to calculate the probability of success, p, after getting a side we need.\n\nThe probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nev <- c()\nfor(i in 6:1){\n  ev[i] <- 6/i\n  \n}\n# Expected rolls for each 1 through 6th side\nrev(ev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.0 1.2 1.5 2.0 3.0 6.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Total \nsum(ev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.7\n```\n\n\n:::\n:::\n\n\n## Variance\n\nIf $X$ has a finite mean $E[X]=\\mu$, then $E[(X-\\mu)^2]$ is finite and called the [variance of $X$]{.blue} which we write as $\\sigma^2$ or $Var[X]$.\n\n\n## Variance{.smaller}\n\n$$\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}$$\n\n- \"The variance of X is equal to the expected value of X-squared, minus the square of X's expected value.\"\n- $\\sigma^2=E[X^2]-E[X]^2$ is a useful identity in proofs and derivations\n\n## Standard Deviations\n\nA standard deviation is just the square root of the variance\n\n$$\\sigma=\\sqrt{Var[X]}$$\n\nStandard deviations are useful for describing:\n\n- A typical deviation from the mean/Expected value\n- The width or spread of a distribution\n\n\n\n## Covariance and correlation{.smaller}\n\n:::{.nonincremental}\n\nCovariance measures the degree to which two random variables vary together.\n\n- $Cov[X,Y] \\to +$ An increase in $X$ tends to be larger than its mean when $Y$ is larger than its mean\n\n$$Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]$$\n\n- The correlation between $X$ and $Y$ is simply the covariance of $X$ and $Y$ divided by the standard deviation of each.\n\n$$\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}$$\n\n- Normalized covariance to a scale that runs between $[-1,1]$\n\n:::\n\n\n## Properties of Variance and Covariance\n\n- $Cov[X,Y]=E[XY]-E[X]E[Y]$\n\n- $Var[X]=E[X^2]-(E[X])^2$\n\n- $Var[X|Y]=E[X^2|Y]-(E[X|Y])^2$\n\n- $Cov[X,Y]=Cov[X,E[Y|X]]$\n\n- $Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]$\n\n- $Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]$\n\n## What you need to know (WYNK)\n\nHonestly, for this class, you won't need to know these properties. \n\nThey'll show up in proofs and theorems and become important when you're trying to evaluate properties of an estimator (isn't unbiased, is it \"efficient\", or consistent does it have minimum variance?) but that's for another day/course.\n\n\n\n## Summary: Random Variables and Probability Distributions{.smaller}\n\n- [Random variables]{.blue} assign numeric values to each event in an experiment.\n\n- [Probability distributions]{.blue} assign probabilities to the values that a random variable can take.\n\n  - Discrete distributions are described by their pmf and cdf\n  - Continuous distributions by their pdf and cdf\n\n## Summary: Random Variables and Probability Distributions{.smaller}\n\n- Probability distributions let us [*describe the data generating process*]{.blue} and encode information about the world into our models\n\n  - There are lots of distributions\n  - Don't worry about memorizing formulas\n  - Do develop intuitions about the nature of your data generating process (Is my outcome continuous or disrecte, binary or count, etc.)\n\n- Two key features of probability distributions are their:\n\n  - [Expected values]{.blue} probability weighted averages\n  - [Variances]{.blue} which quantify variation around expected values\n\n\n# {{< fa lightbulb >}} Standard Errors for Regression{ background-image=\"https://www.phrases.org.uk/images/cart-before-the-horse.jpg\" background-size=\"contain\" background-opacity=.2}\n\n## Interpreting regressions\n\n- Regression coefficients $(\\beta)$ are crucial for [substantive interpretations]{.blue} (sign and size)\n\n- The [standard errors]{.blue} of these coefficients $(SE(\\beta))$ are the key to evaluating the [statistical significance]{.blue} of these coefficients\n\n## What's a standard error? {.smaller}\n\n- The [standard error]{.blue} of an estimate is the [standard deviation]{.blue} of the theoretical [sampling distribution]{.blue} \n\n- A sampling distribution is a distribution of the estimates we would observe in  [repeated sampling]{.blue}\n\n  - Example: Re-run the 1978 CPS, we get different respondents, and thus different estimates.\n  \n- [Standard errors]{.blue} describe the width of the sampling distribution \n  - How much our estimates might vary from the true (population) value from sample to sample.\n\n- [Standard errors]{.blue} can be used to construct intervals and conduct tests that [quantify our uncertainty]{.blue} about our estimate\n\n## {.smaller}\n#### Standard errors of regression coefficients \n\nFor a linear regression written in matrix notation:\n\n$$\ny = X\\beta + \\epsilon\n$$\n\n OLS yields estimates of $\\beta$, $\\hat{\\beta}$ by minimizing the sum of squared residuals\n\n$$\n\\hat{\\beta} = (X'X)^{-1}X'y\n$$\n\n## {.smaller}\n#### Standard errors of regression coefficients \n\n[One can show](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) under a [set of assumptions](https://en.wikipedia.org/wiki/Linear_regression#Assumptions) that variance-covariance matrix of $\\hat{\\beta}$ is\n\n$$\n\\begin{aligned}\nE[(\\hat{\\beta} -\\beta)-(\\hat{\\beta} -\\beta)'] &= \\sigma^2(X'X)^{-1}\\\\\n& = \\begin{bmatrix}\nVar(\\hat\\beta_0) & Cov(\\hat{\\beta_0},\\hat{\\beta_1}) & \\cdots & Cov(\\hat{\\beta_0},\\hat{\\beta_k}))\\\\\nCov(\\hat\\beta_1,\\hat{\\beta_0}) & Var(\\hat{\\beta_1},) & \\cdots & Cov(\\hat{\\beta_1},\\hat{\\beta_k}))\\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nCov(\\hat\\beta_k,\\hat{\\beta_0}) & Cov(\\hat{\\beta_k},\\hat{\\beta_1}) & \\cdots & Var(\\hat{\\beta_k})\\\\\n\\end{bmatrix}\n\\end{aligned}\n$$\nWhere we can estimate $\\sigma^2$ with $\\hat{\\sigma}^2$ the mean $(1/n-k)$ squared error $(\\epsilon'\\epsilon)$ of the regression\n\n$$\n\\hat{\\sigma}^2 = \\frac{\\epsilon'\\epsilon}{n-k}\n$$\n\n\n\n## {.smaller}\n#### Standard errors of regression coefficients\n\nThe standard error for the $k$th coefficient $\\beta_k$ is simply the square root of the of the $k$th diagnoal element of the [variance-covariance matrix]{.blue}\n\n$$\n\\text{SE}(\\beta_k) = \\sqrt{Var(\\beta_k)}\n$$\n\n## Robust Standard Errors{.smaller}\n\n$\\sigma^2(X'X)^{-1}$ is a good estimate of the variance of $\\hat{\\beta}$ if the errors in a regression are [independent]{.blue} and [identically distributed]{.blue} (iid).\n\nThese turn out to be [strong assumptions]{.blue} that are violated when there is:\n\n- [Non-constant error variance]{.blue} (aka heteroskedasticity)\n  - The variance among the treated units tends to be higher than the variance among control units\n- [Autocorrelation]{.blue}\n  - We observe the [same unit]{.blue} over [multiple periods]{.blue} (Say RI in 2016, 2018, 2020)\n- [Clustering]{.blue}\n  - Respondents in RI are more similar to each other than respondents in MA\n\n## Robust Standard Errors{.smaller}\n\n\n- [Robust standard errors]{.blue} are ways of [calculating standard errors]{.blue} for regressions when we think the assumption of [IID errors]{.blue} is unrealistic\n\n  - The assumption of IID is almost always unrealistic...\n\n- We call these of estimators [robust]{.blue} because they provide [consistent](https://en.wikipedia.org/wiki/Consistent_estimator) estimates of the SE even when errors are not [independent]{.blue} and [identically distributed]{.blue}.\n\n## Robust standard errors in R\n\n![](images/08_robust.jpg)\n\n## `lm_robust()` {.smaller}\n\nIn this weeks lab we will get practice using the `lm_robust()` function from the `estimatr`.\n\nAs you will see, `lm_robust()` provides a convenient way to: \n\n- calculate a variety of robust [standard errors]{.blue} using the `se_type = \"stata\"` argument for example to get the SEs Stata uses\n- include [fixed effects]{.blue} using `fixed_effects = ~ st + year` argument\n- cluster standard errors by some grouping id variable `cluster=st`\n- generate estimates quickly using the [Cholesky Decomposition]{.blue}\n\n\n# {{<fa magnifying-glass >}} Previewing Lab 8{.inverse}\n\n## Overview\n\nThe goals of this weeks lab are to:\n\n- Help develop your inuition behind the Two-way Fixed Effects Estimator\n\n- Learn how to estimate models with fixed effects and robust clustered standard errors using `lm_robust()`\n\n- Interpret the marginal effects of interaction models\n\n## Recreating Figure 2 \n\n:::panel-tabset\n\n## Task\n\nQuestion 8 from last week's lab asked you to recreate Figure 2 from @Grumbach2022-zj\n\n\n\n## {{< fa code >}} Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nload(url(\"https://pols1600.paultesta.org/files/data/cps_clean.rda\"))\n\n# Calculate turnout by age group and SDR\ncps %>% \n  group_by(age_group, SDR) %>% \n  summarise(\n    prop_vote = mean(dv_voted, na.rm=T)\n  ) -> fig2_df\n\n# Recreate figure 2\nfig2_df %>% \n  filter(!is.na(age_group)) %>% \n  ggplot(aes(age_group,prop_vote,fill = SDR))+\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") -> fig2\n```\n:::\n\n\n\n## {{<fa chart-line>}} Fig 2\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/fig2-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## Q3.1 Describing variation across states {.smaller}\n\n:::panel-tabset\n\n## Task\n\nQ3 will ask you to describe variation in turnout across states and years, and policy.\n\nLet's get a little practice calculating turnout across states\n\n\n## {{< fa code >}} Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate turnout by state\ncps %>% \n  group_by(st) %>% \n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) %>% \n  mutate(\n    st = fct_reorder(st, turnout)\n    ) -> df_state\n\n\n\n# Visualize turnout by state\ndf_state %>% \n  ggplot(aes(turnout,st))+\n  geom_bar(stat = \"identity\") -> fig3_1\n```\n:::\n\n\n\n## {{<fa chart-line>}} Fig 3.1\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/fig3_1-1.png){width=960}\n:::\n:::\n\n\n\n## {{< fa lightbulb >}} Fixed Effects\n\nAs you can see, there is considerable variation in average turnout across States\n\nQ3.2 will ask you to describe similar variation across years.\n\nQ3.3 will then ask you to look at variation across SDR policy within a single state.\n\nThe goal these questions is to help illustrate motivation for including [fixed effects]{.blue} as way of generalizing the logic of a [difference in differences]{.blue} design\n\n:::\n\n## References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.31/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}