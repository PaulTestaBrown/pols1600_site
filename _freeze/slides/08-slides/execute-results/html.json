{
  "hash": "1be2b7a3fabd0e6c71792a7e8981fe5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 08:\"\nsubtitle: \"Probably too much Probability Theory\"\nauthor: \"Paul Testa\"\nformat: \n  revealjs:\n    theme: [default, brownslides.scss]\n    logo: images/pols1600_hex.png\n    footer: \"POLS 1600\"\n    multiplex: false\n    transition: fade\n    slide-number: c\n    incremental: true\n    center: false\n    menu: true\n    scrollable: true\n    highlight-style: github\n    progress: true\n    code-overflow: wrap\n    # include-after-body: title-slide.html\n    title-slide-attributes:\n      align: left\n      data-background-image: images/pols1600_hex.png\n      data-background-position: 90% 50%\n      data-background-size: 40%\nexecute: \n  echo: true\nfilters:\n  - openlinksinnewpage\n---\n\n\n\n\n\n\n\n\n\n\nclass: inverse, center, middle \\# Overview\n\n## General Plan\n\n-   Setup\n-   Lab Preview\n-   Lecture\n    -   Probability Theory\n    -   Conditional Probability\n    -   Random variables and probability distributions\n    -   Expected values and variances\n\n# Lab: Predicting Election Outcomes\n\nAvailable [here](https://pols1600.paultesta.org/labs/08-lab.Rmd)\n\n1.  Get set up to work (5 minutes)\n\n2.  Calculate Obama's expected electoral vote share on November 3, 2008 (the day before the election) (10 minutes)\n\n3.  Simulate a 1000 elections for November 3, 2008 using the betting market prices a measure of the probability that Obama wins or loses a state (10 minutes)\n\n4.  Display the results of your simulation with a histogram. (5 minutes)\n\n5.  Transform these probabilities to reduce the likelihood that Obama wins states like Alabama and increase the likelihood that Obama wins states like California (5 minutes)\n\n6.  Simulate another 1000 elections using these transformed probabilities. Compare the results to your initial simulation. (10 minutes)\n\n7.  Calculate Obama's expected total number of votes for each day in the 120 days before the 2008 election (15 minutes)\n\n8.  Simulate 100 elections for each of the 120 days before the election, plot the results of your simulation. (20 minutes)\n\n# ðŸ’ª\n\n## Get set up to work\n\n## New packages\n\nAnd tools for doing Permutations and Combinations\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstall.packages(\"gtools\")\n```\n:::\n\n\n## Packages for today\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nthe_packages <- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\", \"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #<<\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\", \n  \"gtools\" #<<\n)\n```\n:::\n\n\n## Define a function to load (and if needed install) packages\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak <- function(pkg){\n    new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n```\n:::\n\n\n## Load packages for today\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nipak(the_packages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign           zoo        gtools \n         TRUE          TRUE          TRUE          TRUE \n```\n\n\n:::\n:::\n\n\n# ðŸ’ª\n\n## Load Data for today\n\n## What we'll cover today\n\n-   Probability\n\n    -   Probability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur\n\n    -   Probability follows three simple rules (from which numerous theorems follow)\n\n-   Conditional Probability\n\n    -   Conditional probability allow us to describe how our beliefs about one event change after observing another event(s)\n\n    -   We can update beliefs using Bayes Rule\n\n-   Random Variables and Probability Distributions\n\n    -   Random variables *map* events in the world onto numbers\n\n    -   Probability distributions describe the likelihood that random variables take certain values\n\n    -   The expected value of a random variable is a *probability weighted average* that tells us the most likely value a variable will take\n\n# ðŸ’¡\n\n# Probability\n\n## Probability\n\n-   Probability describes the likelihood of an event happening.\n\n-   Statistics uses probability to quantify uncertainty about estimates and hypotheses.\n\n-   To do this, we will need to understand:\n\n-   Definitions (**experiment**, **sample space**, **events**,)\n\n-   Three *rules* of probability (**Kolmogorov axioms**)\n\n-   Two interpretations interpreting probabilities (**Frequentist** and **Bayesian**)\n\n    -   Some rules for counting\n\n## Experiments, sample spaces, sets, and events\n\n-   In probability theory, an **experiment** describes a repeatable process where the outcome is uncertain\n\n    -   Processes where the outcomes are uncertain are called *non-deterministic* or *stochastic*\n\n-   The **sample space** of an experiment is the **set** $(\\Omega$ \"omega\", or $S$) of all the possible outcomes of an experiment\n\n-   Sets can be:\n\n    -   empty $( A: \\{\\emptyset\\}$\n    -   a single event $( Coin: \\{\\text{Heads}\\}$\n    -   multiple events $( Odd\\, \\#s: \\{\\text{1,3,5}\\}$\n    -   infinite $(\\mathbb{R}: \\text{ The set of real numbers}\\{ -\\infty \\dots +\\infty\\}$)\n\n-   An **event**, $(E$ or $A)$ is a **subset** of outcomes in the sample space\n\n    -   The sample space for a coin flip is $\\Omega = \\{\\text{Heads, Tails}\\}$\n    -   The event Heads is a subset of $\\Omega$\n\nbackground-image:url(\"https://www.playmonster.com/wp-content/uploads/2019/09/1000_set_pkgcontents-1.png\") background-size:contain\n\n## Operations on Sets\n\n-   Empty Set: $\\emptyset$ a set with no elements\n-   Subset:\n    -   Let $D$ be the set outcomes for a 6-side die: $D=\\{1,2,3,4,5,6\\}$\n    -   $Primes=\\{2,3,5\\}$\n    -   $Primes \\subset D \\iff \\forall X \\in Primes, X \\in D$\n-   Unions\n    -   $A \\cup B = \\{X:X \\in A \\lor X \\in B \\}$\n    -   Either $A$, $B$ or both $A and B$ occur\n-   Intersections\n    -   $A \\cap B = \\{X:X \\in A \\land X \\in B \\}$\n    -   Both $A$ and $B$ occur\n-   Complements\n    -   $A'=A^\\complement = \\{X:X\\notin A\\}$\n    -   $A'=A^\\complement$ means $A$ does not occur\n    -   $\\emptyset^\\complement=S$ and $S^\\complement=\\emptyset$\n\nbackground-image:url(\"https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png\") background-size:contain\n\n[Source](https://www.onlinemathlearning.com/union-set.html)\n\n## Three Rules of Probability\n\n-   Probability is defined by three *rules* or assumptions called the [**Kolmogorov Axioms**](https://win-vector.com/2020/09/19/kolmogorovs-axioms-of-probability-even-smarter-than-you-have-been-told/)\n\n1.  The probability of any event $A$ is nonnegative\n\n$$Pr(A) \\geq 0 $$\n\n2.  The probability that one of the outcomes in the same space occurs is 1\n\n$$Pr(\\Omega) = 1 $$\n\n3.  If events $A$ and $B$ are mutually exclusive, then:\n\n$$Pr(A \\text{ or } B) = Pr(A) + Pr(B)$$ \\## The Addition Rule\n\nFor any given events, $A$ and $B$, the **addition** rule says we can find the probability of either $A$ or $B$ occurring:\n\n$$Pr(A \\cup B) = Pr(A \\text{ or } B) = Pr(A) + Pr(B) - \\underbrace{Pr(A \\text{ and } B)}_{\\text{aka } Pr(A \\cap B)}$$ In words: The probability of either A or B occurring is the probability that A occurs plus the probability that B occurs - minus the probability that both occur (so that we're not double counting...)\n\nbackground-image:url(\"https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png\") background-size:contain\n\n[Source](https://www.onlinemathlearning.com/union-set.html)\n\n## The Law of Total Probability (Part 1)\n\nFor any event two events, $A$ and $B$, the probability of $A$ $(Pr(A)$ can be **decomposed** into the sum of the probabilities of two **mutually exclusive** events:\n\n$$Pr(A) = Pr(A \\text{ and } B) + Pr(A \\text{ and } B^{\\complement})$$\n\nbackground-image:url(\"https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png\") background-size:contain\n\n[Source](https://www.onlinemathlearning.com/union-set.html)\n\n## Two interpretations of probablity\n\n-   Probabilities are defined by these three axioms\n\n-   The are two broad ways of interpreting what probabilities mean:\n\n    -   Frequentist\n\n    -   Bayesian\n\n## Frequentist interpretations of probability\n\n-   Probability describes how likely it is that some event happens.\n\n    -   Flip a fair coin, the probability of heads is Pr(Heads) = 0.5\n\n--\n\n-   **Frequentist:** view this probability as the limit of the relative frequency of an event over repeated trials.\n\n$$Pr(E) = \\lim_{n \\to \\infty} \\frac{n_{E}}{n} \\approx \\frac{ \\text{# of Times E happened}}{\\text{Total # of Trials}}$$\n\n-   Thinking about probability as a relative frequency, requires us to know how to the number of times an event occurred.\n\nbackground-image:url(\"https://www.kindpng.com/picc/m/99-991302_transparent-sesame-street-count-clipart-count-sesame-street.png\") background-size:contain\n\n## How many elements in a set?\n\nA set can be:\n\n-   **Countably Finite:**\n\n    -   Roll a die, there six possible outcomes $\\{1,2,3,4,5,6\\}$\n\n--\n\n-   **Countably Infinite:**\n\n    -   Number of rolls until a six appears ${1,2,3,\\dots}$\n\n--\n\n-   **Uncountably Infinite:**\n\n    -   All the real numbers between 0.25 and 0.75\n\n## The Fundamental Counting Principle\n\nThe Fundamental Counting Principle says that if there $x$ ways to do one thing and $z$ ways to do another then, then are their are $x \\times z$ total ways of doing both tasks.\n\nMore generally, if there are\n\n-   If there are $j$ tasks or decision stages\n\n-   And $k$ choices at each decision stage $j$ such that $n_{j,k_j} = k$\n\n-   The total number of possible outcomes is the product of the number number of choices $k$ and each stage, $j$ $\\prod_{j=1}^{j} n_{j,k_j} = n_{1,k_1}*n_{2,k_2}*\\dots*n_{j,k_j}$\n\n    -   That is, just multiply the number of choices at each stage\n\n## How many different outfits could you have made me wear?\n\n-   **Palette**: $\\{\\text{Fall, Winter, Spring, Summer}\\}$\n\n-   **Jacket**: $\\{\\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\\}$\n\n-   **Top**: $\\{\\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\\}$\n\n-   **Pant**: $\\{\\text{Slacks, Khakis, Jeans, Shorts}\\}$\n\n-   **Shoe**: $\\{\\text{Dress Shoe, Dress boot, Jordans, Dunks, Basketball, Crocs}\\}$\n\n-   **Tie**: $\\{\\text{Repp, Pattern, Knit, Bow, No tie}\\}$\n\n-   **Pattern**: $\\{\\text{Simple, Stripes, Checks, Graphic}\\}$\n\n-   How many decision stages?\n\n-   How many choices at each stage?\n\n-   How many possible outfits?\n\n## How many different suggested outfits could you have made me wear?\n\n-   How many decision stages? $j=7$\n\n-   How many choices at each stage?\n\n    -   **Palette**: $\\{\\text{Fall, Winter, Spring, Summer}\\} = n_{1,4} = 4$\n    -   **Jacket**: $\\{\\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\\} = n_{2,5} = 5$\n    -   **Top**: $\\{\\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\\} = n_{3,4} = 4$\n    -   **Pant**: $\\{\\text{Slacks, Khakis, Jeans, Shorts}\\} = n_{4,4} = 4$\n    -   **Shoe**: $\\{\\text{Dress Shoe, Boots, Jordans, Dunks, Basketball, Crocs}\\} = n_{5,6} = 6$\n    -   **Tie**: $\\{\\text{Repp, Pattern, Knit, Bowtie, No tie}\\} = n_{6,7} = 5$\n    -   **Pattern**: $\\{\\text{Simple, Stripes, Checks, Graphic}\\} = n_{7,4} = 4$\n\n-   How many possible suggestions? $\\prod_{i=1}^{j} n_{jk} = 4 \\times 5 \\times 4 \\times 4 \\times 6 \\times 5 \\times 4 = 38,400 \\text{ outfits}$\n\n#### And yet you chose this:\n\n<blockquote class=\"twitter-tweet\">\n\n<p lang=\"en\" dir=\"ltr\">\n\nNaive professor in class survey: \"Help me with my fit\"<br>Students: \"Ok, bet. Tuxedo, crocs, and a jersey\" <a href=\"https://t.co/NPmBZhsfen\">https://t.co/NPmBZhsfen</a> <a href=\"https://t.co/cWlytcZ12h\">pic.twitter.com/cWlytcZ12h</a>\n\n</p>\n\nâ€” Paul Testa (@ProfPaulTesta) <a href=\"https://twitter.com/ProfPaulTesta/status/1636820286950522894?ref_src=twsrc%5Etfw\">March 17, 2023</a>\n\n</blockquote>\n\n\n```{=html}\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n```\n\n## Perumations and Combinations\n\nWhen we're counting how many \"events\" we can construct from a set of elements, our answer depends on whether we:\n\n-   Can distinguish the **order** of elements\n\n-   Allow for elements to be **repeated**\n\nSituations where we can distinguish the order of elements being selected are described by **Permutations**\n\nSituations where we cannot distinguish the order of elements being selected are described by **Combinations**\n\nIn both cases, the total number of permutations or combinations, varies, depending on whether we allow repeats or not.\n\n## Factorials\n\nThe symbol `!` is the factorial function. `5!` is read as \"five factorial\" and is a shorthand way of writing:\n\n$$5! = 5\\times 4\\times 3\\times 2 \\times 1$$\n\nCombined with the fundamental counting principle, factorials let us mathematically represent situations where\n\n-   order does or does not matter\n\n-   repeats are or are not allowed.\n\n\n::: {.cell layout-align=\"center\"}\n\n|            |Order Matters |Repetition Allowed |Formula                                                                      |Example = {abc}                                                                                                                |\n|:-----------|:-------------|:------------------|:----------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|\n|Permutation |Yes           |Yes                |&nbsp; ${}_{n}P_k =  n^k$                                                    |P(3,3) = Pick three letters in order with replacement = $3^3 = 27$ permutations                                                |\n|Perumation  |Yes           |No                 |&nbsp; ${}_{n}P_k = \\frac{n!}{(n-k)!}$                                       |P(3,3) = Pick three letters in order without replacement = $\\frac{3!}{(3-3)!} = \\frac{3*2*1}{1} = 6$ permutations              |\n|Combination |No            |No                 |&nbsp; ${}_{n}C_k = \\binom{n}{k} = \\frac{{}_{n}P_k}{k!} \\frac{n!}{k!(n-k)!}$ |C(3,3) = Choose three letters without replacement = $\\frac{3!}{3!(3-3)!} = \\frac{3*2*1}{3*2*1} = 1$ combination                |\n|Combination |No            |Yes                |&nbsp; ${}_{n}C_k = \\binom{n + k - 1}{k} =\\frac{(n + k -1)!}{k!(n-1)!}$      |C(3,3) = Choose three letters with replacement = $\\frac{3+3 -1!}{3!(3-1)!} = \\frac{5*4*3*2*1}{(3*2*1)(2*1)} = 10$ combinations |\n:::\n\n\n## Permutations and Combinations\n\nCounting turns out to be vary useful for a number of questions/problems in social science.\n\nFor example, in a randomized experiment, how we randomize treatment assignments can have big implications for how we conduct statistical inference.\n\n-   In short, we will \"analyze as we randomize\"\n-   Compare what we observed from one possible way of assigning treatment, to what we could have observed under different assignments, if some claim (hypothesis) were true\n-   Simple random assignment (equal probability, flipping a coin) yields a lot of possible assignments\n-   Complete random assignment, fixes the \\# treated (e.g. N/2), ruling cases where everyone or no-one gets the treatment\n-   Similarly, paired random assignment randomizes within pairs.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Randomization         |Total # of Treatment Assignments |N=4 |N=8 |N=16   |\n|:---------------------|:--------------------------------|:---|:---|:------|\n|Coin Flip             |&nbsp; $N^2$                     |16  |256 |65,536 |\n|Completely Randomized |&nbsp; $\\binom{N}{N/2}$          |6   |70  |12870  |\n|Pair-Randomized       |&nbsp; $2^{N/2}$                 |4   |16  |256    |\n\n\n:::\n:::\n\n\n## Frequentist interpretations of probability\n\n-   Probabilities from a Frequentist perspective are defined by *fixed* and *unknown* **parameters**\n\n-   The goal of statistics for a frequentist is to learn about these parameters from data.\n\n-   Frequentist statistics often ask questions like \"What is the probability of observing some data $Y$, given a hypothesis about the true value of parameter(s), $\\theta$, that generated it.\n\n## Frequentist interpretations of probability\n\nFor example, suppose we wanted to test whether a coin is \"fair\" $(p = Pr(Heads) = .5; q = Pr(Tails) = 1-p = .5).$ We could:\n\n-   Flip a fair coin 10 times. Our estimate of the $Pr(H)$ is the number of heads divided by 10. It could be 0.5, but also 0 or 1, or some number in between.\n\n-   Flip a coin 100 times and our estimate will be closer to the true $paramter$.\n\n-   Flip a coin an $\\infty$ amount of times and the relative frequency will converge to the true parameter/ $(Pr(H) = \\lim_{n \\to \\infty} \\frac{n_{H}}{n} = p = 0.5 \\text{ for a fair coin})$\n\n## Bayesian interpretations of probability\n\n-   Frequentist interpretations make sense for describing processes that we could easily repeat (e.g. Coin flips, Surveys, Experiments)\n\n--\n\n-   But feel more convoluted when trying to describe events like \"the probability of that Biden wins reelection.\"\n\n--\n\n-   Bayesian interpretations of probability view probabilities as subjective beliefs.\n\n--\n\n-   The task for a Bayesian statistics is to update these *prior* beliefs () based on a model of the *likelihood* of observing some data to form new beliefs after observing the data (called *posterior beliefs*).\n\n-   Bayesians do this using *Bayes Rule*, which says:\n\n$$\\text{posterior} \\propto \\text{likelihood} \\times \\text{prior}$$ More formally:\n\n$$\\underbrace{Pr(\\theta|Y)}_{\\text{Posterior}} \\propto \\underbrace{Pr(Y|\\theta)}_{\\text{Likelihood}}) \\times \\underbrace{Pr(\\theta)}_{\\text{Prior}}$$\n\nbackground-image:url(\"https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png\") background-size:contain\n\n## Bayesian vs Frequentists\n\nOur two main tools for doing statistical inference in this course\n\n-   Hypothesis Testing\n-   Interval Estimation\n\nFollow largely from frequentist interpretations of probability\n\n--\n\nThe differences between Bayesian and Frequentist frameworks, are both philosophical and technical in nature\n\n-   Is probability a relative frequency or subjective belief? How do we form and use prior beliefs\n\n-   Bayesian statistics relies heavily on algorhithms for [Markov Chain Monte-Carlo](http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf) simulations made possible by advances in computing.\n\n--\n\nFor most of the questions in this course, these two frameworks will yield similar (even identical) conclusions.\n\n-   Sometimes it's helpful to think like a Bayesian, others, like a frequentist\n-   All models are wrong, some models are useful.\n\n## Summary: Probability\n\n-   Probability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur\n\n-   Probabilities are:\n\n    -   Non-negative\n    -   Unitary\n    -   Additive\n\n-   Two different interpretations of probability:\n\n    -   Frequentists: Probability is a long run relative frequency\n    -   Bayesians: Probability reflect subjective beliefs which we update upon observing data\n\n-   It helps to know how to count. When we count it matters if we do so with or without **replacement** and with our without regard to order\n\n# ðŸ’¡\n\n# Conditional Probability\n\n### Conditional Probability: Definition\n\nThe conditional probability that event **A** occurred, given that event **B** occurred is written as $Pr(A|B)$ (*\"The probability of A given B\"*) and defined as:\n\n$$Pr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)} = \\frac{\\text{Probability of Both A and B}}{\\text{Probability of B}}$$\n\n-   $Pr(A \\cap B)$ is the same as $Pr(A \\text{ and } B)$ is the **joint probability** of both events occurring\n\n-   $Pr(B)$ is the **marginal probability** of B occuring\n\n### Conditional Probability: Multiplication Rule\n\nJoint probabilities are **symmetrical**. $Pr(A \\cap B) = Pr(B \\cap A)$.\n\nBy rearranging terms:\n\n$$Pr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)}$$ We get the **multiplication rule**:\n\n$$Pr(A \\cap B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A)$$ \\## The Law of Total Probability (Part 2)\n\nWe can use multiplication rule to derive an alternative form of the law of total probability:\n\n$$Pr(A) = Pr(A|B)Pr(B) + Pr(A|B^\\complement)Pr(B^\\complement)$$\n\n## QSS Example (p. 256): Race and Gender in the FL Voter File\n\n`FLVoters` contains a random sample of 10,000 voters\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(qss)\ndata(\"FLVoters\")\ndim(FLVoters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10000     6\n```\n\n\n:::\n\n```{.r .cell-code}\nFLVoters <- na.omit(FLVoters)\ndim(FLVoters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9113    6\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(FLVoters)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     surname county VTD age gender  race\n1     PIEDRA    115  66  58      f white\n2      LYNCH    115  13  51      m white\n4    LATHROP    115  80  54      m white\n5     HUMMEL    115   8  77      f white\n6 CHRISTISON    115  55  49      m white\n7      HOMAN    115  84  77      f white\n```\n\n\n:::\n:::\n\n\n## Marginal Probabilities: Gender\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Pr(Gender)\ntable(FLVoters$gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   f    m \n4883 4230 \n```\n\n\n:::\n\n```{.r .cell-code}\nmargin_gender <- prop.table(table(FLVoters$gender))\nmargin_gender\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n        f         m \n0.5358279 0.4641721 \n```\n\n\n:::\n:::\n\n\n## Marginal Probabilities: Race\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Pr(Race)\ntable(FLVoters$race)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   asian    black hispanic   native    other    white \n     175     1194     1192       29      310     6213 \n```\n\n\n:::\n\n```{.r .cell-code}\nmargin_race <- prop.table(table(FLVoters$race))\nmargin_race\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n      asian       black    hispanic      native       other       white \n0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 \n```\n\n\n:::\n:::\n\n\n## Cross Tab of race and gender\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(FLVoters$gender,FLVoters$race, useNA = \"ifany\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n    asian black hispanic native other white\n  f    83   678      666     17   158  3281\n  m    92   516      526     12   152  2932\n```\n\n\n:::\n:::\n\n\n## Joint probability of race and gender\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njoint_p <- prop.table(table(FLVoters$gender,FLVoters$race))\njoint_p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   \n          asian       black    hispanic      native       other       white\n  f 0.009107868 0.074399210 0.073082410 0.001865467 0.017337869 0.360035115\n  m 0.010095468 0.056622408 0.057719741 0.001316800 0.016679469 0.321738176\n```\n\n\n:::\n:::\n\n\n### Marginal probabilities from joint probabilities:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob_tab<-addmargins(table(FLVoters$gender,FLVoters$race))\nprob_tab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     \n      asian black hispanic native other white  Sum\n  f      83   678      666     17   158  3281 4883\n  m      92   516      526     12   152  2932 4230\n  Sum   175  1194     1192     29   310  6213 9113\n```\n\n\n:::\n:::\n\n\n### Marginal probabilities from joint probabilities: Gender\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Marginal probability of Gender\nprob_tab[,\"Sum\"]/prob_tab[\"Sum\",\"Sum\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        f         m       Sum \n0.5358279 0.4641721 1.0000000 \n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table(table(FLVoters$gender))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n        f         m \n0.5358279 0.4641721 \n```\n\n\n:::\n:::\n\n\n### Marginal probabilities from joint probabilities: Race\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Marginal probability of Race\nprob_tab[\"Sum\",]/prob_tab[\"Sum\",\"Sum\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      asian       black    hispanic      native       other       white \n0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 \n        Sum \n1.000000000 \n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table(table(FLVoters$race))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n      asian       black    hispanic      native       other       white \n0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 \n```\n\n\n:::\n:::\n\n\n### Total Probability: Pr(Black)\n\n$$Pr(Black)=Pr(Black \\cap female) + Pr(Black \\cap male)$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprop.table(table(FLVoters$gender,FLVoters$race))[\"f\",\"black\"] +\n  prop.table(table(FLVoters$gender,FLVoters$race))[\"m\",\"black\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1310216\n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table(table(FLVoters$race))[\"black\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    black \n0.1310216 \n```\n\n\n:::\n:::\n\n\n### Conditional Probability: Pr(Black\\|Female)\n\n$$Pr(Black|Female) = \\frac{Pr(Black \\text{ and } Female)}{Pr(Female)} \\approx \\frac{0.074}{0.536} \\approx 0.139$$\n\n## Independence\n\nEvents $A$ and $B$ are independent if\n\n$$Pr(A|B) = Pr(A) \\text{ and } Pr(B|A) = Pr(B)$$\n\nConceputally, If $A$ and $B$ are **independent** knowing whether $B$ occurred, tells us nothing about $A$, and so the conditional probability of $A$ given $B$, $Pr(A|B)$ is equal to the unconditional, or marginal probability, $Pr(A)$\n\nFormally, two events are statistically independent if and only if the joint probability is equal to product of the marginal probabilities\n\n$$Pr(A\\text{ and }B) = Pr(A)Pr(B)$$\n\nRace and gender are approximately independent in these data\n\n.pull-left\\[\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(x = c(margin_race*margin_gender[\"f\"]),\n     y = joint_p[\"f\",],\n     xlim = c(0,.4),\n     ylim = c(0,.4),\n     xlab = \"Pr(race)*Pr(female)\",\n     ylab = \"Pr(race and gender)\"\n             )\nabline(0,1)\n```\n:::\n\n\n\\] .pull-right\\[\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\\]\n\n## Conditional Independence\n\nWe can extend the concept of independence to situations with more than two events:\n\nIf events $A$, $B$, and $C$ are jointly independent then:\n\n$$Pr(A \\cap B \\cap C) = Pr(A)Pr(B)Pr(C)$$\n\nJoint independence implies pairwise independence and conditional independence:\n\n$$Pr(A \\cap B | C) = Pr(A|C)Pr(B|C)$$ But not the reverse.\n\n## Bayes Rule\n\nBayes rule is theorem for how we should update our beliefs about $A$ given that $B$ occurred:\n\n$$Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}$$ Where\n\n-   $Pr(A)$ is called the prior probability of A (our initial belief)\n\n-   $Pr(A|B)$ is called the posterior probability of A given B (our updated belief after observing B)\n\n### What's the probability you have Covid-19 given a positive test\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$\n\n|               \\| Has Covid \\| Does Not Have Covid \\|\n\n\\|\\|-\\|\\| \\| Test Positive \\| True Positive \\| False Positive \\| \\| Test Negative \\| False Negative \\| True Negative \\|\n\n### What's the probability you have Covid-19 given a positive test\n\nLet's assume:\n\n-   1 out 100 people have Covid-19\n\n-   Our test correctly identifies true positives 95 percent of the time (sensitivity = True Positive Rate)\n\n-   Our test correctly identifies true negatives 95 percent of the time (specifity = True Negative Rate)\n\n### What's the probability you have Covid-19 given a positive test\n\nIn a sample of 100,000 people then:\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$\n\n|               \\| Has Covid \\| Does Not Have Covid \\|\n\n\\|\\|-\\|\\| \\| Test Positive \\| 950 \\| 4950 \\| \\| Test Negative \\| 50 \\| 94050 \\|\n\n-   $Pr(+|Covid) = 950/(1000) \\approx 0.95$\\\n-   $Pr(Covid) = 1000/100000 \\approx 0.01$\\\n-   $Pr(+) = Pr(+|Covid) + Pr(+|Covid)= .95*.01 + .05*.99 \\approx 0.059$\n\n### What's the probability you have Covid-19 given a positive test\n\nConverting this table into marginal probabilities\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$\n\n|               \\| Has Covid \\| Does Not Have Covid \\| Prob(Test)\n\n\\|\\|-\\|\\|\\| \\| Test Positive \\| 0.0095 \\| 0.0495 \\|0.059 \\| Test Negative \\| 0.0005 \\| 0.9505 \\| 0.941 \\| Prob(Covid) \\| 0.01 \\| 0.99 \\| 1\n\n$$Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}$$ $$Pr(Covid|Test +) = \\frac{0.95 \\times 0.01}{0.059} \\approx 0.16$$ \\## What if we took a second test?\n\nWe could use posterior belief as our prior:\n\n$$Pr(Covid|2nd +) = \\frac{0.95 \\times 0.16}{0.16\\times0.95 + (1-0.16)\\times 0.95 } \\approx 0.783$$ Now we're much more confident that we have Covid-19\n\n# ðŸ’¡\n\n# Random Variables and Probability Distributions\n\n## Random Variables\n\n-   Random variables assign numeric values to each event in an experiment.\n\n    -   Mutually exclusive and exhaustive, together cover the entire sample space.\n\n-   Discrete random variables take on finite, or [countably infinite](http://mathworld.wolfram.com/CountablyInfinite.html) distinct values.\n\n-   Continuous variables can take on an uncountably infinite number of values.\n\n## Example: Toss Two Coins\n\n-   $S={TT,TH,HT,HH}$\n-   Let $X$ be the number of heads\n    -   $X(TT)=0$\n    -   $X(TH)=1$\n    -   $X(HT)=1$\n    -   $X(HH)=2$\n\n## Probability Distributions\n\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\n\nThe can be represented in terms of:\n\n-   Probability Mass/Density Functions\n    -   Discrete variables have probability mass functions (PMF)\n    -   Continuous variables have probability density functions (PDF)\n-   Cumulative Density Functions\n    -   Discrete: Summation of discrete probabilities\n    -   Continuous: Integration over a range of values\n\n## Discrete distributions\n\n-   **Probability Mass Function (pmf):** $f(x)=p(X=x)$\n\n-   Assigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\n\n-   **Cumulative Distribution Function (cdf)** $F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)$\n\n    -   Sum of the probability mass for events less than or equal to $x_j$\n\n## Example: Toss Two coins\n\n-   $S={TT,TH,HT,HH}$\n-   Let $X$ be the number of heads\n    -   $X(TT)=0$\n    -   $X(TH)=1$\n    -   $X(HT)=1$\n    -   $X(HH)=2$\n-   $f(X=0)=p(X=0)=1/4$\n-   $f(X=1)=p(X=1)=1/2$\n-   $F(X\\leq 1) = p(X \\leq 1)= 3/4$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X\\<=2) = 1/6 + 1/6 = 1/3\n\n## Continuous distributions\n\n-   **Probability Density Functions (PDF):** $f(x)$\n    -   Assigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\n    -   But... since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n-   **Cumulative Distribution Function (CDF)** $F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx$\n    -   Instead of summing up to a specific value (discrete) we integrate over all possible values up to $x$\n    -   Probability of having a value less than x\n\n## Integrals\n\nFirst, a brief aside on integral calculus:\n\nWhat's the area of the rectangle? $base\\times height$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Integrals\n\nHow would we find the area under a curve?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Integrals\n\nWell suppose we added up the areas of a bunch of rectangles roughly whose height's approximated the height of the curve?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nCan we do any better?\n\n## Integrals\n\nLet's make the rectangles smaller\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get's even better:\n\n## Link between PDF and CDF\n\nIf $$F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx $$\n\nThen by the [fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus)\n\n$$\\frac{d}{dx}F(x)=f(x)$$\n\nIn words\n\n-   the PDF $(f(x))$is the derivative (rate of change) of the CDF $(F(X))$\n\n-   the CDF describes the area under the curve defined by f(x) up to x\n\n## Properties of the CDF\n\n-   $0\\leq F(x) \\leq 1$\n\n-   $F$ is non-decreasing and right continuous\n\n-   $\\lim_{x\\to-\\infty}F(x)=0$\n\n-   $\\lim_{x\\to\\infty}F(x)=1$\n\n-   For all $a,b \\in \\mathbb{R}$ s.t. $a<b$\n\n$$p(a < X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx $$\n\n## Recall the PMF and CDF of a die\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-27-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## What's the probability\n\n-   $p(X=1)...p(X=6) = 1/6$\n\n-   $p( 2 < X \\leq 5) = F(5)-F(2)=5/6-2/6=3/6=1/2$\n\n# Common Probablity Distirbutions\n\nIn this course, we'll use probability distributions to\n\n-   Model the data generating process as a function of parameters we can estimate\n\n-   To perform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)\n\nThere are a lot of probability distributions:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.math.wm.edu/~leemis/chart/UDR/BaseImage.png){fig-align='center' width=60%}\n:::\n:::\n\n\n![Source](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html)\n\nFortunately, the distributions you need to know to really master data science, is probably more something like\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/4854/1*szMCjXuMDfKu6L9T9c34wg.png){fig-align='center' width=60%}\n:::\n:::\n\n\n![Source](https://www.datasciencecentral.com/common-probability-distributions-the-data-scientist-s-crib-sheet/)\n\nAnd the distributions we'll work with the most in this class are an even smaller subset.\n\n## Bernoulli Random Variables\n\nLet's start with our old friend the coin flip\n\nA coin flip is an example of a **Bernoulli random variable** defined by 1 parameter $p$, the probability of success. It has a pmf of\n\n$$f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array} \n    \\right.$$\n\nAnd a CDF of\n\n$$F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x<1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x<1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array} \n    \\right.$$\n\nNote that in our coin flip example $p=0.5$ but it need not. Just imagine a weighted coin like the Patriots use at Foxborough\n\n## Uniform Distribution\n\nOur fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range $[a,b]$, the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of\n\n$$f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                \\frac{1}{b-a}& \\mathrm{if\\ } a \\leq x\\leq b \\\\\n                0 & \\text{otherwise} \\\\\n        \\end{array} \n    \\right.$$\n\nAnd a CDF:\n\n$$F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                        0 & x <a \\\\\n                \\frac{x-a}{b-a}& \\mathrm{if\\ } a \\leq x < b \\\\\n                1 & x \\geq b \\\\\n        \\end{array} \n    \\right.$$\n\nWe won't run into uniform distributions all that often except in examples like rolling a fair sided die, but often they're used in Bayesian analysis as a form of uninformative prior.\n\n## Binomial Distributions\n\nThe binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.\n\nThe key parameters are the number of trials $n$ and the probability of success for each trial $p$ and the pdf of a binomial distribution is:\n\n$$f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n$$ So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThe binomial distribution will come in handy when trying to model binary outcomes.\n\n## Poisson Distributions\n\nWhat would happen if you let the $n$ in a binomial distribution go to infinity and $p$ go to 0 so that $np$ stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter $\\lambda$ which represents rate at which events occur.\n\n$$f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}$$\n\nWe use these distributions to try and predict to predict the [probability of a given number of events occurring in a fixed interval of time.](https://towardsdatascience.com/poisson-distribution-intuition-and-derivation-1059aeab90d) Things like how many acts of political participation would a voter engage in over a year.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-32-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-33-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Geometric Distributions\n\nWhat if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:\n\n$$f(x)=p(1-p) ^{x}$$\n\nGeometric and related distributions are useful for describing the time until an event occurs\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-34-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Exponential Distributions\n\nTaking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a $\\lambda = \\frac{1}{\\beta}$ rate parameter\n\n$$f(x)=\\frac{1}{\\beta}\\exp\\left[-x/\\beta\\right]$$\n\n[Cioffa-Revilla (1984)](https://www.jstor.org/stable/1963367) uses an exponential distribution to model the stability of Italian governments.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-35-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Normal Distribution\n\nFinally, there's the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter $\\mu$ that determines the center of a distribution and a scale parameter $\\sigma^2$ that determines the spread of a distribution\n\n$$f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]$$\n\nStandard normal: $X \\sim N(\\mu =0,\\sigma^2=1)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-36-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n-   As we'll see normal distributions tend to arise when ever you're summing variables.\n\n-   That is sum together a bunch of values from almost any distribution and the **distribution of their sums** tends to follow a normal distribution.\n\n-   Since lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)\n\nConsider a binomial distribution with N=100 and p=.5.\n\nThe pmf of this variable (black lollipops) follows a distribution that's closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.\n\nA relationship explained more generally by the Central Limit Theorem, which we'll cover next week.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-37-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n### What's the $p(X \\leq 0)$ for a normal distirbution with mean 0 and sd 1\n\nSince the normal distribution is so common, it's useful to get practice working with it's pdf and cdf.\n\nConsider the following question: If X is normally distributed variable with $\\mu=0$ and $\\sigma=1$, what's the probability that X is less than 0 $p(X\\leq0)=?$ We could solve:\n\n$$\\int_{-\\infty}^{0}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.5$$\n\nBut R's `pnorm()` function will quickly tell us\n\n-   $p(X\\leq0)=$ 0.5\n\nAnd we can visualize this as follows:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-38-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nConsider some other questions?\n\n-   $p(X=0)=0$\n    -   The probability that a continuous variable is exactly some value is always 0.\n-   $p(X<0)=0.5$\n-   $p(-1< X< 1)$\n-   $p(-2< X< 2)$\n\n### p(-1 \\< X \\< 1)\n\n-   $p(-1< X< 1)=pr(X<1)-pr(X<-1)$\n\n$$\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.841-0.158=0.682$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-39-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n### p(-2 \\< X \\< 2)\n\n-   $p(-2< X\\leq 2)=$ 0.9544997\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-slides_files/figure-revealjs/unnamed-chunk-40-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nWe'll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0.\n\n## Expected Value\n\nA (probability) weighted average of the possible outcomes of a random variable, often labeled $\\mu$\n\nDiscrete:\n\n$$\\mu_X=E(X)=\\sum xp(x)$$\n\nContinuous\n\n$$\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx$$\n\n## What's the expected value of a 1 roll of fair die?\n\n$$\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}$$\n\n## Properties of Expected Values\n\n-   $E(c)=c$\n\n-   $E(a+bX)=a+bE[X]$\n\n-   $E[E[X]]=X$\n\n-   $E[E[Y|X]]=E[Y]$\n\n-   $E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx$\n\n-   $E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)$\n\n-   $E[XY]=E[X]E[Y]$ if $X$ and $Y$ are independent\n\n## How many times would you have to roll a fair die to get all six sides?\n\nWe can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success. The expected value of a geometric variable is:\n\n$$\\begin{align*}E(X)&=\\sum_{k=1}^{\\infty}kp(1-p)^{k-1} \\\\\n&=p\\sum_{k=1}^{\\infty}k(1-p)^{k-1} \\\\\n&=p\\left(-\\frac{d}{dp}\\sum_{k=1}^{\\infty}(1-p)^k\\right) \\text{(Chain rule)} \\\\\n&=p\\left(-\\frac{d}{dp}\\frac{1-p}{p}\\right) \\text{(Geometric Series)} \\\\\n&=p\\left(\\frac{d}{dp}\\left(1-\\frac{1}{p}\\right)\\right)=p\\left(\\frac{1}{p^2}\\right)=\\frac1p\\end{align*}$$\n\nFor this question, we need to calculate the probability of success, p, after getting a side we need.\n\nThe probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nev <- c()\nfor(i in 6:1){\n  ev[i] <- 6/i\n  \n}\n# Expected rolls for each 1 through 6th side\nrev(ev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.0 1.2 1.5 2.0 3.0 6.0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Total \nsum(ev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.7\n```\n\n\n:::\n:::\n\n\n## Variance\n\nIf $X$ has a finite mean $E[X]=\\mu$, the $E[(X-\\mu)^2]$ is finite and called the variance of $X$ which we write as $\\sigma^2$ or $Var[X]$.\n\nNote:\n\n$$\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}$$\n\n-   \"The variance of X is equal to the expected value of X-squared, minus the square of X's expected value.\"\n-   $\\sigma^2=E[X^2]-E[X]^2$ is a useful identity in proofs and derivations\n\n## Variance and Standard Deviations\n\nWe often think of variances $Var[X]$ as describing the spread of a distribution\n\n$$\\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2$$\n\nA standard deviation is just the square root of the variance\n\n$$\\sigma=\\sqrt{Var[X]}$$\n\n## Covariance\n\nCovariance measures the degree to which two random variables vary together.\n\n-   $Cov[X,Y] \\to +$ An increase in $X$ tends to be larger than its mean when $Y$ is larger than its mean\n\n$$Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]$$\n\n## Properties of Variance and Covariance\n\n-   $Cov[X,Y]=E[XY]-E[X]E[Y]$\n\n-   $Var[X]=E[X^2]-(E[X])^2$\n\n-   $Var[X|Y]=E[X^2|Y]-(E[X|Y])^2$\n\n-   $Cov[X,Y]=Cov[X,E[Y|X]]$\n\n-   $Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]$\n\n-   $Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]$\n\nWYNK: Honestly, for this class, you won't need to know these properties. They'll show up in proofs and theorems and become important when you're trying to evaluate properties of an estimator (isn't unbiased, is it \"efficient\", or consistent does it have minimum variance?) but that's for another day/course.\n\n## Correlation\n\n-   The correlation between $X$ and $Y$ is simply the covariance of $X$ and $Y$ divided by the standard deviation of each.\n\n$$\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}$$\n\n-   Normalize covariance to a scale that runs between \\[-1,1\\]\n\n## Question: If two variables have zero covariance, are they independent?\n\nNo\n\nA trivial example\n\n-   $p(X=x)=1/3$ for $x = -1,0,1$ and let $Y=X^2$\n-   Y clearly depends on X even though their covariance is 0\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np=1/3\nx=-1:1\ny=x^2\ncor(x,y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n## Summary: Random Variables and Probability Distributions\n\n-   Random variables assign numeric values to each event in an experiment.\n\n-   Probability distributions assign probabilities to the values that a Random variable can take.\n\n    -   Discrete distributions are despribed by their pmf and cdf\n    -   Continuous distributions by their pdf and cdf\n\n-   Probability distributions let us *describe the data generating process* and encode information about the world into our models\n\n    -   There are lots of distributions\n    -   Don't worry about memorizing formulas\n    -   Do develop intuitions about the nature of your data generating process (Is my outcome continuous or disrecte, binary or count, etc.)\n\n-   Two key features of probability distributions are their:\n\n    -   Expected values\n    -   Variances\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}