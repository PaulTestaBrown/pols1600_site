---
title: "Week 12:"
subtitle: "Review, Presentations, and Workshops"
author: "Paul Testa"
output:
  xaringan::moon_reader:
    css: ["default", "css/brown.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
---

```{r}
#| label = "setup",
#| include = FALSE
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300,
  fig.align = "center", out.width = "80%", cache = FALSE)
library("tidyverse")
```

```{r}
#| label = "xaringan-tile-view",
#| echo = FALSE
xaringanExtra::use_tile_view()
```

```{r}
#| label = "xaringanExtra-clipboard",
#| echo = FALSE
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```



```{r}
#| label = "packages",
#| include = F
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Graphics:
  "scatterplot3d", #<<
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)
```

```{r}
#| label = "ipak",
#| include = F
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

```


```{r}
#| label = "loadpackages",
#| cache = F,
#| include = F
ipak(the_packages)
```


class: inverse, center, middle
# Overview

---
## General Plan

- Setup
- Feedback
- Review
  - Statistical Inference
  - Causal Inference
  - Linear Models
  - Confidence intervals and Hypothesis tests
- Presentations


---
class:inverse, middle, center
# üí™
## Get set up to work

---
## New packages

First we'll install some packages that you will need for your presentations

```{r}
#| eval = FALSE
# Uncomment and run:
# install.packages(remotes)
# remotes::install_github('yihui/xaringan')
# devtools::install_github("gadenbuie/xaringanExtra")
# install.packages("xaringanthemer")

```



---
## Packages for today


```{r}
#| ref.label = c("packages")

```

---
## Define a function to load (and if needed install) packages


```{r}
#| ref.label = "ipak"
```

---
## Load packages for today

```{r}
#| ref.label = "loadpackages"
```



---
class:inverse, middle, center
# üì¢
## Feedback

---
## Feedback on Drafts

- Posted before class on Thursday.

- If you haven't submitted a file on Canvas, do it by COB today.

- For Thursday's class:
  
  - Today and tomorrow, work on translating your draft into a slide presentation
  
  - Come to class with a set of tasks to work on for your presentation.

- Sunday, May 1st, upload final presentations to Canvas

- Sunday, May 8, upload final papers to Canvas
  - You can have a 1-week extension, just email, no questions asked, but must submit by May 15.
  



---
class:inverse, middle, center
# üîç
## Review


---
## What I hope you've learned:

Core Concepts:

- Statistical Inference

- Causal Inference

- Linear Models

- Confidence intervals and Hypothesis tests

Key Skills:

- How to load, transform, summarize, and visualize data

- How to estimate, evaluate, present, and interpret linear models



---
class:inverse, middle, center
# üîç
# Core Concepts

---
## Statistical Inference:

- Statistical inference involves quantifying uncertainty about what could have happened

- We describe uncertainty about what could have happened with distributions

- We can generate these distributions via

  - simulation (Bootstrapping and permutations)
  
  - analytic theory (Limit Theorems)

- We quantify uncertainty using confidence intervals and hypothesis tests



---
## Causal Inference

- Causal inference involves making counterfactual claims about what would have happened had some causal factor $(Z)$ been present or absent.

- The fundamental problem of causal inference is that for an individual observation, we only observe one of many potential outcomes $(Y(Z))$.

- The statistical solution to this problem moves from individual causal effects ($\tau_i = Y_i(1) - Y_i(0)$) to average causal effects $(\tau = ATE = E[Y(1)]-E[Y(0)])$

- Experimental designs identify the ATE by randomly assigning treatment $\to$ $Y(1), Y(0), X, U, \perp Z$
  
- Observational designs approximate the experimental ideal based on identifying assumptions that claim conditional independence $\to$ $Y(1), Y(0), X, U, \perp Z |X$. 
  
  - Difference in Difference $\to$ Parallel Trends
  - Regression Discontinuity $\to$ Continuity at the cut off
  - Instrumental Variables $\to$ The exclusion restriction
  - Regression $\to$ Selection on observable

---
## Linear Regression

- Linear regression provides a linear estimate of the Conditional Expectation Function

  - Bivariate: $E[Y|x] = \beta_0 + \beta_1 x +\epsilon$
  - Multiple regression: $E[Y|X] = X\beta +\epsilon  = \beta_0 + \beta_1 x_1 \dots \beta_k x_k +\epsilon$

- Ordinary Least Linear regression finds coefficients $\beta$ by minimizing the sum of squared errors $(\sum \epsilon^2)$

- Linear regressions partition variance in the outcome into variance explained by the model $(X\beta)$ and variance not explained by the model ($\epsilon$). 

  - A model's $R^2$ describes the proportion of the overall variance in outcome explained by the predictors

---
## Linear Regression

- The coefficient on a predictor describes how the outcome is expected to change with a 1-unit change in that predictor

- Controlling for multiple variables isolates the variation in the outcome explained by one predictor by removing (controlling for) the variation in the outcome and that predictor explained by the other predictors.

  - We control for covariates that are common causes of both our key predictor and our outcome to address omitted variable bias (spurious correlation)

  - We avoid controlling for covariates that our common consequences of our outcome and predictor (collider bias)

---
## Linear Regression

- We present linear regression using regression tables where:

  - Each column corresponds to a model
  - Each row corresponds to a coefficient in the model (with standard errors in parentheses and asterisks denoting p<0.05)

- We use generalized linear models to help us incorporate information about our outcomes to improve our models' predictions

  - Logistic regression is commonly used to model binary outcomes
  
  - Poisson regression is commonly used to model counts
  

- Plots of predicted values can help us interpret more complicated regression models such as:

  - polynomial regressions where the marginal effect of one predictor varies non-linearly
  
  - interaction models, where the marginal effect of one predictor varies with the value of another predictor
  
  - generalized linear models 


---
## Confidence intervals

- A confidence interval describes a range of plausible values for the true (population) value of our estimate

- Our confidence is about the interval:

  - $(1-\alpha)\times 100%$ of the intervals we could construct in repeated sampling are expected to contain the true (population) value of the thing we're estimating

- To construct a confidence interval we need: 

  - An estimate $(\hat{\theta})$
  - A standard error $(\hat{\sigma_{\hat{\theta}}})$ (the standard deviation of sampling distribution)
  - A critical value derived from the hypothetical sampling $(z_{\alpha/2})$

- With these three components the $(1-\alpha)\times 100%$ is $\hat{\theta}\pm z_{\alpha/2} \times \hat{\sigma_{\hat{\theta}}}$

- We report confidence intervals in text: $\beta = 0.9$ $[0.7, 0.11]$

- We interpret estimates as being statistically significant, if 0 is outside the confidence interval


---
## Hypothesis Tests

- A hypothesis test quantifies how likely it is that we would observe what we did (our test statistic), if some claim about the world were true (our hypothesis).

- Typically, we test a null hypothesis that expresses our belief that their is no relationship between variables.

  - $\tau = E[Y|Z=1] - E[Y|Z=0] = 0 \to$ No average treatment effect 
  - $\beta = 0 \to$ No relationship between predictor and outcome 

- If our claim were true, then under the null, our test statistic would have a distribution centered around the truth.

- We can describe this distribution via:

  - simulation (e.g. permuting the outcome)
  - analytic theory (CLT)

- We quantify our uncertainty using a p-value which describes the probability of observing a test statistic as extreme or more extreme in a world where our null hypothesis was true

  - If our p-value is small (p < 0.05), we reject the null hypothesis 

  - If our p-value is large (p > 0.05), we fail to reject the null, or retain the null hypothesis


---
class:inverse, middle, center
# üîç
# Key Skills

---
## How to load, explore and transform data

```{r}
#| eval = F
# Load data
load("df.rda")
df <- readr::read_csv("df.rda")
library(tidyverse)
## Explore data
head(df)
table(df$y)

# Transfrom data:
df %>%
  mutate(
    dv = ifelse(var < 0, NA, y),
    iv = case_when(
      x == 1 ~ "Low",
      x == 2 ~ "Medium"
      x == 3 ~ "High",
      T ~ NA_character_
    ),
    covar_std = scale(z)
  ) -> df
```

---
## How to summarize data

```{r}
#| eval = F
summary(df$dv)

df %>%
  group_by(iv) %>%
  summarize(
    min = min(dv,na.rm = T),
    median = median(dv,na.rm = T),
    mean = median(dv,na.rm = T),
  )
```


---
## How to visualize data

```{r}
#| eval = F
# data
df %>%
  # aesthetics
  ggplot(aes(x = iv, y = dv))+
  # geometries
  geom_point() -> figure1
      
```



---
## How to estimate and evaluate

```{r}
#| eval = F
# Estimate models
m1 <- lm(dv ~ iv, df)
m2 <- lm(dv ~ iv + covar_std, df)
m3 <- glm(dv ~ iv + covar_std, df, family = binomial)

# evaluate models
summary(m1)
confint(m2)

```

---
## How to present, and interpret linear models

```{r}
#| eval = F
# Regression Table
texreg::htmlreg(list(m1, m2, m3))
```

```{r}
#| eval = F
# Produce Predicted values

pred_df <- expand_grid(
  iv = c("Low","Medium","High"),
  covar_std = 0
)

pred_df_m2 <- cbind(pred_df, predict(m2, newdata = pred_df), interval = "confidence")

# Plot Predicted values
pred_df_m2 %>%
  ggplot(aes(iv, fit))+
  geom_pointrange(aes(ymin = lwr, ymax = upr))
```


---
class: inverse, center, middle
# üí°
# Final Presentations

---
## Final Presentations

- Next Tuesday your groups will present some of the findings from your projects

  - 10 Minutes per group
  
  - 8-12 slides (15 max)
  
  - 2 Minute Q&A

- On Thursday, we will work through the templates you've been provided

- Don't have to present the finished product


---
## Final Presentation Structure

1. Motivation

2. Research Question

3. Theory

4. Expectations

5. Data 
  - Summary
  - Descriptive Table and/or Figure (Optional)

6. Design

7. Results

  - Summary
  - Table (Optional)
  - Figure (Optional)

8. Conclusion

  - Appendices (Extra Slides Optional)

---
## Template

Let's open up the template and explore.
