---
title: "POLS 1600"
subtitle: "Multiple Regression"
date: last-modified
date-format: "[Updated ]MMM D, YYYY"
format: 
  revealjs:
    theme: brownslides.scss
    logo: images/pols1600_hex.png
    footer: "POLS 1600"
    multiplex: false
    transition: fade
    slide-number: c
    incremental: true
    center: false
    menu: true
    scrollable: true
    highlight-style: github
    progress: true
    code-overflow: wrap
    chalkboard: true
    html-math-method: mathjax
    # include-after-body: title-slide.html
    title-slide-attributes:
      align: left
      data-background-image: images/pols1600_hex.png
      data-background-position: 90% 50%
      data-background-size: 40%
filters:
  - openlinksinnewpage
execute: 
  eval: true
  echo: true
  warning: false
  message: false
  cache: true
---


```{r}
#| label: init
#| echo: false
#| results: hide
#| warning: false 
#| message: false

library(tidyverse)
library(labelled)
library(haven)
library(DeclareDesign)
library(easystats)
library(texreg)
```



# {{< fa map-location>}} Overview {.inverse}

## Class Plan

- Announcements (20)
  - [Assignment 1: Research Questions](https://pols1600.paultesta.org/assignments/a1) graded by Thursday, March 6
  - [Assignment 2  Data:](https://pols1600.paultesta.org/assignments/a2) now due Sunday March 16
  - [Assignment 3  Data:](https://pols1600.paultesta.org/assignments/a2) now due Sunday March 30 ? (Or should it be right before spring break?)
- Review: Simple Linear Regression and Lab 5 (15-20 min)
- Preview: Setup for Lab 6 (10-15 min)
- Estimating and Interpreting Multiple Regression (25-30 min)
- Difference-in-Differences (15-20 min)


## Annoucements


## Assignment 2

- Prompt posted on website [here]{https://pols1600.paultesta.org/assignments/a2}

- Uploaded to Canvas by [Sunday, March 16]{.blue}

- Revised Research Question

- Implied Linear Model

- Code to load possible data 

## Feedback

- Not a lot

- So let's try to get those numbers up and will summarize the results of the [survey](https://brown.co1.qualtrics.com/jfe/form/SV_aVqL4lh92U7igGG){target="_blank"} next week



## Packages for today

```{r}
#| label: packages
#| echo: true

## Pacakges for today
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)

## Define a function to load (and if needed install) packages

#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

## Install (if needed) and load libraries in the_packages
ipak(the_packages)
```

  

# {{< fa magnifying-glass>}} Lab 5 & Simple Linear Regression {.inverse}

## Review: Key Concepts from the Lab

## Review: Simple Linear Regression

Let's pick up where we left off on Thursday. First we'll need to run some code to get to recreate our data

```{r}
#| label: labdata

# ---- Load data ----
## Covid-19 Data
load(url("https://pols1600.paultesta.org/files/data/covid.rda"))
## Presidential Election Data
load(url("https://pols1600.paultesta.org/files/data/pres_df.rda"))

# ---- Recode Covid Data ----
territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )

# Filter out Territories and create state variable
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)%>%
  mutate(
    state = administrative_area_level_2
  )

# Calculate new cases, new cases per capita, and 7-day average

covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000,
    new_cases_pc_7day = zoo::rollmean(new_cases_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us

# Recode facemask policy

covid_us %>%
mutate(
  # Recode facial_coverings to create face_masks
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ),
    # Turn face_masks into a factor with ordered policy levels
    face_masks = factor(face_masks,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -> covid_us

# Create year-month and percent vaccinated variables

covid_us %>%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(year, 
                       str_pad(month, width = 2, pad=0), 
                       sep = "-"),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us

# Recode Deaths
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = new_deaths / population *100000,
    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA ),
    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, 
                                     k = 14, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us

# ---- Recode Presidential Election Data ----

# Transform Presidential Election data
pres_df %>%
  mutate(
    year_election = year,
    state = str_to_title(state),
    # Fix DC
    state = ifelse(state == "District Of Columbia", "District of Columbia", state)
  ) %>%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%>%
  filter(year == 2020) %>%
  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes
         ) %>%
  pivot_wider(names_from = party_simplified,
              values_from = candidatevotes) %>%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes*100,
    rep_voteshare = REPUBLICAN/totalvotes*100,
    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,"Trump","Biden")))
  ) -> pres2020_df

# ---- Merge Data ----

dim(covid_us)
dim(pres2020_df)
covid_df <- covid_us %>% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(covid_us) # Same number of rows as covid_us w/ 8 additional columns


```

## {.smaller}
### Calculating Conditional Means

::: panel-tabset

## Q6
Now let's revisit question 6, which asked you to calculate some conditional means:

- Overall
- Before the vaccine was widely available
- After the vaccine was widely available



## Data
```{r}
#| label: q6_code

# ---- Deaths: Overall ----
covid_df %>%
  group_by(winner)%>%
  summarise(
    new_deaths = mean(new_deaths, na.rm=T),
    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),
  ) %>% 
  mutate(
    comparison = "Overall"
  ) -> deaths_overall

# ---- Deaths: Pre Vaccine ----
covid_df %>%
  filter(date < "2021-04-19") %>%
  group_by(winner)%>%
  summarise(
    new_deaths = mean(new_deaths, na.rm=T),
    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),
  ) %>% 
  mutate(
    comparison = "Pre Vaccine"
  ) -> deaths_pre_vax

# ---- Deaths: Post Vaccine ----
covid_df %>%
  filter(date >= "2021-04-19") %>%
  group_by(winner)%>%
  summarise(
    new_deaths = mean(new_deaths, na.rm=T),
    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),
  ) %>% 
  mutate(
    comparison = "Post Vaccine"
  ) -> deaths_post_vax

# ---- Tidy outputs for display ----

deaths_tab <- deaths_overall %>% 
  bind_rows(
  deaths_pre_vax,
  deaths_post_vax
) %>% 
  mutate(
    comparison = factor(
      comparison,
      levels = c("Overall","Pre Vaccine", "Post Vaccine")
      )
  )


```

## Table

```{r}
#| lable: q6_tab

knitr::kable(deaths_tab) %>% 
  kableExtra::kable_styling()
```

## {{< fa code >}}-Fig 

```{r}
#| lable: q6fig_code

# 1. Data
deaths_tab %>% 
  # 2. Aesthetics
  ggplot(
    aes(winner, new_deaths_pc_7day,
        fill = winner)
  ) +
  # 3. Geometries
  geom_bar(stat = "identity") +
  # 4. Facets
  facet_grid(~ comparison) +
  # 5. Labels and Themes
  guides(fill = "none") +
  labs(
    x = "State's won by",
    y = "Average # of Deaths per 100k\n(7-day rolling average)",
    title = "Red States have more Covid-19 deaths per capita after vaccine"
  )+
  theme_minimal()+
  theme(title = element_text(size = 10,face = "bold")) -> fig_q6

```

## {{< fa chart-line >}}-Fig 

```{r}
#| label: fig_q6
#| echo: false

fig_q6
```

:::

## {.smaller}  
#### Using OLS to estimate conditional means 

:::: panel-tabset

## Q7
Question 7 asked you estimate the following OLS models:

$$ \text{New Deaths} = \beta_0 + \beta_1 \text{Election Winner} + \epsilon $$

$$ \text{7-day average of New Deaths (per 100k)} = \beta_0 + \beta_1 \text{Election Winner} + \epsilon $$

::: {.callout-note}
Recall `winner` is a [factor]{.blue} whose levels we set to be `c("Trump","Biden")`. 

`lm()` converts factors into [binary indicators]{.blue}. Here `0="Trump"` and `1="Biden"`
:::


## {{< fa code >}}

```{r}
#| label: q7code

m1_lab <- lm(new_deaths ~ winner, covid_df)
m2_lab <- lm(new_deaths_pc_7day ~ winner, covid_df)
```

## Output

```{r}
#| label: q7out

m1_lab
# Just the coefficients
coef(m2_lab)
# Coefficients with summary stats (for later)
summary(m2_lab)

```


## Table

```{r}
#| label: q7tab
#| results: asis
texreg::htmlreg(list(m1_lab,m2_lab),
                custom.header = list(
                  "DV:" = 1:2
                ),
                custom.model.names = c(
                  "new_deaths",
                  "new_deaths_pc_7day"
                ))
```

::::


## {.smaller} 
### Q7: When the CEF is Linear, OLS = CEF


::: panel-tabset

## {{< fa lightbulb >}}

If the [CEF]{.blue} is linear, then [OLS = CEF]{.blue}

In a [saturated]{.blue} linear model, every [group]{.blue} in the data can be represented by a [coefficient]{.blue} or combination of coefficients in the model. 

## {{< fa code >}} -CEF

```{r}
#| label: code_q7cef

covid_df %>%
  mutate(
    biden01 = ifelse(winner == "Biden",1,0)
  ) %>% 
  ggplot(aes(biden01, new_deaths_pc_7day))+
  geom_jitter(size=.25,alpha=.15)+
  stat_summary(col="red")+
  stat_summary(aes(label=round(..y..,2)),
               geom = "text", 
               position = position_nudge(y = 1),
               col = "red",
               fun = mean) +
  theme_minimal()-> fig_q7cef


```

## {{< fa chart-line >}}-CEF
```{r}
#| label: fig_q7cef
#| echo: false

fig_q7cef
```


## {{< fa code >}}-OLS

```{r}
#| label: code_q7m2

fig_q7cef +
  stat_smooth(method = "lm",se = F)+
  stat_summary(aes(label=round(..y..,2)),
               geom = "text", 
               position = position_nudge(y = .05),
               col = "red",
               fun = mean)+
  coord_cartesian(ylim=c(0,.45))+
  geom_segment(aes(
    x = 0,
    xend = 0,
    y = coef(m2_lab)[1],
    yend = coef(m2_lab)[1] + coef(m2_lab)[2]
  ),
  col = "blue",
  linetype = "dashed")+
  geom_segment(aes(
    x = 0,
    xend = 1,
    y = coef(m2_lab)[1] + coef(m2_lab)[2],
    yend = coef(m2_lab)[1] + coef(m2_lab)[2]
  ),
  col = "blue",
  linetype = "dashed")+
  annotate("text",
           label = expression(paste(beta[0]," = " )),
           x = 0, 
           y = coef(m2_lab)[1] + 0.075,
           hjust = "center",
           col = "blue"
           )+
    annotate("text",
           label = expression(paste(beta[1]," = 0.29 - 0.34 = -0.05" )),
           x = 0, 
           y = coef(m2_lab)[1] +coef(m2_lab)[2] - 0.05,
           hjust = "left",
           col = "blue"
           )+
  annotate("text",
           label = expression(
             paste(beta[0]," + ", beta[1], " = " )),
           x = 1, 
           y = coef(m2_lab)[1] + 0.05,
           hjust = "center",
           col = "blue"
           ) -> fig_q7ols

  


```

## {{< fa chart-line >}}-OLS

```{r}
#| label: fig_q7ols
#| echo: false

fig_q7ols

```

:::

## {.smaller}
#### Q8: Vote shares, vaccinations, and deaths

::: panel-tabset

## Q9

Q9 asked you to fit [three]{.blue} models exploring the relationship between:

$$ \text{m3} =\text{14-day average of New Deaths (per 100k)} = \beta_0 + \beta_1 \text{Percent Vaccinated} $$
$$ \text{m4} =\text{Percent Vaccinated} = \beta_0 + \beta_1 \text{Republican Vote Share} $$
$$ \text{m5} =\text{14-day average of New Deaths (per 100k)} = \beta_0 + \beta_1 \text{Republican Vote Share} $$

On September 23, 2021, when Leonhardt was writing

## {{< fa code >}}-OLS

```{r}
#| label: q8_code
# Deaths modeled by percent vaccinated on 2021-09-23
m3_lab <- lm(new_deaths_pc_14day ~ percent_vaccinated, 
         covid_df,
         subset = date == "2021-09-23")

#  Percent vaccinated modeled by Republican vote share on 2021-09-23
m4_lab <- lm(percent_vaccinated ~ rep_voteshare, 
         covid_df,
         subset = date == "2021-09-23")

# Deaths modeled by Republican vote share on 2021-09-23
m5_lab <- lm(new_deaths_pc_14day ~ rep_voteshare, 
         covid_df,
         subset = date == "2021-09-23")

```

## Results

```{r}
#| label: q8_out

coef(m3_lab)
coef(m4_lab)
coef(m5_lab)
```


:::

## {.smaller}
#### Q9: Visualizing Vote Shares and Deaths

::: panel-tabset

## Q9

Q9 asks you to visualize the results of the model `m5`

Let's take a basic figure, and make it fetch!

## {{< fa code >}}-Basic

```{r}
#| label: q9_basic_code

covid_df %>%
  # Only use observations from September 23, 2021
  filter(date == "2021-09-23") %>%
  # Exclude DC
  filter(state != "District of Columbia") %>%
  # Set aesthetics
  ggplot(aes(x = rep_voteshare,
             y = new_deaths_pc_14day))+
  # Set geometries
  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+
  # Include the linear regression of lm(new_deaths_pc_14day ~ rep_voteshare)
  geom_smooth(method = "lm", se=F,
              col = "grey", linetype =2) -> fig_m5_lab


```


## {{< fa chart-line >}}-Basic

```{r}
#| label: q9_basic_fig

fig_m5_lab
```


## {{< fa code >}}-Fetch

```{r}
#| label: q9_fetch_code

fig_m5_lab +
  # two way gradient, blue states -> blue, red states -> red, swing -> grey
  scale_color_gradient2(
    midpoint = 50,
    low = "blue", mid = "grey", high = "red",
    guide = "none")+
  # Vertical line at 50% threshold
  geom_vline(xintercept = 50, 
             col = "grey",linetype = 3)+
  # Add labels
  ggrepel::geom_text_repel(aes(label = state_po), size=2)+
  # theme with minimal lines
  theme_classic()+
  # Labels
  labs(
    x = "Republican Vote Share\n 2020 Presidential Election",
    y = "New Covid-19 Deaths per 100kresidents\n (14-day average)",
    title = "Partisan Gaps in Covid-19 Deaths at the State Level",
    subtitle = "Data from Sept. 23, 2021"
  ) -> fig_m5_lab_fetch

```


## {{< fa chart-line >}}-Fetch

```{r}
#| label: q9_fetch_fig

fig_m5_lab_fetch
```


:::

## Q10: Alternative Explanations

- Finally Q10 asked you to consider some possible [alternative explanations]{.blue} or [omitted variables]{.blue} that might explain the [positive  relationship]{.blue}, between Republican Vote Share and Covid-19 deaths.

- In this week's lab, we'll see how we can use [multiple regression]{.blue} to evaluate these claims.


# {{< fa code >}}Previewing Lab 6

## Red Covid

The core thesis of Red Covid is something like the following:

> Since Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.

## {.smaller}
#### Testing Alternative Explanations of Red Covid 

- A skeptic might argue that this relationship is [spurious]{.blue}. 

- There are lots of ways that Red States differ from Blue States -- demographics, economics, geography, culture -- that might explain the differences in Covid-19 deaths.

## {.smaller}
#### Testing Alternative Explanations of Red Covid 

- In Lab 6, we use multiple regression to try and [control for]{.blue} the following [alternative explanations]{.blue}:

  - Differences in [median age]{.blue}
  - Differences in [median income]{.blue}

- To do this, we need to merge in some additional state level data from the census.

## Loading data from the Census

If you worked through the instructions [here]() and  installed `tidycensus` and a Census API key on your machine, you should be able to run the following:

```{r}
#| label: acs_data
acs_df <- tidycensus::get_acs(geography = "state", 
              variables = c(med_income = "B19013_001",
                            med_age = "B01002_001"), 
              year = 2019)
```

If not, no worries, just uncomment the code below:

```{r}
# Uncomment if get_acs() doesn't work:
# load(url("https://pols1600.paultesta.org/files/data/acs_df.rda"))

```

## Tidy Census Data{.smaller}

::: panel-tabset

## Task

`get_acs()` returns data whose [unit of analysis]{.blue} is roughly a [state-variable]{.blue}

We need to [reshape]{.blue} `acs_df` using `pivot_wider()` so that the [unit of analysis]{.blue} is just a [state]{.blue}, and each column variable is a [column]{.blue}

## {{< fa table >}}-Old

```{r}
acs_df
```


## {{< fa code >}}-Tidy
```{r}
#| label: acs_tidy
acs_df %>%
  mutate(
    state = NAME,
  ) %>%
  select(state, variable, estimate) %>%
  pivot_wider(names_from = variable,
              values_from = estimate) -> acs_df
```

## {{< fa table >}}-New

```{r}
acs_df
```

:::

## Merge Census data into Covid data{.smaller}

::::: panel-tabset


## Task

::::{.nonincremental}

Now we can [merge]{.blue} `acs_df` into `covid_us` using the `left_join()` function

::: {.callout-note}
- In the code, we'll save the output of joining `acs_df` to `covid_us` to a [new dataframe]{.blue} called `covid_df` to avoid potentially [duplicating columns]{.blue}  
:::

::::

## {{< fa code >}}-Merge
```{r}
#| label: acs_merge

dim(acs_df)
dim(acs_df)
dim(covid_us)
# Merge tmp with acs_df and save as final covid_df file
covid_df <- covid_df %>% left_join(
  acs_df,
  by = c("state" = "state")
)
dim(covid_df)  # Same number of rows as tmp w/ 2 additional columns

```

:::::

## Subset Data{.smaller}

::::: panel-tabset

## Task

::::{.nonincremental}
Next, we'll [subset]{.blue} the data to include just the values from variables we'll use in the lab on from September 23, 2021 using:

- `filter()`
- `select()`

::: {.callout-note}
- Again, we'll save this output to a new object called `covid_lab`  
:::

::::

## {{< fa code >}}-Subset

```{r}
the_vars <- c(
  # Covid variables
  "state","state_po","date","new_deaths_pc_14day", "percent_vaccinated",
  # Election variables
  "winner","rep_voteshare",
  # Demographic variables
  "med_age","med_income","population")


covid_lab <- covid_df %>%
  filter( date == "2021-09-23") %>%
  select(all_of(the_vars)) %>%
  ungroup()

length(the_vars)
dim(covid_lab)
```

## {{< fa table >}}-Subsetted Data

```{r}
covid_lab
```

:::::


## Standardized Variables{.smaller}

::::{.nonincremental}

When numeric [variables]{.blue} are measured on [different scales]{.blue}, it can be useful to construct [standardized]{.blue} measures, sometimes called [z-scores]{.blue}

$$z\text{-scores of x} = \frac{x_i - \mu_{x}}{\sigma_x}$$

- The *z-score* of Age is

$$z\text{-scores of Age} = \frac{\text{Age}_i - \text{Average Age}}{\text{Standard Deviation of Age}}$$

::: {.callout-note}
- Standardized variables [all]{.blue} have a [mean of 0]{.blue} and a [standard deviation of 1]{.blue}
- Standardizing variables helps us [interpret coefficients]{.blue} in [multiple regressions]{.blue} with predictors [measured on different scales]{.blue} 
:::

::::

## Standardizing variables for the lab{.smaller}


::::: panel-tabset

## Task

::::{.nonincremental}

Create [standardized versions]{.blue} of

- `rep_voteshare`
- `med_age`
- `med_income`
- `percent_vaccinated`

::: {.callout-note}
- We'll use the suffix `_std` to distinguish the [standardized]{.blue} variables from the [original]{.blue} variables
:::

::::
## {{< fa code >}}-Standardize

```{r}
covid_lab %>%
  mutate(
    rep_voteshare_std = (rep_voteshare - mean(rep_voteshare)) / sd(rep_voteshare),
    med_age_std = ( med_age - mean( med_age)) / sd( med_age),
    med_income_std = (med_income - mean(med_income)) / sd(med_income),
    percent_vaccinated_std = (percent_vaccinated - mean(percent_vaccinated)) / sd(percent_vaccinated)
  ) -> covid_lab
```

## {{< fa table >}}-Compare

```{r}
covid_lab %>%
  summarise(
    mn_rep_vote = mean(rep_voteshare),
    mn_rep_vote_std = round(mean(rep_voteshare_std),2),
    sd_rep_vote = sd(rep_voteshare),
    sd_rep_vote_std = sd(rep_voteshare_std)
  )
```


## {{< fa chart-line >}}-Compare

```{r}
covid_lab %>% 
  ggplot(aes(rep_voteshare_std,rep_voteshare))+
  geom_point()
```


:::::

## Save Data

Finally, I'll save the data for Thursday's lab

```{r}
#| eval: false
# Don't run this code
save(covid_lab, file = "../files/data/06_lab.rda")

```

And on Thursday, we'll be able to load the `covid_lab` just by running:

```{r}
load(url("https://pols1600.paultesta.org/files/data/06_lab.rda"))
```

# {{< fa lightbulb >}} Multiple Regression     {.inverse}

## Conceptual: Multiple Regression

:::{.nonincremental}

- Multiple linear regression generalizes simple regression to models with multiple predictors

$$y = \beta_0 + \beta_1x_{1} +\beta_2x_{2} \dots \beta_kx_k + \epsilon$$
More compactly written in matrix notation:

$$y = X\beta + \epsilon$$
Where:

$$
\overbrace{\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}}^{y}
=
\overbrace{
\begin{bmatrix}
	 1  & x_{1,1}&\dots & x_{1,k}\\
 1  & x_{2,1}&\dots & x_{2,k}\\
 \vdots&\vdots&  & \vdots \\
 1  & x_{n,1}&\dots & x_{n,k}\\
\end{bmatrix}}^{X}
\overbrace{\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots \\
\beta_k\\
\end{bmatrix}}^{\beta}
+
\overbrace{\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{bmatrix}}^{\epsilon}
$$

:::

## Conceptual: Multiple Regression

- Regression models [partition variance]{.blue}

- Separate variation in the outcome ($y$) into variation [explained by the predictors]{.blue} in the model and the [residual]{.blue} variation [not explained]{.blue} by these predictors

- [Regression coefficients]{.blue} tell us how the outcome 
$y$ is expected to change if $x$ changes by one unit, [holding constant]{.blue} or [controlling for]{.blue} other predictors in the model.



## Practical: Multiple Regression

:::{.nonincremental}

- We estimate linear models in R using the `lm()` function using the `+` to [add predictors]{.blue}

- We use the `*` to include the main effects $(\beta_1 x, \beta_2z)$ and [interactions]{.blue} $(\beta_3 (x\cdot z))$of two predictors

```{r}
#| eval: false

lm(y ~ x + z, data = df)
lm(y ~ x*z, data = df) # Is a shortcut for:
lm(y ~ x + z + x:z, data = df)

```

:::

## Technical: Simple Linear Regression {.smaller}

:::{.nonincremental}


- Simple linear regression chooses a $\hat{\beta_0}$ and $\hat{\beta_1}$ to minimize the Sum of Squared Residuals (SSR):

$$
\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\hat{\beta_0}+\hat{\beta_1}x_i))^2
$$
Which yields:

$$\begin{aligned} 
\beta_0 &= \bar{y} - \beta_1 \bar{x}\\
\beta_1 &= \frac{Cov(x,y)}{Var(x)}
\end{aligned}$$

:::

## Technical: Multiple Regression {.smaller}

:::{.nonincremental}


- Multiple linear regression chooses a vector of coefficients $\hat{\beta}$ to minimize the Sum of Squared Residuals (SSR):

$$
\textrm{Find }\widehat{\beta} \text{ argmin }_{\widehat{\beta}} \sum \epsilon^2=\epsilon^\prime\epsilon=(y-X\widehat{\beta})^\prime(y-X\widehat{\beta})
$$
Which yields

$$
\hat{\beta} = (X'X)^{-1}X'y
$$

:::

## Theoretical: Multiple Regression {.smaller}

:::{.nonincremental}


- Multiple Linear regression [still]{.blue} provides a [linear estimate]{.blue} of the conditional expectation function (CEF): $E[Y|X]$ where $Y$ is now a function of [multiple predictors]{.blue}, $X$

:::

# {{< fa lightbulb >}} Estimating and Interpretting Multiple Regression{.inverse}


## {.smaller}

#### Estimating and Interpretting Multiple Regressions 

:::: panel-tabset


## Task

:::{.nonincremental}

Let's [load, inspect, and recode]{.blue} some data from the [2016 NES](https://electionstudies.org/data-center/2016-time-series-study/) and explore the relationship between political interest and evaluations of presidential candidates:

-   Political Interest: "How interested are you in in politics?
    -   Very Interested
    -   Somewhat interested
    -   Not very Interested
    -   Not at all Interested
-   Feeling Thermometer: "... On the feeling thermometer scale of 0 to 100, how would you rate"
    -   Donald Trump
    -   Hillary Clinton

:::

## {{< fa table >}} NES

```{r}
# Load data from 2016 NES
load(url("https://pols1600.paultesta.org/files/data/nes.rda"))


```

## {{< fa table >}} HLO

```{r}
# Take a quick look at the data
dim(nes)
head(nes)
table(nes$pol_interest)
summary(nes$ft_trump)
summary(nes$ft_hrc)

```

## {{< fa code >}} Wrangle

```{r}
nes %>%
  mutate(
    income = ifelse(faminc > 16, NA, faminc),
    interested = ifelse(pol_interest==3,T,F),
    pol_interest_f = factor(case_when(
      pol_interest == 0 ~ "Not at all Interested",
      pol_interest == 1 ~ "Not very Interested",
      pol_interest == 2 ~ "Somewhat Interested",
      pol_interest == 3 ~ "Very Interested"
    )),
    tc_diff = abs(ft_trump - ft_hrc)
  ) -> nes
```

::::

## {.smaller}  

#### Estimating and Interpreting Models

:::: panel-tabset


## Models

Let's estimate the following models:



$$\text{m1: tc_diff} = \beta_0 + \beta_1\text{interested}+\epsilon $$ 

$$\text{m2: tc_diff} = \beta_0 + \beta_1\text{pol_interest}+\epsilon$$ 

$$\text{m3: tc_diff} = \beta_0 + \beta_1\text{pol_interest_f} +\epsilon$$ 

$$\text{m4: tc_diff} = \beta_0 + \beta_1\text{interested} + \beta_2\text{age} +\epsilon$$ 

$$\text{m5: tc_diff} = \beta_0 + \beta_1\text{interested} + \beta_2\text{age} + \beta_3\text{interested} \times \text{age}+\epsilon$$ 

$$\text{m6: tc_diff} = \beta_0 + \beta_1\text{age} + \beta_2\text{income} +\epsilon$$

$$\text{m7: tc_diff} = \beta_0 + \beta_1\text{age} + \beta_2\text{income} + \beta_4\text{age}\times \text{income}+\epsilon $$

## Skills

:::{.nonincremental}

We'll use:

- `lm()` to [estimate models]{.blue}
- `coef()` and `summary()` to [examine]{.blue} our results
- `htmlreg()`to [format]{.blue} our results
- `data.frame()` to create [prediction dataframes]{.blue}
- `predict()` to produce [predicted values]{.blue} and `cbind()` to combine these predicted back into the [prediction dataframes]{.blue} for plotting
- `ggplot()` to display and interpret our models' [predictions]{.blue}

:::

::::

## Regression Tables

- Academic articles are littered with regression tables.

- Below we'll see how to:

  - produce regression tables in R
  - use some heuristics to interpret regression tables

- We'll cover the [why]{.blue} with greater depth and care later in the course, for now, let's focus on the [what and how]{.blue}

## Making Regression Tables in R

:::: panel-tabset

## Task
We can make a very simple regression table using the `htmlreg` function from the `texreg` package

:::{.callout-note}
To properly display your results you need to add `#| results: asis` the following to the code chunk header

:::

## {{< fa code >}}
```{{r}}
#| echo: false
#| results: asis
texreg::htmlreg(
  list(m1_lab,m2_lab, m3_lab,m4_lab,m5_lab)
)
```

## {{< fa table >}}

```{r}
#| echo: false
#| results: asis
texreg::htmlreg(
  list(m1_lab,m2_lab, m3_lab,m4_lab,m5_lab)
)
```

::::

## Interpreting Regression Tables (Stargazing){.smaller}

:::{.nonincremental}
- Each column is a model
- Each row is a coefficient from that model with its [standard error]{.blue} (more to come) in parentheses below
- We interpret coefficients by looking at their [sign, size, and significance]{.blue}
  - Coefficients with asterisks `*` are [statistically significant]{.blue} (more to come)
  - It is unlikely that we would see a coefficient this big or bigger if the true coefficient were 0
- Rule of thumb:

$$\text{If }\frac{\beta}{se} > 2 \to \text{Statistically Significant}$$
:::

## `m1`: A binary indicator {.smaller}

::: panel-tabset

## m1

$$\text{m1: tc_diff} = \beta_0 + \beta_1\text{interested}+\epsilon $$ 



```{r}
#| label: m1
m1 <- lm(tc_diff ~ interested, nes)
coef(m1)
mean(nes$tc_diff[nes$interested == F], na.rm=T)
mean(nes$tc_diff[nes$interested == T], na.rm=T)
mean(nes$tc_diff[nes$interested == T], na.rm=T) -
  mean(nes$tc_diff[nes$interested == F], na.rm=T)

```

## {{< fa table >}}-Table

```{r}
#| label: m1tab
#| results: asis
htmlreg(m1)
```


## {{< fa code >}}-Figure

```{r}
#| label: m1figcode
# Create Prediction Data Frame

pred_df1 <- expand_grid(
  interested = c(F,T)
)

pred_df1  <- cbind(
  pred_df1 ,
  fit = predict(m1, pred_df1 )
)

# Produce figure

pred_df1 %>% 
  ggplot(aes(interested, fit))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(interested)),
    aes(x= interested,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point(col="red") -> fig_tc_m1




```

## {{< fa chart-line >}}-m1

```{r}
#| label: m1fig
#| echo: false

fig_tc_m1
```

:::

## `m2`: A numerical predictor {.smaller}

::: panel-tabset

## m2

$$\text{m2: tc_diff} = \beta_0 + \beta_1\text{pol_interest}+\epsilon$$ 



```{r}
#| label: m2
m2 <- lm(tc_diff ~ pol_interest, nes)
round(coef(m2),2)


```

## {{< fa table >}}-Table

```{r}
#| label: m2tab
#| results: asis
htmlreg(list(m1,m2))
```


## {{< fa code >}}-Figure

```{r}
#| label: m2figcode
# Create Prediction Data Frame

pred_df2 <- expand_grid(
  pol_interest = na.omit(sort(unique(nes$pol_interest)))
)

pred_df2  <- cbind(
  pred_df2 ,
  fit = predict(m2, pred_df2 )
)

# Produce figure

pred_df2 %>% 
  ggplot(aes(pol_interest, fit,
             ))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(pol_interest)),
    aes(x= pol_interest,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_line(col="red") +
  geom_point(col="red") -> fig_tc_m2




```

## {{< fa chart-line >}}-m2

```{r}
#| label: m2fig
#| echo: false

fig_tc_m2
```

:::


## `m3`: A categorical indicator {.smaller}

::: panel-tabset

## m3

$$\text{m3: tc_diff} = \beta_0 + \beta_1\text{pol_interest_f} +\epsilon$$ 



```{r}
#| label: m3
m3 <- lm(tc_diff ~ pol_interest_f, nes)
round(coef(m3),2)

nes %>% 
  group_by(pol_interest_f) %>%
  filter(!is.na(pol_interest_f)) %>% 
  summarise(
    mean = mean(tc_diff,na.rm = T),
    beta = round(mean - coef(m3)[1],3)
  )
```


## {{<fa eye >}} lm()

`lm()` converts the factor `pol_interest_f` into binary indicators for every value of `pol_interest_f` excluding "Not at all Interested", the first level of the factor. 

"Not at all Interested" is the [reference category]{.blue} because all the other coefficients describe how the means for other levels of `pol_interest_f` differ from "Not at all Interested"

```{r}
cbind(
m3$model[26:30,],
model.matrix(m3)[26:30,]
)

```

## {{< fa table >}}-Table

```{r}
#| label: m3tab
#| results: asis
htmlreg(list(m1,m2,m3))
```


## {{< fa code >}}-Figure

```{r}
#| label: m3figcode
# Create Prediction Data Frame

pred_df3 <- expand_grid(
  pol_interest_f = na.omit(sort(unique(nes$pol_interest_f)))
)

pred_df3  <- cbind(
  pred_df3 ,
  fit = predict(m3, pred_df3 )
)

# Produce figure

pred_df3 %>% 
  ggplot(aes(pol_interest_f, fit,
             col = pol_interest_f))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(pol_interest_f)),
    aes(x= pol_interest_f,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point() -> fig_tc_m3




```

## {{< fa chart-line >}}-m3

```{r}
#| label: m3fig
#| echo: false

fig_tc_m3
```

## {{< fa chart-line >}}-m2 vs m3

```{r}
#| label: m3figm2
#| echo: false

nes %>% 
  filter(!is.na(pol_interest)) %>% 
  ggplot(
    aes(pol_interest,
        tc_diff,
        col = pol_interest_f)
  )+
  stat_smooth(method = "lm", col = "red",
              se =F)+
  stat_summary(fun = mean)+
  geom_point(size=.2, alpha =.2)


```


:::

## `m4`: A binary and numerical predictor {.smaller}

::: panel-tabset

## m4

$$\text{m4: tc_diff} = \beta_0 + \beta_1\text{interested} + \beta_2\text{age} +\epsilon$$ 



```{r}
#| label: m4
m4 <- lm(tc_diff ~ interested + age, nes)
coef(m4)


```

## {{< fa table >}}-Table

```{r}
#| label: m4tab
#| results: asis
htmlreg(list(m1, m4))
```


## {{< fa code >}}-Figure

```{r}
#| label: m4figcode
# Create Prediction Data Frame

pred_df4 <- expand_grid(
  interested = c(F,T),
  age = seq(min(nes$age, na.rm = T),
            max(nes$age, na.rm=T),
            length.out = 10)
)

pred_df4  <- cbind(
  pred_df4 ,
  fit = predict(m4, pred_df4 )
)

# Produce figure

pred_df4 %>% 
  ggplot(aes(age, fit,
             col = interested,
             group = interested))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(interested)) %>%
      filter(!is.na(age))
      ,
    aes(x= age,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point()+
  geom_line() -> fig_tc_m4




```

## {{< fa chart-line >}}-m4

```{r}
#| label: m4fig
#| echo: false

fig_tc_m4
```

## {{< fa chart-line >}}-m4 in 3d

```{r}
#| label: m4fig3d
#| echo: false
library(plotly)
library(tidymodels)
library(parsnip)


mesh_size <- 1
margin <- 0

nes_df <- nes %>% 
  filter(!is.na(age)) %>% 
  filter(!is.na(interested)) %>% 
  mutate(
    interest01 = as.numeric(interested),
    Interest = factor(ifelse(interested == T, "Interested", "Not Interested"))
  )

model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(tc_diff ~ interest01 + age, nes_df)

x_min <- min(nes_df$interest01) - margin
x_max <- max(nes_df$interest01) - margin
y_min <- min(nes_df$age) - margin
y_max <- max(nes_df$age) - margin
xrange <- seq(x_min, x_max, mesh_size)
yrange <- seq(y_min, y_max, mesh_size)
xy <- pracma::meshgrid(x = xrange, y = yrange)
xx <- xy$X
yy <- xy$Y
dim_val <- dim(xx)
xx1 <- matrix(xx, length(xx), 1)
yy1 <- matrix(yy, length(yy), 1)
final <- data.frame(interest01 = xx1, age = yy1)
pred <- model %>%
  predict(final)

pred <- pred$.pred
pred <- matrix(pred, dim_val[1], dim_val[2])

pred_col <- pred
pred_col[,1] <- 1
pred_col[,2] <- 1


nes_df %>% 
  plot_ly(
  x = ~interest01,
  y = ~age,
  z = ~ tc_diff,
  color = ~Interest,
   type = "scatter3d", 
  mode = "markers",
  showscale=FALSE
) %>% 
  add_surface(x=xrange, y=yrange, z=pred, alpha = 0.65, 
              name = 'pred_surface',
              type = "surface",
              surfacecolor = pred_col,
              cauto=F
              ) -> fig_tc_m4_3d
fig_tc_m4_3d



```


:::

## {.smaller}

#### `m5`: An interaction between a binary and numerical predictor 

::: panel-tabset

## m5

$$\text{m5: tc_diff} = \beta_0 + \beta_1\text{interested} + \beta_2\text{age} + \beta_3\text{interested} \times \text{age}+\epsilon$$ 



```{r}
#| label: m5
m5 <- lm(tc_diff ~ interested*age, nes)
coef(m5)


```

## {{< fa table >}}-Table

```{r}
#| label: m5tab
#| results: asis
htmlreg(list(m1, m4, m5))
```


## {{< fa code >}}-Figure

```{r}
#| label: m5figcode
# Create Prediction Data Frame

pred_df5 <- expand_grid(
  interested = c(F,T),
  age = seq(min(nes$age, na.rm = T),
            max(nes$age, na.rm=T),
            length.out = 10)
)

pred_df5  <- cbind(
  pred_df5 ,
  fit = predict(m5, pred_df5 )
)

# Produce figure

pred_df5 %>% 
  ggplot(aes(age, fit,
             col = interested,
             group = interested))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(interested)) %>%
      filter(!is.na(age))
      ,
    aes(x= age,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point()+
  geom_line() -> fig_tc_m5




```

## {{< fa chart-line >}}-m5

```{r}
#| label: m5fig
#| echo: false

fig_tc_m5
```

## {{< fa chart-line >}}-m5 in 3d

```{r}
#| label: m5fig3d
#| echo: false


mesh_size <- 1
margin <- 0

nes_df <- nes %>% 
  filter(!is.na(age)) %>% 
  filter(!is.na(interested)) %>% 
  mutate(
    interest01 = as.numeric(interested),
    Interest = factor(ifelse(interested == T, "Interested", "Not Interested"))
  )

model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(tc_diff ~ interest01*age, nes_df)

x_min <- min(nes_df$interest01) - margin
x_max <- max(nes_df$interest01) - margin
y_min <- min(nes_df$age) - margin
y_max <- max(nes_df$age) - margin
xrange <- seq(x_min, x_max, mesh_size)
yrange <- seq(y_min, y_max, mesh_size)
xy <- pracma::meshgrid(x = xrange, y = yrange)
xx <- xy$X
yy <- xy$Y
dim_val <- dim(xx)
xx1 <- matrix(xx, length(xx), 1)
yy1 <- matrix(yy, length(yy), 1)
final <- data.frame(interest01 = xx1, age = yy1)
pred <- model %>%
  predict(final)

pred <- pred$.pred
pred <- matrix(pred, dim_val[1], dim_val[2])

pred_col <- pred
pred_col[,1] <- 1
pred_col[,2] <- 1


nes_df %>% 
  plot_ly(
  x = ~interest01,
  y = ~age,
  z = ~ tc_diff
)  %>% 
  add_markers(size = 5,) %>% 
  add_surface(x=xrange, y=yrange, z=pred, alpha = 0.65, 
              name = 'pred_surface',
              type = "surface",
              cauto=T
              ) %>% 
  hide_legend() %>% 
  hide_colorbar() -> fig_tc_m5_3d

fig_tc_m5_3d



```


:::

## {.smaller}

#### `m6`: Two numerical predictors 

::: panel-tabset

## m6

$$\text{m6: tc_diff} = \beta_0 + \beta_1\text{age} + \beta_2\text{income} +\epsilon$$



```{r}
#| label: m6
m6 <- lm(tc_diff ~ age + income, nes)
coef(m6)


```

## {{< fa table >}}-Table

```{r}
#| label: m6tab
#| results: asis
htmlreg(list( m6))
```


## {{< fa code >}}-Figure

```{r}
#| label: m6figcode
# Create Prediction Data Frame

pred_df6_age <- expand_grid(
  age = seq(min(nes$age, na.rm = T),
            max(nes$age, na.rm=T),
            length.out = 10),
  # Hold income constant at mean value
  income = mean(nes$income,na.rm=T)
)

pred_df6_income <- expand_grid(
  income = seq(min(nes$income, na.rm = T),
            max(nes$income, na.rm=T),
            length.out = 16),
  # Hold income constant at mean value
  age = mean(nes$age,na.rm=T)
)

pred_df6_age  <- cbind(
  pred_df6_age ,
  fit = predict(m6, pred_df6_age )
)

pred_df6_income  <- cbind(
  pred_df6_income ,
  fit = predict(m6, pred_df6_income )
)
# Produce figure

pred_df6_age %>% 
  ggplot(aes(age, fit,
             ))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(income)) %>%
      filter(!is.na(age))
      ,
    aes(x= age,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point()+
  geom_line() +
  labs(title ="Age holding income constant") -> fig_tc_m6_age

pred_df6_income %>% 
  ggplot(aes(income, fit,
             ))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(income)) %>%
      filter(!is.na(age))
      ,
    aes(x= income,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point()+
  geom_line() +
  labs(title = "Income holding age constant") -> fig_tc_m6_income


fig_tc_m6 <- ggpubr::ggarrange(fig_tc_m6_age, fig_tc_m6_income)

```

## {{< fa chart-line >}}-m6

```{r}
#| label: m6fig
#| echo: false

fig_tc_m6
```

## {{< fa chart-line >}}-m6 in 3d

```{r}
#| label: m6fig3d
#| echo: false





mesh_size <- 1
margin <- 0

nes_df <- nes %>% 
  filter(!is.na(age)) %>% 
  filter(!is.na(income)) %>% 
  mutate(
    interest01 = as.numeric(interested),
    Interest = factor(ifelse(interested == T, "Interested", "Not Interested"))
  )

model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(tc_diff ~ age + income, nes_df)

x_min <- min(nes_df$income) - margin
x_max <- max(nes_df$income) - margin
y_min <- min(nes_df$age) - margin
y_max <- max(nes_df$age) - margin
xrange <- seq(x_min, x_max, mesh_size)
yrange <- seq(y_min, y_max, mesh_size)
xy <- pracma::meshgrid(x = xrange, y = yrange)
xx <- xy$X
yy <- xy$Y
dim_val <- dim(xx)
xx1 <- matrix(xx, length(xx), 1)
yy1 <- matrix(yy, length(yy), 1)
final <- data.frame(income = xx1, age = yy1)
pred <- model %>%
  predict(final)

pred <- pred$.pred
pred <- matrix(pred, dim_val[1], dim_val[2])

pred_col <- pred
pred_col[,1] <- 1
pred_col[,2] <- 1


nes_df %>% 
  plot_ly(
  x = ~income,
  y = ~age,
  z = ~ tc_diff
) %>% 
  add_markers(size = 5,) %>% 
  add_surface(x=xrange, y=yrange, z=pred, alpha = 0.65, 
              name = 'pred_surface',
              type = "surface",
              cauto=T
              ) %>% 
  hide_legend() %>% 
  hide_colorbar()-> fig_tc_m6_3d
fig_tc_m6_3d 


```


:::

## {.smaller}

#### `m7`: Interaction between two numerical predictors 

::: panel-tabset

## m7

$$\text{m7: tc_diff} = \beta_0 + \beta_1\text{age} + \beta_2\text{income} + \beta_4\text{age}\times \text{income}+\epsilon $$


```{r}
#| label: m7

m7 <- lm(tc_diff ~ age*income, nes)
coef(m7)


```

## {{< fa table >}}-Table

```{r}
#| label: m7tab
#| results: asis
htmlreg(list( m6, m7))
```


## {{< fa code >}}-Figure

```{r}
#| label: m7figcode
# Create Prediction Data Frame

pred_df7 <- expand_grid(
  age = seq(19, 95,
            by = 4),
  # Hold income constant at mean value
  income = seq(min(nes$income, na.rm = T),
            max(nes$income, na.rm=T),
            length.out = 16)
)



pred_df7  <- cbind(
  pred_df7 ,
  fit = predict(m7, pred_df7 )
)

# Produce figure

# Marginal effect of age at min, median, and max values of income
pred_df7 %>%
  mutate(
    income_at = case_when(
      income == 1 ~ "01",
      income == median(nes$income,na.rm=T) ~ "05",
      income == 16 ~ "16",
      T ~ NA_character_
    )
  ) %>% 
  filter(!is.na(income_at)) %>% 
  ggplot(aes(age, fit,
             ))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(income)) %>%
      filter(!is.na(age))
      ,
    aes(x= age,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point(aes(col =income_at,
                 group = income_at))+
  geom_line(aes(col =income_at,
                group = income_at)
            ) -> fig_tc_m7_age

# Marginal effect of income at min, median, and max values of age
pred_df7 %>%
  mutate(
    age_at = case_when(
      age == min(nes$age, na.rm=T) ~ "19",
      age == 47 ~ "47", # close enough ...
      age ==  max(nes$age, na.rm=T) ~ "95",
      T ~ NA_character_
    )
  ) %>% 
  filter(!is.na(age_at)) %>% 
  ggplot(aes(income, fit,
             ))+
  # Add raw data
  geom_point(
    data = nes %>% 
      filter(!is.na(income)) %>%
      filter(!is.na(age))
      ,
    aes(x= income,
        y= tc_diff),
    alpha = .1,
    size = .2
  ) +
  geom_point(aes(col =age_at,
                 group = age_at))+
  geom_line(aes(col =age_at,
                group = age_at)
            ) -> fig_tc_m7_income


fig_tc_m7 <- ggpubr::ggarrange(fig_tc_m7_age, 
                               fig_tc_m7_income,
                               legend = "bottom")

```

## {{< fa chart-line >}}-m7

```{r}
#| label: m7fig
#| echo: false

fig_tc_m7

```

## {{< fa chart-line >}}-m7 in 3d

```{r}
#| label: m7fig3d
#| echo: false



mesh_size <- 1
margin <- 0

nes_df <- nes %>% 
  filter(!is.na(age)) %>% 
  filter(!is.na(income)) %>% 
  mutate(
    interest01 = as.numeric(interested),
    Interest = factor(ifelse(interested == T, "Interested", "Not Interested"))
  )

model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(tc_diff ~ age*income, nes_df)

x_min <- min(nes_df$income) - margin
x_max <- max(nes_df$income) - margin
y_min <- min(nes_df$age) - margin
y_max <- max(nes_df$age) - margin
xrange <- seq(x_min, x_max, mesh_size)
yrange <- seq(y_min, y_max, mesh_size)
xy <- pracma::meshgrid(x = xrange, y = yrange)
xx <- xy$X
yy <- xy$Y
dim_val <- dim(xx)
xx1 <- matrix(xx, length(xx), 1)
yy1 <- matrix(yy, length(yy), 1)
final <- data.frame(income = xx1, age = yy1)
pred <- model %>%
  predict(final)

pred <- pred$.pred
pred <- matrix(pred, dim_val[1], dim_val[2])




nes_df %>% 
  plot_ly(
  x = ~income,
  y = ~age,
  z = ~ tc_diff
) %>% 
  add_markers(size = 5,) %>% 
  add_surface(x=xrange, y=yrange, z=pred, alpha = 0.65, 
              name = 'pred_surface',
              type = "surface",
              cauto=T
              ) %>% 
  hide_legend() %>% 
  hide_colorbar()-> fig_tc_m7_3d

fig_tc_m7_3d 


```


:::



# {{< fa lightbulb >}} Difference-in-Differences {.inverse}

## Motivating Example: What causes Cholera? {.smaller background-image=https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc, background-opacity=.3}

- In the 1800s, cholera was thought to be transmitted through the air.

- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.

- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. 


## Notation {.smaller}

Let's adopt a little notation to help us think about the logic of Snow's design:

- $D$: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)

- $T$: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).

- $Y_{di}(t)$ the potential outcome of unit $i$ 

  - $Y_{1i}(t)$ the potential outcome of unit $i$ when treated between the two periods 

  - $Y_{0i}(t)$ the potential outcome of unit $i$ when control between the two periods 


## Causal Effects {.smaller}

The individual causal effect for unit i at time t is:

$$\tau_{it} = Y_{1i}(t) − Y_{0i}(t)$$

What we observe is 

$$Y_i(t) = Y_{0i}(t)\cdot(1 − D_i(t)) + Y_{1i}(t)\cdot D_i(t)$$

$D$ only equals 1, when $T$ equals 1, so we never observe $Y_0i(1)$ for the treated units. 

In words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.


## Average Treatment on Treated {.smaller}

Our goal is to estimate the average effect of treatment on treated (ATT):


$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

That is, what would have happened in Lambeth, had their water company not moved their pipes


## Average Treatment on Treated {.smaller}

Our goal is to estimate the average effect of treatment on treated (ATT):

We we can observe is:

|               | Pre-Period (T=0)  | Post-Period (T=1)  |
|-|--|-|
| Treated $D_{i}=1$  |  $E[Y_{0i}(0)\vert D_i = 1]$ | $E[Y_{1i}(1)\vert D_i = 1]$  |
| Control $D_i=0$  |  $E[Y_{0i}(0)\vert D_i = 0]$ | $E[Y_{0i}(1)\vert D_i = 0]$  |


## Data {.smaller}

Because potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):

```{r}
#| label = "choleradat",
#| echo = F
snow <- tibble(Company = c("Lambeth (D=1)", "Southwark and Vauxhall (D=0)"),
               `1849 (T=0)` = c(85,135),
               `1854 (T=1)` = c(19,147),

               )

knitr::kable(snow)

```


## How can we estimate the effect of moving pumps upstream? {.smaller}

Recall, our goal is to estimate the effect of the the treatment on the treated:

$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

Let's conisder some strategies Snow could take to estimate this quantity:


## Before vs after comparisons:{.smaller}

:::{.nonincremental}
- Snow could have compared Labmeth in 1854 $(E[Y_i(1)|D_i = 1] = 19)$ to Lambeth in 1849 $(E[Y_i(0)|D_i = 1]=85)$, and claimed that moving the pumps upstream led to **66 fewer cholera deaths.** 

- Assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved $(E[Y_{0i}(1)|D_i = 1])$.

- A skeptic might argue that Lambeth in 1849 $\neq$ Lambeth in 1854


```{r}
#| echo: false
knitr::kable(snow) |> 
  kable_styling() |> 
  row_spec(1, bold=T,color = "blue")
```

:::


## Treatment-Control comparisons in the Post Period. {.smaller}

:::{.nonincremental}

- Snow could have compared outcomes between Lambeth and S&V in 1954  ($E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to **128 fewer deaths.**

- Here the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved $(E[Y_{0i}(1)|D_i = 1])$

- Again, our skeptic could argue  Lambeth $\neq$ S&V 

```{r}
#| echo: false
knitr::kable(snow) |> 
  kable_styling() |> 
  kableExtra::column_spec(3, bold=T, color="red")
```

:::

## Difference in Differences {.smaller}

:::{.nonincremental}
To address these concerns, Snow employed what we now call a [difference-in-differences]{.blue} design, 

There are two, equivalent ways to view this design. 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$

- Difference 1: Average change between Treated and Control  in Post Period

- Difference 2: Average change between Treated and Control  in Pre Period

:::

## Difference in Differences {.smaller}

:::{.nonincremental}

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$
Is equivalent to: 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\}}_{\text{Post - Pre |Treated }}− \overbrace{\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Post-Pre|Control}}$$


- Difference 1: Average change between Treated over time
- Difference 2: Average change between Control over time

:::

## Difference in Differences {.smaller}


You'll see the DiD design represented both ways, but they produce the same result:

$$
\tau_{ATT} = (19-147) - (85-135) = -78
$$

$$
\tau_{ATT} = (19-85) - (147-135) = -78
$$


## Identifying Assumption of a Difference in Differences Design {.smaller}

The key assumption in this design is what's known as the parallel trends assumption: $E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]$ 

- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&V

## Parallel Trends

```{r}
#| label: paralleltrends
#| echo: false

snow_g <- tibble(
  Period = c(0,0,3,3,0,3),
  Treatment = c(0,1,0, 1,1,1),
  Line = c(1,1,1,1,2,2),
  Company = c("S&V","Lambeth","S&V","Lambeth","Lambeth (D=0)","Lambeth (D=0)"),
  Deaths = c(135,85,147,19,85,97)
)

line_df <- tibble(
  line_type = c("solid","solid","dotted","dotted","solid","solid"),
  line_col = c("red","red","red","red","blue","blue")
)

snow_g %>%
  ggplot(aes(Period,Deaths, col=Company))+
  geom_point()+
  geom_line(linetype=line_df$line_type
            )+
  scale_color_manual(values = c("red","red","blue"))+
  geom_segment(aes(x=3.1,xend=3.1,y=19,yend=147), linetype = 2, col= "gray")+
  annotate(geom="text",x = 3.3,y=125, label = "1",hjust=.5)+
  geom_segment(aes(x=3.2,xend=3.2,y=19,yend=97), linetype = 2,col="gray")+
  annotate(geom="text",x = 3.3,y=55, label = "3",hjust=-.5)+
  geom_segment(aes(x=-.1,xend=-.1,y=85,yend=135), linetype = 2,col="gray")+
  annotate(geom="text",x = -.1,y=120, label = "2",hjust=1.5)+
  xlim(-2,6)+
  scale_x_continuous(breaks = c(0,3),labels = c("Pre","Post"))+
  theme_bw() -> snow_p

snow_p


```

## Using linear regression to estimate a Difference in Difference {.smaller}

:::: panel-tabset

## Concept

- Recall that linear regression provides a...
  - linear estimate of the conditional expectation function
- In the canonincal pre-post, treated and control DiD, $\beta_3$ from the following linear regression will give us the ATT:

:::{.fragment}

$$
y = \beta_0 + \beta_1 Post + \beta_2 Treated + \underbrace{\beta_3Post\times Treated}_{\tau_{ATT}}
$$

:::

## Code

```{r}
#| echo: true
#| label: didcode

cholera_df <- tibble(
  Period = factor(c("Pre","Pre","Post","Post"),
                  levels = c("Pre","Post")),
  Year = c(1849,1849, 1854,1854),
  Treated = factor(c("Control","Treated","Control","Treated")),
  Company = c("S&V","Lambeth","S&V","Lambeth"),
  Deaths = c(135,85,147,19)
)

m_did <- lm(Deaths~Period + Treated + Period:Treated, cholera_df)

m_did

```

## Data

```{r}
#| echo: false
#| label: diddata


DT::datatable(cholera_df)
```


## DiD

```{r}
#| echo: false
#| results: asis
#| label: didtab

texreg::htmlreg(m_did,
                custom.coef.names = c("(Intercept)",
                                      "Post (1854)",
                                      "Treated (Lambeth)",
                                      "Post X Treated (DID)")
                )
```

## Figure

```{r}
#| echo: false
#| label: didfig


snow_g %>%
  ggplot(aes(Period,Deaths,col = Company))+
  geom_point()+
  geom_line()+
  xlim(-1,4)+
  annotate(geom = "text", x = 0, y = 135,
           label = expression(beta[0]), hjust=0.5, vjust =-.5)+
  geom_segment(aes(x=0, xend = 3, y=135, yend=135), 
               linetype = 1, col ="gray30",
               arrow = arrow(length = unit(0.01, "npc")))+
  geom_segment(aes(x=3, xend = 3, y=135, yend=147), 
               linetype = 1, col ="gray30",
               arrow = arrow(length = unit(0.01, "npc")))+
  annotate(geom = "text", x = 3, y = 141,
           label = expression(beta[1]), hjust=-0.5, vjust =0.5,
           col = "gray30") +
  geom_segment(aes(x=3.2, xend = 3.2, y=147, yend=97), 
               linetype = 1, col ="gray50",
               arrow = arrow(length = unit(0.01, "npc")))+
  annotate(geom = "text", x = 3, y = 122.5,
           label = expression(beta[2]), hjust=-0.5, vjust =0.5,col = "gray50")+
  geom_segment(aes(x=3, xend = 3, y=97, yend=19), 
               linetype = 1, col ="gray70",
               arrow = arrow(length = unit(0.01, "npc")))+
  annotate(geom = "text", x = 3, y = 58,
           label = expression(beta[3]), hjust=-0.5, vjust =0.5,col = "gray70")+
  scale_x_continuous(breaks = c(0,3),labels = c("Pre","Post"))+
  theme_bw() -> did_p

did_p

```







:::{.nonincremental}

- $\beta_0=$ Outcome in control (S&V) before treatment
- $\beta_1=$ Fixed, [unit invariant]{.blue} differences between [pre]{.blue} and [post]{.blue} periods
- $\beta_2=$ Fixed, [time invariant]{.blue} differences between [treated]{.blue} and [control]{.blue}
- $\beta_3=$ [Difference-in-Differences]{.blue} = $E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$

:::

::::




## Summary {.smaller}

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
- Differencing twice accounts for fixed differences [across units]{.blue} and [between periods]{.blue}
  - But not time varying differences across units...
  
- The key identifying assumption of a DiD design is the assumption of [parallel trends]{.blue}
  - Absent treatment, treated and control groups
would see the same changes over time.
  - Hard to prove, possible to test



## Extensions and limitations {.smaller}

- Diff-in-Diff easy to estimate with linear regression
- Generalizes to multiple periods and treatment interventions
  - More pre-treatment periods allow you assess "parallel trends" assumption
- Alternative methods 
  - Synthetic control
  - Event Study Designs
- What if you have multiple treatments or treatments that come and go?
  - Panel Matching
  - Generalized Synthetic control


## Applications{.smaller}

- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment

- [Abadie, Diamond, & Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany

- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?





# {{< fa home >}} Summary {.inverse}

## Summary - Linear Regression{.smaller}

- Linear regression produces linear estimates of the Conditional Expectation Function

- We estimate linear regression using `lm()`

:::{.fragment}

```{r}
#| eval: false
m1 <- lm( y ~ x1 + x2 + x3, data = df)
```

:::

- We interpret linear regression by: looking at the [sign]{.blue}, [size]{.blue}, and, eventually, [significance]{.blue} of coefficients

  - the intercept $(\beta_0)$ corresponds to the model's prediction when every other predictor is zero
  - the other $\beta$s describe how $y$ changes with a [unit change] in $x$, controlling for other predictors in the model
  
- We present our results using [regression tables]{.blue} and [figures]{.blue} showing [predicted values]{.blue}


## Summary - Difference-in-Differences{.smaller}

- Difference-in-Differences (DiD) is a powerful research design for observational data that combines a [pre-post comparisons]{.blue} with a [treated and control]{.blue} comparisons
  
- Differencing twice accounts for fixed differences [across units]{.blue} and [between periods]{.blue}

- DiD relies on an assumption of [parallel trends]{.blue}

- We can use linear regression to estimate and generalize DiD designs


## References