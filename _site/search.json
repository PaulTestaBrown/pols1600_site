[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This class is an introduction to applied statistics as practiced in political science. It is computing intensive, and, as such, will enable students to execute basic quantitative analyses of social science data using the linear model with statistical inference arising from re-sampling and permutation based techniques as applied in the R statistical computing language with RStudio. By the end of the course, a successful student will be able to find social science data online, download it, analyze it, and write about how the analyses bear on focused social science or policy questions.\n\n\n\nMore than anything I assume a willingness to engage with mathematics, data analysis, computer programming, and the practice of social science thinking and writing. I also assume you’ve taken at least one class in algebra at the level taught in most high schools in the United States and have used a personal computer to read and type email and other documents and have some experience with the Internet.\nI also assume that you will read the syllabus and that you keep up to date on changes in the syllabus which will be announced in class. You should not expect a response to emails that ask a question already answered in the syllabus.\nThis is an experimental class so you should expect that the syllabus will change throughout the term. Make sure you have the syllabus with the latest date stamp. I will announce syllabus changes via the emails sent from Canvas.\n\n\n\nNeither the University nor I tolerate cheating or plagiarism. The Brown Writing Center defines plagiarism as ``appropriating another person’s ideas or words (spoken or written) without attributing those word or ideas to their true source.’’ The consequences for plagiarism are often severe, and can include suspension or expulsion. This course will follow the guidelines in the Academic Code for determining what is and isn’t plagiarism:\n\nIn preparing assignments a student often needs or is required to employ outside sources of information or opinion. All such sources should be listed in the bibliography. Citations and footnote references are required for all specific facts that are not common knowledge and about which there is not general agreement. New discoveries or debatable opinions must be credited to the source, with specific references to edition and page even when the student restates the matter in his or her own words. Word-for-word inclusion of any part of someone else’s written or oral sentence, even if only a phrase or sentence, requires citation in quotation marks and use of the appropriate conventions for attribution. Citations should normally include author, title, edition, and page. (Quotations longer than one sentence are generally indented from the text of the essay, without quotation marks, and identified by author, title, edition, and page.) Paraphrasing or summarizing the contents of another’s work is not dishonest if the source or sources are clearly identified (author, title, edition, and page), but such paraphrasing does not constitute independent work and may be rejected by the instructor. Students who have questions about accurate and proper citation methods are expected to consult reference guides as well as course instructors.\n\nWe will discuss specific information about your written work in class in more detail, but if you are unsure of how to properly cite material, please ask for clarification. If you are having difficulty with writing or would like more information or assistance, consult the Writing Center, the Brown library and/or the Academic Code for more information.\n\n\n\nAll students and the instructor must be respectful of others in the classroom. If you ever feel that the classroom environment is discouraging your participation or problematic in any way, please contact me.\n\n\n\nBrown University is committed to full inclusion of all students. Please inform me if you have a disability or other condition that might require accommodations or modification of any of these course procedures. You may speak with me after class or during office hours. For more information contact Student and Employee Accessibility Services at 401-863-9588 or SEAS@brown.edu.\n\n\n\nAny student with a documented disability is welcome to contact me as early in the semester as possible so that we may arrange reasonable accommodations. As part of this process, please be in touch with Student Accessibility Services by calling 401-863-9588 or online\n\n\n\nThis course is designed to support an inclusive learning environment where diverse perspectives are recognized, respected and seen as a source of strength. It is my intent to provide materials and activities that are respectful of various levels of diversity: mathematical background, previous computing skills, gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Toward that goal:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official Brown records, please let me know!\nIf there are things going on inside or outside of class that are affecting your performance in class, please don’t hesitate to talk to me, provide anonymous feedback through our course survey, or contact one of Brown’s Academic Deans."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "This class is an introduction to applied statistics as practiced in political science. It is computing intensive, and, as such, will enable students to execute basic quantitative analyses of social science data using the linear model with statistical inference arising from re-sampling and permutation based techniques as applied in the R statistical computing language with RStudio. By the end of the course, a successful student will be able to find social science data online, download it, analyze it, and write about how the analyses bear on focused social science or policy questions."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "",
    "text": "More than anything I assume a willingness to engage with mathematics, data analysis, computer programming, and the practice of social science thinking and writing. I also assume you’ve taken at least one class in algebra at the level taught in most high schools in the United States and have used a personal computer to read and type email and other documents and have some experience with the Internet.\nI also assume that you will read the syllabus and that you keep up to date on changes in the syllabus which will be announced in class. You should not expect a response to emails that ask a question already answered in the syllabus.\nThis is an experimental class so you should expect that the syllabus will change throughout the term. Make sure you have the syllabus with the latest date stamp. I will announce syllabus changes via the emails sent from Canvas."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "",
    "text": "Neither the University nor I tolerate cheating or plagiarism. The Brown Writing Center defines plagiarism as ``appropriating another person’s ideas or words (spoken or written) without attributing those word or ideas to their true source.’’ The consequences for plagiarism are often severe, and can include suspension or expulsion. This course will follow the guidelines in the Academic Code for determining what is and isn’t plagiarism:\n\nIn preparing assignments a student often needs or is required to employ outside sources of information or opinion. All such sources should be listed in the bibliography. Citations and footnote references are required for all specific facts that are not common knowledge and about which there is not general agreement. New discoveries or debatable opinions must be credited to the source, with specific references to edition and page even when the student restates the matter in his or her own words. Word-for-word inclusion of any part of someone else’s written or oral sentence, even if only a phrase or sentence, requires citation in quotation marks and use of the appropriate conventions for attribution. Citations should normally include author, title, edition, and page. (Quotations longer than one sentence are generally indented from the text of the essay, without quotation marks, and identified by author, title, edition, and page.) Paraphrasing or summarizing the contents of another’s work is not dishonest if the source or sources are clearly identified (author, title, edition, and page), but such paraphrasing does not constitute independent work and may be rejected by the instructor. Students who have questions about accurate and proper citation methods are expected to consult reference guides as well as course instructors.\n\nWe will discuss specific information about your written work in class in more detail, but if you are unsure of how to properly cite material, please ask for clarification. If you are having difficulty with writing or would like more information or assistance, consult the Writing Center, the Brown library and/or the Academic Code for more information."
  },
  {
    "objectID": "syllabus.html#community-standards",
    "href": "syllabus.html#community-standards",
    "title": "Syllabus",
    "section": "",
    "text": "All students and the instructor must be respectful of others in the classroom. If you ever feel that the classroom environment is discouraging your participation or problematic in any way, please contact me."
  },
  {
    "objectID": "syllabus.html#accessibility",
    "href": "syllabus.html#accessibility",
    "title": "Syllabus",
    "section": "",
    "text": "Brown University is committed to full inclusion of all students. Please inform me if you have a disability or other condition that might require accommodations or modification of any of these course procedures. You may speak with me after class or during office hours. For more information contact Student and Employee Accessibility Services at 401-863-9588 or SEAS@brown.edu."
  },
  {
    "objectID": "syllabus.html#academic-accommodations",
    "href": "syllabus.html#academic-accommodations",
    "title": "Syllabus",
    "section": "",
    "text": "Any student with a documented disability is welcome to contact me as early in the semester as possible so that we may arrange reasonable accommodations. As part of this process, please be in touch with Student Accessibility Services by calling 401-863-9588 or online"
  },
  {
    "objectID": "syllabus.html#diversity-and-inclusion",
    "href": "syllabus.html#diversity-and-inclusion",
    "title": "Syllabus",
    "section": "",
    "text": "This course is designed to support an inclusive learning environment where diverse perspectives are recognized, respected and seen as a source of strength. It is my intent to provide materials and activities that are respectful of various levels of diversity: mathematical background, previous computing skills, gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Toward that goal:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official Brown records, please let me know!\nIf there are things going on inside or outside of class that are affecting your performance in class, please don’t hesitate to talk to me, provide anonymous feedback through our course survey, or contact one of Brown’s Academic Deans."
  },
  {
    "objectID": "syllabus.html#requirements",
    "href": "syllabus.html#requirements",
    "title": "Syllabus",
    "section": "Requirements",
    "text": "Requirements\nTo accomplish this metamorphosis, we’ll need the following:\n\nSome math\nSome programming and computing skills\nSome general life skills"
  },
  {
    "objectID": "syllabus.html#math",
    "href": "syllabus.html#math",
    "title": "Syllabus",
    "section": "Math",
    "text": "Math\nYou either already know, or will learn, all the math you need to take this course.1 We’ll go over some key theorems of probability and statistics in class, emphasizing conceptual understanding (often illustrated via simulation) over formal proofs.2. Along the way, we’ll need some calculus and linear algebra to make our lives easier, and so we’ll briefly review this material together in class."
  },
  {
    "objectID": "syllabus.html#computing",
    "href": "syllabus.html#computing",
    "title": "Syllabus",
    "section": "Computing",
    "text": "Computing\nDoing quantitative, empirical social science research requires working with data. Today, working with data requires a computer and statistical software. I assume that you have, or will acquire, a laptop that you will bring to class. In terms of software, there are many possible options. In this class,R.3.\nAll the slides, notes, and assignments in this class are produced using R Markdown, a free, open-source tool for creating reproducible research. It’s a short but steep learning curve, the benefits of which (pretty documents, nicely formatted tables and figures, easy integration with citation managers) far outweigh the costs (finicky syntax)"
  },
  {
    "objectID": "syllabus.html#general",
    "href": "syllabus.html#general",
    "title": "Syllabus",
    "section": "General",
    "text": "General\nLike any course, success in this class requires preparation, participation and perseverance. In terms of preparation, I expect that you will have done the readings and submitted your assignments on time (more on that below). In short, you’ll get out of this class what you put in. In terms of participation, I expect that you will come to class eager to learn and engage with that week’s topics. If you have a question, ask it. If you’re getting an error, share it. In some ways, your job is to make errors. To paraphrase Joyce: people of genius make no mistakes. Our errors are volitional and portals of discovery. While this experience can be challenging and frustrating, it is also incredibly rewarding. I fully expect you persevere through the problems and difficulties that inevitably arise in this course, and will do everything I can to help in this process."
  },
  {
    "objectID": "syllabus.html#class",
    "href": "syllabus.html#class",
    "title": "Syllabus",
    "section": "Class",
    "text": "Class\nThis course meets two times a week for 80 minutes on Tuesdays and Thursdays. Tuesday’s class will be devoted to lecture, demonstration and review. Recorded versions of these lectures will be provided on Canvas after class. Thursday’s class will focus on applications of these concepts through brief labs where you’ll work with real data from a variety of sources. I assume that you will come to class having done each week’s assigned readings and reviewed material from the previous week’s lectures and labs. Slides and labs are available on Canvas and https://pols1600.paultesta.org"
  },
  {
    "objectID": "syllabus.html#attendance",
    "href": "syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nYou may miss two classes without it having any effect on the attendance portion of your grade. After two absences, each additional absence (without a written note from the University) will reduce your final grade by 1 percent."
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Syllabus",
    "section": "Readings",
    "text": "Readings\nImai (2022) required textbooks for the course (Estimated cost: \\(\\sim\\)$38.50 for the ebook, $55.00 for the paperback):\n\nImai, Kosuke. 2022. Quantitative Social Science An Introduction in Tidyverse. Princeton University Press.\n\nMost chapters are spread over multiple weeks. You should read this text with your laptop and R Studio open. Execute the code in the main text and ideally try to complete the assignments and exercises at the end of the chapter.4\nAdditional readings will be listed below and available to download on the course website and Canvas."
  },
  {
    "objectID": "syllabus.html#labs",
    "href": "syllabus.html#labs",
    "title": "Syllabus",
    "section": "Labs",
    "text": "Labs\nThe bulk of the work and learning you’ll do in the course comes in the form of weekly labs in which you’ll explore a given data set or paper using R. You’ll be given an R Markdown document that will guide you through a set of exercises to teach concepts covered in the lectures and reading. You’ll code in R and summaries of your findings in R Markdown. You will compile your document to produce an html document which you will submit on Canvas by the end of each class.\nAll work in this class MUST BE SUBMITTED ONLINE VIA CANVAS.\nYou will work in groups on these labs. One member of your group will submit a lab. One question from the lab will be randomly selected for grading."
  },
  {
    "objectID": "syllabus.html#tutorials",
    "href": "syllabus.html#tutorials",
    "title": "Syllabus",
    "section": "Tutorials",
    "text": "Tutorials\nIn addition to weekly labs, you will also work through weekly tutorials made available to you through the `qsslearnr’ package. These tutorials provide you with an opportunity to practice your programming and review concepts from the text and lecture. After completing each tutorial, you will download your progress report and upload this file to Canvas by midnight on Friday each week a tutorial is assigned. If you upload a report by Friday, you receive a grade of 100 % on that Tutorial. If you upload a report after Friday, you receive a grade of 50 %. If you do not upload a report, you receive a grade of 0 %. There are 11 total tutorials for the course. Your lowest grade on the Tutorials will be dropped.\nThese Tutorials are for your personal benefit. You may collaborate with peers, but you must submit your own file."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nIn addition to weekly labs, you will complete periodic group assignments developing an original research presentation applying skills you have learned in this class to a topic of your choosing. All assignments are due the Friday after the class with which they are associated.\nThe timeline of assignments for your final paper is as follows:\n\nWeek 4: Research Topics\nWeek 6: Identifying Datasets\nWeek 8: Data Explorations\nWeek 11: Draft of Research Presentation\nWeek 12: Research Presentations\nWeek 13: Final Paper\n\nAssignments must be submitted on time to Canvas. No late work will be accepted without prior approval of the instructor or a note from the University."
  },
  {
    "objectID": "syllabus.html#grades",
    "href": "syllabus.html#grades",
    "title": "Syllabus",
    "section": "Grades",
    "text": "Grades\nYour final grade for this course will be calculated as follows:\n\n5 % Class attendance\n10 % Class involvement and participation\n10 % Tutorials\n30 % Labs\n20 % Assignments not including the final Paper\n25 % Final Project\n\nLabs, assignments excluding the final presentation, will be graded graded out of 100 roughly on a ✓ + (100, completed on time, acceptable), ✓ (85, completed on time, passable), ✓ - (0 not submitted on time, unacceptable). The lowest two lab grades will be dropped from your final lab grade. Tutorials are graded on pass (submitted on time = 100 % ) - fail (not submitted =0) based submitting your progress report from the tutorial by Friday each week. If you submit a Tutorial after the week it’s do, you will receive partial credit (50 %). Your final projects will be graded on 100-point scales with rubrics provided beforehand.\nIncomplete Work Assignments not turned in will be counted as zero in the calculation of the final grade.\nComputers in class Please bring your laptops if you have them. We will install R and RStudio together. If you do not own a laptop, you can still work in a group of other people who have laptops and will be able to complete the in-class worksheets without a problem. In fact, it is ideal if each group of 2-4 people works with one laptop and then shares the work among themselves. Of course, feel free to work on your own outside of class."
  },
  {
    "objectID": "syllabus.html#time",
    "href": "syllabus.html#time",
    "title": "Syllabus",
    "section": "Time",
    "text": "Time\nThis course meets 27 times over 13 weeks in the semester. Each class is 80 minutes long so you should expect to spend approximately 36 hours total in class; approximately 4 hours per week reading the textbook and reviewing material (42 hours total); approximately 22 hours on tutorials each week, approximately 30 hours on assignments for the final paper; approximately 50 hours researching, writing, and revising your final presentation; and at least .5 hours meeting with me in person to discuss your work (Estimated Total Time: 180.5 hours)"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not the same as all the math you need to know be a successful, methodologically sophisticated political scientist. But it’s a start, and one that will hopefully help you figure out what additional training you’ll need.↩︎\nWe’ll do the proofs as well, but your focus should be on making sure you understand concepts and implications rather than specific derivations↩︎\nAvailable for free at https://cran.r-project.org/. Python is also increasingly common among social scientists.↩︎\nSeriously. Working carefully through these examples will be incredibly helpful and rewarding. If you’re taking the time to read this footnote, send me picture of a cute animal and I’ll add 1 point of extra credit to your final paper grade. See, your hard work is already paying off.↩︎"
  },
  {
    "objectID": "class/09-class.html",
    "href": "class/09-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#readings",
    "href": "class/09-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#lecture",
    "href": "class/09-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#lab",
    "href": "class/09-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#assignments",
    "href": "class/09-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/10-class.html",
    "href": "class/10-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#readings",
    "href": "class/10-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#lecture",
    "href": "class/10-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#lab",
    "href": "class/10-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#assignments",
    "href": "class/10-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/08-class.html",
    "href": "class/08-class.html",
    "title": "Probability I",
    "section": "",
    "text": "Imai (2022) Chapter 6",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#readings",
    "href": "class/08-class.html#readings",
    "title": "Probability I",
    "section": "",
    "text": "Imai (2022) Chapter 6",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#lecture",
    "href": "class/08-class.html#lecture",
    "title": "Probability I",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#lab",
    "href": "class/08-class.html#lab",
    "title": "Probability I",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#assignments",
    "href": "class/08-class.html#assignments",
    "title": "Probability I",
    "section": "Assignments",
    "text": "Assignments\n\n Complete QSS Tutorial 7: Probability 1\n\n\nlearnr::run_tutorial(\"07-probability1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here\n Turn in Assignment 2: Revised Questions",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/05-class.html",
    "href": "class/05-class.html",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 2 & 4\n Red Covid New York Times, 27 September, 2021",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#readings",
    "href": "class/05-class.html#readings",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 2 & 4\n Red Covid New York Times, 27 September, 2021",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#lecture",
    "href": "class/05-class.html#lecture",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#lab",
    "href": "class/05-class.html#lab",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#assignments",
    "href": "class/05-class.html#assignments",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\nlearnr::run_tutorial(\"05-prediction1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/index.html",
    "href": "class/index.html",
    "title": "General Workflow for POLS 1600",
    "section": "",
    "text": "Before class on Tuesday\n\nDo the assigned readings\nReview last week’s lab and slides\n\nTuesday: Come to class ready to engage with lecture and slides\nWednesday:\n\nStart the tutorials for that week\nDownload, render, and skim Thursday’s lab\n\nThursday: Come to class ready to work through that week’s lab\nFriday: Upload that week’s tutorials to Canvas",
    "crumbs": [
      "Class",
      "Overview",
      "General Workflow for POLS 1600"
    ]
  },
  {
    "objectID": "class/12-class.html",
    "href": "class/12-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#readings",
    "href": "class/12-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#lecture",
    "href": "class/12-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#lab",
    "href": "class/12-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#assignments",
    "href": "class/12-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/13-class.html",
    "href": "class/13-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#readings",
    "href": "class/13-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#lecture",
    "href": "class/13-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#lab",
    "href": "class/13-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#assignments",
    "href": "class/13-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/01-class.html",
    "href": "class/01-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#readings",
    "href": "class/01-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#slides",
    "href": "class/01-class.html#slides",
    "title": "Data and Measurement",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#lecture",
    "href": "class/01-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#lab",
    "href": "class/01-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#assignments",
    "href": "class/01-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "slides/06-slides.html#class-plan",
    "href": "slides/06-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements (20)\n\nAssignment 1: Research Questions due Tuesday, March 12\nAssignment 2 Data: due Friday March 22\n\nFeedback\nReview: Simple Linear Regression and Lab 5 (15-20 min)\nPreview: Setup for Lab 6 (10-15 min)\nEstimating and Interpreting Multiple Regression (25-30 min)\nDifference-in-Differences (15-20 min)"
  },
  {
    "objectID": "slides/06-slides.html#annoucements",
    "href": "slides/06-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/06-slides.html#assignment-1",
    "href": "slides/06-slides.html#assignment-1",
    "title": "POLS 1600",
    "section": "Assignment 1",
    "text": "Assignment 1\n\nPrompt posted on website here\n3 possible research questions uploaded to Canvas by Next Tuesday, March 12\nThe ideal experiment = “What would you have to randomly assign to create a meaningful counterfactual comparison”\nObservational study = “What could you say with data that likely exist?”\nFeedback will help you narrow down a topic for final project"
  },
  {
    "objectID": "slides/06-slides.html#assignment-2",
    "href": "slides/06-slides.html#assignment-2",
    "title": "POLS 1600",
    "section": "Assignment 2",
    "text": "Assignment 2\n\nPrompt posted on website [here]{https://pols1600.paultesta.org/assignments/a2}\nUploaded to Canvas by Friday, March 22\nRevised Research Question\nImplied Linear Model\nCode to load possible data"
  },
  {
    "objectID": "slides/06-slides.html#feedback",
    "href": "slides/06-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback\n\nOnly 9 people completed the survey last week…\nSo let’s try to get those numbers up and will summarize the results of the survey next week"
  },
  {
    "objectID": "slides/06-slides.html#packages-for-today",
    "href": "slides/06-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/06-slides.html#review-key-concepts-from-the-lab",
    "href": "slides/06-slides.html#review-key-concepts-from-the-lab",
    "title": "POLS 1600",
    "section": "Review: Key Concepts from the Lab",
    "text": "Review: Key Concepts from the Lab"
  },
  {
    "objectID": "slides/06-slides.html#review-simple-linear-regression",
    "href": "slides/06-slides.html#review-simple-linear-regression",
    "title": "POLS 1600",
    "section": "Review: Simple Linear Regression",
    "text": "Review: Simple Linear Regression\nLet’s pick up where we left off on Thursday. First we’ll need to run some code to get to recreate our data\n\n# ---- Load data ----\n## Covid-19 Data\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n## Presidential Election Data\nload(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n# ---- Recode Covid Data ----\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n# Recode Deaths\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# ---- Recode Presidential Election Data ----\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df\n\n# ---- Merge Data ----\n\ndim(covid_us)\n\n[1] 53678    60\n\ndim(pres2020_df)\n\n[1] 51  9\n\ncovid_us &lt;- covid_us %&gt;% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us) # Same number of rows as covid_us w/ 8 additional columns\n\n[1] 53678    68"
  },
  {
    "objectID": "slides/06-slides.html#section",
    "href": "slides/06-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Calculating Conditional Means\n\nQ6DataTable-Fig-Fig\n\n\nNow let’s revisit question 6, which asked you to calculate some conditional means:\n\nOverall\nBefore the vaccine was widely available\nAfter the vaccine was widely available\n\n\n\n\n# ---- Deaths: Overall ----\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  ) %&gt;% \n  mutate(\n    comparison = \"Overall\"\n  ) -&gt; deaths_overall\n\n# ---- Deaths: Pre Vaccine ----\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  ) %&gt;% \n  mutate(\n    comparison = \"Pre Vaccine\"\n  ) -&gt; deaths_pre_vax\n\n# ---- Deaths: Post Vaccine ----\ncovid_us %&gt;%\n  filter(date &gt;= \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  ) %&gt;% \n  mutate(\n    comparison = \"Post Vaccine\"\n  ) -&gt; deaths_post_vax\n\n# ---- Tidy outputs for display ----\n\ndeaths_tab &lt;- deaths_overall %&gt;% \n  bind_rows(\n  deaths_pre_vax,\n  deaths_post_vax\n) %&gt;% \n  mutate(\n    comparison = factor(\n      comparison,\n      levels = c(\"Overall\",\"Pre Vaccine\", \"Post Vaccine\")\n      )\n  )\n\n\n\n\nknitr::kable(deaths_tab) %&gt;% \n  kableExtra::kable_styling()\n\n\n\n\nwinner\nnew_deaths\nnew_deaths_pc_7day\ncomparison\n\n\n\n\nTrump\n19.34867\n0.3402479\nOverall\n\n\nBiden\n22.04853\n0.2871843\nOverall\n\n\nTrump\n22.82048\n0.4000294\nPre Vaccine\n\n\nBiden\n30.61051\n0.3801906\nPre Vaccine\n\n\nTrump\n17.07402\n0.3016571\nPost Vaccine\n\n\nBiden\n16.30683\n0.2257111\nPost Vaccine\n\n\n\n\n\n\n\n\n\n\n# 1. Data\ndeaths_tab %&gt;% \n  # 2. Aesthetics\n  ggplot(\n    aes(winner, new_deaths_pc_7day,\n        fill = winner)\n  ) +\n  # 3. Geometries\n  geom_bar(stat = \"identity\") +\n  # 4. Facets\n  facet_grid(~ comparison) +\n  # 5. Labels and Themes\n  guides(fill = \"none\") +\n  labs(\n    x = \"State's won by\",\n    y = \"Average # of Deaths per 100k\\n(7-day rolling average)\",\n    title = \"Red States have more Covid-19 deaths per capita after vaccine\"\n  )+\n  theme_minimal()+\n  theme(title = element_text(size = 10,face = \"bold\")) -&gt; fig_q6"
  },
  {
    "objectID": "slides/06-slides.html#section-1",
    "href": "slides/06-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Using OLS to estimate conditional means\n\nQ7OutputTable\n\n\nQuestion 7 asked you estimate the following OLS models:\n\\[ \\text{New Deaths} = \\beta_0 + \\beta_1 \\text{Election Winner} + \\epsilon \\]\n\\[ \\text{7-day average of New Deaths (per 100k)} = \\beta_0 + \\beta_1 \\text{Election Winner} + \\epsilon \\]\n\n\n\n\n\n\nNote\n\n\nRecall winner is a factor whose levels we set to be c(\"Trump\",\"Biden\").\nlm() converts factors into binary indicators. Here 0=\"Trump\" and 1=\"Biden\"\n\n\n\n\n\n\nm1_lab &lt;- lm(new_deaths ~ winner, covid_us)\nm2_lab &lt;- lm(new_deaths_pc_7day ~ winner, covid_us)\n\n\n\n\nm1_lab\n\n\nCall:\nlm(formula = new_deaths ~ winner, data = covid_us)\n\nCoefficients:\n(Intercept)  winnerBiden  \n      19.35         2.70  \n\n# Just the coefficients\ncoef(m2_lab)\n\n(Intercept) winnerBiden \n 0.34024787 -0.05306357 \n\n# Coefficients with summary stats (for later)\nsummary(m2_lab)\n\n\nCall:\nlm(formula = new_deaths_pc_7day ~ winner, data = covid_us)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0016 -0.2205 -0.1340  0.0966  5.8550 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.340248   0.002487  136.83   &lt;2e-16 ***\nwinnerBiden -0.053064   0.003475  -15.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3978 on 52447 degrees of freedom\n  (1229 observations deleted due to missingness)\nMultiple R-squared:  0.004427,  Adjusted R-squared:  0.004408 \nF-statistic: 233.2 on 1 and 52447 DF,  p-value: &lt; 2.2e-16\n\n\n\n\ntexreg::htmlreg(list(m1_lab,m2_lab),\n                custom.header = list(\n                  \"DV:\" = 1:2\n                ),\n                custom.model.names = c(\n                  \"new_deaths\",\n                  \"new_deaths_pc_7day\"\n                ))\n\n\nStatistical models\n\n\n\n\n \n\n\nDV:\n\n\n\n\n \n\n\nnew_deaths\n\n\nnew_deaths_pc_7day\n\n\n\n\n\n\n(Intercept)\n\n\n19.35***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.39)\n\n\n(0.00)\n\n\n\n\nwinnerBiden\n\n\n2.70***\n\n\n-0.05***\n\n\n\n\n \n\n\n(0.55)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n52755\n\n\n52449\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/06-slides.html#section-3",
    "href": "slides/06-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Q7: When the CEF is Linear, OLS = CEF\n\n -CEF-CEF-OLS-OLS\n\n\nIf the CEF is linear, then OLS = CEF\nIn a saturated linear model, every group in the data can be represented by a coefficient or combination of coefficients in the model.\n\n\n\ncovid_us %&gt;%\n  mutate(\n    biden01 = ifelse(winner == \"Biden\",1,0)\n  ) %&gt;% \n  ggplot(aes(biden01, new_deaths_pc_7day))+\n  geom_jitter(size=.25,alpha=.15)+\n  stat_summary(col=\"red\")+\n  stat_summary(aes(label=round(..y..,2)),\n               geom = \"text\", \n               position = position_nudge(y = 1),\n               col = \"red\",\n               fun = mean) +\n  theme_minimal()-&gt; fig_q7cef\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig_q7cef +\n  stat_smooth(method = \"lm\",se = F)+\n  stat_summary(aes(label=round(..y..,2)),\n               geom = \"text\", \n               position = position_nudge(y = .05),\n               col = \"red\",\n               fun = mean)+\n  coord_cartesian(ylim=c(0,.45))+\n  geom_segment(aes(\n    x = 0,\n    xend = 0,\n    y = coef(m2_lab)[1],\n    yend = coef(m2_lab)[1] + coef(m2_lab)[2]\n  ),\n  col = \"blue\",\n  linetype = \"dashed\")+\n  geom_segment(aes(\n    x = 0,\n    xend = 1,\n    y = coef(m2_lab)[1] + coef(m2_lab)[2],\n    yend = coef(m2_lab)[1] + coef(m2_lab)[2]\n  ),\n  col = \"blue\",\n  linetype = \"dashed\")+\n  annotate(\"text\",\n           label = expression(paste(beta[0],\" = \" )),\n           x = 0, \n           y = coef(m2_lab)[1] + 0.075,\n           hjust = \"center\",\n           col = \"blue\"\n           )+\n    annotate(\"text\",\n           label = expression(paste(beta[1],\" = 0.29 - 0.34 = -0.05\" )),\n           x = 0, \n           y = coef(m2_lab)[1] +coef(m2_lab)[2] - 0.05,\n           hjust = \"left\",\n           col = \"blue\"\n           )+\n  annotate(\"text\",\n           label = expression(\n             paste(beta[0],\" + \", beta[1], \" = \" )),\n           x = 1, \n           y = coef(m2_lab)[1] + 0.05,\n           hjust = \"center\",\n           col = \"blue\"\n           ) -&gt; fig_q7ols"
  },
  {
    "objectID": "slides/06-slides.html#section-5",
    "href": "slides/06-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "Q8: Vote shares, vaccinations, and deaths\n\nQ9-OLSResults\n\n\nQ9 asked you to fit three models exploring the relationship between:\n\\[ \\text{m3} =\\text{14-day average of New Deaths (per 100k)} = \\beta_0 + \\beta_1 \\text{Percent Vaccinated} \\] \\[ \\text{m4} =\\text{Percent Vaccinated} = \\beta_0 + \\beta_1 \\text{Republican Vote Share} \\] \\[ \\text{m5} =\\text{14-day average of New Deaths (per 100k)} = \\beta_0 + \\beta_1 \\text{Republican Vote Share} \\]\nOn September 23, 2021, when Leonhardt was writing\n\n\n\n# Deaths modeled by percent vaccinated on 2021-09-23\nm3_lab &lt;- lm(new_deaths_pc_14day ~ percent_vaccinated, \n         covid_us,\n         subset = date == \"2021-09-23\")\n\n#  Percent vaccinated modeled by Republican vote share on 2021-09-23\nm4_lab &lt;- lm(percent_vaccinated ~ rep_voteshare, \n         covid_us,\n         subset = date == \"2021-09-23\")\n\n# Deaths modeled by Republican vote share on 2021-09-23\nm5_lab &lt;- lm(new_deaths_pc_14day ~ rep_voteshare, \n         covid_us,\n         subset = date == \"2021-09-23\")\n\n\n\n\ncoef(m3_lab)\n\n       (Intercept) percent_vaccinated \n         2.4232235         -0.0330773 \n\ncoef(m4_lab)\n\n  (Intercept) rep_voteshare \n   84.3326265    -0.5731175 \n\ncoef(m5_lab)\n\n  (Intercept) rep_voteshare \n  -0.36296799    0.01888996"
  },
  {
    "objectID": "slides/06-slides.html#section-6",
    "href": "slides/06-slides.html#section-6",
    "title": "POLS 1600",
    "section": "",
    "text": "Q9: Visualizing Vote Shares and Deaths\n\nQ9-Basic-Basic-Fetch-Fetch\n\n\nQ9 asks you to visualize the results of the model m5\nLet’s take a basic figure, and make it fetch!\n\n\n\ncovid_us %&gt;%\n  # Only use observations from September 23, 2021\n  filter(date == \"2021-09-23\") %&gt;%\n  # Exclude DC\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14day))+\n  # Set geometries\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # Include the linear regression of lm(new_deaths_pc_14day ~ rep_voteshare)\n  geom_smooth(method = \"lm\", se=F,\n              col = \"grey\", linetype =2) -&gt; fig_m5_lab\n\n\n\n\nfig_m5_lab\n\n\n\n\n\n\n\n\n\n\n\nfig_m5_lab +\n  # two way gradient, blue states -&gt; blue, red states -&gt; red, swing -&gt; grey\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # Vertical line at 50% threshold\n  geom_vline(xintercept = 50, \n             col = \"grey\",linetype = 3)+\n  # Add labels\n  ggrepel::geom_text_repel(aes(label = state_po), size=2)+\n  # theme with minimal lines\n  theme_classic()+\n  # Labels\n  labs(\n    x = \"Republican Vote Share\\n 2020 Presidential Election\",\n    y = \"New Covid-19 Deaths per 100kresidents\\n (14-day average)\",\n    title = \"Partisan Gaps in Covid-19 Deaths at the State Level\",\n    subtitle = \"Data from Sept. 23, 2021\"\n  ) -&gt; fig_m5_lab_fetch\n\n\n\n\nfig_m5_lab_fetch"
  },
  {
    "objectID": "slides/06-slides.html#q10-alternative-explanations",
    "href": "slides/06-slides.html#q10-alternative-explanations",
    "title": "POLS 1600",
    "section": "Q10: Alternative Explanations",
    "text": "Q10: Alternative Explanations\n\nFinally Q10 asked you to consider some possible alternative explanations or omitted variables that might explain the positive relationship, between Republican Vote Share and Covid-19 deaths.\nIn this week’s lab, we’ll see how we can use multiple regression to evaluate these claims."
  },
  {
    "objectID": "slides/06-slides.html#red-covid",
    "href": "slides/06-slides.html#red-covid",
    "title": "POLS 1600",
    "section": "Red Covid",
    "text": "Red Covid\nThe core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States."
  },
  {
    "objectID": "slides/06-slides.html#section-7",
    "href": "slides/06-slides.html#section-7",
    "title": "POLS 1600",
    "section": "",
    "text": "Testing Alternative Explanations of Red Covid\n\nA skeptic might argue that this relationship is spurious.\nThere are lots of ways that Red States differ from Blue States – demographics, economics, geography, culture – that might explain the differences in Covid-19 deaths."
  },
  {
    "objectID": "slides/06-slides.html#section-8",
    "href": "slides/06-slides.html#section-8",
    "title": "POLS 1600",
    "section": "",
    "text": "Testing Alternative Explanations of Red Covid\n\nIn Lab 6, we use multiple regression to try and control for the following alternative explanations:\n\nDifferences in median age\nDifferences in median income\n\nTo do this, we need to merge in some additional state level data from the census."
  },
  {
    "objectID": "slides/06-slides.html#loading-data-from-the-census",
    "href": "slides/06-slides.html#loading-data-from-the-census",
    "title": "POLS 1600",
    "section": "Loading data from the Census",
    "text": "Loading data from the Census\nIf you worked through the instructions here and installed tidycensus and a Census API key on your machine, you should be able to run the following:\n\nacs_df &lt;- tidycensus::get_acs(geography = \"state\", \n              variables = c(med_income = \"B19013_001\",\n                            med_age = \"B01002_001\"), \n              year = 2019)\n\nIf not, no worries, just uncomment the code below:\n\n# Uncomment if get_acs() doesn't work:\n# load(url(\"https://pols1600.paultesta.org/files/data/acs_df.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#tidy-census-data",
    "href": "slides/06-slides.html#tidy-census-data",
    "title": "POLS 1600",
    "section": "Tidy Census Data",
    "text": "Tidy Census Data\n\nTask-Old-Tidy-New\n\n\nget_acs() returns data whose unit of analysis is roughly a state-variable\nWe need to reshape acs_df using pivot_wider() so that the unit of analysis is just a state, and each column variable is a column\n\n\n\nacs_df\n\n# A tibble: 104 × 5\n   GEOID NAME       variable   estimate    moe\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1 01    Alabama    med_age        39      0.2\n 2 01    Alabama    med_income  50536    304  \n 3 02    Alaska     med_age        34.3    0.1\n 4 02    Alaska     med_income  77640   1015  \n 5 04    Arizona    med_age        37.7    0.2\n 6 04    Arizona    med_income  58945    266  \n 7 05    Arkansas   med_age        38.1    0.1\n 8 05    Arkansas   med_income  47597    328  \n 9 06    California med_age        36.5    0.1\n10 06    California med_income  75235    232  \n# ℹ 94 more rows\n\n\n\n\n\nacs_df %&gt;%\n  mutate(\n    state = NAME,\n  ) %&gt;%\n  select(state, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable,\n              values_from = estimate) -&gt; acs_df\n\n\n\n\nacs_df\n\n# A tibble: 52 × 3\n   state                med_age med_income\n   &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alabama                 39        50536\n 2 Alaska                  34.3      77640\n 3 Arizona                 37.7      58945\n 4 Arkansas                38.1      47597\n 5 California              36.5      75235\n 6 Colorado                36.7      72331\n 7 Connecticut             41        78444\n 8 Delaware                40.6      68287\n 9 District of Columbia    34        86420\n10 Florida                 42        55660\n# ℹ 42 more rows"
  },
  {
    "objectID": "slides/06-slides.html#merge-census-data-into-covid-data",
    "href": "slides/06-slides.html#merge-census-data-into-covid-data",
    "title": "POLS 1600",
    "section": "Merge Census data into Covid data",
    "text": "Merge Census data into Covid data\n\nTask-Merge\n\n\n\nNow we can merge acs_df into covid_us using the left_join() function\n\n\n\n\n\n\nNote\n\n\n\nIn the code, we’ll save the output of joining acs_df to covid_us to a new dataframe called covid_df to avoid potentially duplicating columns\n\n\n\n\n\n\n\n\n\ndim(acs_df)\n\n[1] 52  3\n\ndim(acs_df)\n\n[1] 52  3\n\ndim(covid_us)\n\n[1] 53678    68\n\n# Merge tmp with acs_df and save as final covid_df file\ncovid_df &lt;- covid_us %&gt;% left_join(\n  acs_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_df)  # Same number of rows as tmp w/ 2 additional columns\n\n[1] 53678    70"
  },
  {
    "objectID": "slides/06-slides.html#subset-data",
    "href": "slides/06-slides.html#subset-data",
    "title": "POLS 1600",
    "section": "Subset Data",
    "text": "Subset Data\n\nTask-Subset-Subsetted Data\n\n\n\nNext, we’ll subset the data to include just the values from variables we’ll use in the lab on from September 23, 2021 using:\n\nfilter()\nselect()\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, we’ll save this output to a new object called covid_lab\n\n\n\n\n\n\n\n\n\nthe_vars &lt;- c(\n  # Covid variables\n  \"state\",\"state_po\",\"date\",\"new_deaths_pc_14day\", \"percent_vaccinated\",\n  # Election variables\n  \"winner\",\"rep_voteshare\",\n  # Demographic variables\n  \"med_age\",\"med_income\",\"population\")\n\n\ncovid_lab &lt;- covid_df %&gt;%\n  filter( date == \"2021-09-23\") %&gt;%\n  select(all_of(the_vars)) %&gt;%\n  ungroup()\n\nlength(the_vars)\n\n[1] 10\n\ndim(covid_lab)\n\n[1] 51 10\n\n\n\n\n\ncovid_lab\n\n# A tibble: 51 × 10\n   state       state_po date       new_deaths_pc_14day percent_vaccinated winner\n   &lt;chr&gt;       &lt;I&lt;chr&gt;&gt; &lt;date&gt;                   &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt; \n 1 Minnesota   MN       2021-09-23               0.222               61.0 Biden \n 2 California  CA       2021-09-23               0.295               60.8 Biden \n 3 Florida     FL       2021-09-23               1.61                58.4 Trump \n 4 Wyoming     WY       2021-09-23               0.938               43.9 Trump \n 5 South Dako… SD       2021-09-23               0.291               52.4 Trump \n 6 Kansas      KS       2021-09-23               0.559               53.9 Trump \n 7 Nevada      NV       2021-09-23               0.700               51.8 Biden \n 8 Virginia    VA       2021-09-23               0.379               62.5 Biden \n 9 Washington  WA       2021-09-23               0.532               63.2 Biden \n10 Oregon      OR       2021-09-23               0.439               62.3 Biden \n# ℹ 41 more rows\n# ℹ 4 more variables: rep_voteshare &lt;dbl&gt;, med_age &lt;dbl&gt;, med_income &lt;dbl&gt;,\n#   population &lt;int&gt;"
  },
  {
    "objectID": "slides/06-slides.html#standardized-variables",
    "href": "slides/06-slides.html#standardized-variables",
    "title": "POLS 1600",
    "section": "Standardized Variables",
    "text": "Standardized Variables\n\nWhen numeric variables are measured on different scales, it can be useful to construct standardized measures, sometimes called z-scores\n\\[z\\text{-scores of x} = \\frac{x_i - \\mu_{x}}{\\sigma_x}\\]\n\nThe z-score of Age is\n\n\\[z\\text{-scores of Age} = \\frac{\\text{Age}_i - \\text{Average Age}}{\\text{Standard Deviation of Age}}\\]\n\n\n\n\n\n\nNote\n\n\n\nStandardized variables all have a mean of 0 and a standard deviation of 1\nStandardizing variables helps us interpret coefficients in multiple regressions with predictors measured on different scales"
  },
  {
    "objectID": "slides/06-slides.html#standardizing-variables-for-the-lab",
    "href": "slides/06-slides.html#standardizing-variables-for-the-lab",
    "title": "POLS 1600",
    "section": "Standardizing variables for the lab",
    "text": "Standardizing variables for the lab\n\nTask-Standardize-Compare-Compare\n\n\n\nCreate standardized versions of\n\nrep_voteshare\nmed_age\nmed_income\npercent_vaccinated\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll use the suffix _std to distinguish the standardized variables from the original variables\n\n\n\n\n\n\n\n\ncovid_lab %&gt;%\n  mutate(\n    rep_voteshare_std = (rep_voteshare - mean(rep_voteshare)) / sd(rep_voteshare),\n    med_age_std = ( med_age - mean( med_age)) / sd( med_age),\n    med_income_std = (med_income - mean(med_income)) / sd(med_income),\n    percent_vaccinated_std = (percent_vaccinated - mean(percent_vaccinated)) / sd(percent_vaccinated)\n  ) -&gt; covid_lab\n\n\n\n\ncovid_lab %&gt;%\n  summarise(\n    mn_rep_vote = mean(rep_voteshare),\n    mn_rep_vote_std = round(mean(rep_voteshare_std),2),\n    sd_rep_vote = sd(rep_voteshare),\n    sd_rep_vote_std = sd(rep_voteshare_std)\n  )\n\n# A tibble: 1 × 4\n  mn_rep_vote mn_rep_vote_std sd_rep_vote sd_rep_vote_std\n        &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1        49.2               0        12.0               1\n\n\n\n\n\ncovid_lab %&gt;% \n  ggplot(aes(rep_voteshare_std,rep_voteshare))+\n  geom_point()"
  },
  {
    "objectID": "slides/06-slides.html#save-data",
    "href": "slides/06-slides.html#save-data",
    "title": "POLS 1600",
    "section": "Save Data",
    "text": "Save Data\nFinally, I’ll save the data for Thursday’s lab\n\n# Don't run this code\nsave(covid_lab, file = \"../files/data/06_lab.rda\")\n\nAnd on Thursday, we’ll be able to load the covid_lab just by running:\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#conceptual-multiple-regression",
    "href": "slides/06-slides.html#conceptual-multiple-regression",
    "title": "POLS 1600",
    "section": "Conceptual: Multiple Regression",
    "text": "Conceptual: Multiple Regression\n\n\nMultiple linear regression generalizes simple regression to models with multiple predictors\n\n\\[y = \\beta_0 + \\beta_1x_{1} +\\beta_2x_{2} \\dots \\beta_kx_k + \\epsilon\\] More compactly written in matrix notation:\n\\[y = X\\beta + \\epsilon\\] Where:\n\\[\n\\overbrace{\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}}^{y}\n=\n\\overbrace{\n\\begin{bmatrix}\n     1  & x_{1,1}&\\dots & x_{1,k}\\\\\n1  & x_{2,1}&\\dots & x_{2,k}\\\\\n\\vdots&\\vdots&  & \\vdots \\\\\n1  & x_{n,1}&\\dots & x_{n,k}\\\\\n\\end{bmatrix}}^{X}\n\\overbrace{\\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1\\\\\n\\vdots \\\\\n\\beta_k\\\\\n\\end{bmatrix}}^{\\beta}\n+\n\\overbrace{\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{bmatrix}}^{\\epsilon}\n\\]"
  },
  {
    "objectID": "slides/06-slides.html#conceptual-multiple-regression-1",
    "href": "slides/06-slides.html#conceptual-multiple-regression-1",
    "title": "POLS 1600",
    "section": "Conceptual: Multiple Regression",
    "text": "Conceptual: Multiple Regression\n\nRegression models partition variance\nSeparate variation in the outcome (\\(y\\)) into variation explained by the predictors in the model and the residual variation not explained by these predictors\nRegression coefficients tell us how the outcome \\(y\\) is expected to change if \\(x\\) changes by one unit, holding constant or controlling for other predictors in the model."
  },
  {
    "objectID": "slides/06-slides.html#practical-multiple-regression",
    "href": "slides/06-slides.html#practical-multiple-regression",
    "title": "POLS 1600",
    "section": "Practical: Multiple Regression",
    "text": "Practical: Multiple Regression\n\n\nWe estimate linear models in R using the lm() function using the + to add predictors\nWe use the * to include the main effects \\((\\beta_1 x, \\beta_2z)\\) and interactions \\((\\beta_3 (x\\cdot z))\\)of two predictors\n\n\nlm(y ~ x + z, data = df)\nlm(y ~ x*z, data = df) # Is a shortcut for:\nlm(y ~ x + z + x:z, data = df)"
  },
  {
    "objectID": "slides/06-slides.html#technical-multiple-regression",
    "href": "slides/06-slides.html#technical-multiple-regression",
    "title": "POLS 1600",
    "section": "Technical: Multiple Regression",
    "text": "Technical: Multiple Regression\n\n\nSimple linear regression chooses a \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize the Sum of Squared Residuals (SSR):\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\hat{\\beta_0}+\\hat{\\beta_1}x_i))^2\\]\n\nMultiple linear regression chooses a vector of coefficients \\(\\hat{\\beta}\\) to minimize the Sum of Squared Residuals (SSR):\n\n\\[\\textrm{Find }\\widehat{\\beta} \\text{ argmin }_{\\widehat{\\beta}} \\sum \\epsilon^2=\\epsilon^\\prime\\epsilon=(y-X\\widehat{\\beta})^\\prime(y-X\\widehat{\\beta})\\]"
  },
  {
    "objectID": "slides/06-slides.html#theoretical-multiple-regression",
    "href": "slides/06-slides.html#theoretical-multiple-regression",
    "title": "POLS 1600",
    "section": "Theoretical: Multiple Regression",
    "text": "Theoretical: Multiple Regression\n\n\nMultiple Linear regression still provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\) where \\(Y\\) is now a function of multiple predictors, \\(X\\)"
  },
  {
    "objectID": "slides/06-slides.html#section-9",
    "href": "slides/06-slides.html#section-9",
    "title": "POLS 1600",
    "section": "",
    "text": "Estimating and Interpretting Multiple Regressions\n\nTask NES HLO Wrangle\n\n\n\nLet’s load, inspect, and recode some data from the 2016 NES and explore the relationship between political interest and evaluations of presidential candidates:\n\nPolitical Interest: “How interested are you in in politics?\n\nVery Interested\nSomewhat interested\nNot very Interested\nNot at all Interested\n\nFeeling Thermometer: “… On the feeling thermometer scale of 0 to 100, how would you rate”\n\nDonald Trump\nHillary Clinton\n\n\n\n\n\n\n# Load data from 2016 NES\nload(url(\"https://pols1600.paultesta.org/files/data/nes.rda\"))\n\n\n\n\n# Take a quick look at the data\ndim(nes)\n\n[1] 1200   14\n\nhead(nes)\n\n  caseid    state age gender educ faminc pid7 ideo5 pol_interest church_atd\n1    745  Alabama  19      2    3     97    5     3            1          2\n2   1115  Alabama  46      1    3      3    1     1            3          6\n3    258  Alabama  59      2    2      6    2     3            2          1\n4    126  Alabama  55      2    4      6    1     2            3          5\n5    414  Alabama  66      1    3      8    7     4            3          3\n6    523  Alabama  61      1    2     97    1     2            3          6\n  bornagain01 ft_trump ft_hrc vote_choice\n1           0        3     19       Other\n2           0        0     36         HRC\n3           1       22      2        &lt;NA&gt;\n4           0        1     80         HRC\n5           1      100      3        &lt;NA&gt;\n6           1        0    100        &lt;NA&gt;\n\ntable(nes$pol_interest)\n\n\n  0   1   2   3 \n 78 178 348 568 \n\nsummary(nes$ft_trump)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    2.00   30.00   38.38   72.00  100.00       3 \n\nsummary(nes$ft_hrc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    3.00   44.00   42.99   76.00  100.00       1 \n\n\n\n\n\nnes %&gt;%\n  mutate(\n    income = ifelse(faminc &gt; 16, NA, faminc),\n    interested = ifelse(pol_interest==3,T,F),\n    pol_interest_f = factor(case_when(\n      pol_interest == 0 ~ \"Not at all Interested\",\n      pol_interest == 1 ~ \"Not very Interested\",\n      pol_interest == 2 ~ \"Somewhat Interested\",\n      pol_interest == 3 ~ \"Very Interested\"\n    )),\n    tc_diff = abs(ft_trump - ft_hrc)\n  ) -&gt; nes"
  },
  {
    "objectID": "slides/06-slides.html#section-10",
    "href": "slides/06-slides.html#section-10",
    "title": "POLS 1600",
    "section": "",
    "text": "Estimating and Interpreting Models\n\nModelsSkills\n\n\nLet’s estimate the following models:\n\\[\\text{m1: tc_diff} = \\beta_0 + \\beta_1\\text{interested}+\\epsilon \\]\n\\[\\text{m2: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest}+\\epsilon\\]\n\\[\\text{m3: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest_f} +\\epsilon\\]\n\\[\\text{m4: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} +\\epsilon\\]\n\\[\\text{m5: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} + \\beta_3\\text{interested} \\times \\text{age}+\\epsilon\\]\n\\[\\text{m6: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} +\\epsilon\\]\n\\[\\text{m7: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} + \\beta_4\\text{age}\\times \\text{income}+\\epsilon \\]\n\n\n\nWe’ll use:\n\nlm() to estimate models\ncoef() and summary() to examine our results\nhtmlreg()to format our results\ndata.frame() to create prediction dataframes\npredict() to produce predicted values and cbind() to combine these predicted back into the prediction dataframes for plotting\nggplot() to display and interpret our models’ predictions"
  },
  {
    "objectID": "slides/06-slides.html#regression-tables",
    "href": "slides/06-slides.html#regression-tables",
    "title": "POLS 1600",
    "section": "Regression Tables",
    "text": "Regression Tables\n\nAcademic articles are littered with regression tables.\nBelow we’ll see how to:\n\nproduce regression tables in R\nuse some heuristics to interpret regression tables\n\nWe’ll cover the why with greater depth and care later in the course, for now, let’s focus on the what and how"
  },
  {
    "objectID": "slides/06-slides.html#making-regression-tables-in-r",
    "href": "slides/06-slides.html#making-regression-tables-in-r",
    "title": "POLS 1600",
    "section": "Making Regression Tables in R",
    "text": "Making Regression Tables in R\n\nTask\n\n\nWe can make a very simple regression table using the htmlreg function from the texreg package\n\n\n\n\n\n\nNote\n\n\nTo properly display your results you need to add #| results: asis the following to the code chunk header\n\n\n\n\n\n```{r}\n#| echo: false\n#| results: asis\ntexreg::htmlreg(\n  list(m1_lab,m2_lab, m3_lab,m4_lab,m5_lab)\n)\n```\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\n\n\n\n\n(Intercept)\n\n\n19.35***\n\n\n0.34***\n\n\n2.42***\n\n\n84.33***\n\n\n-0.36\n\n\n\n\n \n\n\n(0.39)\n\n\n(0.00)\n\n\n(0.30)\n\n\n(2.66)\n\n\n(0.20)\n\n\n\n\nwinnerBiden\n\n\n2.70***\n\n\n-0.05***\n\n\n \n\n\n \n\n\n \n\n\n\n\n \n\n\n(0.55)\n\n\n(0.00)\n\n\n \n\n\n \n\n\n \n\n\n\n\npercent_vaccinated\n\n\n \n\n\n \n\n\n-0.03***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.01)\n\n\n \n\n\n \n\n\n\n\nrep_voteshare\n\n\n \n\n\n \n\n\n \n\n\n-0.57***\n\n\n0.02***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n0.44\n\n\n0.71\n\n\n0.31\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n0.43\n\n\n0.70\n\n\n0.29\n\n\n\n\nNum. obs.\n\n\n52755\n\n\n52449\n\n\n51\n\n\n51\n\n\n51\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/06-slides.html#interpreting-regression-tables-stargazing",
    "href": "slides/06-slides.html#interpreting-regression-tables-stargazing",
    "title": "POLS 1600",
    "section": "Interpreting Regression Tables (Stargazing)",
    "text": "Interpreting Regression Tables (Stargazing)\n\n\nEach column is a model\nEach row is a coefficient from that model with its standard error (more to come) in parentheses below\nWe interpret coefficients by looking at their sign, size, and significance\n\nCoefficients with asterisks * are statistically significant (more to come)\nIt is unlikely that we would see a coefficient this big or bigger if the true coefficient were 0\n\nRule of thumb:\n\n\\[\\text{If }\\frac{\\beta}{se} &gt; 2 \\to \\text{Statistically Significant}\\]"
  },
  {
    "objectID": "slides/06-slides.html#m1-a-binary-indicator",
    "href": "slides/06-slides.html#m1-a-binary-indicator",
    "title": "POLS 1600",
    "section": "m1: A binary indicator",
    "text": "m1: A binary indicator\n\nm1-Table-Figure-m1\n\n\n\\[\\text{m1: tc_diff} = \\beta_0 + \\beta_1\\text{interested}+\\epsilon \\]\n\nm1 &lt;- lm(tc_diff ~ interested, nes)\ncoef(m1)\n\n   (Intercept) interestedTRUE \n      47.84500       12.74655 \n\nmean(nes$tc_diff[nes$interested == F], na.rm=T)\n\n[1] 47.845\n\nmean(nes$tc_diff[nes$interested == T], na.rm=T)\n\n[1] 60.59155\n\nmean(nes$tc_diff[nes$interested == T], na.rm=T) -\n  mean(nes$tc_diff[nes$interested == F], na.rm=T)\n\n[1] 12.74655\n\n\n\n\nhtmlreg(m1)\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n\n\n \n\n\n(1.26)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n\n\n \n\n\n(1.81)\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df1 &lt;- expand_grid(\n  interested = c(F,T)\n)\n\npred_df1  &lt;- cbind(\n  pred_df1 ,\n  fit = predict(m1, pred_df1 )\n)\n\n# Produce figure\n\npred_df1 %&gt;% \n  ggplot(aes(interested, fit))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(interested)),\n    aes(x= interested,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point(col=\"red\") -&gt; fig_tc_m1"
  },
  {
    "objectID": "slides/06-slides.html#m2-a-numerical-predictor",
    "href": "slides/06-slides.html#m2-a-numerical-predictor",
    "title": "POLS 1600",
    "section": "m2: A numerical predictor",
    "text": "m2: A numerical predictor\n\nm2-Table-Figure-m2\n\n\n\\[\\text{m2: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest}+\\epsilon\\]\n\nm2 &lt;- lm(tc_diff ~ pol_interest, nes)\nround(coef(m2),2)\n\n (Intercept) pol_interest \n       40.80         6.01 \n\n\n\n\nhtmlreg(list(m1,m2))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n40.80***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.35)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n \n\n\n\n\n \n\n\n(1.81)\n\n\n \n\n\n\n\npol_interest\n\n\n \n\n\n6.01***\n\n\n\n\n \n\n\n \n\n\n(0.98)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.03\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.03\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df2 &lt;- expand_grid(\n  pol_interest = na.omit(sort(unique(nes$pol_interest)))\n)\n\npred_df2  &lt;- cbind(\n  pred_df2 ,\n  fit = predict(m2, pred_df2 )\n)\n\n# Produce figure\n\npred_df2 %&gt;% \n  ggplot(aes(pol_interest, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(pol_interest)),\n    aes(x= pol_interest,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_line(col=\"red\") +\n  geom_point(col=\"red\") -&gt; fig_tc_m2"
  },
  {
    "objectID": "slides/06-slides.html#m3-a-categorical-indicator",
    "href": "slides/06-slides.html#m3-a-categorical-indicator",
    "title": "POLS 1600",
    "section": "m3: A categorical indicator",
    "text": "m3: A categorical indicator\n\nm3 lm()-Table-Figure-m3-m2 vs m3\n\n\n\\[\\text{m3: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest_f} +\\epsilon\\]\n\nm3 &lt;- lm(tc_diff ~ pol_interest_f, nes)\nround(coef(m3),2)\n\n                      (Intercept) pol_interest_fNot very Interested \n                            46.10                              1.73 \npol_interest_fSomewhat Interested     pol_interest_fVery Interested \n                             2.14                             14.49 \n\nnes %&gt;% \n  group_by(pol_interest_f) %&gt;%\n  filter(!is.na(pol_interest_f)) %&gt;% \n  summarise(\n    mean = mean(tc_diff,na.rm = T),\n    beta = round(mean - coef(m3)[1],3)\n  )\n\n# A tibble: 4 × 3\n  pol_interest_f         mean  beta\n  &lt;fct&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n1 Not at all Interested  46.1  0   \n2 Not very Interested    47.8  1.73\n3 Somewhat Interested    48.2  2.14\n4 Very Interested        60.6 14.5 \n\n\n\n\nlm() converts the factor pol_interest_f into binary indicators for every value of pol_interest_f excluding “Not at all Interested”, the first level of the factor.\n“Not at all Interested” is the reference category because all the other coefficients describe how the means for other levels of pol_interest_f differ from “Not at all Interested”\n\ncbind(\nm3$model[26:30,],\nmodel.matrix(m3)[26:30,]\n)\n\n   tc_diff        pol_interest_f (Intercept) pol_interest_fNot very Interested\n26      96 Not at all Interested           1                                 0\n27      93   Somewhat Interested           1                                 0\n28      95       Very Interested           1                                 0\n29      75   Somewhat Interested           1                                 0\n30       1   Not very Interested           1                                 1\n   pol_interest_fSomewhat Interested pol_interest_fVery Interested\n26                                 0                             0\n27                                 1                             0\n28                                 0                             1\n29                                 1                             0\n30                                 0                             0\n\n\n\n\nhtmlreg(list(m1,m2,m3))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n40.80***\n\n\n46.10***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.35)\n\n\n(3.53)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n \n\n\n \n\n\n\n\n \n\n\n(1.81)\n\n\n \n\n\n \n\n\n\n\npol_interest\n\n\n \n\n\n6.01***\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.98)\n\n\n \n\n\n\n\npol_interest_fNot very Interested\n\n\n \n\n\n \n\n\n1.73\n\n\n\n\n \n\n\n \n\n\n \n\n\n(4.23)\n\n\n\n\npol_interest_fSomewhat Interested\n\n\n \n\n\n \n\n\n2.14\n\n\n\n\n \n\n\n \n\n\n \n\n\n(3.90)\n\n\n\n\npol_interest_fVery Interested\n\n\n \n\n\n \n\n\n14.49***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(3.76)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.03\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df3 &lt;- expand_grid(\n  pol_interest_f = na.omit(sort(unique(nes$pol_interest_f)))\n)\n\npred_df3  &lt;- cbind(\n  pred_df3 ,\n  fit = predict(m3, pred_df3 )\n)\n\n# Produce figure\n\npred_df3 %&gt;% \n  ggplot(aes(pol_interest_f, fit,\n             col = pol_interest_f))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(pol_interest_f)),\n    aes(x= pol_interest_f,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point() -&gt; fig_tc_m3"
  },
  {
    "objectID": "slides/06-slides.html#m4-a-binary-and-numerical-predictor",
    "href": "slides/06-slides.html#m4-a-binary-and-numerical-predictor",
    "title": "POLS 1600",
    "section": "m4: A binary and numerical predictor",
    "text": "m4: A binary and numerical predictor\n\nm4-Table-Figure-m4-m4 in 3d\n\n\n\\[\\text{m4: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} +\\epsilon\\]\n\nm4 &lt;- lm(tc_diff ~ interested + age, nes)\ncoef(m4)\n\n   (Intercept) interestedTRUE            age \n    32.1757944      9.7739375      0.3533876 \n\n\n\n\nhtmlreg(list(m1, m4))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n32.18***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.71)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n9.77***\n\n\n\n\n \n\n\n(1.81)\n\n\n(1.84)\n\n\n\n\nage\n\n\n \n\n\n0.35***\n\n\n\n\n \n\n\n \n\n\n(0.05)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.07\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.07\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df4 &lt;- expand_grid(\n  interested = c(F,T),\n  age = seq(min(nes$age, na.rm = T),\n            max(nes$age, na.rm=T),\n            length.out = 10)\n)\n\npred_df4  &lt;- cbind(\n  pred_df4 ,\n  fit = predict(m4, pred_df4 )\n)\n\n# Produce figure\n\npred_df4 %&gt;% \n  ggplot(aes(age, fit,\n             col = interested,\n             group = interested))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(interested)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() -&gt; fig_tc_m4"
  },
  {
    "objectID": "slides/06-slides.html#section-13",
    "href": "slides/06-slides.html#section-13",
    "title": "POLS 1600",
    "section": "",
    "text": "m5: An interaction between a binary and numerical predictor\n\nm5-Table-Figure-m5-m5 in 3d\n\n\n\\[\\text{m5: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} + \\beta_3\\text{interested} \\times \\text{age}+\\epsilon\\]\n\nm5 &lt;- lm(tc_diff ~ interested*age, nes)\ncoef(m5)\n\n       (Intercept)     interestedTRUE                age interestedTRUE:age \n        39.7626421         -6.7040163          0.1822814          0.3396523 \n\n\n\n\nhtmlreg(list(m1, m4, m5))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n32.18***\n\n\n39.76***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.71)\n\n\n(3.62)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n9.77***\n\n\n-6.70\n\n\n\n\n \n\n\n(1.81)\n\n\n(1.84)\n\n\n(5.55)\n\n\n\n\nage\n\n\n \n\n\n0.35***\n\n\n0.18*\n\n\n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.08)\n\n\n\n\ninterestedTRUE:age\n\n\n \n\n\n \n\n\n0.34**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.11)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.07\n\n\n0.08\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.07\n\n\n0.08\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df5 &lt;- expand_grid(\n  interested = c(F,T),\n  age = seq(min(nes$age, na.rm = T),\n            max(nes$age, na.rm=T),\n            length.out = 10)\n)\n\npred_df5  &lt;- cbind(\n  pred_df5 ,\n  fit = predict(m5, pred_df5 )\n)\n\n# Produce figure\n\npred_df5 %&gt;% \n  ggplot(aes(age, fit,\n             col = interested,\n             group = interested))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(interested)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() -&gt; fig_tc_m5"
  },
  {
    "objectID": "slides/06-slides.html#section-14",
    "href": "slides/06-slides.html#section-14",
    "title": "POLS 1600",
    "section": "",
    "text": "m6: Two numerical predictors\n\nm6-Table-Figure-m6-m6 in 3d\n\n\n\\[\\text{m6: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} +\\epsilon\\]\n\nm6 &lt;- lm(tc_diff ~ age + income, nes)\ncoef(m6)\n\n(Intercept)         age      income \n 33.1994121   0.4002682   0.1573334 \n\n\n\n\nhtmlreg(list( m6))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n33.20***\n\n\n\n\n \n\n\n(3.35)\n\n\n\n\nage\n\n\n0.40***\n\n\n\n\n \n\n\n(0.06)\n\n\n\n\nincome\n\n\n0.16\n\n\n\n\n \n\n\n(0.29)\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1049\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df6_age &lt;- expand_grid(\n  age = seq(min(nes$age, na.rm = T),\n            max(nes$age, na.rm=T),\n            length.out = 10),\n  # Hold income constant at mean value\n  income = mean(nes$income,na.rm=T)\n)\n\npred_df6_income &lt;- expand_grid(\n  income = seq(min(nes$income, na.rm = T),\n            max(nes$income, na.rm=T),\n            length.out = 16),\n  # Hold income constant at mean value\n  age = mean(nes$age,na.rm=T)\n)\n\npred_df6_age  &lt;- cbind(\n  pred_df6_age ,\n  fit = predict(m6, pred_df6_age )\n)\n\npred_df6_income  &lt;- cbind(\n  pred_df6_income ,\n  fit = predict(m6, pred_df6_income )\n)\n# Produce figure\n\npred_df6_age %&gt;% \n  ggplot(aes(age, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() +\n  labs(title =\"Age holding income constant\") -&gt; fig_tc_m6_age\n\npred_df6_income %&gt;% \n  ggplot(aes(income, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= income,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() +\n  labs(title = \"Income holding age constant\") -&gt; fig_tc_m6_income\n\n\nfig_tc_m6 &lt;- ggpubr::ggarrange(fig_tc_m6_age, fig_tc_m6_income)"
  },
  {
    "objectID": "slides/06-slides.html#section-15",
    "href": "slides/06-slides.html#section-15",
    "title": "POLS 1600",
    "section": "",
    "text": "m7: Interaction between two numerical predictors\n\nm7-Table-Figure-m7-m7 in 3d\n\n\n\\[\\text{m7: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} + \\beta_4\\text{age}\\times \\text{income}+\\epsilon \\]\n\nm7 &lt;- lm(tc_diff ~ age*income, nes)\ncoef(m7)\n\n(Intercept)         age      income  age:income \n38.22449671  0.29406374 -0.78888643  0.01993978 \n\n\n\n\nhtmlreg(list( m6, m7))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n33.20***\n\n\n38.22***\n\n\n\n\n \n\n\n(3.35)\n\n\n(5.80)\n\n\n\n\nage\n\n\n0.40***\n\n\n0.29*\n\n\n\n\n \n\n\n(0.06)\n\n\n(0.12)\n\n\n\n\nincome\n\n\n0.16\n\n\n-0.79\n\n\n\n\n \n\n\n(0.29)\n\n\n(0.94)\n\n\n\n\nage:income\n\n\n \n\n\n0.02\n\n\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1049\n\n\n1049\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df7 &lt;- expand_grid(\n  age = seq(19, 95,\n            by = 4),\n  # Hold income constant at mean value\n  income = seq(min(nes$income, na.rm = T),\n            max(nes$income, na.rm=T),\n            length.out = 16)\n)\n\n\n\npred_df7  &lt;- cbind(\n  pred_df7 ,\n  fit = predict(m7, pred_df7 )\n)\n\n# Produce figure\n\n# Marginal effect of age at min, median, and max values of income\npred_df7 %&gt;%\n  mutate(\n    income_at = case_when(\n      income == 1 ~ \"01\",\n      income == median(nes$income,na.rm=T) ~ \"05\",\n      income == 16 ~ \"16\",\n      T ~ NA_character_\n    )\n  ) %&gt;% \n  filter(!is.na(income_at)) %&gt;% \n  ggplot(aes(age, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point(aes(col =income_at,\n                 group = income_at))+\n  geom_line(aes(col =income_at,\n                group = income_at)\n            ) -&gt; fig_tc_m7_age\n\n# Marginal effect of income at min, median, and max values of age\npred_df7 %&gt;%\n  mutate(\n    age_at = case_when(\n      age == min(nes$age, na.rm=T) ~ \"19\",\n      age == 47 ~ \"47\", # close enough ...\n      age ==  max(nes$age, na.rm=T) ~ \"95\",\n      T ~ NA_character_\n    )\n  ) %&gt;% \n  filter(!is.na(age_at)) %&gt;% \n  ggplot(aes(income, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= income,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point(aes(col =age_at,\n                 group = age_at))+\n  geom_line(aes(col =age_at,\n                group = age_at)\n            ) -&gt; fig_tc_m7_income\n\n\nfig_tc_m7 &lt;- ggpubr::ggarrange(fig_tc_m7_age, \n                               fig_tc_m7_income,\n                               legend = \"bottom\")"
  },
  {
    "objectID": "slides/06-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/06-slides.html#motivating-example-what-causes-cholera",
    "title": "POLS 1600",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/06-slides.html#notation",
    "href": "slides/06-slides.html#notation",
    "title": "POLS 1600",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/06-slides.html#causal-effects",
    "href": "slides/06-slides.html#causal-effects",
    "title": "POLS 1600",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/06-slides.html#average-treatment-on-treated",
    "href": "slides/06-slides.html#average-treatment-on-treated",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/06-slides.html#average-treatment-on-treated-1",
    "href": "slides/06-slides.html#average-treatment-on-treated-1",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n\n\n\n\n\n\n\n\n\nPre-Period (T=0)\nPost-Period (T=1)\n\n\n\n\nTreated \\(D_{i}=1\\)\n\\(E[Y_{0i}(0)\\vert D_i = 1]\\)\n\\(E[Y_{1i}(1)\\vert D_i = 1]\\)\n\n\nControl \\(D_i=0\\)\n\\(E[Y_{0i}(0)\\vert D_i = 0]\\)\n\\(E[Y_{0i}(1)\\vert D_i = 0]\\)"
  },
  {
    "objectID": "slides/06-slides.html#data-1",
    "href": "slides/06-slides.html#data-1",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/06-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/06-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "POLS 1600",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/06-slides.html#before-vs-after-comparisons",
    "href": "slides/06-slides.html#before-vs-after-comparisons",
    "title": "POLS 1600",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/06-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/06-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "POLS 1600",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-1",
    "href": "slides/06-slides.html#difference-in-differences-1",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-2",
    "href": "slides/06-slides.html#difference-in-differences-2",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\] Is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-3",
    "href": "slides/06-slides.html#difference-in-differences-3",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/06-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/06-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "POLS 1600",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V"
  },
  {
    "objectID": "slides/06-slides.html#parallel-trends",
    "href": "slides/06-slides.html#parallel-trends",
    "title": "POLS 1600",
    "section": "Parallel Trends",
    "text": "Parallel Trends"
  },
  {
    "objectID": "slides/06-slides.html#using-lm-to-estimate-diff-in-diff",
    "href": "slides/06-slides.html#using-lm-to-estimate-diff-in-diff",
    "title": "POLS 1600",
    "section": "Using lm to estimate Diff-in-Diff",
    "text": "Using lm to estimate Diff-in-Diff\n\n Data lm() Diff-in-Diff\n\n\n\ncholera_df &lt;- tibble(\n  Location = c(\"S&V\",\"Lambeth\",\"S&V\",\"Lambeth\"),\n  Treated = c(0,1,0, 1),\n  Time = c(0,0,1,1),\n  Deaths = c(135,85,147,19)\n)\ncholera_df\n\n# A tibble: 4 × 4\n  Location Treated  Time Deaths\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 S&V            0     0    135\n2 Lambeth        1     0     85\n3 S&V            0     1    147\n4 Lambeth        1     1     19\n\n\n\n\n\\[\n\\text{Deaths} = \\beta_0 + \\beta_1\\text{Treated} + \\beta_2\\text{Time} + \\beta_3 \\text{Treated}\\times\\text{Time}\n\\]\n\ndiff_in_diff &lt;- lm(Deaths ~ Treated + Time + Treated:Time, cholera_df)\ndiff_in_diff\n\n\nCall:\nlm(formula = Deaths ~ Treated + Time + Treated:Time, data = cholera_df)\n\nCoefficients:\n (Intercept)       Treated          Time  Treated:Time  \n         135           -50            12           -78  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_0=\\) Outcome in control (S&V) before treatment\n\\(\\beta_1=\\) Fixed, time invariant differences between treated and control\n\\(\\beta_2=\\) Fixed, unit invariant differences between pre and post periods\n\\(\\beta_3=\\) Difference-in-Differences = \\(E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\)"
  },
  {
    "objectID": "slides/06-slides.html#summary",
    "href": "slides/06-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\nDifferencing twice accounts for fixed differences across units and between periods\n\nBut not time varying differences across units…\n\nThe key identifying assumption of a DiD design is the assumption of parallel trends\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to test"
  },
  {
    "objectID": "slides/06-slides.html#extensions-and-limitations",
    "href": "slides/06-slides.html#extensions-and-limitations",
    "title": "POLS 1600",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nDiff-in-Diff easy to estimate with linear regression\nGeneralizes to multiple periods and treatment interventions\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\n\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/06-slides.html#applications",
    "href": "slides/06-slides.html#applications",
    "title": "POLS 1600",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?"
  },
  {
    "objectID": "slides/06-slides.html#summary---linear-regression",
    "href": "slides/06-slides.html#summary---linear-regression",
    "title": "POLS 1600",
    "section": "Summary - Linear Regression",
    "text": "Summary - Linear Regression\n\nLinear regression produces linear estimates of the Conditional Expectation Function\nWe estimate linear regression using lm()\n\n\n\nm1 &lt;- lm( y ~ x1 + x2 + x3, data = df)\n\n\n\nWe interpret linear regression by: looking at the sign, size, and, eventually, significance of coefficients\n\nthe intercept \\((\\beta_0)\\) corresponds to the model’s prediction when every other predictor is zero\nthe other \\(\\beta\\)s describe how \\(y\\) changes with a [unit change] in \\(x\\), controlling for other predictors in the model\n\nWe present our results using regression tables and figures showing predicted values"
  },
  {
    "objectID": "slides/06-slides.html#summary---difference-in-differences",
    "href": "slides/06-slides.html#summary---difference-in-differences",
    "title": "POLS 1600",
    "section": "Summary - Difference-in-Differences",
    "text": "Summary - Difference-in-Differences\n\nDifference-in-Differences (DiD) is a powerful research design for observational data that combines a pre-post comparisons with a treated and control comparisons\nDifferencing twice accounts for fixed differences across units and between periods\nDiD relies on an assumption of parallel trends\nWe can use linear regression to estimate and generalize DiD designs"
  },
  {
    "objectID": "slides/06-slides.html#references",
    "href": "slides/06-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/03-slides.html#class-plan",
    "href": "slides/03-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan\n\nCausal Inference\nNotation for Causal Inference\nCausal Identification\nCausal Identification in Experiments\nresume data from QSS\nExplore Broockman and Kalla (2016)"
  },
  {
    "objectID": "slides/03-slides.html#annoucements",
    "href": "slides/03-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nRevised Schedule\n\nLecture 3 today\nRead  Broockman and Kalla (2016) pdf\nLab 3 next week\nWeek 4 content spread out over weeks 5/6/7 condensed\nMore updates to come"
  },
  {
    "objectID": "slides/03-slides.html#group-assignments",
    "href": "slides/03-slides.html#group-assignments",
    "title": "POLS 1600",
    "section": "Group Assignments",
    "text": "Group Assignments"
  },
  {
    "objectID": "slides/03-slides.html#what-did-we-like",
    "href": "slides/03-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/03-slides.html#what-did-we-dislike",
    "href": "slides/03-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/03-slides.html#setup",
    "href": "slides/03-slides.html#setup",
    "title": "POLS 1600",
    "section": "Setup",
    "text": "Setup\nEvery time you work in R\n\nSave your file to your course or project folder\nSet your working directory\nLoad, and if needed, install packages\nMaybe change some global options in your .Rmd file"
  },
  {
    "objectID": "slides/03-slides.html#setting-your-working-directory",
    "href": "slides/03-slides.html#setting-your-working-directory",
    "title": "POLS 1600",
    "section": "Setting your working directory:",
    "text": "Setting your working directory:\n\nMy default code for setting a working directory is:\n\n\nThis is really just a reminder to someone else using my code that they need to have their working directories set up correctly\nR Studio sets the working directory automatically, when you knit the file\nWhen I work on a file, I set the working directory manually"
  },
  {
    "objectID": "slides/03-slides.html#setting-your-working-directory-when-working-live",
    "href": "slides/03-slides.html#setting-your-working-directory-when-working-live",
    "title": "POLS 1600",
    "section": "Setting your working directory when working “Live”",
    "text": "Setting your working directory when working “Live”"
  },
  {
    "objectID": "slides/03-slides.html#packages-for-today",
    "href": "slides/03-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\n  \n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \n  \"haven\", \"labelled\",\n  \n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \n  \"ggthemes\", \"ggpubr\", \"GGally\",\n  \"scales\", \"dagitty\", \"ggdag\", #&lt;&lt;\n  \n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\n  \"qss\" #&lt;&lt;\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\nkableExtra         DT  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally     scales \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n   dagitty      ggdag    COVID19       maps    mapdata        qss \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE"
  },
  {
    "objectID": "slides/03-slides.html#review-data-wrangling",
    "href": "slides/03-slides.html#review-data-wrangling",
    "title": "POLS 1600",
    "section": "Review: Data wrangling",
    "text": "Review: Data wrangling"
  },
  {
    "objectID": "slides/03-slides.html#data-transformations",
    "href": "slides/03-slides.html#data-transformations",
    "title": "POLS 1600",
    "section": "Data transformations",
    "text": "Data transformations\n\n\nYou want to:\n\nLoad some data\nCombine multiple functions\nLook at your data\nRecode your data\nTransform your data\n\n\n\nYou could use\n\nread_* functions\n%&gt;% the “pipe” operator\nglimpse() head(), filter(), select(), arrange()\nmutate(), case_when(), ifelse()\nsummarize(), group_by()"
  },
  {
    "objectID": "slides/03-slides.html#data-visualization",
    "href": "slides/03-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nThe grammar of graphics\nAt minimum you need:\n\ndata\naesthetic mappings\ngeometries\n\nHey Jude, make a sad plot and make it better by:\n\nlabels\nthemes\nstatistics\ncooridnates\nfacets\ntransforming your data before plotting"
  },
  {
    "objectID": "slides/03-slides.html#causal-claims-imply-counterfactual-comparisons",
    "href": "slides/03-slides.html#causal-claims-imply-counterfactual-comparisons",
    "title": "POLS 1600",
    "section": "Causal claims imply counterfactual comparisons",
    "text": "Causal claims imply counterfactual comparisons\n\n\n\nCausal claims imply claims about counterfactuals\nWhat would have happened if we were to change some aspect of the world?"
  },
  {
    "objectID": "slides/03-slides.html#whats-the-counter-factual-for-these-claims",
    "href": "slides/03-slides.html#whats-the-counter-factual-for-these-claims",
    "title": "POLS 1600",
    "section": "What’s the counter factual for these claims:",
    "text": "What’s the counter factual for these claims:\n\nForeign aid increases develop\nWikileaks cost Hillary Clinton the 2016 election\nDemocracies don’t fight wars with other democracies\nUniversal Pre-K improves child development"
  },
  {
    "objectID": "slides/03-slides.html#casual-claims-are-are-all-around-us",
    "href": "slides/03-slides.html#casual-claims-are-are-all-around-us",
    "title": "POLS 1600",
    "section": "Casual claims are are all around us",
    "text": "Casual claims are are all around us"
  },
  {
    "objectID": "slides/03-slides.html#casual-claims-are-are-all-around-us-1",
    "href": "slides/03-slides.html#casual-claims-are-are-all-around-us-1",
    "title": "POLS 1600",
    "section": "Casual claims are are all around us",
    "text": "Casual claims are are all around us\nWhat are some questions that interest you?\nWhat are the counterfactual comparisons they imply?"
  },
  {
    "objectID": "slides/03-slides.html#how-to-represent-causal-claims",
    "href": "slides/03-slides.html#how-to-represent-causal-claims",
    "title": "POLS 1600",
    "section": "How to represent causal claims",
    "text": "How to represent causal claims\nIn this course, we will use two forms of notation to describe our causal claims.\n\nDirected Acyclic Graphs (DAGs, next lecture)\nPotential Outcomes Notation"
  },
  {
    "objectID": "slides/03-slides.html#general-notation-variables",
    "href": "slides/03-slides.html#general-notation-variables",
    "title": "POLS 1600",
    "section": "General Notation: Variables",
    "text": "General Notation: Variables\n\nY our outcome of interest\nD an indicator of treatment status\n\nD=1 \\(\\to\\) treated\nD=0 \\(\\to\\) not treated (control)\n\nZ an of assignment status\n\nZ=1 \\(\\to\\) assigned to treatment\nZ=0 \\(\\to\\) assigned to control\n\nX a covariate or predictor we can measure/observe\nU unmeasured covariates"
  },
  {
    "objectID": "slides/03-slides.html#expected-values",
    "href": "slides/03-slides.html#expected-values",
    "title": "POLS 1600",
    "section": "Expected Values",
    "text": "Expected Values\n\n\nThe \\(E[Y]\\) reads as “the expected value of Y”\n\\(E[Y]\\) is defined as a probability weighted average based on the unconditional probability of Y ( \\(f(y)\\) )\n\n\\[\\operatorname{E}[Y] = \\int_{-\\infty}^\\infty y f(y)\\, dy\\]"
  },
  {
    "objectID": "slides/03-slides.html#conditional-expectations",
    "href": "slides/03-slides.html#conditional-expectations",
    "title": "POLS 1600",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\n\n\nThe \\(E[Y|X=x]\\) reads as “the expected value of Y conditional on the value of X”\n\\(E[Y|X=x]\\) is defined as a probability weighted average of Y based on the conditional probability of Y given X ( \\(y f_{Y|X}(y|x)\\) )\n\n\\[\\operatorname{E}[Y \\vert X=x] = \\int_{-\\infty}^\\infty y f (y\\vert x) \\, dy\\]"
  },
  {
    "objectID": "slides/03-slides.html#estimands-estimators-and-estimates",
    "href": "slides/03-slides.html#estimands-estimators-and-estimates",
    "title": "POLS 1600",
    "section": "Estimands, Estimators and Estimates",
    "text": "Estimands, Estimators and Estimates\n\nEstimand the thing we want to know.\n\nSometimes called a parameter (\\(\\theta\\), “theta”) or quantity of interest\nThe expected value of heights in POLS 1600 (\\(\\theta =E[X]\\))\n\nEstimator a rule or method for calculating an estimate of our estimand\n\nAn average of a sample of 10 student’s heights in POLS 1600\n\\(\\hat{\\theta} = \\bar{x} = 1/n*\\sum_1^{n} x_i\\)\n\nEstimate: a value produced by our estimator for some data\n\nThe average of our 10 person sample is 5'10''"
  },
  {
    "objectID": "slides/03-slides.html#error-and-bias",
    "href": "slides/03-slides.html#error-and-bias",
    "title": "POLS 1600",
    "section": "Error and Bias",
    "text": "Error and Bias\nWe’ll talk about lots of types of bias throughout this course.\nFormally, we’ll say an estimate, \\(\\hat{\\theta}\\) (“theta hat”) is an unbiased estimator of a parameter, \\(\\theta\\) (“theta”) if:\n\\[\nE[\\hat{\\theta}] = \\theta\n\\]\nBias or error, \\(\\epsilon\\), is the difference between our estimate and the truth\n\\[\n\\epsilon = \\hat{\\theta} -\\theta\n\\]\nAn estimator is unbiased if, on average, the errors equal 0\n\\[\nE[\\epsilon] = E[\\hat{\\theta} -\\theta] = 0\n\\]"
  },
  {
    "objectID": "slides/03-slides.html#bias-vs.-variance",
    "href": "slides/03-slides.html#bias-vs.-variance",
    "title": "POLS 1600",
    "section": "Bias vs. variance",
    "text": "Bias vs. variance"
  },
  {
    "objectID": "slides/03-slides.html#the-bias-variance-tradeoff",
    "href": "slides/03-slides.html#the-bias-variance-tradeoff",
    "title": "POLS 1600",
    "section": "The bias-variance tradeoff",
    "text": "The bias-variance tradeoff"
  },
  {
    "objectID": "slides/03-slides.html#potential-outcomes-notation",
    "href": "slides/03-slides.html#potential-outcomes-notation",
    "title": "POLS 1600",
    "section": "Potential outcomes notation",
    "text": "Potential outcomes notation\n\n\\(Y_i(1)\\) describes individual \\(i\\)’s outcome, \\(Y_i\\) if they received the treatment \\((D_i = 1)\\)\n\nShorthand for \\(Y_i(D_i=1)\\)\nPaul’s Covid-19 status (\\(Y_i\\)) with the vaccine (\\(D_i = 1\\))\n\n\\(Y_i(0)\\) describes individual \\(i\\)’s outcome, \\(Y_i\\) if they did not receive the treatment \\((D_i = 0)_i\\)\n\nShorthand for \\(Y_i(D_i=0)\\)\nPaul’s Covid-19 status (\\(Y_i\\)) without the vaccine (\\(D_i=0\\))\n\n\n\nThe treatment received determines which potential outcome we actually observe:\n\\[\nY_i = (1 - D_i)*Y_i(0) + D_i*Y_i(1)\n\\]\nPotential outcomes are fixed, but we only observe one (of many) potential outcomes \\(\\to\\) Fundamental Problem of Causal Inference"
  },
  {
    "objectID": "slides/03-slides.html#fundamental-problem-of-causal-inference",
    "href": "slides/03-slides.html#fundamental-problem-of-causal-inference",
    "title": "POLS 1600",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\nThe individual causal effect (ICE), \\(\\tau_i\\), is defined as\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\n\nThe fundamental problem of causal inference is that we only ever see one potential outcome for an individual, and so it’s impossible to know the causal effect of some intervention for that individual\nThe ICE is unidentified"
  },
  {
    "objectID": "slides/03-slides.html#identification",
    "href": "slides/03-slides.html#identification",
    "title": "POLS 1600",
    "section": "Identification",
    "text": "Identification\n\nIdentification refers to what we can learn from the data available\nA quantity of interest is identified if, with infinite data it can only take one value\nMathematically, we’ll sometimes say a coefficient in an equation is unidentified if\n\nWe have more predictors than observations, or\nSome of predictors are linear combinations of other predictors."
  },
  {
    "objectID": "slides/03-slides.html#causal-identification-1",
    "href": "slides/03-slides.html#causal-identification-1",
    "title": "POLS 1600",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCasual Identification refers to “the assumptions needed for statistical estimates to be given a causal interpretation” Keele (2015)\nWhat’s Your Casual Identification Strategy What are the assumptions that make your research design credible?\nIdentification &gt; Estimation"
  },
  {
    "objectID": "slides/03-slides.html#observational-vs-experimental-designs",
    "href": "slides/03-slides.html#observational-vs-experimental-designs",
    "title": "POLS 1600",
    "section": "Observational vs Experimental Designs",
    "text": "Observational vs Experimental Designs\n\nExperimental designs are studies in which a causal variable of interest, the treatement, is manipulated by the researcher to examine its causal effects on some outcome of interest\nObservational designs are studies in which a causal variable of interest is determined by someone/thing other than the researcher (nature, governments, people, etc.)"
  },
  {
    "objectID": "slides/03-slides.html#the-fpoci-is-a-problem-of-missing-data",
    "href": "slides/03-slides.html#the-fpoci-is-a-problem-of-missing-data",
    "title": "POLS 1600",
    "section": "The FPoCI is a problem of missing data",
    "text": "The FPoCI is a problem of missing data\nRecall that an individual causal effect \\(\\tau_i\\), is defined as:\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\nThe problem is that for any one individual, we only observe \\(Y_i(1)\\) or \\(Y_i(0)\\), but never both.\n\nIf Paul got the vaccine \\((Y_{Paul}(Vaxxed)=\\text{Covid Free})\\), then we don’t know what Paul’s health status would have been, had he not got the vaccine \\((Y_{Paul}(Unvaxxed) =???)\\)"
  },
  {
    "objectID": "slides/03-slides.html#a-statistical-solution-to-the-fpoci",
    "href": "slides/03-slides.html#a-statistical-solution-to-the-fpoci",
    "title": "POLS 1600",
    "section": "A statistical solution to the FPoCI",
    "text": "A statistical solution to the FPoCI\nRather than focus individual causal effects:\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\nWe focus on average causal effects (Average Treatment Effects [ATEs]):\n\\[\nE[\\tau_i] = \\overbrace{E[Y_i(1) - Y_i(0)]}^{\\text{Average of a difference}} = \\overbrace{E[Y_i(1)] - E[Y_i(0)]}^{\\text{Difference of Averages}}\n\\]\nWhen does the difference of averages provide us with a good estimate of the average difference?\nLet’s consider a simple example"
  },
  {
    "objectID": "slides/03-slides.html#does-eating-chocolate-make-you-happy",
    "href": "slides/03-slides.html#does-eating-chocolate-make-you-happy",
    "title": "POLS 1600",
    "section": "Does eating chocolate make you happy?",
    "text": "Does eating chocolate make you happy?\n\n\\(Y_i\\) happiness measured on a 0-10 scale\n\\(D_i\\) whether a person ate chocolate \\((D=1)\\) or fruit \\((D = 0)\\)\n\\(Y_i(1)\\) a person’s happiness eating chocolate\n\\(Y_i(0)\\) a person’s happiness eating fruit\n\\(X_i\\) a person’s self-reported preference \\((X_i \\in\\) {chocolate, fruit })"
  },
  {
    "objectID": "slides/03-slides.html#section",
    "href": "slides/03-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Potential Outcomes:\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(\\tau_i\\)\n\n\n\n\n7\n3\n4\n\n\n8\n6\n2\n\n\n5\n4\n1\n\n\n4\n3\n1\n\n\n6\n10\n-4\n\n\n8\n9\n-1\n\n\n5\n4\n1\n\n\n7\n8\n-1\n\n\n4\n3\n1\n\n\n6\n0\n6\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i(1)]\\)\n\\(E[Y_i(0)]\\)\n\\(E[\\tau_i]\\)\n\n\n\n\n6\n5\n1\n\n\n\n\n\n\n\nIf we could observe everyone’s potential outcomes, we could calculate the ICE\nOn average eating chocolate increases happiness by 1 point on our 10-point scale (ATE = 1)\nSuppose we conducted a study and let folks select what they wanted to eat."
  },
  {
    "objectID": "slides/03-slides.html#section-1",
    "href": "slides/03-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Potential Outcomes:\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(\\tau_i\\)\n\n\n\n\n7\n3\n4\n\n\n8\n6\n2\n\n\n5\n4\n1\n\n\n4\n3\n1\n\n\n6\n10\n-4\n\n\n8\n9\n-1\n\n\n5\n4\n1\n\n\n7\n8\n-1\n\n\n4\n3\n1\n\n\n6\n0\n6\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i(1)]\\)\n\\(E[Y_i(0)]\\)\n\\(ATE\\)\n\n\n\n\n6\n5\n1\n\n\n\n\n\n\nObserved Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n1\n5\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n0\n9\n\n\nchocolate\n1\n5\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n1\n6\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n5.57\n9\n-3.43"
  },
  {
    "objectID": "slides/03-slides.html#section-2",
    "href": "slides/03-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Observed Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n1\n5\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n0\n9\n\n\nchocolate\n1\n5\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n1\n6\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n5.57\n9\n-3.43\n\n\n\n\n\n\nSelection Bias\n\nOur estimate of the ATE is biased by the fact that folks who prefer fruit seem to be happier than folks who prefer chocolate in this example\nIn general, selection bias occurs when folks who receive the treatment differ systematically from folks who don’t\nWhat if instead of letting people pick and choose, we randomly assigned half our respondents to chocolate and half to receive fruit"
  },
  {
    "objectID": "slides/03-slides.html#section-3",
    "href": "slides/03-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Potential Outcomes:\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(\\tau_i\\)\n\n\n\n\n7\n3\n4\n\n\n8\n6\n2\n\n\n5\n4\n1\n\n\n4\n3\n1\n\n\n6\n10\n-4\n\n\n8\n9\n-1\n\n\n5\n4\n1\n\n\n7\n8\n-1\n\n\n4\n3\n1\n\n\n6\n0\n6\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i(1)]\\)\n\\(E[Y_i(0)]\\)\n\\(ATE\\)\n\n\n\n\n6\n5\n1\n\n\n\n\n\n\nRandomly Assigned Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.2\n5.2\n1"
  },
  {
    "objectID": "slides/03-slides.html#section-4",
    "href": "slides/03-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Randomly Assigned Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.2\n5.2\n1\n\n\n\n\n\n\nRandom Assignment\n\nWhen treatment has been randomly assigned, a difference in sample means provides an unbiased estimate of the ATE\nThe fact that our \\(\\hat{ATE} = ATE\\) in this example is pure coincidence.\nIf we randomly assigned treatment a different way, we’d get a different estimate.\nIn general unbiased estimators will tend to be neither too high nor too low (e.g. \\(E[\\hat{\\theta} - \\theta] = 0\\)])"
  },
  {
    "objectID": "slides/03-slides.html#estimating-an-average-treatment-effect",
    "href": "slides/03-slides.html#estimating-an-average-treatment-effect",
    "title": "POLS 1600",
    "section": "Estimating an Average Treatment Effect",
    "text": "Estimating an Average Treatment Effect\nIf we treatment has been randomly assigned, we can estimate the ATE by taking the difference of means between treatment and control:\n\\[\n\\begin{align*}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{align*}\n\\]\nThat is, the ATE is causally identified by the difference of means estimator in an experimental design"
  },
  {
    "objectID": "slides/03-slides.html#section-5",
    "href": "slides/03-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "Random Assignment 1\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.2\n5.2\n1\n\n\n\n\n\n\nRandom Assignment 2\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n0\n3\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n1\n6\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n1\n7\n\n\nchocolate\n0\n3\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.6\n2.8\n3.8\n\n\n\n\n\n\nRandom Assignment 3\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n0\n6\n\n\nchocolate\n1\n5\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n0\n9\n\n\nchocolate\n0\n4\n\n\nfruit\n1\n7\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n5.4\n5.8\n-0.4"
  },
  {
    "objectID": "slides/03-slides.html#distribution-of-sample-ates",
    "href": "slides/03-slides.html#distribution-of-sample-ates",
    "title": "POLS 1600",
    "section": "Distribution of Sample ATEs",
    "text": "Distribution of Sample ATEs"
  },
  {
    "objectID": "slides/03-slides.html#why-random-assignment-matters",
    "href": "slides/03-slides.html#why-random-assignment-matters",
    "title": "POLS 1600",
    "section": "Why Random Assignment Matters?",
    "text": "Why Random Assignment Matters?\nFormally, randomly assigning treatments creates statistical independence \\((\\unicode{x2AEB})\\) between treatment ( \\(D\\) ) and potential outcomes ( \\(Y(1),Y(0)\\) ) as well as any observed ( \\(X\\) ) or unobserved confounders ( \\(U\\) ):\n\\[Y_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\\]\nPractically, what this means is that what we can observe ( differences in conditional means for treated and control ), provide good (unbiased) estimates of what we’re trying to learn about (Average Treatment Effects)"
  },
  {
    "objectID": "slides/03-slides.html#causal-identification-with-experimental-designs",
    "href": "slides/03-slides.html#causal-identification-with-experimental-designs",
    "title": "POLS 1600",
    "section": "Causal Identification with Experimental Designs",
    "text": "Causal Identification with Experimental Designs\nCausal identification for experimental designs requires very few assumptions:\n\nIndependence (Satisfied by Randomization)\n\n\\(Y(1), Y(0),X,U, \\perp D\\)\n\nSUTVA Stable Unit Treatment Value Assumption (Depends on features of the design)\n\nNo interference between units \\(Y_i(d_1, d_2, \\dots, d_N) = Y_i(d_i)\\)\nNo hidden values of the treatment/Variation in the treatment"
  },
  {
    "objectID": "slides/03-slides.html#random-assignment-creates-testable-implications",
    "href": "slides/03-slides.html#random-assignment-creates-testable-implications",
    "title": "POLS 1600",
    "section": "Random assignment creates testable implications",
    "text": "Random assignment creates testable implications\n\nIf treatment has been randomly assigned, we would expect treatment and control groups to look similar in terms of pre-treatment covariates\n\nWe can show covariate balance by comparing the means in each treatment group\n\nIf the treatment had an effect, than we can credibly claim that that effect was due to the presence or absence of the treatment, and not some alternative explanation.\nThis type of clean apples-to-apples counterfactual comparison is what people mean when they talk about an experimental ideal"
  },
  {
    "objectID": "slides/03-slides.html#no-causation-without-manipulation",
    "href": "slides/03-slides.html#no-causation-without-manipulation",
    "title": "POLS 1600",
    "section": "No Causation without Manipulation?",
    "text": "No Causation without Manipulation?\n\n“No causation without manipulation” - Holland (1986)\nCausal effects are well defined when we can imagine manipulating (changing) the value of \\(D_i\\) and only \\(D_i\\)\nBut what about the “effects” of things like:\n\nRace\nSex\nDemocracy\n\nStudying the effects of these factors requires strong theory and clever design Sen and Wasow (2016)"
  },
  {
    "objectID": "slides/03-slides.html#the-resume-experimento",
    "href": "slides/03-slides.html#the-resume-experimento",
    "title": "POLS 1600",
    "section": "The Resume Experimento",
    "text": "The Resume Experimento\nLet’s take a look at the resume experiment from your text book and compare some of Imai’s code to its tidyverse equivalent\n\n# make sure qss package is loaded\nlibrary(qss)\ndata(\"resume\")"
  },
  {
    "objectID": "slides/03-slides.html#high-level-overview-p.-34",
    "href": "slides/03-slides.html#high-level-overview-p.-34",
    "title": "POLS 1600",
    "section": "High level Overview (p. 34)",
    "text": "High level Overview (p. 34)\n\ndim(resume)\n\n[1] 4870    4\n\nhead(resume)\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n5    Carrie female white    0\n6       Jay   male white    0"
  },
  {
    "objectID": "slides/03-slides.html#high-level-overview-p.-34-1",
    "href": "slides/03-slides.html#high-level-overview-p.-34-1",
    "title": "POLS 1600",
    "section": "High level Overview (p. 34)",
    "text": "High level Overview (p. 34)\n\nsummary(resume)\n\n  firstname             sex                race                call        \n Length:4870        Length:4870        Length:4870        Min.   :0.00000  \n Class :character   Class :character   Class :character   1st Qu.:0.00000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.00000  \n                                                          Mean   :0.08049  \n                                                          3rd Qu.:0.00000  \n                                                          Max.   :1.00000"
  },
  {
    "objectID": "slides/03-slides.html#crosstabs",
    "href": "slides/03-slides.html#crosstabs",
    "title": "POLS 1600",
    "section": "Crosstabs",
    "text": "Crosstabs\n\nrace.call.tab &lt;- table(race = resume$race,\n                       call = resume$call)\nrace.call.tab\n\n       call\nrace       0    1\n  black 2278  157\n  white 2200  235\n\naddmargins(race.call.tab)\n\n       call\nrace       0    1  Sum\n  black 2278  157 2435\n  white 2200  235 2435\n  Sum   4478  392 4870"
  },
  {
    "objectID": "slides/03-slides.html#tidy-crosstab",
    "href": "slides/03-slides.html#tidy-crosstab",
    "title": "POLS 1600",
    "section": "Tidy crosstab",
    "text": "Tidy crosstab\n\nresume %&gt;%\n  group_by(race, call)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  pivot_wider(names_from = call, values_from = n)\n\n# A tibble: 2 × 3\n# Groups:   race [2]\n  race    `0`   `1`\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 black  2278   157\n2 white  2200   235"
  },
  {
    "objectID": "slides/03-slides.html#calculating-call-back-rates",
    "href": "slides/03-slides.html#calculating-call-back-rates",
    "title": "POLS 1600",
    "section": "Calculating Call Back Rates",
    "text": "Calculating Call Back Rates\n\n# Overall\nsum(race.call.tab[,2])/nrow(resume)\n\n[1] 0.08049281\n\n# Black names\ncb_bl &lt;- sum(race.call.tab[1,2])/sum(race.call.tab[1,])\n# White Names\ncb_wh &lt;- sum(race.call.tab[2,2])/sum(race.call.tab[2,])\n\n# ATE\ncb_wh - cb_bl\n\n[1] 0.03203285"
  },
  {
    "objectID": "slides/03-slides.html#calculating-call-back-rates-with-group_by",
    "href": "slides/03-slides.html#calculating-call-back-rates-with-group_by",
    "title": "POLS 1600",
    "section": "Calculating Call Back Rates with group_by()",
    "text": "Calculating Call Back Rates with group_by()\n\nresume %&gt;%\n  group_by(race)%&gt;%\n  summarise(\n    call_back = mean(call)\n  )\n\n# A tibble: 2 × 2\n  race  call_back\n  &lt;chr&gt;     &lt;dbl&gt;\n1 black    0.0645\n2 white    0.0965"
  },
  {
    "objectID": "slides/03-slides.html#factor-variables-in-base-r",
    "href": "slides/03-slides.html#factor-variables-in-base-r",
    "title": "POLS 1600",
    "section": "Factor variables in Base R",
    "text": "Factor variables in Base R\n\nresume$type &lt;- NA\nresume$type[resume$race == \"black\" & resume$sex == \"female\"] &lt;- \"BlackFemale\"\nresume$type[resume$race == \"black\" & resume$sex == \"male\"] &lt;- \"BlackMale\"\nresume$type[resume$race == \"white\" & resume$sex == \"female\"] &lt;- \"WhiteFemale\"\nresume$type[resume$race == \"white\" & resume$sex == \"male\"] &lt;- \"WhiteMale\""
  },
  {
    "objectID": "slides/03-slides.html#factor-variables-in-tidy-r",
    "href": "slides/03-slides.html#factor-variables-in-tidy-r",
    "title": "POLS 1600",
    "section": "Factor variables in Tidy R",
    "text": "Factor variables in Tidy R\n\nresume %&gt;%\n  mutate(\n    type_tidy = case_when(\n      race == \"black\" & sex == \"female\" ~ \"BlackFemale\",\n      race == \"black\" & sex == \"male\" ~ \"BlackMale\",\n      race == \"white\" & sex == \"female\" ~ \"WhiteFemale\",\n      race == \"white\" & sex == \"male\" ~ \"WhiteMale\"\n    )\n  ) -&gt; resume"
  },
  {
    "objectID": "slides/03-slides.html#comparing-approaches",
    "href": "slides/03-slides.html#comparing-approaches",
    "title": "POLS 1600",
    "section": "Comparing approaches",
    "text": "Comparing approaches\n\ntable(base= resume$type, tidy= resume$type_tidy)\n\n             tidy\nbase          BlackFemale BlackMale WhiteFemale WhiteMale\n  BlackFemale        1886         0           0         0\n  BlackMale             0       549           0         0\n  WhiteFemale           0         0        1860         0\n  WhiteMale             0         0           0       575"
  },
  {
    "objectID": "slides/03-slides.html#visualizing-call-back-rates-by-name",
    "href": "slides/03-slides.html#visualizing-call-back-rates-by-name",
    "title": "POLS 1600",
    "section": "Visualizing Call Back Rates by Name",
    "text": "Visualizing Call Back Rates by Name\n\nCodeFigure\n\n\n\nresume %&gt;%\n  group_by(race, sex,firstname)%&gt;%\n  summarize(\n    Y = mean(call),\n    n = n()\n  )%&gt;%\n  arrange(desc(Y)) %&gt;%\n  mutate(\n    firstname = forcats::fct_reorder(firstname,Y)\n  )%&gt;%\n  ggplot(aes(Y, firstname,col=race, size=n))+\n  geom_point() + \n  facet_grid(sex~.,scales = \"free_y\")"
  },
  {
    "objectID": "slides/03-slides.html#reading-academic-papers",
    "href": "slides/03-slides.html#reading-academic-papers",
    "title": "POLS 1600",
    "section": "Reading Academic Papers",
    "text": "Reading Academic Papers\n\nReading academic papers is a skill and takes practice.\nYou should aim to answer the following:\n\nWhat’s the research question?\nWhat’s the theoretical framework?\nWhat’s the empirical design?\nWhat’s are the main results?"
  },
  {
    "objectID": "slides/03-slides.html#study-design-a-placebo-controlled-field-experiment",
    "href": "slides/03-slides.html#study-design-a-placebo-controlled-field-experiment",
    "title": "POLS 1600",
    "section": "Study Design :A placebo-controlled field experiment",
    "text": "Study Design :A placebo-controlled field experiment\n\nRecruited from voter files to complete a baseline survey\nAmong those who complete the survey, half are assigned to receive an intervention and half are assigned to receive a placebo\nOnly some are actually home or open the door when the canvassers knock.\nThese people are then recruited to participate in a series of surveys 3 days, 3 weeks, 6 weeks, and 3 months after the initial intervention."
  },
  {
    "objectID": "slides/03-slides.html#data-for-thursday",
    "href": "slides/03-slides.html#data-for-thursday",
    "title": "POLS 1600",
    "section": "Data for Thursday",
    "text": "Data for Thursday\nLet’s load the data from the orginal study\n\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))"
  },
  {
    "objectID": "slides/03-slides.html#codebook",
    "href": "slides/03-slides.html#codebook",
    "title": "POLS 1600",
    "section": "Codebook",
    "text": "Codebook\n\ncompleted_baseline whether someone completed the baseline survey (“Survey”) or not (“No Survey”)\ntreatment_assigned what intervention someone who completed the baseline survey was assigned two (treatment= “Trans-Equality”, placebo = “Recycling”)\nanswered_door whether someone answered the door (“Yes”) or not (“No”) when a canvasser came to their door\ntreatment_group the treatment assignments of those who answered the door (treatment= “Trans-Equality”, placebo = “Recycling”)\nvf_age the age of the person in years\nvf_female the respondent’s sex (female = 1, male = 0)\nvf_democrat whether the person was a registered Democract (Democrat=1, 0 otherwise)\nvf_white whether the person was white (White=1, 0 otherwise)\nvf_vg_12 whether the person voted in the 2012 general election (voted = 1, 0 otherwise)"
  },
  {
    "objectID": "slides/03-slides.html#hlo",
    "href": "slides/03-slides.html#hlo",
    "title": "POLS 1600",
    "section": "HLO",
    "text": "HLO\n\nglimpse(df)\n\nRows: 68,378\nColumns: 14\n$ completed_baseline &lt;chr&gt; \"No Survey\", \"No Survey\", \"No Survey\", \"No Survey\",…\n$ treatment_assigned &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ answered_door      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ treatment_group    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ vf_age             &lt;dbl&gt; 23.00000, 38.00000, 48.00000, 49.20192, 49.20192, 4…\n$ vf_female          &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, …\n$ vf_democrat        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ vf_white           &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ vf_vg_12           &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ therm_trans_t0     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t1     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t2     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t3     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t4     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…"
  },
  {
    "objectID": "slides/03-slides.html#study-design",
    "href": "slides/03-slides.html#study-design",
    "title": "POLS 1600",
    "section": "Study Design",
    "text": "Study Design\n\ntable(df$completed_baseline)\n\n\nNo Survey    Survey \n    66553      1825 \n\ntable(df$treatment_assigned)\n\n\n     Recycling Trans-Equality \n           913            912 \n\ntable(df$answered_door)\n\n\n  No  Yes \n1324  501 \n\ntable(df$treatment_group)\n\n\n     Recycling Trans-Equality \n           255            246"
  },
  {
    "objectID": "slides/03-slides.html#assessing-balance-in-covariates",
    "href": "slides/03-slides.html#assessing-balance-in-covariates",
    "title": "POLS 1600",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\ndf %&gt;%\n  filter(completed_baseline == \"Survey\") %&gt;%\n  select(treatment_assigned,\n         starts_with(\"vf_\"))%&gt;%\n  group_by(treatment_assigned)%&gt;%\n  summarise(\n    across(starts_with(\"vf_\"), mean)\n    ) -&gt; pretreatment_balance"
  },
  {
    "objectID": "slides/03-slides.html#assessing-balance-in-covariates-1",
    "href": "slides/03-slides.html#assessing-balance-in-covariates-1",
    "title": "POLS 1600",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\npretreatment_balance\n\n# A tibble: 2 × 6\n  treatment_assigned vf_age vf_female vf_democrat vf_white vf_vg_12\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Recycling            46.3     0.593       0.463    0.209    0.757\n2 Trans-Equality       47.7     0.582       0.488    0.217    0.719"
  },
  {
    "objectID": "slides/03-slides.html#assessing-balance-in-covariates-2",
    "href": "slides/03-slides.html#assessing-balance-in-covariates-2",
    "title": "POLS 1600",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\nCodeBalance\n\n\n\n# Rearrange data\npretreatment_balance %&gt;%\n  # Pivot columns except treatement assigned\n  pivot_longer(names_to = \"covariate\", values_to =  \"value\", -treatment_assigned) %&gt;%\n  # Pivot rows two two columns for treatment and placebo\n  pivot_wider(names_from = treatment_assigned) %&gt;%\n  # Calculate covariate balance\n  mutate(\n    Difference = `Trans-Equality` - Recycling\n  )\n\n\n\n\n\n# A tibble: 5 × 4\n  covariate   Recycling `Trans-Equality` Difference\n  &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 vf_age         46.3             47.7      1.40   \n2 vf_female       0.593            0.582   -0.0103 \n3 vf_democrat     0.463            0.488    0.0246 \n4 vf_white        0.209            0.217    0.00790\n5 vf_vg_12        0.757            0.719   -0.0375"
  },
  {
    "objectID": "slides/03-slides.html#summary-1",
    "href": "slides/03-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nCausal Claims involve counterfactual comparisons\nThe fundamental problem of causal inference is that for an individual only observe one of many potential outcomes\nCausal identification refers to the assumptions necessary to generate credible causal estimates\nIdentification for experimental designs follows from the random assignment of treatment which allows us to produce unbiased estimates of the Average Treatment Effect"
  },
  {
    "objectID": "slides/03-slides.html#references",
    "href": "slides/03-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nBroockman, David, and Joshua Kalla. 2016. “Durably reducing transphobia: A field experiment on door-to-door canvassing.” Science 352 (6282): 220–24."
  },
  {
    "objectID": "slides/10-slides.html#class-plan",
    "href": "slides/10-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nTopics:\n\nSampling distributions and standard errors (15 minutes)\nConfidence intervals (15 minutes )\nHypothesis testing (15 minutes)\nQuantifying uncertainty for regression (15 minutes)"
  },
  {
    "objectID": "slides/10-slides.html#announcements",
    "href": "slides/10-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\nFinal lab this week.\nNext week’s lecture:\n\nWorkshop/Review/special topics\nAsk questions in the survey\n\nFeedback on A2 by tomorrow."
  },
  {
    "objectID": "slides/10-slides.html#setup-packages-for-today",
    "href": "slides/10-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\", \"boot\", \"modelr\" ,\"purrr\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo          boot \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       modelr         purrr \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/10-slides.html#feedback",
    "href": "slides/10-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/10-slides.html#what-did-we-like",
    "href": "slides/10-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/10-slides.html#what-did-we-dislike",
    "href": "slides/10-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/10-slides.html#what-are-we-streaming",
    "href": "slides/10-slides.html#what-are-we-streaming",
    "title": "POLS 1600",
    "section": "What are we streaming",
    "text": "What are we streaming"
  },
  {
    "objectID": "slides/10-slides.html#goals",
    "href": "slides/10-slides.html#goals",
    "title": "POLS 1600",
    "section": "Goals:",
    "text": "Goals:\n\nA sampling distribution is a theoretical distribution of estimates obtained in repeated sampling\n\nWhat could have happened?\n\nA standard error (SE) is the standard deviation of the sampling distribution\nWe can calculate SEs via simulation and analytically\nWe can use SEs to construct confidence intervals and conduct hypothesis tests allowing us to quantify uncertainty"
  },
  {
    "objectID": "slides/10-slides.html#populations-and-samples",
    "href": "slides/10-slides.html#populations-and-samples",
    "title": "POLS 1600",
    "section": "Populations and samples",
    "text": "Populations and samples\n\nPopulation: All the cases from which you could have sampled\nParameter: A quantity or quantities of interest often generically called \\(\\theta\\) (“theta”). What we want to learn about our population\nSample: A (random) draw of observations from that population\nSample Size: The number of observations in your draw (without replacement)"
  },
  {
    "objectID": "slides/10-slides.html#estimators-estimates-and-statistics",
    "href": "slides/10-slides.html#estimators-estimates-and-statistics",
    "title": "POLS 1600",
    "section": "Estimators, estimates, and statistics",
    "text": "Estimators, estimates, and statistics\n\nEstimator: A rule for calculating an estimate of our parameter of interest.\nEstimate: The value produced by some estimator for some parameter from some data. Often called \\(\\hat{\\theta}\\)\nUnbiased estimators: \\(E(\\hat{\\theta})=E(\\theta)\\) On average, the estimates produced by some estimator will be centered around the truth\nConsistent estimates: \\(\\lim_{n\\to \\infty} \\hat{\\theta_N} = \\theta\\) As the sample size increases, the estimates from an estimator converge in probability to the parameter value\nStatistic: A summary of the data (mean, regression coefficient, \\(R^2\\)). An estimator without a specified target of inference"
  },
  {
    "objectID": "slides/10-slides.html#distrubtions-and-standard-errors",
    "href": "slides/10-slides.html#distrubtions-and-standard-errors",
    "title": "POLS 1600",
    "section": "Distrubtions and standard errors",
    "text": "Distrubtions and standard errors\n\nSampling Distribution: How some estimate would vary if you took repeated samples from the population\nStandard Error: The standard deviation of the sampling distribution\nResampling Distribution: How some estimate would vary if you took repeated samples from your sample WITH REPLACEMENT\n\n“Sampling from our sample, as the sample was sampled from the population.”"
  },
  {
    "objectID": "slides/10-slides.html#sampling-distributions",
    "href": "slides/10-slides.html#sampling-distributions",
    "title": "POLS 1600",
    "section": "Sampling distributions",
    "text": "Sampling distributions\n\nOverview CodeN = 10N = 30N = 300Comments\n\n\n\n\nTreat the 2024 NES pilot as the population\nTake repeated samples of size N = 10, 30, 300\nFor each sample of size N, calculate the sample mean of age\nPlot the distribution of sample means (i.e. the sampling distribution)\n\n\n\n\n\n# Load Data\nload(url(\"https://pols1600.paultesta.org/files/data/nes24.rda\"))\n\n# ---- Population ----\n\n# Population average\nmu_age &lt;- mean(df$age, na.rm=T)\n# Population standard deviation\nsd_age &lt;- sd(df$age, na.rm = T)\n\n# ---- Function to Take Repeated Samples From Data ----\n\nsample_data_fn &lt;- function(\n    dat=df, var=age, samps=1000, sample_size=10,\n    resample = F){\n  if(resample == F){\n  df &lt;- tibble(\n  sim = 1:samps,\n  distribution = \"Sampling\",\n  size = sample_size,\n  sample_from = \"Population\",\n  pop_mean = dat %&gt;% pull(!!enquo(var)) %&gt;% mean(., na.rm=T),\n  pop_sd = dat %&gt;% pull(!!enquo(var)) %&gt;% sd(., na.rm=T),\n  se_asymp = pop_sd / sqrt(size),\n  ll_asymp = pop_mean - 1.96*se_asymp,\n  ul_asymp = pop_mean + 1.96*se_asymp,\n) %&gt;% \n  mutate(\n    sample = purrr::map(sim, ~ slice_sample(dat %&gt;% select(!!enquo(var)), n = sample_size, replace = F)),\n    sample_mean = purrr::map_dbl(sample, \\(x) x %&gt;% pull(!!enquo(var)) %&gt;% mean(.,na.rm=T)),\n    ll = sample_mean - 1.96*sd(sample_mean),\n    ul = sample_mean + 1.96*sd(sample_mean)\n  )\n  }\n  if(resample == T){\n    df &lt;- tibble(\n  sim = 1:samps,\n  distribution = \"Resampling\",\n  size = sample_size,\n  sample_from = \"Sample\",\n  pop_mean = dat %&gt;% pull(!!enquo(var)) %&gt;% mean(., na.rm=T),\n  pop_sd = dat %&gt;% pull(!!enquo(var)) %&gt;% sd(., na.rm=T),\n  se_asymp = pop_sd / sqrt(size),\n  ll_asymp = pop_mean - 1.96*se_asymp,\n  ul_asymp = pop_mean + 1.96*se_asymp,\n) %&gt;% \n  mutate(\n    sample = purrr::map(sim, ~ slice_sample(dat %&gt;% select(!!enquo(var)), n = sample_size, replace = T)),\n    sample_mean = purrr::map_dbl(sample, \\(x) x %&gt;% pull(!!enquo(var)) %&gt;% mean(.,na.rm=T))\n  )\n  }\n  return(df)\n}\n\n# ---- Plot Single Distribution -----\n\nplot_distribution &lt;- function(the_pop,the_samp, the_var, ...){\n  mu_pop &lt;- the_pop %&gt;% pull(!!enquo(the_var)) %&gt;% mean(., na.rm=T)\n  mu_samp &lt;- the_samp %&gt;% pull(!!enquo(the_var)) %&gt;% mean(., na.rm=T)\n  ll &lt;- the_pop %&gt;% pull(!!enquo(the_var)) %&gt;% as.numeric() %&gt;%  min(., na.rm=T)\n  ul &lt;- the_pop %&gt;% pull(!!enquo(the_var)) %&gt;% as.numeric() %&gt;% max(., na.rm=T)\n  p&lt;- the_samp %&gt;% \n    ggplot(aes(!!enquo(the_var)))+\n    geom_density()+\n    geom_rug()+\n    theme_void()+\n    geom_vline(xintercept = mu_samp, col = \"red\")+\n    geom_vline(xintercept = mu_pop, col = \"grey40\",linetype = \"dashed\")+\n    xlim(ll,ul)\n  return(p)\n}\n\n# ---- Plot multiple distributions ----\n\nplot_samples &lt;- function(pop, x, variable,n_rows = 4, ...){\n  sample_plots &lt;- x$sample[1:(4*n_rows)] %&gt;% \n  purrr::map( \\(x) plot_distribution(the_pop=pop, the_samp = x, \n                                     the_var = !!enquo(variable)))\n  p &lt;- wrap_elements(wrap_plots(sample_plots[1:(4*n_rows)], ncol=4))\n  return(p)\n  \n}\n\n# ---- Plot Combined Figure ----\n\nplot_figure_fn &lt;- function(\n    d=df, \n    v=age, \n    sim=1000, \n    size=10,\n    rows = 4){\n  # Population average\n  mu &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% mean(., na.rm=T)\n  sd &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% sd(., na.rm=T)\n  se &lt;- sd/sqrt(size)\n  # Range\n  ll &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;%  min(., na.rm=T)\n  ul &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;% max(., na.rm=T)\n  # Population standard deviation\n  # Sample data\n  samp_df &lt;- sample_data_fn(dat=d, var = !!enquo(v), samps = sim, sample_size = size)\n  # Plot Population\n  p_pop &lt;- d %&gt;%\n    ggplot(aes(!!enquo(v)))+\n      geom_density(col =\"grey60\")+\n      geom_rug(col = \"grey60\", )+\n      geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n      theme_void()+\n      labs(title =\"Population\")+\n      xlim(ll,ul)+\n      theme(plot.title = element_text(hjust = 0))\n\n  \n  p_samps &lt;- plot_samples(pop=d, x= samp_df,variable = !!enquo(v),\n                          n_rows = rows)\n  p_samps &lt;- p_samps + \n    ggtitle(paste(\"Repeated samples of size N =\",size,\"from the population\"))+\n    theme(plot.title = element_text(hjust = 0.5), \n          plot.background = element_rect(\n            fill = NA, colour = 'black', linewidth = 2)\n          )\n  \n  \n  p_dist &lt;- samp_df %&gt;% \n  ggplot(aes(sample_mean))+\n  geom_density(col=\"red\",aes(y= after_stat(ndensity)))+\n  geom_rug(col=\"red\")+\n  geom_density(data = df, aes(!!enquo(v), y= after_stat(ndensity)),\n               col=\"grey60\")+\n  geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n  xlim(ll,ul)+\n  theme_void()+\n    labs(\n      title = \"Sampling Distribution\"\n    )+  theme(plot.title = element_text(hjust = 0))\n  \n  range_upper_df &lt;- tibble(\n  x = seq( ((ll+ul)/2 -5), ((ll+ul)/2 +5), length.out = 20),\n  xend = seq(ll-5, ul+5, length.out = 20),\n  y = rep(9, 20),\n  yend = rep(1, 20)\n)\np_upper &lt;- range_upper_df %&gt;% \n  ggplot(aes(x=x, xend = xend, y=y,yend=yend))+\n  geom_segment(\n    arrow = arrow(length = unit(0.05, \"npc\"))\n  )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),\n              xlim =c(ll-5,ul+5),clip=\"off\")\n  # Lower\n  range_df &lt;- samp_df %&gt;% \n  summarise(\n    min = min(sample_mean),\n    max = max(sample_mean),\n    mean = mean(sample_mean)\n  )\n  \n  plot_df &lt;- tibble(\n  id = 1:50,\n  # x = sort(rnorm(50, mu, sd)),\n  x = sort(runif(50, ll, ul)),\n  xend = sort(rnorm(50, mu, se)),\n  y = 9,\n  yend = 1\n)\n\np_lower &lt;- plot_df %&gt;%\n  ggplot(aes(x,y, group =id))+\n  geom_segment(aes(xend=xend, yend=yend),\n               col = \"red\",arrow = arrow(length = unit(0.05, \"npc\"))\n               )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),xlim = c(ll,ul),clip=\"off\")\n\n  \n  design &lt;-\"##AAAA##\n            ##AAAA##\n            ##AAAA##\n            BBBBBBBB\n            BBBBBBBB\n            #CCCCCC#\n            #CCCCCC#\n            #CCCCCC#\n            #CCCCCC#\n            DDDDDDDD\n            DDDDDDDD\n            ##EEEE##\n            ##EEEE##\n            ##EEEE##\"\n  \n  fig &lt;- p_pop / p_upper / p_samps / p_lower / p_dist +\n    plot_layout(design = design)\n  return(fig)\n\n\n  \n  \n  \n}\n\n# ---- Samples and Figures Varying Sample Size ----\n## N = 10\nset.seed(1234)\nsamp_n10 &lt;- sample_data_fn(sample_size  = 10, samps = 1000)\nset.seed(1234)\nfig_n10 &lt;- plot_figure_fn(v=age,size = 10)\n\n## N = 30\nset.seed(1234)\nsamp_n30 &lt;- sample_data_fn(sample_size  = 30, samps = 1000)\nset.seed(1234)\nfig_n30 &lt;- plot_figure_fn(size = 30,rows=4)\n\n## N = 300\nset.seed(1234)\nsamp_n300 &lt;- sample_data_fn(sample_size  = 300, samps = 1000)\nset.seed(1234)\nfig_n300 &lt;- plot_figure_fn(size = 300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the sample sample size increases:\n\nThe width of the sampling distribution decreases (LLN)\nThe shape of the sampling distribution approximates a Normal distribution (CLT)"
  },
  {
    "objectID": "slides/10-slides.html#standard-errors",
    "href": "slides/10-slides.html#standard-errors",
    "title": "POLS 1600",
    "section": "Standard errors",
    "text": "Standard errors\n\nOverview Code SEs Coverage\n\n\n\n\nThe standard error (SE) is simply the standard deviation of the sampling distribution.\nThe SE decreases as the sample size increases (by the LLN):\nApproximately 95% of the sample means will be within 2 SEs of the population mean (CLT)\n\n\n\n\n\nse_df &lt;- tibble(\n  `Sample Size` = factor(paste(\"N =\",c(10,30, 300))),\n  se = c(sd(samp_n10$sample_mean),\n         sd(samp_n30$sample_mean),\n         sd(samp_n300$sample_mean)),\n  SE = paste(\"SE =\", round(se,2)),\n  ll = mu_age,\n  ul = mu_age + se,\n  y = c(.3,.3,.45),\n  yend = y\n)\n\nci_df &lt;- tibble(\n  `Sample Size` = factor(paste(\"N =\",c(10,30, 300))),\n  se = c(sd(samp_n10$sample_mean),\n         sd(samp_n30$sample_mean),\n         sd(samp_n300$sample_mean)),\n  mu = mu_age,\n  ll = round(mu_age - 1.96 *se,2),\n  ul = round(mu_age + 1.96 *se,2),\n  ci = paste(\"95 % Coverage Interval [\",ll,\";\",ul,\"]\",sep=\"\"),\n  y = c(.3,.3,.45),\n  yend = y\n)\nsim_df &lt;- samp_n10 %&gt;% \n  bind_rows(samp_n30) %&gt;% \n  bind_rows(samp_n300) %&gt;% \n  mutate(\n    `Sample Size` = factor(paste(\"N =\",size))\n    ) %&gt;% \n  left_join(ci_df) %&gt;% \n  mutate(\n    Coverage = case_when(\n      sample_mean &gt; ll_asymp & sample_mean &lt; ul_asymp  & size == 10~ \"#F8766D\",\n      sample_mean &gt; ll_asymp & sample_mean &lt; ul_asymp  & size == 30~ \"#00BA38\",\n      sample_mean &gt; ll_asymp & sample_mean &lt; ul_asymp  & size == 300~ \"#619CFF\",\n      T ~ \"grey\"\n    )\n  )\n\n\n\nfig_se &lt;- sim_df %&gt;% \n  ggplot(aes(sample_mean, col = `Sample Size`))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = mu_age, linetype = \"dashed\")+\n  theme_minimal()+\n  facet_wrap(~`Sample Size`, ncol=1)+\n  ylim(0,.5)+\n  guides(col=\"none\")+\n  geom_segment(\n    data = se_df,\n    aes(x= ll, xend =ul, y = y, yend = yend)\n  )+\n  geom_text(\n    data = se_df,\n    aes(x = ul, y =y, label = SE),\n    hjust = -.25\n  ) +\n  labs(\n    y = \"\",\n    x = \"Sampling Distributions of Sample Means\",\n    title = \"Standard Errors decrease with Sample Size\"\n  )\n\nfig_coverage &lt;- sim_df %&gt;% \n  ggplot(aes(sample_mean,col=`Sample Size`))+\n  geom_density()+\n  geom_rug(col=sim_df$Coverage)+\n  geom_vline(xintercept = mu_age, linetype = \"dashed\")+\n  theme_minimal()+\n  facet_wrap(~`Sample Size`, ncol=1)+\n  ylim(0,.55)+\n  guides(col=\"none\")+\n  geom_segment(\n    data = ci_df,\n    aes(x= ll, xend =ul, y = y, yend = yend)\n  )+\n  geom_text(\n    data = ci_df,\n    aes(x = mu, y =y, label = ci),\n    hjust = .5,\n    nudge_y =.1\n  ) +\n  labs(\n    y = \"\",\n    x = \"Sampling Distributions of Sample Means\",\n    title = \"Approximately 95% of sample means are within 2 SE of the population mean\"\n  )"
  },
  {
    "objectID": "slides/10-slides.html#how-do-we-calculate-a-standard-error-from-a-single-sample",
    "href": "slides/10-slides.html#how-do-we-calculate-a-standard-error-from-a-single-sample",
    "title": "POLS 1600",
    "section": "How do we calculate a standard error from a single sample?",
    "text": "How do we calculate a standard error from a single sample?"
  },
  {
    "objectID": "slides/10-slides.html#calculating-standard-errors",
    "href": "slides/10-slides.html#calculating-standard-errors",
    "title": "POLS 1600",
    "section": "Calculating standard errors",
    "text": "Calculating standard errors\n\nTwo ApproachesN = 10N = 30N = 300Simulation vs Analytic\n\n\n\n\nSimulation:\n\nTreat sample as population\nSample with replacement (“bootstrapping”)\nEstimate SE from standard deviation of resampling distribution (“plug-in principle”)\n\nAnalytic\n\nCharacterize sampling distribution from sample mean and variance via asymptotic theory (the LLT and CLT)\nFor a sample mean, \\(\\bar{x}\\)\n\n\n\\[\nSE_{\\bar{x}} = \\frac{\\sigma_x}{\\sqrt(n)}\n\\]\n\n\n\n\nplot_resampling_fn &lt;- function(d=df, v=age, sim=1000, size=10,rows=3){\n  # Population average\n  mu &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% mean(., na.rm=T)\n  # Population standard deviation and SE\n  sd &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% sd(., na.rm=T)\n  se &lt;- sd/sqrt(size)\n  # Range\n  ll &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;%  min(., na.rm=T)\n  ul &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;% max(., na.rm=T)\n  # Resampling with replace\n  # Draw 1 Sample\n  sample &lt;- sample_data_fn(dat=d, var = !!enquo(v), samps = 1, sample_size = size, resample = F)\n  samp_df &lt;- as.data.frame(sample$sample)\n  # Resample from sample with replacement\n  resamp_df &lt;- sample_data_fn(dat=samp_df, var = !!enquo(v), samps = sim, sample_size = size, resample = T)\n  # Plot Population\n  p_pop &lt;- d %&gt;%\n    ggplot(aes(!!enquo(v)))+\n      geom_density(col =\"grey60\")+\n      geom_rug(col = \"grey60\", )+\n      geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n      theme_void()+\n      labs(title =\"Population\")+\n      xlim(ll,ul)+\n      theme(plot.title = element_text(hjust = 0))\n\n  p_samp &lt;- plot_distribution(the_pop = d,\n                              the_samp = samp_df,\n                              the_var = age)+\n    labs(title =\"Sample\")+\n      xlim(ll,ul)+\n      theme(plot.title = element_text(hjust = 0))\n  \n  p_samps &lt;- plot_samples(pop=d, x= resamp_df,variable = !!enquo(v), n_rows =rows)\n  p_samps &lt;- p_samps + \n    ggtitle(paste(\"Repeated samples with replacement\\nof size N =\",size,\"from sample\"))+\n    theme(plot.title = element_text(hjust = 0.5), \n          plot.background = element_rect(\n            fill = NA, colour = 'black', linewidth = 2)\n          )\n  \n  # Resampling Distribution\n  \n  \n  p_dist &lt;- resamp_df %&gt;% \n  ggplot(aes(sample_mean))+\n  geom_density(col=\"red\",aes(y= after_stat(ndensity)))+\n  geom_rug(col=\"red\")+\n  geom_density(data = df, aes(!!enquo(v), y= after_stat(ndensity)),\n               col=\"grey60\")+\n  geom_vline(xintercept = unique(resamp_df$pop_mean), col=\"red\", linetype=\"solid\")+\n  geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n  xlim(ll,ul)+\n  theme_void()+\n    labs(\n      title = \"Reampling Distribution\"\n    )+  theme(plot.title = element_text(hjust = 0))\n  \n   range_upper_df &lt;- tibble(\n  x = seq( ((ll+ul)/2 -5), ((ll+ul)/2 +5), length.out = 20),\n  xend = seq(ll-5, ul+5, length.out = 20),\n  y = rep(9, 20),\n  yend = rep(1, 20)\n)\np_upper &lt;- range_upper_df %&gt;% \n  ggplot(aes(x=x, xend = xend, y=y,yend=yend))+\n  geom_segment(\n    arrow = arrow(length = unit(0.05, \"npc\"))\n  )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),\n              xlim =c(ll-5,ul+5),clip=\"off\")\n  # Lower\n  range_df &lt;- resamp_df %&gt;% \n  summarise(\n    min = min(sample_mean),\n    max = max(sample_mean),\n    mean = mean(sample_mean)\n  )\n  \n  plot_df &lt;- tibble(\n  id = 1:50,\n  # x = sort(rnorm(50, mu, sd)),\n  x = sort(runif(50, ll, ul)),\n  xend = sort(rnorm(50, unique(resamp_df$pop_mean), se)),\n  y = 9,\n  yend = 1\n)\n\np_lower &lt;- plot_df %&gt;%\n  ggplot(aes(x,y, group =id))+\n  geom_segment(aes(xend=xend, yend=yend),\n               col = \"red\",arrow = arrow(length = unit(0.05, \"npc\"))\n               )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),xlim = c(ll,ul),clip=\"off\")\n\n  \n  design &lt;-\"##AAAA##\n            ##AAAA##\n            ##AAAA##\n            ##BBBB##\n            ##BBBB##\n            ##BBBB##            \n            CCCCCCCC\n            CCCCCCCC\n            #DDDDDD#\n            #DDDDDD#\n            #DDDDDD#\n            #DDDDDD#\n            EEEEEEEE\n            EEEEEEEE\n            ##FFFF##\n            ##FFFF##\n            ##FFFF##\"\n  \n  fig &lt;- p_pop / p_samp /p_upper / p_samps / p_lower / p_dist +\n    plot_layout(design = design)\n  return(fig)\n\n\n  \n  \n  \n}\nset.seed(123)\nresamp_n10 &lt;- sample_data_fn(\n  dat = sample_data_fn(samps = 1, sample_size = 10, resample = T)$sample %&gt;%  as.data.frame(),\n  sample_size = 10, \n  resample = T)\nset.seed(123)\nfig_n10_bs &lt;- plot_resampling_fn(size=10)\n\nset.seed(12345)\nresamp_n30 &lt;- sample_data_fn(\n  dat = sample_data_fn(samps = 1, sample_size = 30, resample = T)$sample %&gt;%  as.data.frame(),\n  samps = 1000, sample_size = 30, resample = T)\n\nset.seed(12345)\nfig_n30_bs &lt;- plot_resampling_fn(size=30)\n\nset.seed(1234)\nresamp_n300 &lt;- sample_data_fn(\n  dat = sample_data_fn(samps = 1, sample_size = 300, resample = T)$sample %&gt;%  as.data.frame(),\n  samps = 1000, sample_size = 300, resample = T)\nset.seed(1234)\nfig_n300_bs &lt;- plot_resampling_fn(size=300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap SE\nAnalytic SE\n\n\n\n\n5.74\n5.61\n\n\n2.75\n3.24\n\n\n1.07\n1.02"
  },
  {
    "objectID": "slides/10-slides.html#confidence-intervals-1",
    "href": "slides/10-slides.html#confidence-intervals-1",
    "title": "POLS 1600",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nConfidence intervals:\n\nprovide a way of quantifying uncertainty about estimates\ndescribe a range of plausible values for an estimate\nare a function of the standard error of the estimate, and the a critical value determined by \\(\\alpha\\), which describes the degree of confidence we want"
  },
  {
    "objectID": "slides/10-slides.html#section-1",
    "href": "slides/10-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Calculating a confidence interval\n\nSteps Code Fig 1 Fig 2 Fig 3Comments\n\n\n\n\nChoose level of confidence \\((1-\\alpha)\\times 100%\\)\n\n\\(\\alpha = 0.05\\), corresponds to a 95% confidence level.\n\nDerive the sampling distribution of the estimator\n\nSimulation: bootstrap re-sampling\nAnalytically: computing its mean and variance.\n\nCompute the standard error\nCompute the critical value \\(z_{\\alpha/2}\\)\n\nas the \\(1.96 = \\Phi(z_{0.5/2})\\) for a 95% CI\n\nCompute the lower and upper confidence limits\n\nlower limit = \\(\\hat{\\theta} - z_{\\alpha/2}\\times SE\\)\nupper limit = \\(\\hat{\\theta} + z_{\\alpha/2}\\times SE\\)\n\n\n\n\n\n\nresamp_df &lt;- \n  resamp_n10 %&gt;% \n  bind_rows(resamp_n30) %&gt;% \n  bind_rows(resamp_n300) %&gt;% \n  mutate(\n    `Sample Size` = factor(paste(\"N =\",size))\n    )\n\nresamp_ci_df &lt;- tibble(\n  `Sample Size` = factor(paste(\"N =\",c(10,30,300))),\n  mu = unique(resamp_df$pop_mean),\n  ll = unique(resamp_df$ll_asymp),\n  ul = unique(resamp_df$ul_asymp),\n  y = c(.3, .3,.5)\n)\n\nfig_ci1 &lt;- resamp_df %&gt;% \n  ggplot(aes(sample_mean,\n             col = `Sample Size`))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = mu_age, linetype = \"dashed\")+\n  geom_vline(data = resamp_ci_df,\n             aes(xintercept = mu,\n                 col = `Sample Size`))+\n  geom_segment(data = resamp_ci_df,\n               aes(x = ll, xend =ul, y = y, yend =y,\n                   col = `Sample Size`))+\n  facet_wrap(~`Sample Size`, ncol=1)+\n  theme_minimal()+\n  labs(\n    y = \"\",\n    x = \"Resampling Distribution\",\n    title = \"95% Confidence Intervals\"\n  )\n  \n\nsamp_ci_df &lt;- samp_n10 %&gt;% \n  bind_rows(samp_n30) %&gt;% \n  bind_rows(samp_n300) %&gt;% \n  mutate(\n    `Sample Size` = factor(paste(\"N =\",size))\n    ) %&gt;% \n  mutate(\n    Coverage = case_when(\n      pop_mean &gt; ll & pop_mean &lt; ul ~ \"red\",\n      T ~ \"black\"\n    )\n  )\n\nfig_ci2 &lt;- samp_ci_df %&gt;% \n  filter(sim %in% 1:100) %&gt;% \n  filter(size == 10) %&gt;% \n  ggplot(aes(y = sample_mean, x= sim))+\n  geom_pointrange(aes(ymin = ll, ymax =ul, col=Coverage))+\n  geom_hline(yintercept = mu_age, linetype = \"dashed\")+\n  coord_flip()+\n  theme_minimal()+\n  guides(col = \"none\")+\n  facet_wrap(~`Sample Size`)\n\nfig_ci3 &lt;- samp_ci_df %&gt;% \n  filter(sim %in% 1:100) %&gt;% \n  ggplot(aes(y = sample_mean, x= sim))+\n  geom_pointrange(aes(ymin = ll, ymax =ul, col=Coverage))+\n  geom_hline(yintercept = mu_age, linetype = \"dashed\")+\n  coord_flip()+\n  theme_minimal()+\n  guides(col = \"none\")+\n  facet_wrap(~`Sample Size`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 shows 3 confidences intervals for 3 samples of different sizes (N = 10, 30, 300). The CIs for N = 10 and N = 300, intervals contain the truth (include the population mean). By chance, the CI for N=30 falls outside of the truth.\nFigure 2 shows that our confidence is about the property of the interval. Over repeated sampling, 95% of the intervals would contain the truth, 5% percent would not.\n\nIn any one sample, the population parameter either is or is not within the interval.\n\nFigure 3, shows that while the width of the interval declines with the sample size, the coverage properties remains the same."
  },
  {
    "objectID": "slides/10-slides.html#interpreting-confidence-intervals",
    "href": "slides/10-slides.html#interpreting-confidence-intervals",
    "title": "POLS 1600",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nConfidence intervals give a range of values that are likely to include the true value of the parameter \\(\\theta\\) with probability \\((1-\\alpha) \\times 100\\%\\)\n\n\\(\\alpha = 0.05\\) corresponds to a “95-percent confidence interval”\n\nOur “confidence” is about the interval\nIn repeated sampling, we expect that \\((1-\\alpha) \\times 100\\%\\) of the intervals we construct would contain the truth.\nFor any one interval, the truth, \\(\\theta\\), either falls within in the lower and upper bounds of the interval or it does not."
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-hypothesis-test",
    "href": "slides/10-slides.html#what-is-a-hypothesis-test",
    "title": "POLS 1600",
    "section": "What is a hypothesis test",
    "text": "What is a hypothesis test\n\nA formal way of assessing statistical evidence. Combines\n\nDeductive reasoning distribution of a test statistic, if the a null hypothesis were true\nInductive reasoning based on the test statistic we observed, how likely is it that we would observe it if the null were true?"
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-test-statistic",
    "href": "slides/10-slides.html#what-is-a-test-statistic",
    "title": "POLS 1600",
    "section": "What is a test statistic?",
    "text": "What is a test statistic?\n\nA way of summarizing data\n\ndifference of means\ncoefficients from a linear model\ncoefficients from a linear model divided by their standard errors\nR^2\nSums of ranks\n\n\n\n\n\n\n\n\n\nNote\n\n\nDifferent test statistics may be more or less appropriate depending on your data and questions."
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-null-hypothesis",
    "href": "slides/10-slides.html#what-is-a-null-hypothesis",
    "title": "POLS 1600",
    "section": "What is a null hypothesis?",
    "text": "What is a null hypothesis?\n\nA statement about the world\n\nOnly interesting if we reject it\nWould yield a distribution of test statistics under the null\nTypically something like “X has no effect on Y” (Null = no effect)\nNever accept the null can only reject"
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-p-value",
    "href": "slides/10-slides.html#what-is-a-p-value",
    "title": "POLS 1600",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nA p-value is a conditional probability summarizing the likelihood of observing a test statistic as far from our hypothesis or farther, if our hypothesis were true."
  },
  {
    "objectID": "slides/10-slides.html#how-do-we-do-hypothesis-testing",
    "href": "slides/10-slides.html#how-do-we-do-hypothesis-testing",
    "title": "POLS 1600",
    "section": "How do we do hypothesis testing?",
    "text": "How do we do hypothesis testing?\n\nPosit a hypothesis (e.g. \\(\\beta = 0\\))\nCalculate the test statistic (e.g. \\((\\hat{\\beta}-\\beta)/se_\\beta\\))\nDerive the distribution of the test statistic under the null via simulation or asymptotic theory\nCompare the test statistic to the distribution under the null\nCalculate p-value (Two Sided vs One sided tests)\nReject or fail to reject/retain our hypothesis based on some threshold of statistical significance (e.g. p &lt; 0.05)"
  },
  {
    "objectID": "slides/10-slides.html#outcomes-of-hypothesis-tests",
    "href": "slides/10-slides.html#outcomes-of-hypothesis-tests",
    "title": "POLS 1600",
    "section": "Outcomes of hypothesis tests",
    "text": "Outcomes of hypothesis tests\n\n\nTwo conclusions from of a hypothesis test: we can reject or fail to reject a hypothesis test.\nWe never “accept” a hypothesis, since there are, in theory, an infinite number of other hypotheses we could have tested.\n\nOur decision can produce four outcomes and two types of error:\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative\n\n\n\n\nType 1 Errors: False Positive Rate (p &lt; 0.05)\nType 2 Errors: False negative rate (1 - Power of test)"
  },
  {
    "objectID": "slides/10-slides.html#quantifying-uncertainty-in-regression-1",
    "href": "slides/10-slides.html#quantifying-uncertainty-in-regression-1",
    "title": "POLS 1600",
    "section": "Quantifying uncertainty in regression",
    "text": "Quantifying uncertainty in regression\n\nOverview Raw SEs CIs Coefficient plot\n\n\nHow do income and education shape political participation?\nLet’s fit the following model\n\\[\ny = \\beta_0 + \\beta_1\\text{income} + \\beta_2 \\text{education} + \\epsilon\n\\]\n\nm1 &lt;- lm_robust(dv_participation ~   education + income, df)\n\nAnd unpack the output\n\n\n\ntidy(m1) %&gt;% \n  mutate_if(is.numeric, \\(x) round(x, 3)) -&gt; m1_sum\nm1_sum\n\n         term estimate std.error statistic p.value conf.low conf.high   df\n1 (Intercept)    0.312     0.080     3.910   0.000    0.155     0.468 1684\n2   education    0.167     0.024     6.891   0.000    0.119     0.214 1684\n3      income    0.007     0.010     0.671   0.502   -0.014     0.028 1684\n           outcome\n1 dv_participation\n2 dv_participation\n3 dv_participation\n\n\n\n\nhtmlreg(m1,include.ci=F) \n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.31***\n\n\n\n\n \n\n\n(0.08)\n\n\n\n\neducation\n\n\n0.17***\n\n\n\n\n \n\n\n(0.02)\n\n\n\n\nincome\n\n\n0.01\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.29\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\nhtmlreg(m1,include.ci=T) \n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.31*\n\n\n\n\n \n\n\n[ 0.16; 0.47]\n\n\n\n\neducation\n\n\n0.17*\n\n\n\n\n \n\n\n[ 0.12; 0.21]\n\n\n\n\nincome\n\n\n0.01\n\n\n\n\n \n\n\n[-0.01; 0.03]\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.29\n\n\n\n\n\n\n* 0 outside the confidence interval.\n\n\n\n\n\n\n\nm1_coefplot &lt;- m1_sum %&gt;% \n  ggplot(aes(term, estimate))+\n  geom_pointrange(aes(ymin = conf.low, ymax =conf.high))+\n  geom_hline(yintercept = 0, linetype = \"dashed\")+\n  coord_flip()+\n  labs(\n    y = \"Estimate\",\n    x = \"\",\n    title = \"Coefficient plot\"\n  )+\n  theme_minimal()"
  },
  {
    "objectID": "slides/10-slides.html#estimates",
    "href": "slides/10-slides.html#estimates",
    "title": "POLS 1600",
    "section": "Estimates",
    "text": "Estimates\n\nEstimateComments\n\n\nThe estimate column are the regression coefficients, \\(\\beta\\)\nRecall, lm_robust() calculates these:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\n\n\nTip\n\n\n\\(\\beta\\)s describe substantive relationships between predictors (income, education) and the outcome (political participation)\n\n\n\n\n\n\ncoef(m1)\n\n(Intercept)   education      income \n0.311609712 0.166755964 0.007034253 \n\nX &lt;- model.matrix(m1,data=df)\ny &lt;- model.frame(m1)$dv_participation\nbetas &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbetas\n\n                   [,1]\n(Intercept) 0.311609712\neducation   0.166755964\nincome      0.007034253\n\n\n\n\nA unit increases in education is associated with about 0.16 more acts of political participation, while a unit increase in income is associated with 0.007 more acts of participation.\nNote that both income and education are measured with ordinal scales\n\nget_value_labels(df$educ)\n\n    No HS credential High school graduate         Some college \n                   1                    2                    3 \n       2-year degree        4-year degree            Post-grad \n                   4                    5                    6 \n\n\nSuch that it might be unreasonable to assume cardinality (going from a 1 to 2 is the same as going from a 3 to 4)\n\nConsider treating as factor / recoding variable"
  },
  {
    "objectID": "slides/10-slides.html#section-4",
    "href": "slides/10-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Standard errors & confidence intervals\n\nSEs and CI Code SEs SEsComments\n\n\nThe default standard errors from lm_robust() are calculated as follows\n\\[\nSE_{\\beta} = (X'X)^{-1}X'\\text{diag}\\left[\\frac{e_i^2}{1-h_{ii}}\\right]X(X'X)^{-1}\n\\]\nWhich we could also obtain via bootstrapping.\nThe confidence intervals are calculated as follows:\n\\[\nCI = \\beta \\pm 1.96\\times SE_\\beta\n\\]\n\n\n\n# 0 Set seed\nset.seed(123)\n\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df %&gt;% select(dv_participation, income, education), 1000)\n# Estimate Boostrapped Models\nm1_bs &lt;- purrr::map(boot$strap, ~ lm_robust(dv_participation ~  income + education, data = .))\n\n# Tidy coefficients\nm1_bs_df &lt;- map_df(m1_bs, tidy, .id = \"id\")\nm1_asymp_df &lt;- tidy(m1) %&gt;% \n  mutate(\n    term = factor(term)\n  ) %&gt;% \n  select(term,estimate, std.error,conf.low, conf.high) %&gt;% \n  mutate(\n    ll = conf.low,\n    ul = conf.high,\n    y = 1.1,\n    type = \"Analytic\"\n  )\n\nm1_bs_ci_df &lt;- m1_bs_df %&gt;%\n  mutate(\n    term = factor(term)\n  ) %&gt;% \n  group_by(term) %&gt;% \n  summarise(\n  beta = mean(estimate,na.rm=T),\n  se = sd(estimate,na.rm=T)\n  ) %&gt;% \n  mutate(\n  ll = beta - 1.96*se,\n  ul = beta + 1.96*se,\n  y = 1.05,\n  type = \"Bootstrap\"\n) \n\n# Compare SEs\n\ncompare_m1_se_tab &lt;-\n  tibble(\n    `Predictor` = m1_bs_ci_df$term,\n    Estimate = m1_asymp_df$estimate,\n    `SE` = m1_asymp_df$std.error,\n     `CI` = paste(\"[\", round(m1_asymp_df$ll,2),\n                  \"; \", round(m1_asymp_df$ul,2),\"]\",\n                  sep =\"\"),\n    `SE ` = m1_bs_ci_df$se,\n    `CI ` = paste(\"[\", round(m1_bs_ci_df$ll,2),\n                  \"; \", round(m1_bs_ci_df$ul,2),\"]\",\n                  sep =\"\"),\n  )\n\n\n# Figure\nfig_m1_bs &lt;- m1_bs_df %&gt;% \n  ggplot(aes(estimate))+\n  geom_density(aes(y=after_stat(ndensity)))+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype = \"dashed\")+\n  facet_wrap(~term,scales = \"free\")+\n  theme_minimal()+\n  ylim(0, 1.2)+\n  geom_vline(\n    data = m1_asymp_df,\n    aes(xintercept = estimate)\n  ) +\n  geom_segment(\n    data = m1_bs_ci_df,\n    aes(x = ll, xend = ul,\n        y = y, yend = y,\n        col = \"Bootstrap\")\n    \n  ) +\n  geom_segment(\n    data = m1_asymp_df,\n    aes(x = ll, xend = ul,\n        y = y, yend = y,\n        col = \"Analytic\")\n    \n  ) +\n  labs(\n    col = \"Confidence Interval\",\n    x = \"Bootstrapped Sampling Distribution\\n of Coefficients\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytic\n\n\nBootstrap\n\n\n\nPredictor\nEstimate\nSE\nCI\nSE\nCI\n\n\n\n\n(Intercept)\n0.3116\n0.0797\n[0.16; 0.47]\n0.0805\n[0.15; 0.47]\n\n\neducation\n0.1668\n0.0242\n[0.12; 0.21]\n0.0248\n[0.12; 0.22]\n\n\nincome\n0.0070\n0.0105\n[-0.01; 0.03]\n0.0107\n[-0.01; 0.03]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe main takeaway here is that for linear models, bootstrapped SEs and CIs are quite similar to those obtained via analytically (via math and asymptotic theory)\nFor common estimators and large samples, we’ll generally use analytic SEs (quicker)\nFor less common estimators (ratios of estimates), analytic estimates of the SEs may not exist. Bootstrapping will still provide valid SEs, provided we “sample from the sample, as the sample was drawn from the population”"
  },
  {
    "objectID": "slides/10-slides.html#test-statistics-and-p-values",
    "href": "slides/10-slides.html#test-statistics-and-p-values",
    "title": "POLS 1600",
    "section": "Test statistics and p-values",
    "text": "Test statistics and p-values\n\nOverview CodeComments\n\n\nThe test statistic (“t-stat”) reported by lm() and lm_robust() is our observed coefficient, \\(\\hat{\\beta}\\) minus our hypothesized value \\(\\beta\\) (e.g. 0), divided by the standard error of \\(\\hat{\\beta}\\).\n\\[t= \\frac{\\hat\\beta-\\beta}{\\widehat{SE}_{\\hat{\\beta}}} \\sim \\text{Students's } t \\text{ with } n-k \\text{ degrees of freedom}\\] Which follows a \\(t\\) distribution – like a Normal with “heavier tails” (e.g. more probability assigned to extreme values)\n\n\n\n# Calculate t-stats\n\nt_stat_df &lt;- tibble(\n  x= seq(-3,3,length.out = 20),\n  p = dt(x,df=m1$df[1] )\n)\n\n\nm1_tstat_educ &lt;- t_stat_df %&gt;% \n  ggplot(aes(x=x,y=p))+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"line\",\n    xlim = c(\n      min(c(-3, abs(m1$statistic[2])*-1 -1)),\n      max(c(3, abs(m1$statistic[2])+1))\n      )\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(m1$statistic[2],4)\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(-4, abs(m1$statistic[2])*-1)\n  )+\n  geom_vline(xintercept = m1$statistic[2],\n             col = \"blue\",\n             linetype = \"dashed\")+\n   geom_vline(xintercept = m1$statistic[2]*-1,\n             col = \"blue\",\n             linetype = \"dashed\")+\n  theme_minimal()+\n  labs(\n    title = \"Education\",\n    subtitle = paste(\"t-stat = \",round(m1$statistic[2],3),\n    \"\\nPr(&gt;|t|) = \",\n    format(round(m1$p.value[2],3),nsmall = 3),\n    sep = \"\"\n    ),\n    x = \"Distribution of t-stat under the Null\"\n  )\n\nm1_tstat_income &lt;- t_stat_df %&gt;% \n  ggplot(aes(x=x,y=p))+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"line\",\n    xlim = c(\n      min(c(-3, abs(m1$statistic[3])*-1 -1)),\n      max(c(3, abs(m1$statistic[3])+1))\n      )\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(m1$statistic[3],4)\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(-4, abs(m1$statistic[3])*-1)\n  )+\n  geom_vline(xintercept = m1$statistic[3],\n             col = \"blue\",\n             linetype = \"dashed\")+\n   geom_vline(xintercept = m1$statistic[3]*-1,\n             col = \"blue\",\n             linetype = \"dashed\")+\n  theme_minimal()+\n  labs(\n    title = \"Income\",\n    subtitle = paste(\"t-stat = \",round(m1$statistic[3],3),\n    \"\\nPr(&gt;|t|) = \",\n    format(round(m1$p.value[3],3),nsmall = 3),\n    sep = \"\"\n    ),\n    x = \"Distribution of t-stat under the Null\"\n  )\n\nfig_pvalue &lt;- m1_tstat_educ + m1_tstat_income\n\n# Compare Pvalues\n\ncompare_m1_pvalue &lt;-\n  tibble(\n    `Predictor` = m1_bs_ci_df$term,\n    Estimate = m1_asymp_df$estimate,\n    SE = m1_sum$std.error,\n    `t-stat` = m1_sum$statistic,\n     `Pr(&gt;abs(t))` = format(round(m1_sum$p.value,3), nsmall=3)\n  )\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nSE\nt-stat\nPr(&gt;abs(t))\n\n\n\n\n(Intercept)\n0.312\n0.080\n3.910\n0.000\n\n\neducation\n0.167\n0.024\n6.891\n0.000\n\n\nincome\n0.007\n0.010\n0.671\n0.502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 4\n\n\n\n\n\n\nThe p-value for the coefficient on education is less than 0.05, while the p-value for income is 0.50.\nIf there was no relationship between education and participation (\\(H_0:\\beta_2=0\\)), it would be quite unlikely that we would observed a test statistic of 6.89 or larger.\nSimilarly, test statistics as larger or larger than 0.671 occurs quite frequently in a world where there is no relationship (\\(H_0:\\beta_3=0\\)) between income and participation.\nThus we reject the null hypothesis for education, but fail to reject the null hypothesis for income in this model."
  },
  {
    "objectID": "slides/10-slides.html#predicted-values",
    "href": "slides/10-slides.html#predicted-values",
    "title": "POLS 1600",
    "section": "Predicted values",
    "text": "Predicted values\n\nOverview Code Table FigComments\n\n\nLet’s explore whether income and education condition each other’s relationship with participation using the following interaction model\n\\[\ny = \\beta_0 +\\beta_1 \\text{educ} + \\beta_2 \\text{inc} + \\beta_3\\text{educ}\\times\\text{inc} + \\epsilon\n\\]\nTo help our interpretations we’ll produce plots of predicted values of participation, at varying levels of income and education.\n\n\n\n# Fit model\nm2 &lt;- lm_robust(dv_participation ~ education*income, df)\n\n\n# Regression Table\nm2_tab &lt;- htmlreg(\n  m2, \n  include.ci = F,\n  digits = 3,\n  stars = c(0.05, 0.10)\n                    )\n\n# Predicted values\n\n# Data frame of values we want to make predictions at\npred_df &lt;-expand_grid(\n  income = sort(unique(df$income)),\n  education = quantile(df$education, na.rm = T)[c(2,4)]\n)\n\n# Combine model predictions\npred_df &lt;- cbind(pred_df, predict(m2, pred_df,\n                                  interval = \"confidence\")$fit)\n\n# Plot predicted values\nfig_m2_pred &lt;- pred_df %&gt;% \n  mutate(\n    Education = ifelse(education == 2, \"High school\",\"College\")\n  ) %&gt;% \n  ggplot(aes(income, fit, group=Education))+\n  geom_ribbon(aes(ymin = lwr, ymax = upr,\n                  fill = Education),\n              alpha=.5)+\n  geom_line()+\n  theme_minimal()+\n  labs(y = \"Predicted Participation\",\n       x = \"Income\",\n       title = \"\")\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.060\n\n\n\n\n \n\n\n(0.151)\n\n\n\n\neducation\n\n\n0.242**\n\n\n\n\n \n\n\n(0.050)\n\n\n\n\nincome\n\n\n0.048**\n\n\n\n\n \n\n\n(0.024)\n\n\n\n\neducation:income\n\n\n-0.011*\n\n\n\n\n \n\n\n(0.006)\n\n\n\n\nR2\n\n\n0.042\n\n\n\n\nAdj. R2\n\n\n0.040\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.286\n\n\n\n\n\n\n**p &lt; 0.05; *p &lt; 0.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow income individuals with a college degree participate at significantly higher rates than individuals with a similar levels of income with only a high school diploma.\nAlternatively, we might say that the college educated tend to participate at similar levels, regardless of their level of income, while income has a marginally positive relationship with participation for those without college degrees.\n\n\n\n\n\n\nNote\n\n\nIs this a causal relationship? What assumptions would we need to make a causal claim about the effects of education on participation?"
  },
  {
    "objectID": "slides/10-slides.html#references",
    "href": "slides/10-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/13-slides-old.html#general-plan",
    "href": "slides/13-slides-old.html#general-plan",
    "title": "Week 13:",
    "section": "General Plan",
    "text": "General Plan\n\nCourse Summary\nCourse Feedback\nAMA\nPaper Workshop\n\nclass: bottom, center background-image: url(“https://www.powerthesaurus.org/_images/terms/summery-synonyms-2.png”) bacground-size: contain"
  },
  {
    "objectID": "slides/13-slides-old.html#what-was-this-course-about",
    "href": "slides/13-slides-old.html#what-was-this-course-about",
    "title": "Week 13:",
    "section": "What was this course about?",
    "text": "What was this course about?\n–\n\nHow would you know?\n\n–\n\nHow would convince someone who thinks they know different?\n\n–\n\nWhat would it take to change what you think you know?"
  },
  {
    "objectID": "slides/13-slides-old.html#why-quantitative-social-science",
    "href": "slides/13-slides-old.html#why-quantitative-social-science",
    "title": "Week 13:",
    "section": "Why Quantitative Social Science?",
    "text": "Why Quantitative Social Science?\nWhat makes for quantitative social science\n\nCompelling?\nUseful?\nHard?\n\n–\nStatistical methods and programming are tools not answers\n–\nProducing knowledge requires us to match theory with empirical design\n–\nBeing “right” is less important than understanding all the ways you might be wrong.\n–\nGood social science is urgent, eclectic, transparent, contingent, and ?"
  },
  {
    "objectID": "slides/13-slides-old.html#what-youve-learned",
    "href": "slides/13-slides-old.html#what-youve-learned",
    "title": "Week 13:",
    "section": "What you’ve learned",
    "text": "What you’ve learned\nSkills and concepts to help understand and practice quantitative social science\n\nCausal Inference is about counterfactual comparisons\n\nExperimental designs \\(\\to\\) random assignment\nObservational design \\(\\to\\) identifying assumptions \\(\\to\\) conditional independence\n\nRegression is a tool for describing relationships\n\nOLS provides a linear approximation of the Conditional Expectation Function\nRegression can be used for descriptive, predictive, and causal inference.\n\nStatistical inference is about quantifying uncertainty about what could have happened\n\nConfidence intervals provide a range of plausible values for we observed\nHypothesis tests describe the conditional probability of observing what we did, if some hypothesis were true.\n\n\nclass:inverse, bottom, center background-image: url(“https://i.imgflip.com/6f1aul.jpg”) bacground-size: cover # 📢 ## Feedback"
  },
  {
    "objectID": "slides/13-slides-old.html#what-we-liked",
    "href": "slides/13-slides-old.html#what-we-liked",
    "title": "Week 13:",
    "section": "What we liked",
    "text": "What we liked"
  },
  {
    "objectID": "slides/13-slides-old.html#what-we-disliked",
    "href": "slides/13-slides-old.html#what-we-disliked",
    "title": "Week 13:",
    "section": "What we disliked",
    "text": "What we disliked"
  },
  {
    "objectID": "slides/13-slides-old.html#what-we-learned",
    "href": "slides/13-slides-old.html#what-we-learned",
    "title": "Week 13:",
    "section": "What we learned",
    "text": "What we learned"
  },
  {
    "objectID": "slides/13-slides-old.html#what-were-still-learning",
    "href": "slides/13-slides-old.html#what-were-still-learning",
    "title": "Week 13:",
    "section": "What we’re still learning",
    "text": "What we’re still learning"
  },
  {
    "objectID": "slides/13-slides-old.html#howd-i-do",
    "href": "slides/13-slides-old.html#howd-i-do",
    "title": "Week 13:",
    "section": "How’d I do?",
    "text": "How’d I do?\n.pull-left[\n]\n–\n.pull-right[\n\n]"
  },
  {
    "objectID": "slides/13-slides-old.html#howd-you-do",
    "href": "slides/13-slides-old.html#howd-you-do",
    "title": "Week 13:",
    "section": "How’d you do?",
    "text": "How’d you do?"
  },
  {
    "objectID": "slides/13-slides-old.html#are-you-happy-you-took-pols-1600",
    "href": "slides/13-slides-old.html#are-you-happy-you-took-pols-1600",
    "title": "Week 13:",
    "section": "Are you happy you took POLS 1600?",
    "text": "Are you happy you took POLS 1600?"
  },
  {
    "objectID": "slides/13-slides-old.html#what-ill-do",
    "href": "slides/13-slides-old.html#what-ill-do",
    "title": "Week 13:",
    "section": "What I’ll Do",
    "text": "What I’ll Do\n\nLectures:\n\nLess is more\nHow much math…\nMore supplemental content (class notes)\nMore active learning\nIntegrating the textbook\n\n\n–\n\nLabs:\n\nSeemed to work ok\nGroup work vs individual learning?\nGreater integration with textbook and lecture\n\n\n–\n\nAssignments and Grading\n\nMostly liked the group project\nTutorials were helpful\nMore individualized assignments/accountability?\n\n\nclass:inverse, middle, center # 🔍 ## AMA"
  },
  {
    "objectID": "slides/13-slides-old.html#ask-you-anthing-ok-boomer",
    "href": "slides/13-slides-old.html#ask-you-anthing-ok-boomer",
    "title": "Week 13:",
    "section": "Ask you anthing? Ok boomer…",
    "text": "Ask you anthing? Ok boomer…\n\nsum(df$ama == \"\")\n\n[1] 7\n\n\n\n“how many cats do you have”\n\n–\nWe’ve had three cats, Isla, Abby and, currently Toby.\n\n“What is the craziest moment from your time in undergrad?”\n\n–\n\n\n\n\n\n\n\n\n\n\n\n\n“I am interested to know why you decided to go to graduate school, what other career paths you were deciding between, and whether you believe you made the right decision to go into academia and why?”\n\n–\n\n\n\n\n\n\n\n\n\n\n“What’s your favorite musical”\n\n–\n\n\n\n\n\n\n\n\n\n\n“If we want to, how should we develop our R skills by ourselves at Brown?\n\n\nBest book you ever read?\n\n\nBook you think someone in their early 20s should read?”"
  },
  {
    "objectID": "slides/02-slides.html#class-plan",
    "href": "slides/02-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncments\nSetup (5 minutes)\nReview\n\nTroubleshooting Errors (5 min)\nData wrangling in R (20 min)\nDescriptive Statistics (10 min)\n\nData Visualization (40 min)\n\nThe grammar of graphics\nBasic plots to describe:\n\nDistributions\nAssociations"
  },
  {
    "objectID": "slides/02-slides.html#announcements",
    "href": "slides/02-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\nManuel’s office hours today\nPaul’s office hours on Thursday\nSubmit tutorials from last week for full credit by this Sunday.\nGroups for the course assigned next week"
  },
  {
    "objectID": "slides/02-slides.html#setup-for-today",
    "href": "slides/02-slides.html#setup-for-today",
    "title": "POLS 1600",
    "section": "Setup for today",
    "text": "Setup for today"
  },
  {
    "objectID": "slides/02-slides.html#libraries",
    "href": "slides/02-slides.html#libraries",
    "title": "POLS 1600",
    "section": "Libraries",
    "text": "Libraries\nThis week we’ll use the following libraries.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  \n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  \n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  \n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)\nthe_packages\n\n [1] \"tinytex\"    \"kableExtra\" \"tidyverse\"  \"lubridate\"  \"forcats\"   \n [6] \"haven\"      \"labelled\"   \"ggmap\"      \"ggrepel\"    \"ggridges\"  \n[11] \"ggthemes\"   \"ggpubr\"     \"GGally\"     \"COVID19\"    \"maps\"      \n[16] \"mapdata\"    \"DT\""
  },
  {
    "objectID": "slides/02-slides.html#installing-and-loading-new-packages",
    "href": "slides/02-slides.html#installing-and-loading-new-packages",
    "title": "POLS 1600",
    "section": "Installing and loading new packages",
    "text": "Installing and loading new packages\nNext we’ll create a function called ipak (thanks Steven) which:\n\nTakes a list of packages (pkg)\nChecks to see if these packages are installed\nInstalls any new packages\nLoads all the packages so we can use them\n\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nAgain, run this code on your machines"
  },
  {
    "objectID": "slides/02-slides.html#installing-and-loading-new-packages-1",
    "href": "slides/02-slides.html#installing-and-loading-new-packages-1",
    "title": "POLS 1600",
    "section": "Installing and loading new packages",
    "text": "Installing and loading new packages\nFinally, let’s use ipak to install and load the_packages\nWhat should we replace some_function and some_input with to do this?\n\nsome_function(some_input)\n\n\n\nipak(the_packages)\n\n   tinytex kableExtra  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally    COVID19 \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n      maps    mapdata         DT \n      TRUE       TRUE       TRUE \n\n\n\nR may ask you to install a package’s dependencies (other packages your package needs). Try entering the number 1 into your console\nR may tell you need to restart R Try saying yes. If it doesn’t start downloading, say no\nR may then ask if you want to compile some packages from source. Type Y into your console. If this doesn’t work, try again, but this time type N when asked"
  },
  {
    "objectID": "slides/02-slides.html#loading-the-covid-19-data",
    "href": "slides/02-slides.html#loading-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Loading the Covid-19 Data",
    "text": "Loading the Covid-19 Data\nLet’s load the Covid-19 data we worked with last week:\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/02-slides.html#section",
    "href": "slides/02-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "XKCD"
  },
  {
    "objectID": "slides/02-slides.html#two-kinds-of-errors",
    "href": "slides/02-slides.html#two-kinds-of-errors",
    "title": "POLS 1600",
    "section": "Two kinds of errors:",
    "text": "Two kinds of errors:\n\nSyntactic\n\nR doesn’t understand how to run your code\nMost common, easy to fix (eventually…)\n\n\n\n\nSemantic\n\nR runs your code but doesn’t give you the expected result\nLess common, harder to fix\n\n\n\n\nMost errors happen because R is looking for something that isn’t there.\nMore discussion here and here"
  },
  {
    "objectID": "slides/02-slides.html#common-syntactic-errors",
    "href": "slides/02-slides.html#common-syntactic-errors",
    "title": "POLS 1600",
    "section": "Common Syntactic Errors",
    "text": "Common Syntactic Errors\n\nUnmatched parentheses or brackets\nMisspelled a name\nForgot a comma\nForgot to install a package or load a library\nForgot to set the working directory/path to a file you want R to use.\nTried to select a column or row that doesn’t exist"
  },
  {
    "objectID": "slides/02-slides.html#fixing-syntactic-errors",
    "href": "slides/02-slides.html#fixing-syntactic-errors",
    "title": "POLS 1600",
    "section": "Fixing Syntactic Errors",
    "text": "Fixing Syntactic Errors\n\nR Studio’s script editor will show a red circle with a white x in next to a line of code it thinks has an error in it.\nHave someone else look at your code (Fresh eyes, paired programming)\nCopy and paste the “general part” of error message into Google.\nKnit your document after each completed code chunk\n\nThis will run the code from top to bottom, and stop when it encounters an error\nTry commenting out the whole chunk, and then uncommenting successive lines of code\n\nBe patient. Don’t be hard are yourself. Remember, errors are portals of discovery."
  },
  {
    "objectID": "slides/02-slides.html#semantic-errors",
    "href": "slides/02-slides.html#semantic-errors",
    "title": "POLS 1600",
    "section": "Semantic Errors",
    "text": "Semantic Errors\n\nYour code runs, but doesn’t produce what you expected.\nLess common; can be harder to identify and fix\nOne example: Two packages have a function with the same name that do different things\n\n\n\n# dplyr::summarize\n# Hmisc::summarize"
  },
  {
    "objectID": "slides/02-slides.html#semantic-errors-1",
    "href": "slides/02-slides.html#semantic-errors-1",
    "title": "POLS 1600",
    "section": "Semantic Errors",
    "text": "Semantic Errors\n\nSome general solutions/practices to avoid semantic errors:\n\nSpecify the package and the function you want: package_name::function_name()\nWrite helpful comments in your code.\nInclude “sanity” checks in your code.\nIf a function should produce an output that’s a data.frame, check to see if it is a data frame\n\n\n\n\n# Here's some pseudo code:\n\n# I expect my_function produces a data frame\nx &lt;- my_function(y) \n\n# Check to see if x is a data frame\n# If x is not a data frame, return an Error\nstopifnot(is.data.frame(x))"
  },
  {
    "objectID": "slides/02-slides.html#why-do-we-need-to-wrangle-data",
    "href": "slides/02-slides.html#why-do-we-need-to-wrangle-data",
    "title": "POLS 1600",
    "section": "Why do we need to “wrangle” data",
    "text": "Why do we need to “wrangle” data\n\nRarely, if ever, do we get data in the exact format we need.\nInstead, before we can get to work, we often need to transform our data in various ways\nSometimes called:\n\nData cleaning/recoding\nData wrangling\nData carpentry\n\nThe end goal is the same: make messy data tidy"
  },
  {
    "objectID": "slides/02-slides.html#tidy-data",
    "href": "slides/02-slides.html#tidy-data",
    "title": "POLS 1600",
    "section": "Tidy data",
    "text": "Tidy data\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value"
  },
  {
    "objectID": "slides/02-slides.html#tools-for-transforming-our-data",
    "href": "slides/02-slides.html#tools-for-transforming-our-data",
    "title": "POLS 1600",
    "section": "Tools for transforming our data",
    "text": "Tools for transforming our data\nLast week we used the following functions:\n\nread_csv() and data() to read and load data in R\nlogical operators like &, |, %in% ==, !=, &gt;,&gt;=,&lt;,&lt;= to make comparisons\nthe pipe command %&gt;% to “pipe” the output of one function into another\nfilter() to pick observations (rows) by their values\narrange() to reorder rows\nselect() to pick variables by their names\nmutate() and case_when() command to create new variables in our data set\nsummarise() to collapse many values into a single value (like a mean or median)\ngroup_by() to apply functions like mutate() and summarise() on a group-by-group basis"
  },
  {
    "objectID": "slides/02-slides.html#common-functions-for-transforming-data",
    "href": "slides/02-slides.html#common-functions-for-transforming-data",
    "title": "POLS 1600",
    "section": "Common functions for transforming data",
    "text": "Common functions for transforming data\nAll of these “verb” functions from the dplyr package (e.g. filter(),mutate()) follow a similar format:\n\nTheir first argument is a data frame\nThe subsequent arguments tell R what to do with the data frame, using the variable names (without quotes)\nThe output is a new data frame\n\nMore"
  },
  {
    "objectID": "slides/02-slides.html#you-trying-to-get-the",
    "href": "slides/02-slides.html#you-trying-to-get-the",
    "title": "POLS 1600",
    "section": "You trying to get the %>%?",
    "text": "You trying to get the %&gt;%?"
  },
  {
    "objectID": "slides/02-slides.html#the-pipe-command",
    "href": "slides/02-slides.html#the-pipe-command",
    "title": "POLS 1600",
    "section": "The pipe command %>%",
    "text": "The pipe command %&gt;%\n\nThe pipe command is way of “chaining” lines of code together, piping the results of one tidyverse function into the next function.\nThe pipe command works because these functions always expect a data frame as their first argument, and always produce a data frame as their output."
  },
  {
    "objectID": "slides/02-slides.html#the-pipe-command-1",
    "href": "slides/02-slides.html#the-pipe-command-1",
    "title": "POLS 1600",
    "section": "The pipe command %>%",
    "text": "The pipe command %&gt;%\n\nsummarise(\n  data = df,\n  mean = mean(var1, na.rm = T),\n  median = median(var1, na.rm = T)\n )\n# Rewrite with a pipe:\n\ndf %&gt;% \n  summarize(\n    mean = mean(var1, na.rm = T),\n    median = median(var1, na.rm = T)    \n  )"
  },
  {
    "objectID": "slides/02-slides.html#wrangling-the-covid-19-data",
    "href": "slides/02-slides.html#wrangling-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Wrangling the Covid-19 data",
    "text": "Wrangling the Covid-19 data\nTo work with the Covid-19 data we did the following:\n\nSubsetted/Filtered the data to exclude US Territories\nCreated new variables from existing variables in the data to use in our final analysis"
  },
  {
    "objectID": "slides/02-slides.html#wrangling-the-covid-19-data-1",
    "href": "slides/02-slides.html#wrangling-the-covid-19-data-1",
    "title": "POLS 1600",
    "section": "Wrangling the Covid-19 data",
    "text": "Wrangling the Covid-19 data\nSpecifically, we did the following:\n\nCreated an object called territories that is a vector containing the names of U.S. territories\nCreated a new dataframe, called covid_us, by filtering out observations from the U.S. territories\nCreated a state variable that is a copy of the administrative_area_level_2\nCreated a variable called new_cases from the confirmed. Create a variable called new_cases_pc that is the number of new Covid-19 cases per 100,000 citizens\nCreated a variable called face_masks from the facial_coverings variable.\nCalculated the average number of new cases, by different levels of face_masks\n\n\nLet’s take some time to make sure we understand everything that was happening."
  },
  {
    "objectID": "slides/02-slides.html#created-an-object-called-territories",
    "href": "slides/02-slides.html#created-an-object-called-territories",
    "title": "POLS 1600",
    "section": "Created an object called territories",
    "text": "Created an object called territories\n\n# - 1. Create territories object\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n\nThe object territories now exists in our environment."
  },
  {
    "objectID": "slides/02-slides.html#created-a-new-dataframe-called-covid_us",
    "href": "slides/02-slides.html#created-a-new-dataframe-called-covid_us",
    "title": "POLS 1600",
    "section": "Created a new dataframe, called covid_us",
    "text": "Created a new dataframe, called covid_us\n\n\nTaskCode\n\n\n\nUse the filter() command to select only the rows where the administrative_area_level_2 is not (!) in (%in%) the territories object\n\n\n\n\n# - 2. Create covid_us data frame\n# How many rows and columns in covid\ndim(covid)\n\n[1] 58809    47\n\n# Filter out obs from US territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\n# covid_us should have fewer rows than covid\ndim(covid_us)\n\n[1] 53678    47"
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-state",
    "href": "slides/02-slides.html#created-a-variable-called-state",
    "title": "POLS 1600",
    "section": "Created a variable called state",
    "text": "Created a variable called state\n\n\nTaskCode\n\n\nCopy administrative_area_level_2 into a new variable called state\n\n\n\n\n\n\nNote\n\n\nNote that we have to save the output of mutate back into covid_us for our state to exist as new column in covid_us\n\n\n\n\n\n\ndim(covid_us)\n\n[1] 53678    47\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2\n  ) -&gt; covid_us\ndim(covid_us)\n\n[1] 53678    48\n\nnames(covid_us)[48]\n\n[1] \"state\""
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-state-1",
    "href": "slides/02-slides.html#created-a-variable-called-state-1",
    "title": "POLS 1600",
    "section": "Created a variable called state",
    "text": "Created a variable called state\nNow there’s a new column in covid_us called state, that we can access by calling covid_us$state\n\ncovid_us$state[1:5] # Just show first 5 observations\n\n[1] \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\"\n\n\n\nWe could have done the same thing in “Base” R\n\ncovid_us$state &lt;- covid_us$administrative_area_level_2\n\n\n\nWhy didn’t we?\n\nConsistent preference for tidyverse &gt; base R\nSaves time when recoding lots of variables\nmutate() plays nicely with functions like group_by()"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases from the confirmed variable",
    "text": "Create a variable called new_cases from the confirmed variable\nThe confirmed variable contains a running total of confirmed cases in a given state on a given day.\nVizualing data helps us understand how we might need to transform our data"
  },
  {
    "objectID": "slides/02-slides.html#visualize-confirmed-variable-for-rhode-island",
    "href": "slides/02-slides.html#visualize-confirmed-variable-for-rhode-island",
    "title": "POLS 1600",
    "section": "Visualize confirmed variable for Rhode Island",
    "text": "Visualize confirmed variable for Rhode Island\n\nCodePlotData\n\n\n\noptions(scipen = 999) # No scientific notation\ncovid_us %&gt;% \n  filter(state == \"Rhode Island\") %&gt;% \n  ggplot(aes(\n    x = date,\n    y = confirmed\n  ))+\n  geom_point()+\n  theme_bw() +\n  labs(title = \"Total Covid-19 cases in Rhode Island\",\n       y = \"Total Cases\",\n       x = \"Date\") -&gt; fig_ri_covid"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable-1",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable-1",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases from the confirmed variable",
    "text": "Create a variable called new_cases from the confirmed variable\n\nTaskCodeData\n\n\nTake the difference between a given day’s value of confirmed and yesterday’s value of confirmed to create a measure of new_cases on a given date for each state\n\n\n\n\n\n\nNote\n\n\n\nUse lag() to shift values in a column down one row in the data\nUse group_by() to respect the state-date structure of the data\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed)\n  ) -&gt; covid_us"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases_pc",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases_pc",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases_pc",
    "text": "Create a variable called new_cases_pc\n\n\nTaskCode - WranglingCode - CheckingData\n\n\n\nScale new_cases by population to create a per capita measure (new_cases_pc)\n\n\n\n\n\n\n\nNote\n\n\nWe can create multiple variables in a single mutate() by separating lines of code with a ,\n\n\n\n\n\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000\n    ) -&gt;covid_us\n\n\n\n\n# Check recoding\ncovid_us %&gt;% \n  # Look at two states\n  filter(state == \"Rhode Island\" | state == \"New York\") %&gt;% \n  # In a small date range\n  filter(date &gt; \"2021-01-01\" & date &lt; \"2021-01-05\") %&gt;% \n  # Select only the columns we want\n  select(state, date, new_cases, new_cases_pc) -&gt; hlo_df\n# save to object hlo_df\n\n\n\n\nhlo_df\n\n# A tibble: 6 × 4\n# Groups:   state [2]\n  state        date       new_cases new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;         &lt;int&gt;        &lt;dbl&gt;\n1 Rhode Island 2021-01-02         0          0  \n2 Rhode Island 2021-01-03         0          0  \n3 Rhode Island 2021-01-04      4759        449. \n4 New York     2021-01-02     15849         81.5\n5 New York     2021-01-03     12232         62.9\n6 New York     2021-01-04     11242         57.8"
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-face_masks",
    "href": "slides/02-slides.html#created-a-variable-called-face_masks",
    "title": "POLS 1600",
    "section": "Created a variable called face_masks",
    "text": "Created a variable called face_masks\n\n\nTaskHLOCodeCheck\n\n\nCreate a variable called face_masks from the facial_coverings that describes the face mask policy experienced by most people in a given state on a given date.\n\n\n\n\n\n\nNote\n\n\n\nUse case_when() inside of mutate() to create a variable that takes certain values when certain logical statements are true\nSeting the levels = c(value1, value2, etc.) argument in factor() lets us control the ordering of categorical/character data.\n\n\n\n\n\n\nRecall, that the facial_coverings variable took on range of substantive values from 0 to 4, but empirically could take both positve and negative values\n\ntable(covid_us$facial_coverings)\n\n\n   -4    -3    -2    -1     0     1     2     3     4 \n  410  5897  7362   275  3893  8604 17424  9191   622 \n\n\n\n\n\ncovid_us %&gt;%\nmutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n\n\n\ncovid_us%&gt;%\n  filter(state == \"Illinois\", date &gt; \"2020-9-28\") %&gt;%\n  select(state, date, facial_coverings, face_masks) %&gt;% \n  slice(1:5)\n\n# A tibble: 5 × 4\n# Groups:   state [1]\n  state    date       facial_coverings face_masks        \n  &lt;chr&gt;    &lt;date&gt;                &lt;int&gt; &lt;fct&gt;             \n1 Illinois 2020-09-29                2 Some requirements \n2 Illinois 2020-09-30                2 Some requirements \n3 Illinois 2020-10-01               -4 Required all times\n4 Illinois 2020-10-02               -4 Required all times\n5 Illinois 2020-10-03               -4 Required all times"
  },
  {
    "objectID": "slides/02-slides.html#addtional-recoding",
    "href": "slides/02-slides.html#addtional-recoding",
    "title": "POLS 1600",
    "section": " Addtional recoding",
    "text": "Addtional recoding\nIn last week’s lab, we also added the following\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(\n      year, \n      str_pad(month, width = 2, pad=0), \n      sep = \"-\"\n      ),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/02-slides.html#working-with-dates",
    "href": "slides/02-slides.html#working-with-dates",
    "title": "POLS 1600",
    "section": " Working with dates",
    "text": "Working with dates\nR treat’s dates differently\n\ncovid_us$date[1:3]\n\n[1] \"2020-01-01\" \"2020-01-02\" \"2020-01-03\"\n\nclass(covid_us$date)\n\n[1] \"Date\"\n\n\nIf R knows a variable is a date, we can extract components of that date, using functions from the lubridate package\n\nyear(covid_us$date[1:3])\n\n[1] 2020 2020 2020\n\nmonth(covid_us$date[1:3])\n\n[1] 1 1 1"
  },
  {
    "objectID": "slides/02-slides.html#the-str_pad-and-paste-function",
    "href": "slides/02-slides.html#the-str_pad-and-paste-function",
    "title": "POLS 1600",
    "section": " The str_pad() and paste() function",
    "text": "The str_pad() and paste() function\n\nThe str_pad() function lets us ‘pad’ strings so that they’re all the same width\n\n\nmonth(covid_us$date[1:3])\n\n[1] 1 1 1\n\nstr_pad(month(covid_us$date[1:3]), width=2, pad = 0)\n\n[1] \"01\" \"01\" \"01\"\n\n\n\nThe paste function lets us paste objects together.\n\n\npaste(year(covid_us$date[1:3]),\n      str_pad(month(covid_us$date[1:3]), width=2, pad = 0),\n      sep = \"-\"\n      )\n\n[1] \"2020-01\" \"2020-01\" \"2020-01\""
  },
  {
    "objectID": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy",
    "href": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy",
    "title": "POLS 1600",
    "section": "Summarizing the averge number of new_cases by face_mask policy",
    "text": "Summarizing the averge number of new_cases by face_mask policy\n\nTaskCodeResults\n\n\nCalculate the mean (average) number of new_cases of Covid-19 when each type of face_mask policy was in effect\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() command will do each calculation inside of summarise() for each level of the grouping variable\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks)) %&gt;%\n  group_by(face_masks) %&gt;%\n  summarize(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  ) -&gt; face_mask_summary\n\n\n\n\nface_mask_summary\n\n# A tibble: 5 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2"
  },
  {
    "objectID": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy-by-month",
    "href": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy-by-month",
    "title": "POLS 1600",
    "section": "Summarizing the averge number of new_cases by face_mask policy by month",
    "text": "Summarizing the averge number of new_cases by face_mask policy by month\n\nTaskCodeResults\n\n\nCalculate the mean (average) number of new_cases of Covid-19 when each type of face_mask policy was in effect for each year_month in our dataset\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() command can group on multiple variables\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  group_by(face_masks, year_month) %&gt;%\n  summarize(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  ) -&gt; cases_by_month_and_policy\n\n\n\n\ncases_by_month_and_policy\n\n# A tibble: 102 × 3\n# Groups:   face_masks [5]\n   face_masks year_month new_cases_pc\n   &lt;fct&gt;      &lt;chr&gt;             &lt;dbl&gt;\n 1 No policy  2020-01        0.000463\n 2 No policy  2020-02        0.00188 \n 3 No policy  2020-03        1.70    \n 4 No policy  2020-04        6.50    \n 5 No policy  2022-04       19.8     \n 6 No policy  2022-05       20.4     \n 7 No policy  2022-06       37.6     \n 8 No policy  2022-07       36.2     \n 9 No policy  2022-08       35.7     \n10 No policy  2022-09       19.0     \n# ℹ 92 more rows\n\n# In base R:\nmean(\n  covid_us$new_cases_pc[\n    covid_us$face_masks == \"No policy\" &\n      covid_us$year_month == \"2020-01\"], na.rm = T)\n\n[1] 0.0004626161"
  },
  {
    "objectID": "slides/02-slides.html#concept-check",
    "href": "slides/02-slides.html#concept-check",
    "title": "POLS 1600",
    "section": " Concept check",
    "text": "Concept check\nSuppose you want to do the following, what function or functions would you use:\n\nRead data into R\nLook at the data to get a high level overview of its structure\nSubset or filter the data to include just observations with certain values\nSelect specific columns from data\nAdd new columns to the data\nSummarize multiple values by collapsing them into a single value\nDoing some function group-by-group?"
  },
  {
    "objectID": "slides/02-slides.html#concept-check-1",
    "href": "slides/02-slides.html#concept-check-1",
    "title": "POLS 1600",
    "section": "Concept check",
    "text": "Concept check\nSuppose you want to do the following, what function or functions would you use:\n\nRead data into R\n\nread_xxx() (tidy), read.xxx() (base)\n\nLook at the data to get a high level overview of its structure\n\nhead(), tail(), glimpse(), table(), summary(), View()\n\nSubset the data to include just obersvations with certain values\n\ndata %&gt;% filter(x &gt; 0), data[data$x &gt; 0], subset(data, x &gt; 0)\n\nSelect specific columns from data\n\ndata$variable, data %&gt;% select(variable1, variable2), data[,c(\"x1\",\"x2\")]\n\nAdd new columns to the data\n\ndata %&gt;% mutate(x = y/10) data$x &lt;- data$y/10\n\nSummarize multiple values by collapsing them into a single value\n\ndata %&gt;% summarise(x_mn = mean(x, na.rm=T))\n\nDoing some function group-by-group?\n\ndata %&gt;% group_by(g) %&gt;% summarise(x_mn = mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/02-slides.html#concept-check-2",
    "href": "slides/02-slides.html#concept-check-2",
    "title": "POLS 1600",
    "section": "Concept check",
    "text": "Concept check\nShould you know exactly how to do all of this?\n\nNO! Of course not. For Pete’s sake, Paul, It’s only the second week\n\n\nWill you learn how to do much of this?\n\n\nMaybe, but I’m feeling pretty overwhelmed…\n\n\nHow will you learn how do these things?\n\n\nWith lots of practice, patience, and repetition motivated by a sense that these skills will help me learn about things I care about"
  },
  {
    "objectID": "slides/02-slides.html#advice-on-learning-how-to-code",
    "href": "slides/02-slides.html#advice-on-learning-how-to-code",
    "title": "POLS 1600",
    "section": "Advice on learning how to code",
    "text": "Advice on learning how to code\n\nIt takes lots of practice and lots of errors\n\nBreak long blocks of code into individual steps to see what’s happening\n\nCreate code chunks and FAFO\n\nJust clean up when you’re done…\n\nOnly dumb question is one you don’t ask\nGoogle, Stack Exchange are your friends\nTry writing out in comments what you want to do in code\nLearn to recognize patterns in the questions/tasks I give you:\n\nCopy and paste code I give\nChange one thing\nFix the error\nAdapt code from class to do a similar thing\n\nLearning to code is much less painful when you have a reason to do it\n\nLet me know what interests you"
  },
  {
    "objectID": "slides/02-slides.html#descriptive-statistics-1",
    "href": "slides/02-slides.html#descriptive-statistics-1",
    "title": "POLS 1600",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWhen social scientists talk about descriptive inference, we’re trying to summarize our data and make claims about what’s typical of our data\n\nWhat’s a typical value\n\nMeasures of central tendency\nmean, median, mode\n\nHow do our data vary around typical values\n\nMeasures of dispersion\nvariance, standard deviation, range, percentile ranges\n\nHow does variation in one variable relate to variation in another\n\nMeasures of association\ncovariance, correlation"
  },
  {
    "objectID": "slides/02-slides.html#using-r-to-summarize-data",
    "href": "slides/02-slides.html#using-r-to-summarize-data",
    "title": "POLS 1600",
    "section": "Using R to Summarize Data",
    "text": "Using R to Summarize Data\nHere are some common ways of summarizing data and how to calculate them with R\n\n\n\nDescription\nUsage\n\n\n\n\nsum\nsum(x)\n\n\nminimum\nmin(x)\n\n\nmaximum\nmax(x)\n\n\nrange\nrange(x)\n\n\nmean\nmean(x)\n\n\nmedian\nmedian(x)\n\n\npercentile\nquantile(x)\n\n\nvariance\nvar(x)\n\n\nstandard deviation\nsd(x)\n\n\nrank\nrank(x)\n\n\n\n\n\nAll of these functions have an argument called na.rm=F. If your data have missing values, you’ll need to set na.rm=F (e.g. mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of typical values\n\nMeans (mean()) all the time\nMedians (median()) useful for describing distributions of variables particularly those with extreme values\nMode useful for characterizing categorical data"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-1",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-1",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of typical variation\n\nvar() important for quantifying uncertainty, but rarely will you be calculating this directly\nsd() a good summary of a typical change in the data.\nrange(), min(), max() useful for exploring data, detecting outliers and potential values that need to be recoded"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-2",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-2",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of association\n\nCovariance (var()) central to describing relationships but generally not something you’ll calculate or interpret directly\nCorrelation (cor()) useful for describing [bivariate] relationships (positive or negative relationships)."
  },
  {
    "objectID": "slides/02-slides.html#what-you-dont-really-need-to-know-for-pols-1600-smaller",
    "href": "slides/02-slides.html#what-you-dont-really-need-to-know-for-pols-1600-smaller",
    "title": "POLS 1600",
    "section": "What you don’t really need to know for POLS 1600 {smaller}",
    "text": "What you don’t really need to know for POLS 1600 {smaller}\nWe won’t spend much time on the formal definitions, math, and proofs\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\n\\[\nM_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx=\\int_{x_i}^\\infty f_x(X)dx=1/2\n\\]\n\nUseful eventually. Not necessary right now."
  },
  {
    "objectID": "slides/02-slides.html#data-visualizaiton",
    "href": "slides/02-slides.html#data-visualizaiton",
    "title": "POLS 1600",
    "section": "Data visualizaiton",
    "text": "Data visualizaiton\nData visualization is an incredibly valuable tool that helps us to\n\nExplore data, uncovering new relationships, as well as potential problems\nCommunicate our results clearly and precisely\n\nTake a look at how the BBC uses R to produce its graphics"
  },
  {
    "objectID": "slides/02-slides.html#data-visualization",
    "href": "slides/02-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data visualization",
    "text": "Data visualization\nToday, we will:\n\nIntroduce the grammar of graphics\nLearn how to apply this grammar with ggplot()\nIntroduce basic plots to describe\n\nUnivariate distributions\nBivariate relations"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics",
    "href": "slides/02-slides.html#the-grammar-of-graphics",
    "title": "POLS 1600",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nInspired by Wilkinson (2005)\n\nA statistical graphic is a mapping of data variables to aes thetic attributes of geom etric objects.\n\nAt a minimum, a graphic contains three core components:\n\ndata: the dataset containing the variables of interest.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars.\n\nIsmay and Kim (2022)"
  },
  {
    "objectID": "slides/02-slides.html#seven-layers-of-graphics",
    "href": "slides/02-slides.html#seven-layers-of-graphics",
    "title": "POLS 1600",
    "section": "Seven Layers of Graphics",
    "text": "Seven Layers of Graphics\nKesari (2018)"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics-in-r",
    "href": "slides/02-slides.html#the-grammar-of-graphics-in-r",
    "title": "POLS 1600",
    "section": "The grammar of graphics in R",
    "text": "The grammar of graphics in R\nIn R, we’ll implement this grammar of graphics using the ggplot package\n\nLet’s take a look at your feedback to last week’s survey and see how we can visualize some of the in formation you provided"
  },
  {
    "objectID": "slides/02-slides.html#what-we-liked",
    "href": "slides/02-slides.html#what-we-liked",
    "title": "POLS 1600",
    "section": "What we liked",
    "text": "What we liked"
  },
  {
    "objectID": "slides/02-slides.html#what-we-disliked",
    "href": "slides/02-slides.html#what-we-disliked",
    "title": "POLS 1600",
    "section": "What we disliked",
    "text": "What we disliked"
  },
  {
    "objectID": "slides/02-slides.html#building-that-figure",
    "href": "slides/02-slides.html#building-that-figure",
    "title": "POLS 1600",
    "section": "Building that figure",
    "text": "Building that figure\n\nLook at the raw data\nRecode the raw data\nMake a basic plot, telling R the data, aesthetics, geometries, and statistics I want it to plot\nTinker with the data and plot’s scales, coordinates, labels and theme to make the figure look better"
  },
  {
    "objectID": "slides/02-slides.html#look-at-the-raw-data",
    "href": "slides/02-slides.html#look-at-the-raw-data",
    "title": "POLS 1600",
    "section": "1. Look at the raw data",
    "text": "1. Look at the raw data\n\ndf$trip\n\n&lt;labelled&lt;double&gt;[12]&gt;: You're on a road trip with friends. Who controls the music?\n [1] NA  3  1  2  3  2  2  2  1  2  2 NA\n\nLabels:\n value\n     1\n     2\n     3\n                                                                                                                               label\n                                                                                                                    The driver, duh.\n                                                                                                           The front seat, of course\n That jerk in the back who you don't even know but seems to have really strong feelings about Billy Joel's \"Only the good die young\""
  },
  {
    "objectID": "slides/02-slides.html#recode-the-raw-data",
    "href": "slides/02-slides.html#recode-the-raw-data",
    "title": "POLS 1600",
    "section": "2. Recode the raw data",
    "text": "2. Recode the raw data\n\ndf %&gt;%\n  mutate(\n    Playist = forcats::as_factor(trip)\n )%&gt;%\n  select(Playist)\n\n# A tibble: 12 × 1\n   Playist                                                                      \n   &lt;fct&gt;                                                                        \n 1  &lt;NA&gt;                                                                        \n 2 \"That jerk in the back who you don't even know but seems to have really stro…\n 3 \"The driver, duh.\"                                                           \n 4 \"The front seat, of course\"                                                  \n 5 \"That jerk in the back who you don't even know but seems to have really stro…\n 6 \"The front seat, of course\"                                                  \n 7 \"The front seat, of course\"                                                  \n 8 \"The front seat, of course\"                                                  \n 9 \"The driver, duh.\"                                                           \n10 \"The front seat, of course\"                                                  \n11 \"The front seat, of course\"                                                  \n12  &lt;NA&gt;"
  },
  {
    "objectID": "slides/02-slides.html#make-a-basic-plot",
    "href": "slides/02-slides.html#make-a-basic-plot",
    "title": "POLS 1600",
    "section": "3. Make a basic plot",
    "text": "3. Make a basic plot\n\nCodePlot\n\n\n\n#|\ndf %&gt;% #&lt;&lt; Raw data\n  mutate(\n    Playlist =forcats::as_factor(trip)\n  ) %&gt;% # Transformed data\n  ggplot(aes(x = Playlist, # Aesthetics\n             fill = Playlist))+\n  geom_bar( # Geometry\n    stat = \"count\" # Statistic\n    ) -&gt; fig_roadtrip\n\n\n\n\nfig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-data",
    "href": "slides/02-slides.html#tinker-with-data",
    "title": "POLS 1600",
    "section": "4.1 Tinker with data",
    "text": "4.1 Tinker with data\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\") -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-fill-aesthetic",
    "href": "slides/02-slides.html#tinker-with-fill-aesthetic",
    "title": "POLS 1600",
    "section": "4.2 Tinker with fill aesthetic",
    "text": "4.2 Tinker with fill aesthetic\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-coordinates",
    "href": "slides/02-slides.html#tinker-with-coordinates",
    "title": "POLS 1600",
    "section": "4.3 Tinker with coordinates",
    "text": "4.3 Tinker with coordinates\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer() +\n  coord_flip() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-labels",
    "href": "slides/02-slides.html#tinker-with-labels",
    "title": "POLS 1600",
    "section": "4.4 Tinker with labels",
    "text": "4.4 Tinker with labels\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")-&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-theme",
    "href": "slides/02-slides.html#tinker-with-theme",
    "title": "POLS 1600",
    "section": "4.4 Tinker with theme",
    "text": "4.4 Tinker with theme\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n   scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")+\n  theme_bw() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#the-final-code",
    "href": "slides/02-slides.html#the-final-code",
    "title": "POLS 1600",
    "section": "The final code",
    "text": "The final code\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n   scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")+\n  theme_bw() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#describing-distributions-and-associations",
    "href": "slides/02-slides.html#describing-distributions-and-associations",
    "title": "POLS 1600",
    "section": "Describing Distributions and Associations",
    "text": "Describing Distributions and Associations\n\nIn the remaining slides, we’ see how to visualize some distributions and associations in the Covid data using:\n\nbarplots\nhistograms\ndensity plots\nboxplots\nline plots\nscatter plots"
  },
  {
    "objectID": "slides/02-slides.html#general-advice-for-making-figures",
    "href": "slides/02-slides.html#general-advice-for-making-figures",
    "title": "POLS 1600",
    "section": "General advice for making figures",
    "text": "General advice for making figures\n\nThink through conceptually how you want to figure to look\n\nDraw it out by hand\n\nMake a basic plot and iterate\nUse summarize() and other data wrangling skills to transform data for plotting\nUse factor() and related functions to control order of labels on axis\nUse google to figure out arcane options of ggplot\nDon’t let the perfect be the enemy of the good"
  },
  {
    "objectID": "slides/02-slides.html#barplots",
    "href": "slides/02-slides.html#barplots",
    "title": "POLS 1600",
    "section": "Barplots",
    "text": "Barplots\n\nQuestionBasic CodeBetter CodeFigure\n\n\nWhat was the most common face mask policy in the data?\n\n\n\ncovid_us %&gt;% \n  ggplot(aes(x=face_masks))+\n  geom_bar(stat = \"count\")\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  ungroup() %&gt;% \n  mutate(\n    face_masks = forcats::fct_infreq(face_masks)\n  ) %&gt;% \n  ggplot(aes(x=face_masks,\n             fill = face_masks))+\n  geom_bar()+\n  geom_text(stat='count', aes(label=..count..), \n            hjust=.5,vjust=-.5)+\n  guides(fill = \"none\")+\n  theme_bw()+\n  labs(\n    x = \"Face Mask Policy \",\n    title = \"\"\n  ) -&gt; fig_barplot"
  },
  {
    "objectID": "slides/02-slides.html#histogram",
    "href": "slides/02-slides.html#histogram",
    "title": "POLS 1600",
    "section": "Histogram",
    "text": "Histogram\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat does the distribution of new Covid-19 cases look like in June 2021\n\n\n\ncovid_us %&gt;% \n  filter(year_month == \"2021-06\") %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() -&gt; fig_hist1\n\n\n\n\nfig_hist1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(year_month == \"2021-06\") %&gt;% \n  filter(new_cases &gt; 0) %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() +\n  labs(\n    title = \"Exclude Negative Values\"\n  ) -&gt; fig_hist2a\n\ncovid_us %&gt;%\n  filter(year_month == \"2021-06\") %&gt;% \n  filter(new_cases &gt; 0) %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() +\n  scale_x_log10()+\n  labs(\n    title = \"Exclude Negative Values & Use log scale\"\n  ) -&gt; fig_hist2b\n\nfig_hist2 &lt;- ggarrange(fig_hist2a, fig_hist2b)"
  },
  {
    "objectID": "slides/02-slides.html#density-plots",
    "href": "slides/02-slides.html#density-plots",
    "title": "POLS 1600",
    "section": "Density Plots",
    "text": "Density Plots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat does the distribution of Covid-19 deaths look like?\n\n\n\ncovid_us %&gt;% \n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = deaths - lag(deaths)\n  ) %&gt;% \n  filter(new_deaths &gt; 0) %&gt;% \n  ggplot(aes(x=new_deaths_pc))+\n  geom_density() -&gt; fig_density1\n\n\n\n\nfig_density1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;% \n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = deaths - lag(deaths),\n    year_f = factor(year)\n  ) %&gt;% \n  filter(new_deaths &gt; 0) %&gt;% \n  ggplot(aes(x=new_deaths_pc,\n             col = year_f))+\n  geom_density() +\n  geom_rug() +\n  scale_x_log10() +\n    facet_wrap(~month)+\n  theme(legend.position = \"bottom\")-&gt; \n  fig_density2"
  },
  {
    "objectID": "slides/02-slides.html#box-plots",
    "href": "slides/02-slides.html#box-plots",
    "title": "POLS 1600",
    "section": "Box plots",
    "text": "Box plots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nHow did the distribution of Covid-19 cases vary by face mask policy?\n\n\n\ncovid_us %&gt;%\n  filter(new_cases_pc &gt; 0) %&gt;% \n  ggplot(aes(x= face_masks, y=new_cases_pc))+\n  scale_y_log10()+\n  geom_boxplot() -&gt; fig_boxplot1\n\n\n\n\nfig_boxplot1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  mutate(\n    Month = lubridate::month(date, label = T)\n  ) %&gt;% \n  filter(new_cases_pc &gt; 0) %&gt;% \n  filter(year == 2020) %&gt;% \n ggplot(aes(x= face_masks, \n            y=new_cases_pc,\n            col = face_masks))+\n  scale_y_log10()+\n  coord_flip() +\n  geom_boxplot()  +\n    facet_wrap(~Month) +\n  theme(\n    legend.position = \"bottom\"\n  )-&gt; fig_boxplot2"
  },
  {
    "objectID": "slides/02-slides.html#line-graphs",
    "href": "slides/02-slides.html#line-graphs",
    "title": "POLS 1600",
    "section": "Line graphs",
    "text": "Line graphs\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nHow did vaccination rates vary by state?\n\n\n\ncovid_us %&gt;%\n  ggplot(\n    aes(x= date,\n        y=percent_vaccinated,\n        group = state\n        ))+\n  geom_line() -&gt; fig_line1\n\n\n\n\nfig_line1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Label = case_when(\n      date == max(date) & percent_vaccinated == max(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      date == max(date) & percent_vaccinated == median(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      date == max(date) & percent_vaccinated == min(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      TRUE ~ NA_character_\n    ),\n    line_alpha = case_when(\n      state %in% c(\"District of Columbia\", \"Nebraska\", \"Wyoming\") ~ 1,\n      T ~ .3\n    ),\n    line_col = case_when(\n      state %in% c(\"District of Columbia\", \"Nebraska\", \"Wyoming\") ~ \"black\",\n      T ~ \"grey\"\n    )\n  ) %&gt;%\n  ggplot(\n    aes(x= date,\n        y=percent_vaccinated,\n        group = state\n        ))+\n  geom_line(\n    aes(alpha = line_alpha,\n        col =line_col)) +\n  geom_text_repel(aes(label = Label),\n                  direction = \"x\",\n                  nudge_y = 2) +\n  guides(\n    alpha = \"none\",\n    col = \"none\"\n  )+\n  xlim(ym(\"2021-01\"), ym(\"2023-01\")) +\n  labs(\n    y = \"Percent Vacinated\",\n    x = \"Date\"\n  ) +\n  theme_bw()-&gt; fig_line2"
  },
  {
    "objectID": "slides/02-slides.html#scatterplots",
    "href": "slides/02-slides.html#scatterplots",
    "title": "POLS 1600",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat’s the relationship between vaccination rates and new cases of Covid-19?\n\n\n\ncovid_us %&gt;%\n  ggplot(\n    aes(x= percent_vaccinated,\n        y=new_cases_pc,\n        ))+\n  geom_point() -&gt; fig_scatter1\n\n\n\n\nfig_scatter1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(year &gt; 2020) %&gt;%\n  filter(month == 6) %&gt;%\n  filter(new_cases_pc &gt; 0) %&gt;%\n  ggplot(\n    aes(x= percent_vaccinated,\n        y=new_cases_pc,\n        ))+\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  facet_wrap(~year_month,ncol =1,\n             scales = \"free_y\")-&gt; fig_scatter2"
  },
  {
    "objectID": "slides/02-slides.html#summary-1",
    "href": "slides/02-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nThe grammar of graphics provides a language for translating data into figures\nAt a minimum figures with ggplot() require three things:\n\ndata\naesthetic mappings\ngeometries\n\nTo produce a figure:\n\nthink about what the end product will look like\ntransform your data\nmap variables onto corresponding aesthetics\ntell R what to do with these aesthetic mappings\nRevise and iterate!\n\nLearning to code is hard, but the more errors you make now, the easier your life will be in the future\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/07-slides.html#class-plan",
    "href": "slides/07-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements (2-3 min)\nSetup (2-3 min)\nFeedback (15 min)\nTopics:\n\nWhat does it mean to control for X\nHow to make predictions with regression\nEvaluating model fit\nDifference-in-Differences\nSet up for Lab 7"
  },
  {
    "objectID": "slides/07-slides.html#goals",
    "href": "slides/07-slides.html#goals",
    "title": "POLS 1600",
    "section": "Goals",
    "text": "Goals\n\nRegression models partition variation in an outcome into variation explained by the model and not explained by the model\nIndividual regression coefficients reflect the variation explained by that predictor, and only that predictor\nPredicted values for regression models aid in substantive interpretation\nMeasures of model fit like \\(R^2\\) can be useful for comparing different regression models\nDifference-in-differences designs combine pre-post and treatment-control comparisons to make stronger causal claims."
  },
  {
    "objectID": "slides/07-slides.html#annoucements",
    "href": "slides/07-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nAssignment 1: Research Questions due Tuesday, March 12\n\nFeedback by class on Thursday\n\nAssignment 2 Data: due Friday March 22"
  },
  {
    "objectID": "slides/07-slides.html#setup-packages-for-today",
    "href": "slides/07-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/07-slides.html#feedback",
    "href": "slides/07-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/07-slides.html#what-did-we-like",
    "href": "slides/07-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/07-slides.html#what-did-we-dislike",
    "href": "slides/07-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/07-slides.html#what-were-good-at",
    "href": "slides/07-slides.html#what-were-good-at",
    "title": "POLS 1600",
    "section": "What we’re good at",
    "text": "What we’re good at"
  },
  {
    "objectID": "slides/07-slides.html#what-were-working-on",
    "href": "slides/07-slides.html#what-were-working-on",
    "title": "POLS 1600",
    "section": "What we’re working on",
    "text": "What we’re working on"
  },
  {
    "objectID": "slides/07-slides.html#how-are-we-doing",
    "href": "slides/07-slides.html#how-are-we-doing",
    "title": "POLS 1600",
    "section": "How are we doing?",
    "text": "How are we doing?"
  },
  {
    "objectID": "slides/07-slides.html#dont-trust-the-polls",
    "href": "slides/07-slides.html#dont-trust-the-polls",
    "title": "POLS 1600",
    "section": "Don’t trust the polls",
    "text": "Don’t trust the polls"
  },
  {
    "objectID": "slides/07-slides.html#what-should-we-do-going-forward",
    "href": "slides/07-slides.html#what-should-we-do-going-forward",
    "title": "POLS 1600",
    "section": "What should we do going forward?",
    "text": "What should we do going forward?\n\nFrom me:\n\nShorter labs\nShorter slides\nMore coding, less copy-pasting\n\nFrom you?"
  },
  {
    "objectID": "slides/07-slides.html#regression-models-partition-variance",
    "href": "slides/07-slides.html#regression-models-partition-variance",
    "title": "POLS 1600",
    "section": "Regression models partition variance",
    "text": "Regression models partition variance\nRegression models partition variance, separating the variation in the outcome into variation explained by the predictors in our model and the remaining variation not explained by these predictors\n\\[\\begin{aligned}\n\\textrm{Total Variance} &= \\textrm{Variance Explained by Model} + \\textrm{Unexplained Variance} \\\\\n\\textrm{Observed} &= \\textrm{Predicted Value} + \\textrm{Error}\\\\\n\\textrm{Y} &=  E[Y|X] + \\epsilon\\\\\n\\textrm{Y} &=  X\\hat{\\beta} + \\hat{\\epsilon}\\\\\n\\textrm{Y} &= \\hat{Y} + \\hat{\\epsilon}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/07-slides.html#section-1",
    "href": "slides/07-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Coefficients describe the unique variance in Y explained by X (and only X)\n\nTask\n\n\nThe coefficients in a regression model describe the variation in the outcome explained by that predictor, and only that predictor.\nLet’s fit three models from last week’s lab and look at how the coefficients change from model to model\n\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\nm1 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)\nm2 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, covid_lab)\nm3 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)\n\n\n\n\nhtmlreg(list(m1, m2, m3)) %&gt;% HTML() %&gt;% browsable()\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n0.07\n\n\n \n(0.05)\n(0.05)\n(0.07)\n\n\nmed_age_std\n \n0.03\n-0.02\n\n\n \n \n(0.05)\n(0.05)\n\n\nmed_income_std\n \n \n-0.22**\n\n\n \n \n \n(0.07)\n\n\nR2\n0.31\n0.31\n0.44\n\n\nAdj. R2\n0.29\n0.28\n0.40\n\n\nNum. obs.\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#why-do-coefficients-change-when-we-control-for-variables",
    "href": "slides/07-slides.html#why-do-coefficients-change-when-we-control-for-variables",
    "title": "POLS 1600",
    "section": "Why do coefficients change when we control for variables?",
    "text": "Why do coefficients change when we control for variables?"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression",
    "href": "slides/07-slides.html#residualized-regression",
    "title": "POLS 1600",
    "section": "Residualized Regression",
    "text": "Residualized Regression\nResidualized regression is way of understanding what it means to control for variables in a regression.\nResidualized regression provides a way of illustrating what we mean when say the coefficients describe the unique variance in Y explained by some predictor \\(x\\) (and only \\(x\\))"
  },
  {
    "objectID": "slides/07-slides.html#whats-a-residual",
    "href": "slides/07-slides.html#whats-a-residual",
    "title": "POLS 1600",
    "section": "What’s a residual",
    "text": "What’s a residual\n\nResiduals represent the part of the outcome variable, not explained by the predictors in a model\n\nDifference between the observed \\(y\\) and the predicted \\(\\hat{y}\\)\n\n\n\\[y = \\overbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2  + \\dots \\beta_j x_j}^{\\text{Predictors}} + \\underbrace{\\epsilon}_{\\text{Residuals}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#residuals-are-uncorrelated-with-x-and-haty",
    "href": "slides/07-slides.html#residuals-are-uncorrelated-with-x-and-haty",
    "title": "POLS 1600",
    "section": "Residuals are uncorrelated with \\(X\\) and \\(\\hat{y}\\)",
    "text": "Residuals are uncorrelated with \\(X\\) and \\(\\hat{y}\\)\n\n\nResiduals are uncorrelated with (orthogonal to) the predictors \\(X\\), and predicted values \\(X\\beta\\)\n\n# Trust but verify\ncor(resid(m2),covid_lab$rep_voteshare_std) \n\n[1] 3.600777e-17\n\ncor(resid(m2),covid_lab$med_age_std)\n\n[1] 7.021125e-17\n\ncor(resid(m2),fitted(m2))\n\n[1] -1.036606e-17"
  },
  {
    "objectID": "slides/07-slides.html#section-4",
    "href": "slides/07-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Residualized Regression\n\n m2 m3\n\n\n\nFor a model like m2 we can recover the coefficient on rep_voteshare_std by:\n\nRegressing new_deaths_pc_14day on med_age_std to get the residual variation in Covid-19 deaths not explained by median age\nRegressing rep_voteshare_std on med_age_std to get the residual variation in Republican Vote Share not explained by median age\nRegressing the residuals from 1. (Deaths not explained by age) on the residuals from 2. (Vote share not explained by age) to obtain the same coefficient from m2 for rep_voteshare_std\n\nThe same principle holds for m3\n\n\n\n\n# 1. Regressing `new_deaths_pc_14da` on `med_age_std`\nm2_death_by_age &lt;- lm(new_deaths_pc_14day ~ med_age_std, covid_lab)\n# Save residuals\ncovid_lab$res_death_no_age &lt;- resid(m2_death_by_age)\n\n# 2. Regressing `rep_voteshare_std` on `med_age_std` \nm2_repvs_by_age &lt;- lm(rep_voteshare_std ~ med_age_std, covid_lab)\n# Save residuals\ncovid_lab$res_repvs_no_age &lt;- resid(m2_repvs_by_age)\n\n# 3. Residualized regression of deaths on Rep Vote Share\nm2_res &lt;- lm(res_death_no_age ~ res_repvs_no_age, covid_lab)\n\n# Mutliple regression\ncoef(m2)[2]\n\nrep_voteshare_std \n         0.230745 \n\n# Residualized regression\ncoef(m2_res)[2]\n\nres_repvs_no_age \n        0.230745 \n\n\n\n\n\n# 1. Regressing `new_deaths_pc_14da` on `med_age_std` and med_income_std\nm3_death_by_age_income &lt;- lm(new_deaths_pc_14day ~ med_age_std + med_income_std, covid_lab)\n# Save residuals\ncovid_lab$res_death_no_age_income &lt;- resid(m3_death_by_age_income)\n\n# 2. Regressing `rep_voteshare_std` on `med_age_std` and med_income_std\nm3_repvs_by_age_income &lt;- lm(rep_voteshare_std ~ med_age_std + med_income_std, covid_lab)\n# Save residuals\ncovid_lab$res_repvs_no_age_income &lt;- resid(m3_repvs_by_age_income)\n\n# 3. Residualized regression of deaths on Rep Vote Share\nm3_res &lt;- lm(res_death_no_age_income ~ res_repvs_no_age_income, covid_lab)\n\n# multiple regression coefficient\ncoef(m3)[2]\n\nrep_voteshare_std \n       0.07140446 \n\n# Same as  residualized regression coefficient\ncoef(m3_res)[2]\n\nres_repvs_no_age_income \n             0.07140446"
  },
  {
    "objectID": "slides/07-slides.html#why-did-the-coefficient-on-rep-vote-share-change-in-m3-but-not-m2",
    "href": "slides/07-slides.html#why-did-the-coefficient-on-rep-vote-share-change-in-m3-but-not-m2",
    "title": "POLS 1600",
    "section": "Why did the coefficient on Rep Vote Share change in m3 but not m2?",
    "text": "Why did the coefficient on Rep Vote Share change in m3 but not m2?"
  },
  {
    "objectID": "slides/07-slides.html#section-7",
    "href": "slides/07-slides.html#section-7",
    "title": "POLS 1600",
    "section": "",
    "text": "Statistical models\n\n\n \nDV: Death\n\n\n \nBaseline\n\n\n\n\n(Intercept)\n0.57***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#section-8",
    "href": "slides/07-slides.html#section-8",
    "title": "POLS 1600",
    "section": "",
    "text": "\\[\n\\text{Covid-19 Deaths} = \\beta_0 + \\beta_1 \\text{Rep Vote Share}\n\\]"
  },
  {
    "objectID": "slides/07-slides.html#section-9",
    "href": "slides/07-slides.html#section-9",
    "title": "POLS 1600",
    "section": "",
    "text": "Statistical models\n\n\n \nDV: Death\nDV: Vote Share\nDV: Res. Deaths\n\n\n \nBaseline\nMutliple\nDeaths\nVote Share\nDeaths\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n-0.00\n-0.00\n\n\n \n(0.05)\n(0.05)\n(0.06)\n(0.14)\n(0.05)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n \n \n \n\n\n \n(0.05)\n(0.05)\n \n \n \n\n\nmed_age_std\n \n0.03\n0.00\n-0.12\n \n\n\n \n \n(0.05)\n(0.06)\n(0.14)\n \n\n\nres_repvs_no_age\n \n \n \n \n0.23***\n\n\n \n \n \n \n \n(0.05)\n\n\nR2\n0.31\n0.31\n0.00\n0.02\n0.31\n\n\nAdj. R2\n0.29\n0.28\n-0.02\n-0.00\n0.30\n\n\nNum. obs.\n51\n51\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#section-10",
    "href": "slides/07-slides.html#section-10",
    "title": "POLS 1600",
    "section": "",
    "text": "\\[\n\\text{Deaths} = \\beta_0 + \\beta_1 \\text{Rep VS} + \\beta_2 \\text{Age}\n\\]\n\n\\(\\beta_1\\) doesn’t change because age has no relationship to deaths in these data"
  },
  {
    "objectID": "slides/07-slides.html#section-11",
    "href": "slides/07-slides.html#section-11",
    "title": "POLS 1600",
    "section": "",
    "text": "Statistical models\n\n\n \nDV: Death\nDV: Vote Share\nDV: Res. Death\n\n\n \nBaseline\nMutliple\nDeaths\nVote Share\nDeaths\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n-0.00\n0.00\n\n\n \n(0.05)\n(0.04)\n(0.04)\n(0.10)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.07\n \n \n \n\n\n \n(0.05)\n(0.07)\n \n \n \n\n\nmed_age_std\n \n-0.02\n-0.03\n-0.22*\n \n\n\n \n \n(0.05)\n(0.05)\n(0.10)\n \n\n\nmed_income_std\n \n-0.22**\n-0.27***\n-0.74***\n \n\n\n \n \n(0.07)\n(0.05)\n(0.10)\n \n\n\nres_repvs_no_age_income\n \n \n \n \n0.07\n\n\n \n \n \n \n \n(0.07)\n\n\nR2\n0.31\n0.44\n0.43\n0.55\n0.02\n\n\nAdj. R2\n0.29\n0.40\n0.40\n0.53\n0.00\n\n\nNum. obs.\n51\n51\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#section-12",
    "href": "slides/07-slides.html#section-12",
    "title": "POLS 1600",
    "section": "",
    "text": "\\[\n\\text{Deaths} = \\beta_0 + \\beta_1 \\text{Rep VS} + \\beta_2 \\text{Age} + \\beta_3 \\text{Income}\n\\]\n\n\\(\\beta_1\\) decreases because after controlling for income there is less unique variation explained only by republican vote share"
  },
  {
    "objectID": "slides/07-slides.html#using-regression-to-produce-predicted-values",
    "href": "slides/07-slides.html#using-regression-to-produce-predicted-values",
    "title": "POLS 1600",
    "section": "Using regression to produce predicted values",
    "text": "Using regression to produce predicted values\nCoefficients in a regression define a formula which produces a predicted value of the outcome \\(y\\) when the predictors \\(X\\) take particular values.\n\\[\n\\begin{aligned}y &= \\overbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2  + \\dots \\beta_j x_j}^{\\text{Predictors}} + \\underbrace{\\epsilon}_{\\text{Residuals}} &\\\\\ny &= \\beta_0 + \\beta_1x_{rvs} + \\beta_2x_{age}+ \\beta_3x_{inc} + \\epsilon & \\text{m3}\\\\\ny &= 0.56 + 0.07x_{rvs} - 0.02 x_{age} - 0.22 x_{inc} + \\hat{\\epsilon} & \\text{estimated m3}\\\\\ny &= 0.56 + 0.07(-0.87) - 0.02(0.62) - 0.22(0.38) + \\hat{\\epsilon} & \\text{prediction for RI} \\\\\n\\overbrace{0.22}^{\\text{Observed}} &= \\underbrace{0.41}_{\\text{Predicted}} + \\overbrace{(-0.19)}^{\\text{Residual}} &\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-slides.html#producing-predicted-values-in-r",
    "href": "slides/07-slides.html#producing-predicted-values-in-r",
    "title": "POLS 1600",
    "section": "Producing Predicted Values in R",
    "text": "Producing Predicted Values in R\nThe basic steps to producing predicted values in R as follows:\n\nFit a model using lm()\nCreate a prediction data frame using expand_grid():\n\nvary the values of the predictor you’re interested in\nhold all the other predictors in your model constant at some typical value.\n\nInput the model from lm() and the prediction data frame, into the predict() function to obtain predicted values.\n\nSave predictions as a new column in your prediction data frame (I generally call them fit)\n\nPlot predicted values in your prediction data frame to help interpret your model"
  },
  {
    "objectID": "slides/07-slides.html#section-13",
    "href": "slides/07-slides.html#section-13",
    "title": "POLS 1600",
    "section": "",
    "text": "Are there decreasing returns to vaccination?\n\nTask m4 Table Predict Fig\n\n\nSuppose we thought the marginal effect – (here, predicted change in deaths from a 1 percent increase in the percent of the population vaccinated) of vaccines varied.\nThere might be large gains from going to low to average rates of vaccination, but after a certain threshold, the decreases in deaths would taper off.\nWe could test this by including a polynomial term I(percent_vaccinated)^2 in our model.\nIncluding a polynomial term, allows the marginal effect to vary, based on the value of the predictor.\nIt’s hard to interpret the coefficients on polynomial terms (or interaction terms) just by looking at coefficients in a table\nInstead, we’ll produce a plot of predicted values to test these claims\n\n\n\nm4 &lt;- lm(new_deaths_pc_14day ~ percent_vaccinated + I(percent_vaccinated^2) + rep_voteshare_std + med_age_std + med_income_std, covid_lab\n           )\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n6.194**\n\n\n\n\n \n\n\n(2.186)\n\n\n\n\npercent_vaccinated\n\n\n-0.169*\n\n\n\n\n \n\n\n(0.077)\n\n\n\n\npercent_vaccinated^2\n\n\n0.001\n\n\n\n\n \n\n\n(0.001)\n\n\n\n\nrep_voteshare_std\n\n\n-0.062\n\n\n\n\n \n\n\n(0.081)\n\n\n\n\nmed_age_std\n\n\n0.053\n\n\n\n\n \n\n\n(0.053)\n\n\n\n\nmed_income_std\n\n\n-0.114\n\n\n\n\n \n\n\n(0.068)\n\n\n\n\nR2\n\n\n0.561\n\n\n\n\nAdj. R2\n\n\n0.512\n\n\n\n\nNum. obs.\n\n\n51\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\npred_df &lt;- expand_grid(\n  percent_vaccinated = sort(unique(covid_lab$percent_vaccinated)),\n  # Set standardized predictors to their means of 0\n  rep_voteshare_std = 0,\n  med_age_std = 0,\n  med_income_std = 0\n)\n\npred_df$fit &lt;- predict(m4, newdata = pred_df)\n\npred_df %&gt;% \n  ggplot(aes(percent_vaccinated, fit))+\n  geom_line()+\n  labs(\n    y = \"Predicted Covid-19 Deaths\\n(per capita, 14-day average)\",\n    x = \"Percent of State's population that's Vaccinated\"\n  ) + \n  theme_minimal() -&gt; fig_m4\n\n\n\nFor a typical state, early increases in vaccination rate are associated with larger declines in predicted deaths from Covid-19"
  },
  {
    "objectID": "slides/07-slides.html#evaluating-model-fit-1",
    "href": "slides/07-slides.html#evaluating-model-fit-1",
    "title": "POLS 1600",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\nModels partition variance. We can summarize the overall fit of our model using measures like \\(R^2\\)\n\\[R^2 = \\frac{\\text{variance(predicted values )}}{ \\text{variance(observed values )}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#r2",
    "href": "slides/07-slides.html#r2",
    "title": "POLS 1600",
    "section": "R^2",
    "text": "R^2\nMore formally, you’ll see \\(R^2\\) defined in terms of “Sums of Squares”\n\nTSS = Total Sum of Squares = Variance of the Outcome\nESS = Explained Sum of Squares = Variance of the Predicted Values\nRSS = Sum of Squared Residuals = Variance of the Residuals\n\n\\[R^2 = \\frac{ESS}{TSS}= 1 - \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "slides/07-slides.html#calculating-r2-in-r",
    "href": "slides/07-slides.html#calculating-r2-in-r",
    "title": "POLS 1600",
    "section": "Calculating \\(R^2\\) in R",
    "text": "Calculating \\(R^2\\) in R\nWe could do it by hand, finding that our model explained about 43 percent of the observed variation deaths.\n\n# ESS / TSS\nvar(m3$fitted.values)/var(m3$model$new_deaths_pc_14day)\n\n[1] 0.4393655\n\n# 1 - RSS/TSS\n1 - var(m3$residuals)/var(m3$model$new_deaths_pc_14day)\n\n[1] 0.4393655\n\n\nBut generally we let the summary() function do it for us:\n\nsummary(m3)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50751 -0.19703 -0.06278  0.20024  0.92320 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04425  12.782  &lt; 2e-16 ***\nrep_voteshare_std  0.07140    0.06654   1.073  0.28869    \nmed_age_std       -0.01692    0.04744  -0.357  0.72296    \nmed_income_std    -0.21669    0.06660  -3.254  0.00211 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.316 on 47 degrees of freedom\nMultiple R-squared:  0.4394,    Adjusted R-squared:  0.4036 \nF-statistic: 12.28 on 3 and 47 DF,  p-value: 4.689e-06"
  },
  {
    "objectID": "slides/07-slides.html#adjusted-r2",
    "href": "slides/07-slides.html#adjusted-r2",
    "title": "POLS 1600",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) Example Figure\n\n\n\n\nOne can show that a models \\(R^2\\) always increases as we add predictors, even when they’re unrelated to the outcome\nThe adjusted \\(R^2\\) adjusts for this by weighting the \\(R^2\\) of a model by the number of predictors\n\n\\[\\text{adj. }R^2 = 1 - \\frac{RSS/(n-k)}{TSS/(n-1)}\\]\n\n\n\n\nex_df &lt;- data.frame(\n  y = rnorm(100) \n  ) %&gt;%\n    bind_cols(\n      data.frame(matrix(rnorm(10000), ncol=100))\n    ) %&gt;% janitor::clean_names()\n\n\nthe_formulas &lt;- list()\nfor(i in 2:51){\n  vars &lt;- names(ex_df)[2:i]\n  the_formulas[[i-1]] &lt;- paste(\"y~\",paste(vars,collapse = \"+\"))\n}\n\nthe_formulas %&gt;% \n  purrr::map(as.formula) %&gt;% \n  purrr::map(lm, data=ex_df) %&gt;% \n  purrr::map(summary) %&gt;% \n  purrr::map_df(glance) -&gt; r2_df\n\nr2_df %&gt;% \n  ggplot(aes(df, r.squared))+\n  geom_point(aes(col = \"R^2\"))+\n  geom_line()+\n  geom_point(aes(y=adj.r.squared,col = \"Adjusted R^2\"))+\n  geom_line(aes(y=adj.r.squared))+\n  labs(\n    x = \"Number of predictors\",\n    y = \"Proportion of Variance Explained\",\n    title = \"Adding unrelated predictors increases a model's R^2\\nwhile the Adjusted R^2 provides a better indicator of poor fit \",\n    col =\"Model fit\"\n  ) -&gt; fig_r2"
  },
  {
    "objectID": "slides/07-slides.html#section-14",
    "href": "slides/07-slides.html#section-14",
    "title": "POLS 1600",
    "section": "",
    "text": "Using \\(R^2\\) to compare models\n\nANOVA m5 Table Anova\n\n\nWhen models are nested (larger models contain all the predictors of smaller models), we can ask, does including the additional predictors in the larger model explain more variation in the outcome than we would expect would happen if we just added additional, random variable.\nFormally we call this process an Analysis of Variance (ANOVA)\nLet’s assess the added predictive power of I(percent_vaccinated^2) by estimating a model without it and comparing models using ANOVA\n\n\n\n# Estimate model without polynomial\nm5 &lt;- lm(new_deaths_pc_14day ~ percent_vaccinated  + rep_voteshare_std + med_age_std + med_income_std, covid_lab\n           )\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n6.194**\n\n\n2.532***\n\n\n\n\n \n\n\n(2.186)\n\n\n(0.657)\n\n\n\n\npercent_vaccinated\n\n\n-0.169*\n\n\n-0.035**\n\n\n\n\n \n\n\n(0.077)\n\n\n(0.012)\n\n\n\n\npercent_vaccinated^2\n\n\n0.001\n\n\n \n\n\n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nrep_voteshare_std\n\n\n-0.062\n\n\n-0.089\n\n\n\n\n \n\n\n(0.081)\n\n\n(0.082)\n\n\n\n\nmed_age_std\n\n\n0.053\n\n\n0.071\n\n\n\n\n \n\n\n(0.053)\n\n\n(0.053)\n\n\n\n\nmed_income_std\n\n\n-0.114\n\n\n-0.119\n\n\n\n\n \n\n\n(0.068)\n\n\n(0.070)\n\n\n\n\nR2\n\n\n0.561\n\n\n0.531\n\n\n\n\nAdj. R2\n\n\n0.512\n\n\n0.490\n\n\n\n\nNum. obs.\n\n\n51\n\n\n51\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\nThe anova suggests that including a polynomial provides a marginal improvement to fit (p &lt; 0.10)\n\nanova(m5, m4)\n\nAnalysis of Variance Table\n\nModel 1: new_deaths_pc_14day ~ percent_vaccinated + rep_voteshare_std + \n    med_age_std + med_income_std\nModel 2: new_deaths_pc_14day ~ percent_vaccinated + I(percent_vaccinated^2) + \n    rep_voteshare_std + med_age_std + med_income_std\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     46 3.9268                              \n2     45 3.6758  1   0.25098 3.0725 0.08644 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/07-slides.html#motivating-example-what-causes-cholera",
    "title": "POLS 1600",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/07-slides.html#notation",
    "href": "slides/07-slides.html#notation",
    "title": "POLS 1600",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/07-slides.html#causal-effects",
    "href": "slides/07-slides.html#causal-effects",
    "title": "POLS 1600",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/07-slides.html#average-treatment-on-treated",
    "href": "slides/07-slides.html#average-treatment-on-treated",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/07-slides.html#average-treatment-on-treated-1",
    "href": "slides/07-slides.html#average-treatment-on-treated-1",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n\n\n\n\n\n\n\n\n\nPre-Period (T=0)\nPost-Period (T=1)\n\n\n\n\nTreated \\(D_{i}=1\\)\n\\(E[Y_{0i}(0)\\vert D_i = 1]\\)\n\\(E[Y_{1i}(1)\\vert D_i = 1]\\)\n\n\nControl \\(D_i=0\\)\n\\(E[Y_{0i}(0)\\vert D_i = 0]\\)\n\\(E[Y_{0i}(1)\\vert D_i = 0]\\)"
  },
  {
    "objectID": "slides/07-slides.html#data",
    "href": "slides/07-slides.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/07-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/07-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "POLS 1600",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/07-slides.html#before-vs-after-comparisons",
    "href": "slides/07-slides.html#before-vs-after-comparisons",
    "title": "POLS 1600",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/07-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/07-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "POLS 1600",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/07-slides.html#difference-in-differences-1",
    "href": "slides/07-slides.html#difference-in-differences-1",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period"
  },
  {
    "objectID": "slides/07-slides.html#difference-in-differences-2",
    "href": "slides/07-slides.html#difference-in-differences-2",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\] Is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/07-slides.html#difference-in-differences-3",
    "href": "slides/07-slides.html#difference-in-differences-3",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/07-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/07-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "POLS 1600",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V"
  },
  {
    "objectID": "slides/07-slides.html#parallel-trends",
    "href": "slides/07-slides.html#parallel-trends",
    "title": "POLS 1600",
    "section": "Parallel Trends",
    "text": "Parallel Trends"
  },
  {
    "objectID": "slides/07-slides.html#using-lm-to-estimate-diff-in-diff",
    "href": "slides/07-slides.html#using-lm-to-estimate-diff-in-diff",
    "title": "POLS 1600",
    "section": "Using lm to estimate Diff-in-Diff",
    "text": "Using lm to estimate Diff-in-Diff\n\n Data lm() Diff-in-Diff\n\n\n\ncholera_df &lt;- tibble(\n  Location = c(\"S&V\",\"Lambeth\",\"S&V\",\"Lambeth\"),\n  Treated = c(0,1,0, 1),\n  Time = c(0,0,1,1),\n  Deaths = c(135,85,147,19)\n)\ncholera_df\n\n# A tibble: 4 × 4\n  Location Treated  Time Deaths\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 S&V            0     0    135\n2 Lambeth        1     0     85\n3 S&V            0     1    147\n4 Lambeth        1     1     19\n\n\n\n\n\\[\n\\text{Deaths} = \\beta_0 + \\beta_1\\text{Treated} + \\beta_2\\text{Time} + \\beta_3 \\text{Treated}\\times\\text{Time}\n\\]\n\ndiff_in_diff &lt;- lm(Deaths ~ Treated + Time + Treated:Time, cholera_df)\ndiff_in_diff\n\n\nCall:\nlm(formula = Deaths ~ Treated + Time + Treated:Time, data = cholera_df)\n\nCoefficients:\n (Intercept)       Treated          Time  Treated:Time  \n         135           -50            12           -78  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_0=\\) Outcome in control (S&V) before treatment\n\\(\\beta_1=\\) Fixed, time invariant differences between treated and control\n\\(\\beta_2=\\) Fixed, unit invariant differences between pre and post periods\n\\(\\beta_3=\\) Difference-in-Differences = \\(E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\)"
  },
  {
    "objectID": "slides/07-slides.html#summary",
    "href": "slides/07-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\nDifferencing twice accounts for fixed differences across units and between periods\n\nBut not time varying differences across units…\n\nThe key identifying assumption of a DiD design is the assumption of parallel trends\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to assess if we have multiple periods of pre-treatment observations"
  },
  {
    "objectID": "slides/07-slides.html#generalizing-diff-in-diff-with-linear-regression",
    "href": "slides/07-slides.html#generalizing-diff-in-diff-with-linear-regression",
    "title": "POLS 1600",
    "section": "Generalizing Diff-in-Diff with Linear Regression",
    "text": "Generalizing Diff-in-Diff with Linear Regression\n\nLinear regression allows us to generalizes Diff-in-Diff to multiple periods and treatment interventions, with fixed effects\n\n\n\\[\ny_{it} = \\overbrace{\\alpha_i}^{\\text{Unit FE}} + \\underbrace{\\gamma_t}_{\\text{Period FE}} + \\overbrace{\\tau*d_{it}}^{\\text{Treatment}} + \\underbrace{X\\beta}_{\\text{Covariates}} + \\epsilon_{it}\n\\]\n\n\nUnit fixed effects \\((\\alpha_i)\\)control for time-invariant differences across units\nPeriod fixed effects \\((\\gamma_i)\\) control for unit-invariant differences across periods\n\\(\\tau\\) corresponds the Difference-in-Difference estimate for a two-way fixed effects regression"
  },
  {
    "objectID": "slides/07-slides.html#extensions-and-limitations",
    "href": "slides/07-slides.html#extensions-and-limitations",
    "title": "POLS 1600",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nInterpretation of two-way fixed effects DiD estimator is complicated…\n\nGoodman-Bacon (2021)\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/07-slides.html#applications",
    "href": "slides/07-slides.html#applications",
    "title": "POLS 1600",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?"
  },
  {
    "objectID": "slides/07-slides.html#replicating-grumbach-and-hill-2022",
    "href": "slides/07-slides.html#replicating-grumbach-and-hill-2022",
    "title": "POLS 1600",
    "section": "Replicating Grumbach and Hill (2022)",
    "text": "Replicating Grumbach and Hill (2022)\n\nIn this week’s lab, we’ll be conducting a partial replication of Grumbach and Hill (2022) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nOn Thursday, we’ll walk through\n\nthe paper’s design and argument\nsetting up and exploring the data\nreproducing some descriptive figures\n\nNext Thursday, we’ll focus on replicating and understanding the main results"
  },
  {
    "objectID": "slides/07-slides.html#general-structure-of-labs-7-8",
    "href": "slides/07-slides.html#general-structure-of-labs-7-8",
    "title": "POLS 1600",
    "section": "General Structure of Labs 7-8",
    "text": "General Structure of Labs 7-8\nLab 7:\n\nSummarize the study\nDownload and load the data\nRecode the data\nMerge the data\nRecreate Figures 1 and 2\n\nLab 8:\n\nEstimate some baseline models to understand Two-Way Fixed Effects\nEstimate some of the models in Figure 3\nExtend the study, perhaps considering SDR by race or gender"
  },
  {
    "objectID": "slides/07-slides.html#reading-grumbach-and-hill-2022",
    "href": "slides/07-slides.html#reading-grumbach-and-hill-2022",
    "title": "POLS 1600",
    "section": "Reading Grumbach and Hill (2022)",
    "text": "Reading Grumbach and Hill (2022)\nReading Grumbach and Hill (2022), focus on being able to answer the following:\n\nWhat’s the research question?\n\nGeneral RQ: First sentence, second paragraph, p. 405\nSpecific RQs: p. 405-406\n\nWhat’s the theoretical framework?\n\nIntro and Theory of Registration, p. 407-409\n\nWhat’s the empirical design?\n\nMethods pp. 409-410\n\nWhat’s are the main results?\n\nResults pp. 410-413\nFigure 3 in particular"
  },
  {
    "objectID": "slides/07-slides.html#q1-download-the-replication-files",
    "href": "slides/07-slides.html#q1-download-the-replication-files",
    "title": "POLS 1600",
    "section": "Q1: Download the replication files",
    "text": "Q1: Download the replication files\nRather than downloading the files directly from the paper’s replication archives, in this lab, we will download the replication files to your computers and then load the data into R from where they’re saved\nPlease click here and let’s download the files together."
  },
  {
    "objectID": "slides/07-slides.html#go-to-the-papers-dataverse",
    "href": "slides/07-slides.html#go-to-the-papers-dataverse",
    "title": "POLS 1600",
    "section": "1. Go to the paper’s dataverse",
    "text": "1. Go to the paper’s dataverse\nPlease click here"
  },
  {
    "objectID": "slides/07-slides.html#log-in-through-brown",
    "href": "slides/07-slides.html#log-in-through-brown",
    "title": "POLS 1600",
    "section": "2. Log in through Brown",
    "text": "2. Log in through Brown"
  },
  {
    "objectID": "slides/07-slides.html#select-all-of-the-files",
    "href": "slides/07-slides.html#select-all-of-the-files",
    "title": "POLS 1600",
    "section": "3. Select all of the files",
    "text": "3. Select all of the files\nMake sure to Select all 11 files in this dataset"
  },
  {
    "objectID": "slides/07-slides.html#download-the-files-in-their-original-format",
    "href": "slides/07-slides.html#download-the-files-in-their-original-format",
    "title": "POLS 1600",
    "section": "4. Download the files in their original format",
    "text": "4. Download the files in their original format"
  },
  {
    "objectID": "slides/07-slides.html#section-16",
    "href": "slides/07-slides.html#section-16",
    "title": "POLS 1600",
    "section": "",
    "text": "5. Save and unzip the downloaded files into your course folder where your labs are saved"
  },
  {
    "objectID": "slides/07-slides.html#q3-load-the-data-into-r",
    "href": "slides/07-slides.html#q3-load-the-data-into-r",
    "title": "POLS 1600",
    "section": "Q3: Load the data into R",
    "text": "Q3: Load the data into R\nIf you’ve saved the dataverse_files into the folder where your lab is saved, you should be able to run the following code after setting the working directory to source file location:\n\n# Remember to set working directory:\n# Session &gt; Set working directory &gt; Source file location\n\n# Load fips_codes\nfips_codes &lt;- read_csv(\"dataverse_files/fips_codes_website.csv\")%&gt;%\n  janitor::clean_names()\n\n# Load policy data\ndata &lt;- readRDS(\"dataverse_files/policy_data_updated.RDS\")%&gt;%\n  janitor::clean_names()\n\n# Load CPS data\ncps &lt;- read_csv(\"dataverse_files/cps_00021.csv\") %&gt;%\n  janitor::clean_names()"
  },
  {
    "objectID": "slides/07-slides.html#summary-2",
    "href": "slides/07-slides.html#summary-2",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/07-slides.html#references",
    "href": "slides/07-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-differences with variation in treatment timing.” Journal of Econometrics 225 (2): 254–77.\n\n\nGrumbach, Jacob M, and Charlotte Hill. 2022. “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.” The Journal of Politics 84 (1): 405–17."
  },
  {
    "objectID": "slides/11-slides.html#class-plan",
    "href": "slides/11-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nCourse Review\nStatistical Inference\nFinal Projects"
  },
  {
    "objectID": "slides/11-slides.html#annoucements",
    "href": "slides/11-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nDo we need to start taking attendance?\nLab 11/Assignment 3 this week\nNo tutorial this week\nNext Tuesday, April 23, Work on Presentations/Drafts\nAssignment 4 now due April 25.\nApril 30, Final Workshop\nMay 2 Class Presentations"
  },
  {
    "objectID": "slides/11-slides.html#pie-me",
    "href": "slides/11-slides.html#pie-me",
    "title": "POLS 1600",
    "section": "Pie me",
    "text": "Pie me"
  },
  {
    "objectID": "slides/11-slides.html#setup-packages-for-today",
    "href": "slides/11-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/11-slides.html#three-modes-of-inference",
    "href": "slides/11-slides.html#three-modes-of-inference",
    "title": "POLS 1600",
    "section": "Three Modes of Inference",
    "text": "Three Modes of Inference\n\nDescriptive\nCausal\nPredictive"
  },
  {
    "objectID": "slides/11-slides.html#descriptive-inference",
    "href": "slides/11-slides.html#descriptive-inference",
    "title": "POLS 1600",
    "section": "Descriptive Inference",
    "text": "Descriptive Inference\n\nSummarize distributions and relationships in data\n\n\nYou should know how to:\n\nCalculate and interpret measures:\nCentral Tendency\nDispersion\nAssociation\n\nLoad, look at, wrangle, and describe data using:\n\nTables\nFigures"
  },
  {
    "objectID": "slides/11-slides.html#data-wrangling",
    "href": "slides/11-slides.html#data-wrangling",
    "title": "POLS 1600",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe process of transforming data into a useable format\n\nYou should know how to:\n\nLoad, look at,and transform data into R\nGet a HLO of the raw data:\n\nUnit of analysis\nDimensions of the data\nQuickly summarize the distributions and values of variables\n\nRecode the data to:\n\nReplace values as NAs\nCreate categories, indicators (0,1), and factors\nTransform predictors (e.g. standardizing predictors)\n\nReshape the data\n\nPivoting columns and rows\nJoining data sets together.\n\nAggregate the data into summaries"
  },
  {
    "objectID": "slides/11-slides.html#data-visualization",
    "href": "slides/11-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nA tool for describing distributions and relationships\n\nYou should know:\n\nThe grammar of graphics:\n\nData\nAesthetic mappings\nGeometries\n\nHow to generate common plots to describe:\n\nDistributions\nRelationships"
  },
  {
    "objectID": "slides/11-slides.html#causal-inference",
    "href": "slides/11-slides.html#causal-inference",
    "title": "POLS 1600",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal Inference requires counterfactual comparisons\n\nYou should know:\n\nPotential outcomes and DAGs\nThe fundamental problem of causal inference\nBias caused by:\n\nConfounding (Coffee and Cancer)\nColliding (Dating Jerks)\n\nCasual Identification in:\n\nExperimental designs\nObservational designs"
  },
  {
    "objectID": "slides/11-slides.html#prediction-with-linear-models",
    "href": "slides/11-slides.html#prediction-with-linear-models",
    "title": "POLS 1600",
    "section": "Prediction with Linear Models",
    "text": "Prediction with Linear Models\n\nLinear regression provides a linear estimate of the conditional expectation function\n\nYou should know:\n\nHow linear regression works\nWhat it means to control for predictors in a multiple regression\nWhen and why we should control for predictors.\nHow to translate substantive claims into empirical expectations for our models\nHow to estimate and interpret these models using tables and figures\nHow to quantify uncertainty about these estimates using confidence intervals and hypothesis tests."
  },
  {
    "objectID": "slides/11-slides.html#probability",
    "href": "slides/11-slides.html#probability",
    "title": "POLS 1600",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event\nRandom variables assign numeric values to all the events that could occur.\nProbability distributions assign probabilities to every value of a random variable. Can be:\n\ndiscrete\ncontinuous\ncharacterized by their expected values and variances\nused to:\ndescribe the data generating process\nquantify uncertainty about estimates"
  },
  {
    "objectID": "slides/11-slides.html#sampling-distributions-and-standard-errors",
    "href": "slides/11-slides.html#sampling-distributions-and-standard-errors",
    "title": "POLS 1600",
    "section": "Sampling Distributions and Standard Errors",
    "text": "Sampling Distributions and Standard Errors\n\nA sampling distribution is a theoretical probability distribution of estimates obtained from taking repeated samples of size \\(n\\) from some population\n\nA distribution of what we could have seen\n\nA standard errors is simply the standard deviation (\\(\\sigma\\)) of the sampling distribution\n\nA measure of how much our estimate could have varied.\n\nLaw of Large Numbers: As \\(N \\to \\infty\\) \\(\\bar{x} \\to \\mu\\)\nCentral Limit Theorem: As \\(N \\to \\infty\\) \\(\\bar{x} \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals",
    "href": "slides/11-slides.html#confidence-intervals",
    "title": "POLS 1600",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nConfidence intervals provide a range of plausible values for our estimate\n\n\nThree components:\n\nPoint Estimate (i.e. a mean, or coefficient)\nConfidence Level (Often 95 percent by convention)\nMargin of Error (+/- some range (typically 2*SD for 95 percent CI))\n\nConfidence is about the interval\n\n95 percent of the intervals construct in this manner would contain the truth."
  },
  {
    "objectID": "slides/11-slides.html#hypothesis-testing",
    "href": "slides/11-slides.html#hypothesis-testing",
    "title": "POLS 1600",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nA hypothesis test quantifies how likely it is that we would observe what we did (our test statistic), if some claim about the world were true (our hypothesis, typically a null ).\nIf our claim were true, then under this null hypothesis, our test statistic would have a distribution centered around the truth.\nA p-value which describes the probability of observing a test statistic as extreme or more extreme in a world where our null hypothesis was true\n\nIf our p-value is small (\\(p &lt; 0.05\\)), we reject the null hypothesis\nIf our p-value is large (\\(p &gt; 0.05\\)), we fail to reject the null, or retain the null hypothesis"
  },
  {
    "objectID": "slides/11-slides.html#section",
    "href": "slides/11-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Relationship between CIs and Hypothsis Testing\n\nConcept Code Table Figure\n\n\nWe can think of a confidence interval as a range of hypotheses we would fail to reject with \\(p &lt; \\alpha\\)\n\n\n\n# Load Data\nload(url(\"https://pols1600.paultesta.org/files/data/nes24.rda\"))\n\n# Fit Model\nm1 &lt;- lm_robust(dv_participation ~   education + income, df,\n                se = \"classical\")\n\n# Range of hypotheses for education\npval_ci_df &lt;- tibble(\n  # Hypothesized Betas for Education\n  Hypothesis = seq(0, .32, length.out = 100),\n  # Test Statistics\n  Statistic = (m1$coefficients[\"education\"] - Hypothesis) /\n  m1$std.error[\"education\"],\n  # P-value for two sided test\n  `p-value` = 2*pt(abs(Statistic), df = m1$df,lower.tail = F)\n)\n\nfig_pval_ci &lt;- pval_ci_df %&gt;% \n  ggplot(aes(Hypothesis, `p-value`))+\n  geom_line()+\n  geom_vline(xintercept = m1$coefficients[\"education\"],\n             linetype = \"solid\",\n             col = \"red\")+\n  geom_vline(xintercept = m1$conf.low[\"education\"],\n             linetype = \"dotted\")+\n  geom_vline(xintercept = m1$conf.high[\"education\"],\n             linetype = \"dotted\")+\n  geom_hline(yintercept = 0.05,\n             linetype = \"dashed\")+\n  labs(\n    x = \"Hypothesized Education, Coefficent\",\n    title = \"Confidence intervals are a range\\nof plausible hypotheses\"\n  )+\n  theme_minimal()\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.31*\n\n\n\n\n \n\n\n[ 0.14; 0.48]\n\n\n\n\neducation\n\n\n0.17*\n\n\n\n\n \n\n\n[ 0.12; 0.21]\n\n\n\n\nincome\n\n\n0.01\n\n\n\n\n \n\n\n[-0.01; 0.03]\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.29\n\n\n\n\n\n\n* 0 outside the confidence interval."
  },
  {
    "objectID": "slides/11-slides.html#four-possible-outcomes-of-a-hypothesis-test",
    "href": "slides/11-slides.html#four-possible-outcomes-of-a-hypothesis-test",
    "title": "POLS 1600",
    "section": "Four Possible Outcomes of a hypothesis Test",
    "text": "Four Possible Outcomes of a hypothesis Test\n\n\n\nFalse Positive: (Type I Error)\n\nRejecting a True \\(H_0\\).\n\\(\\tau = 0\\), but \\(\\hat{\\tau}\\) has a \\(p&lt;0.05\\)\nProbability=\\(\\alpha\\)\n\nTrue Positive: (Correct Decision)\n\nRejecting a false \\(H_0\\):\n\\(\\tau \\neq 0\\), and \\(\\hat{\\tau}\\) has a \\(p&lt;0.05\\)\nOccurs with Probability = \\(1-\\beta\\)\n\n\n\n\nTrue Negative: (Correct Decision) \n\nFailing to reject a True \\(H_0\\):\n\\(\\tau = 0\\), and \\(\\hat{\\tau}\\) has a \\(p&gt;0.05\\)\nOccurs with Probability = \\(1-\\alpha\\)\n\nFalse Negative: (Type II Error)\n\nFailing to reject a false \\(H_0\\).\n\\(\\tau \\neq 0\\) but \\(\\hat{\\tau}\\) has a \\(p&gt;0.05\\)\nOccurs with Probability= \\(\\beta\\)"
  },
  {
    "objectID": "slides/11-slides.html#type-1-and-2-errors",
    "href": "slides/11-slides.html#type-1-and-2-errors",
    "title": "POLS 1600",
    "section": "Type 1 and 2 Errors",
    "text": "Type 1 and 2 Errors\n\nSource"
  },
  {
    "objectID": "slides/11-slides.html#statistical-power",
    "href": "slides/11-slides.html#statistical-power",
    "title": "POLS 1600",
    "section": "Statistical Power",
    "text": "Statistical Power\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nConceptPowerComments\n\n\n\n\nConsider two distributions of statistics under\n\na null of no effect (\\(H_0\\))\nan effect of \\(\\tau\\) (\\(H_1\\))\n\nFor a significance threshold of \\(\\alpha\\) we would:\n\nFail to reject the null \\(\\beta\\) (Type II Errors)\nCorrectly reject the null \\(1 -\\beta\\) (Statistical Power)\n\n\n\n\n\nTry changing \\(\\tau\\) (the effect size), and se (the standard deviation of the effect)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPower is a function of:\n\nSample size (\\(N\\))\n\nLarger samples, smaller standard errors (LLN)\n\nEffect size (\\(\\tau\\))\n\nBigger effects less overlap\n\nSignificance threshold (\\(\\alpha\\))\n\nDecrease Type 1 (False Positives) error leads to increased Type 2 (False Negatives)\n\nThe distribution of the data\n\nVariance, asympotitc approximations"
  },
  {
    "objectID": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "href": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "title": "POLS 1600",
    "section": "Strucutre of Final Paper and Drafts",
    "text": "Strucutre of Final Paper and Drafts\nAssignment 4: Seven sections\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides.html#for-thursday",
    "href": "slides/11-slides.html#for-thursday",
    "title": "POLS 1600",
    "section": "For Thursday",
    "text": "For Thursday\n\nAssignment 3\nDownload template\nCreate shared google drive.\nMake progress on:\n\n\nData (20 percent ~ 4+ paragraphs)\n\n\nDesign (25 percent ~ 5+ paragraphs)\n\n\nResults (25 percent ~ 5+ paragraphs)"
  },
  {
    "objectID": "slides/11-slides.html#motivating-questions",
    "href": "slides/11-slides.html#motivating-questions",
    "title": "POLS 1600",
    "section": "Motivating Questions",
    "text": "Motivating Questions\nIn the reset of today’s class, we’ll get some practice putting together the various skills you need for your drafts by exploring the following:\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\nHave these perceptions about vaccines changed over time?"
  },
  {
    "objectID": "slides/11-slides.html#tasks",
    "href": "slides/11-slides.html#tasks",
    "title": "POLS 1600",
    "section": "Tasks:",
    "text": "Tasks:\nTo explore these questions, we need to\n\nGet setup to work\nLoad our data\nRecode our data\nSummarize our data\nSpecify our expectations\nEstimate models to test these expectations\nPresent and interpret results using\n\nTables\nFigures\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "slides/11-slides.html#new-packages",
    "href": "slides/11-slides.html#new-packages",
    "title": "POLS 1600",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# # Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anesr)"
  },
  {
    "objectID": "slides/11-slides.html#packages-for-today",
    "href": "slides/11-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/11-slides.html#data",
    "href": "slides/11-slides.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nNow that we have anesr installed, let’s load data from the 2016 and 2020 National Election Studies:\n\n# Load data\ndata(timeseries_2016, package = \"anesr\")\ndata(timeseries_2020, package = \"anesr\")\n\nAnd copy those data frames into new dataframes with shorter names\n\n# Rename datasets\nnes16 &lt;- timeseries_2016\nnes20 &lt;- timeseries_2020"
  },
  {
    "objectID": "slides/11-slides.html#finding-variables-outcomes",
    "href": "slides/11-slides.html#finding-variables-outcomes",
    "title": "POLS 1600",
    "section": "Finding variables: Outcomes",
    "text": "Finding variables: Outcomes\nOur primary outcome of interest are beliefs about vaccines.\nVariables V162162x in the 2016 NES and V202383x in the 2020 NES will serve as our primary outcome of interest, summarizing respondents answer to the following question:\n\nDo the health benefits of vaccinations generally outweigh the risks, do the risks outweigh the benefits, or is there no difference?"
  },
  {
    "objectID": "slides/11-slides.html#finding-variables-predictors",
    "href": "slides/11-slides.html#finding-variables-predictors",
    "title": "POLS 1600",
    "section": "Finding variables: Predictors",
    "text": "Finding variables: Predictors\nSimilarly, V161158x in the 2016 NES and V201231x in the 2020 NES will serve our key predictor (respondent’s partisanship).\nFinally, we’ll control respondents’ age, using V161267 in the 2016 NES and V201507x in the 2020 NES"
  },
  {
    "objectID": "slides/11-slides.html#examine-distributions-vaccine-beliefs",
    "href": "slides/11-slides.html#examine-distributions-vaccine-beliefs",
    "title": "POLS 1600",
    "section": "Examine Distributions: Vaccine Beliefs",
    "text": "Examine Distributions: Vaccine Beliefs\nThe variables in the NES datasets are of a class labelled which allows numeric values to have substantive labels\n\nclass(nes16$V162162x)\n\n[1] \"haven_labelled\"\n\n\nOur outcome variable has the following labels:\n\nlabelled::val_labels(nes16$V162162x)\n\n                                   -9. Refused \n                                            -9 \n                                -8. Don't know \n                                            -8 \n-7. No post data, deleted due to incomplete IW \n                                            -7 \n                -6. No post-election interview \n                                            -6 \n                      1. Benefits much greater \n                                             1 \n                2. Benefits moderately greater \n                                             2 \n                  3. Benefits slightly greater \n                                             3 \n                              4. No difference \n                                             4 \n                     5. Risks slightly greater \n                                             5 \n                   6. Risks moderately greater \n                                             6 \n                         7. Risks much greater \n                                             7 \n\n\nAnd distribution of responses:\n\ntable(nes16$V162162x)\n\n\n  -9   -8   -7   -6    1    2    3    4    5    6    7 \n  21   28   86  536 1687  726  258  539   96  211   82"
  },
  {
    "objectID": "slides/11-slides.html#recoding-outcome-variables",
    "href": "slides/11-slides.html#recoding-outcome-variables",
    "title": "POLS 1600",
    "section": "Recoding outcome variables",
    "text": "Recoding outcome variables\n\nTasks Code\n\n\nWhat transformations do we need to make to V162162x in nes16 and V202383x in nes20 so that these variables are suitable for analysis?\n\nRecode negative values to be NA\nReverse code so that higher values indicate greater belief vaccines benefits\nCreate an indicator of people who are vaccine skeptics\n\n\n\n\nnes16 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V162162x &lt; 0, NA, (V162162x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes16 # Save recodes to nes16\n\nnes20 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V202383x &lt; 0, NA, (V202383x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes20 # Save recodes to nes20"
  },
  {
    "objectID": "slides/11-slides.html#recoding-predictors",
    "href": "slides/11-slides.html#recoding-predictors",
    "title": "POLS 1600",
    "section": "Recoding Predictors",
    "text": "Recoding Predictors\n\nTasks Code\n\n\n\nNow we repeat this process for our key predictor, partisanship.\n\nRecode partisanship variables V161158x in nes16 and V201231x in nes20\nCreate indicators from this recoded variable that classify partisanship as categorical variable (with Democrats as the reference category)\n\nAnd our covariate, age variables V161267 in nes16 and V201507x in nes20\n\nRecode negative values to be NA\n\n\n\n\n\nnes16 %&gt;%\n  mutate(\n    pid = ifelse(V161158x &lt; 0, NA, V161158x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V161267 &lt; 0, NA, V161267)\n  ) -&gt; nes16\n\n## Recoding Partisanship (V201231x) in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    pid = ifelse(V201231x &lt; 0, NA, V201231x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V201507x &lt; 0, NA, V201507x)\n  ) -&gt; nes20"
  },
  {
    "objectID": "slides/11-slides.html#progress-report",
    "href": "slides/11-slides.html#progress-report",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data📥\nSpecify our expectations\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "slides/11-slides.html#descriptive-statistics-2016",
    "href": "slides/11-slides.html#descriptive-statistics-2016",
    "title": "POLS 1600",
    "section": "Descriptive statistics (2016)",
    "text": "Descriptive statistics (2016)\n\nTasks Code Table\n\n\n\nCreate the_vars\nSelect these variables\nPivot the data\nCalculate summary statistics\nFormat as an html table\n\n\n\n\n# 1. Create a object with the names of the variables you want to summarize\nthe_vars &lt;- c(\"vaccine_skeptic01\",\"pid\",\"age\")\n# 2. Select these variables\nnes16 %&gt;%\n  select(all_of(the_vars)) %&gt;%\n# 3. Pivot the data\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  # 3. Calculate summary statistics\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) -&gt; sum_df \n\nsum_tab &lt;- \nknitr::kable(sum_df,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =2) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 3, end_row =3)\n\n\n\n\n\n\nDescriptive Statistics\n\n\nVariable\nmin\np25\nMedian\nmean\np75\nmax\nmissing\n\n\n\n\nOutcome\n\n\nvaccine_skeptic01\n0\n0\n0\n0.26\n0\n1\n671\n\n\nKey Predictors\n\n\npid\n1\n2\n4\n3.86\n2\n7\n23\n\n\nCovariates\n\n\nage\n18\n34\n50\n49.58\n34\n90\n121"
  },
  {
    "objectID": "slides/11-slides.html#progress-report-1",
    "href": "slides/11-slides.html#progress-report-1",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data ✅\nSpecify our expectations 📥\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations",
    "href": "slides/11-slides.html#specificying-expecations",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\n\nConsider our first two motivating questions\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\n\nAnd some illustrative stereotypes:\n\n“Republicans are anti-science”\n“Liberal always fall for Goopy pseudo-science”\n“Independents love to do their own research”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-1",
    "href": "slides/11-slides.html#specificying-expecations-1",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\n\nSimilarly, consider our third question:\n\nHave these perceptions about vaccines changed over time?\n\nAnd some similar simplified claims:\n\n“The Covid-19 vaccine is a miracle of modern science”\n“Social media is rife with misinformation about the Covid-19 vaccine”\n“Politicians are politicizing vaccine politics for political benefits”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-2",
    "href": "slides/11-slides.html#specificying-expecations-2",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nOur goal is to take claims/conventional wisdom/theories, and derive their empirical implications:\n\nH1: Partisan Differences in Vaccine Skepticism\n\nH1a: Republicans will be the most skeptical of vaccines\nH1b: Democrats will be the most skeptical of vaccines\nH1c: Independents will be the most skeptical of vaccines"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-3",
    "href": "slides/11-slides.html#specificying-expecations-3",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\n\nH2: Temporal Differences in Vaccine Skepticism\n\nH2a: Vaccine skepticism will decrease from 2016 to 2020 with the widespread roll out of the Covid-19 vaccine\nH2b: Vaccine skepticism will increase from 2016 to 2020 with increased amounts of misinformation about the Covid-19 vaccine\n\nH3: Partisan Difference in Vaccine Skepticism Over Time Partisan differences in Vaccine Skepticism will increase from 2016 to 2020 with the politicization of Covid-19 policies"
  },
  {
    "objectID": "slides/11-slides.html#motivating-your-expectations",
    "href": "slides/11-slides.html#motivating-your-expectations",
    "title": "POLS 1600",
    "section": "Motivating your expectations",
    "text": "Motivating your expectations\nIn your final papers, unlike in these slides, your expectations should be grounded in existing theory, research, and evidence. For the present question, we might cite sources such as:\n\nEnders, Adam M., and Steven M. Smallpage. “Informational cues, partisan-motivated reasoning, and the manipulation of conspiracy beliefs.” Political Communication 36.1 (2019): 83-102.\nStecula, Dominik A., and Mark Pickup. “How populism and conservative media fuel conspiracy beliefs about COVID-19 and what it means for COVID-19 behaviors.” Research & Politics 8.1 (2021): 2053168021993979.\nJennings, Will, et al. “Lack of trust, conspiracy beliefs, and social media use predict COVID-19 vaccine hesitancy.” Vaccines 9.6 (2021): 593.\nHollander, Barry A. “Partisanship, individual differences, and news media exposure as predictors of conspiracy beliefs.” Journalism & Mass Communication Quarterly 95.3 (2018): 691-713."
  },
  {
    "objectID": "slides/11-slides.html#model-specification",
    "href": "slides/11-slides.html#model-specification",
    "title": "POLS 1600",
    "section": "Model Specification",
    "text": "Model Specification\nTranslate these expectations into empirical models requires choices about how to specify our models\n\nHow should we measure/operationalize our outcome\n\nShould we measure beliefs about vaccines with 7-point ordinal scale or as a binary indicator of vaccine skepticism\n\nHow should we measure/operationalize our key predictor(s)\n\nShould we measure partisanship using a 7 point scale or as categorical variable?\n\nWhat should we control for in our model?\n\nFactors likely to predict both our outcome and our key predictor of interest\n\nThere are rarely definitive answers to these questions. Instead, we will often estimate multiple models to try and show that our findings are robust to alternative specifications"
  },
  {
    "objectID": "slides/11-slides.html#model-specification-1",
    "href": "slides/11-slides.html#model-specification-1",
    "title": "POLS 1600",
    "section": "Model Specification",
    "text": "Model Specification\nFor your projects, every group will almost surely estimate some form of the following:\n\nBaseline bivariate model: The simplest test of the relationship between your outcome and key predictor\nMultiple regression model: A test of the robustness of this relationship, controlling for alternative explanations"
  },
  {
    "objectID": "slides/11-slides.html#model-specification-2",
    "href": "slides/11-slides.html#model-specification-2",
    "title": "POLS 1600",
    "section": "Model Specification",
    "text": "Model Specification\nIn practice, I suspect you may estimate multiple regression models such as:\n\nAlternative specifications/operationalizations of outcomes and predictors\nInteraction models to test conditional relationships\nPolynomial models to test non-linear relationships"
  },
  {
    "objectID": "slides/11-slides.html#section-1",
    "href": "slides/11-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Translating Theoretical Claims into Empirical Expectations\nBefore we estimate our models in R, we will write down our models formally and empirical implications of our theoretical expectations in terms of the coefficients of our model.\nFor example, our baseline model might be:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} + X\\beta + \\epsilon\\]\nIf \\(\\beta_1\\) is positive this is consistent with H1a (greater skepticism among Republicans), - If \\(\\beta_2\\) is negative this is consistent with H1b (greater skepticism among Democrats),\n\nBut how could we test H1c – greater skepticism among Independents, who are “4s” on \\(\\text{PID}_{7pt}\\)?"
  },
  {
    "objectID": "slides/11-slides.html#section-2",
    "href": "slides/11-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Translating Theoretical Claims into Empirical Expectations\nWe could fit a polynomial regression, including both partisanship and “partissanship squared” to allow the relationship between partisanship and vaccine skepticism to vary non-linearly\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} +  \\beta_2 \\text{PID}_{7pt}^2+ X\\beta+ \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides.html#section-3",
    "href": "slides/11-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Translating Theoretical Claims into Empirical Expectations\nOr we could estimate a model treating Partisanship as a categorical variable rather than an ordinal interval variable.\nIn our recoding, we set \"Democrat\" to be the first level of the variable pid3cat, so the model R will estimate by default is:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{Ind} +  \\beta_2 \\text{PID}_{Rep}+ X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides.html#testing-differences-over-time",
    "href": "slides/11-slides.html#testing-differences-over-time",
    "title": "POLS 1600",
    "section": "Testing differences over time",
    "text": "Testing differences over time\nTesting Hypotheses 2 and 3 involve making comparisons across models estimated on data from different surveys.\nFormally, testing these expectations is a little more complicated\n\nwe could pool our two surveys together include an interaction term for survey year\n\nFor our purposes, we’ll treat these as more qualitative/exploratory hypotheses:\n\nH2a/b implies overall rates of vaccine skepticism will be lower/higher in 2020 compared to 2016\nH3 implies that whatever partisan differences we find in 2016 should be larger in 2020."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-2",
    "href": "slides/11-slides.html#progress-report-2",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations 📥\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models",
    "href": "slides/11-slides.html#estimating-empirical-models",
    "title": "POLS 1600",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nHaving derived empirical implications of our theoretical expectations expressed in terms of linear regressions, now we simply have to estimate our models in R.\nWhen estimating the same model on different datasets we can write the formulas once\n\nf1 &lt;- formula(vaccine_skeptic01 ~ pid + age)\nf2 &lt;- formula(vaccine_skeptic01 ~ pid + I(pid^2) + age)\nf3 &lt;- formula(vaccine_skeptic01 ~ pid3cat + age)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-1",
    "href": "slides/11-slides.html#estimating-empirical-models-1",
    "title": "POLS 1600",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nAnd then pass it to lm() with different data arguments:\n\nm1_2016 &lt;- lm(formula = f1, data = nes16)\nm1_2020 &lt;- lm(formula = f1, data = nes20)\nm2_2016 &lt;- lm(formula = f2, data = nes16)\nm2_2020 &lt;- lm(formula = f2, data = nes20)\nm3_2016 &lt;- lm(formula = f3, data = nes16)\nm3_2020 &lt;- lm(formula = f3, data = nes20)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-2",
    "href": "slides/11-slides.html#estimating-empirical-models-2",
    "title": "POLS 1600",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\n\nIf you’ve:\n\ncoded your data correctly\ndeveloped clear testable implications from your theoretical expectations\n\nSpecifying and estimating empirical models is straightforward. Literally a few lines of code."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-3",
    "href": "slides/11-slides.html#progress-report-3",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations ✅\nPresent our results 📥\n\nTables\nFigures\nConfidence intervals\nHypothesis testing"
  },
  {
    "objectID": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "href": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "title": "POLS 1600",
    "section": "Presenting and Interpreting Your Results",
    "text": "Presenting and Interpreting Your Results\nPresenting and interpreting your results is requires both art and science.\nYour goal is to tell a story with your results,\nLet’s start by producing a regression table, which provides a concise summary of multiple regression models."
  },
  {
    "objectID": "slides/11-slides.html#regression-tables",
    "href": "slides/11-slides.html#regression-tables",
    "title": "POLS 1600",
    "section": "Regression Tables",
    "text": "Regression Tables\n\nTasks Code Basic Fetch\n\n\n\n\nGiving the variables in substantive names\nReporting coefficients to 3 decimal places\nUsing a single significance threshold of \\(p &lt; 0.05\\)\nGiving the models custom names\nAdding a header to group models by year\nChanging the caption of the table\n\n\n\n\n\n# Basic\ntab_basic &lt;- texreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020)\n)\n\n# Formatted\ntab_fetch &lt;- texreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020),\n  # Reporting coefficients to 3 decimal places\n  digits = 3,\n  # Using a single significance threshold \n  stars = 0.05,\n  # Giving the variables in substantive names\n  custom.coef.names = c(\n    \"(Intercept)\",\n    \"PID (7pt)\",\n    \"Age\",\n    \"PID&lt;sup&gt;2&lt;/sup&gt; (7pt)\",\n    \"Independent\",\n    \"Republican\"\n  ),\n  # Use SE instead o CIs\n  include.ci = F,\n  # Giving the models custom names\n  custom.model.names = paste(\"(\",c(1:6),\")\", sep=\"\"),\n  # Adding a header to group models by year\n  custom.header = list(\"NES 2016\" = 1:3, \"NES 2020\" = 4:6),\n  # Changing the caption of the table\n  caption = \"Partisanship and Vaccine Skepticism\"\n)\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.46***\n\n\n0.35***\n\n\n0.42***\n\n\n0.34***\n\n\n0.32***\n\n\n0.35***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\npid\n\n\n-0.00\n\n\n0.06***\n\n\n \n\n\n0.02***\n\n\n0.04***\n\n\n \n\n\n\n\n \n\n\n(0.00)\n\n\n(0.02)\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n \n\n\n\n\nage\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\npid^2\n\n\n \n\n\n-0.01***\n\n\n \n\n\n \n\n\n-0.00\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n\n\npid3catIndependent\n\n\n \n\n\n \n\n\n0.17***\n\n\n \n\n\n \n\n\n0.20***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\npid3catRepublican\n\n\n \n\n\n \n\n\n-0.02\n\n\n \n\n\n \n\n\n0.10***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#telling-a-story-with-regression",
    "href": "slides/11-slides.html#telling-a-story-with-regression",
    "title": "POLS 1600",
    "section": "Telling a Story with Regression",
    "text": "Telling a Story with Regression\n\nFirst, provide an overview the models presented in the table\n\nExplain what each model is doing conceptually\n\nThen start with your simplest model (first column)\n\nUse this as a chance to explain core concepts from the course\n\nWhat is regression\nHow should I interpret a coefficient substantively\nHow should I interepret the statistical signficance of a give coefficient\n\nAs you move from left to right (simple to more complex)\n\nyou need not interpret every single coefficient in the model\ninstead highlight the factors that are important for the reader to note (e.g. a comparison between one coefficient in model or another.)"
  },
  {
    "objectID": "slides/11-slides.html#example",
    "href": "slides/11-slides.html#example",
    "title": "POLS 1600",
    "section": "Example",
    "text": "Example\nTable 1 presents the results of three specifications exploring the relationship between partisanship and vaccine skepticism using data from the 2016 (Models 1-3) and 2020 (Models 4-5) National Election Studies.\nModels 1 and 4 operationalize partisanship as a 7-point scale, where 1 corresponds to Strong Democrats, 4 to Indepndents, and 7 to Strong Republicans in the 2016 (Model 1) and 2020 (Model 2) surveys.\nModels 2 and 5 allow the relationship between partisanship and vaccine skepticism to vary non-linear again for the 2016 (Model 2) and 2020 (Model 5) elections.\nModels 3 and 6 treat partisanship as categorical variable, describing how Independents and Republicans differ from Democrats, the reference category in these models.\nAll models control age, since (put in substantive justification for controlling for age here)"
  },
  {
    "objectID": "slides/11-slides.html#story-testing-for-partisan-differences",
    "href": "slides/11-slides.html#story-testing-for-partisan-differences",
    "title": "POLS 1600",
    "section": "Story: Testing for Partisan Differences",
    "text": "Story: Testing for Partisan Differences\n\nThe results from Model 1 provide little initial evidence for partisan differences in vaccine skepticism in the 2016 Election.\n\nThe coefficient on the partisanship variable is -0.005, suggesting that a unit increase in partisanship (going from being a Strong Democrat to just a Democrat, or an Independent to an independent who leans Republican), is associated with just a 0.5 percentage point increase in the probability of being a vaccine skeptic (believing that the risks of vaccination outweigh the benefits or that their is no difference in the risks versus benefits).\n\nFurthermore the 95-percent confidence interval for this estimate (-0.011, 0.002) brackets 0, suggesting the true population estimate from this model could be either positive or negative. Similarly, we fail to reject the null hypothesis that the true coefficient on partisanship in this model is 0 as the test statistic for this estimate ( -1.38) corresponds to a p-value of 0.168 suggesting that we would see test statistics this large or larger fairly often when the true relationship was 0.\nIn sum, the results from Model 1 provide little support for any of the expectations described by H1"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "href": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "title": "POLS 1600",
    "section": "Testing for Partisan Differences: Model 2",
    "text": "Testing for Partisan Differences: Model 2\n\nWhile coefficients from Model 1 suggest little evidence of partisan differences in vaccine skepticism, the coefficients on both partisanship, and partisanship squared are statistically significant (p &lt; 0.05)."
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2",
    "href": "slides/11-slides.html#interpreting-model-2",
    "title": "POLS 1600",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nTask FigureInterpretation\n\n\n\nThe coefficients from polynomial regressions can be difficult to interpret jointly and so Figure 1 presents the predicted values from Model 2, holding age constant at its sample mean.\n\n\n\n\npred_df_m2 &lt;- expand_grid(\n  pid = 1:7,\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m2 &lt;- cbind(pred_df_m2, predict(m2_2016,pred_df_m2, interval =\"confidence\"))\n\nfig_m2 &lt;- pred_df_m2 %&gt;%\n  ggplot(aes(pid, fit, ymin =lwr, ymax =upr))+\n  geom_line()+\n  geom_ribbon(alpha=.2, fill=\"grey\")+\n  theme_bw()+\n  labs(x = \"Partisanship\",\n       y = \"Predicted Vaccine Skepticism\",\n       title = \"Independents are the most skeptical of vaccines\",\n       subtitle = \"Data: 2016 NES\"\n       )\n\n\n\n\nfig_m2\n\n\n\n\n\n\n\n\n\n\nWe see from Model 2 that 29.7 percent [27.3%, 32.1%] of Independents in the 2016 NES were predicted to be vaccine skeptics compared to 23.7 percent [20.8%, 26.5%] of Strong Democrats and only 20.1 percent [16.9%, 23.3%] of Strong Republicans."
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-3",
    "href": "slides/11-slides.html#interpreting-model-3",
    "title": "POLS 1600",
    "section": "Interpreting Model 3",
    "text": "Interpreting Model 3\n\nInterpretation Code\n\n\nModel 3 tells a similar story to model 2. Again, adjusting for differences in vaccine skepticism explained by age, Model 3 predicts that 41.7 percent [37.7%, 45.6%] of Independents in the 2016 NES are vaccine skeptics compared to 24.2 percent [22.1%, 26.2%] of Democrats, and 22.6 percent [20.4%, 24.8%] of Republicans.\nNote the coefficients from Model 3 imply that the differences between Independents and Democrats are statistically significant (\\(\\beta_{Ind} = 0.175, p &lt; 0.05\\)), the differences between Republicans and Democrats are not (\\(\\beta_{Rep} = -0.004, p = 0.31\\))\n\n\n\npred_df_m3 &lt;- expand_grid(\n  pid3cat = c(\"Democrat\", \"Independent\",\"Republican\"),\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m3 &lt;- cbind(pred_df_m3, predict(m3_2016,pred_df_m3, interval =\"confidence\"))\npred_df_m3\n\n      pid3cat      age       fit       lwr       upr\n1    Democrat 49.58231 0.2419547 0.2211228 0.2627867\n2 Independent 49.58231 0.4169043 0.3773539 0.4564547\n3  Republican 49.58231 0.2261496 0.2038046 0.2484947"
  },
  {
    "objectID": "slides/11-slides.html#section-5",
    "href": "slides/11-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "tab_fetch\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-differences-over-time",
    "href": "slides/11-slides.html#testing-for-differences-over-time",
    "title": "POLS 1600",
    "section": "Testing for Differences Over Time",
    "text": "Testing for Differences Over Time\nThe results for the 2016 NES suggest political independents are most skeptical of vaccines.\nThe results for 2020 suggest the relationship between partisanship and vaccine skepticism has changed overtime.\n\nThe coefficient on partisanship in model 4 is now positive and statistically significant (p &lt; 0.05), suggesting that as respondents become more Republican, they are more likely to be skeptical of vaccines\nThe coefficients from Model 5 suggest the relationship between partisanship skepticism is non linear, which is confirmed by model 6.\nIn Model 6, we see that independents remain the most skeptical of vaccines in 2020 \\((\\beta = 0.20,\\, p &lt;0.05)\\), but that Republicans now tend to be more skeptical of vaccines than Democrats \\((\\beta = 0.10,\\, p &lt;0.05)\\)\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/05-slides.html#overview-1",
    "href": "slides/05-slides.html#overview-1",
    "title": "POLS 1600",
    "section": "Overview",
    "text": "Overview\n\nAnnouncements\nSetup\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/05-slides.html#learing-goals",
    "href": "slides/05-slides.html#learing-goals",
    "title": "POLS 1600",
    "section": "Learing goals",
    "text": "Learing goals\n\nIntroduce the concept of Directed Acyclic Graphs to describe causal relationships and illustrate potential bias from confounders and colliders\nDiscuss three approaches to covariate adjustment\n\nSubclassification\nMatching\nLinear Regression\n\nBegin discussing three research designs to make causal claims with observational data\n\nDifferences-in-Differences (If there’s time)\nRegression Discontinuity Designs\nInstrumental Variables"
  },
  {
    "objectID": "slides/05-slides.html#annoucements",
    "href": "slides/05-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nSit with your groups (for now)\nUpdated timeline for final projects next week"
  },
  {
    "objectID": "slides/05-slides.html#group-assignments",
    "href": "slides/05-slides.html#group-assignments",
    "title": "POLS 1600",
    "section": "Group Assignments",
    "text": "Group Assignments"
  },
  {
    "objectID": "slides/05-slides.html#what-did-we-like",
    "href": "slides/05-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/05-slides.html#what-did-we-dislike",
    "href": "slides/05-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/05-slides.html#grinding-an-iron-pestle-into-a-needle",
    "href": "slides/05-slides.html#grinding-an-iron-pestle-into-a-needle",
    "title": "POLS 1600",
    "section": "Grinding an Iron Pestle into a Needle",
    "text": "Grinding an Iron Pestle into a Needle\n\n\n\nMe\n\nLess is more\nGo slow\nProvide labs/code earlier\nAdapt assignments/policies\n\n\n\nYou\n\nActive reading\nDo tutorials\nReview labs before class\nReview comments after class\nAsk for help\nDon’t give up!"
  },
  {
    "objectID": "slides/05-slides.html#review-1",
    "href": "slides/05-slides.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review\n\nData wrangling\nDescriptive Statistics\nLevels of understanding\nData visualization"
  },
  {
    "objectID": "slides/05-slides.html#data-wrangling-1",
    "href": "slides/05-slides.html#data-wrangling-1",
    "title": "POLS 1600",
    "section": "Data wrangling",
    "text": "Data wrangling\n\n\n\nYou're learning how to map conceptual tasks to commands in R\n\n\nSkill\nCommon Commands\n\n\n\n\nSetup R\nlibrary(), ipak()\n\n\nLoad data\nread_csv(), load()\n\n\nGet HLO of data\ndf$x, glimpse(), table(), summary()\n\n\nTransform data\n&lt;-, mutate(), ifelse(), case_when()\n\n\nReshape data\npivot_longer(), left_join()\n\n\nSummarize data numerically\nmean(), median(), summarise(), group_by()\n\n\nSummarize data graphically\nggplot(), aes(), geom_"
  },
  {
    "objectID": "slides/05-slides.html#mapping-concepts-to-code",
    "href": "slides/05-slides.html#mapping-concepts-to-code",
    "title": "POLS 1600",
    "section": "Mapping Concepts to Code",
    "text": "Mapping Concepts to Code\n\nTakes time and practice\nDon’t be afraid to FAAFO\nDon’t worry about memorizing everything.\nStatistical programming is necessary to actually do empirical research\nLearning to code will help us understand statistical concepts.\nLearning to think programmatically and algorithmically will help us tackle complex problems"
  },
  {
    "objectID": "slides/05-slides.html#descriptive-statistics",
    "href": "slides/05-slides.html#descriptive-statistics",
    "title": "POLS 1600",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nDescriptive statistics help us describe what’s typical of our data\nWhat’s a typical value in our data\n\nMean\nMedian\nMode\n\nHow much do our data vary?\n\nVariance\nStandard deviation\n\nAs one variable changes how does another change?\n\nCovariance\nCorrelation\n\nDescriptive statistics are:\n\nDiagnostic\nGenerative"
  },
  {
    "objectID": "slides/05-slides.html#levels-of-understanding-in-pols-1600",
    "href": "slides/05-slides.html#levels-of-understanding-in-pols-1600",
    "title": "POLS 1600",
    "section": "Levels of understanding in POLS 1600",
    "text": "Levels of understanding in POLS 1600\n\nConceptual\nPractical\nDefinitional\nTheoretical\n\n\nLet’s illustrate these different levels of understanding about our old friend the mean"
  },
  {
    "objectID": "slides/05-slides.html#mean-conceptual-understanding",
    "href": "slides/05-slides.html#mean-conceptual-understanding",
    "title": "POLS 1600",
    "section": "Mean: Conceptual Understanding",
    "text": "Mean: Conceptual Understanding\nA mean is:\n\nA common and important measure of central tendency (what’s typical)\nIt’s the arithmetic average you learned in school\nWe can think of it as the balancing point of a distribution\nA conditional mean is the average of one variable \\(X\\), when some other variable, \\(Z\\) takes a value \\(z\\)\n\nThink about the average height in our class (unconditional mean) vs the average height among men and women ([conditional means].{blue})"
  },
  {
    "objectID": "slides/05-slides.html#mean-as-a-balancing-point",
    "href": "slides/05-slides.html#mean-as-a-balancing-point",
    "title": "POLS 1600",
    "section": "Mean as a balancing point",
    "text": "Mean as a balancing point\n\nSource"
  },
  {
    "objectID": "slides/05-slides.html#mean-practical",
    "href": "slides/05-slides.html#mean-practical",
    "title": "POLS 1600",
    "section": "Mean: Practical",
    "text": "Mean: Practical\nThere are lots of ways to calculate means in R\n\nThe simplest is to use the mean() function\n\nIf our data have missing values, we need to to tell R to remove them\n\n\n\nmean(df$x, na.rm=T)"
  },
  {
    "objectID": "slides/05-slides.html#conditional-means-practical",
    "href": "slides/05-slides.html#conditional-means-practical",
    "title": "POLS 1600",
    "section": "Conditional Means: Practical",
    "text": "Conditional Means: Practical\n\nTo calculate a conditional mean we could us a logical index [df$z == 1]\n\n\nmean(df$x[df$z == 1], na.rm=T)\n\n\nIf we wanted to a calculate a lot of conditional means we could use the mean() in combination with group_by() and summarise()\n\n\ndf %&gt;% \n  group_by(z)%&gt;%\n  summarise(\n    x = mean(x, na.rm=T)\n  )"
  },
  {
    "objectID": "slides/05-slides.html#mean-definitional",
    "href": "slides/05-slides.html#mean-definitional",
    "title": "POLS 1600",
    "section": "Mean: Definitional",
    "text": "Mean: Definitional\nFormally, we define the arithmetic mean of \\(x\\) as \\(\\bar{x}\\):\n\\[\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}\n\\]\nIn words, this formula says, to calculate the average of x, we sum up all the values of \\(x_i\\) from observation \\(i=1\\) to \\(i=n\\) and then divide by the total number of observations \\(n\\)"
  },
  {
    "objectID": "slides/05-slides.html#mean-definitional-1",
    "href": "slides/05-slides.html#mean-definitional-1",
    "title": "POLS 1600",
    "section": "Mean: Definitional",
    "text": "Mean: Definitional\n\nIn this class, I don’t put a lot of weight on memorizing definitions (that’s what Google’s for).\nBut being comfortable with “the math” is important and useful\nDefinitional knowledge is a prerequisite for understanding more theoretical claims."
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical",
    "href": "slides/05-slides.html#mean-theoretical",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nSuppose I asked you to show that the sum of deviations from a mean equals 0?\n\\[\n\\text{Claim:} \\sum_{i=1}^n (x_i -\\bar{x}) = 0\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical-1",
    "href": "slides/05-slides.html#mean-theoretical-1",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nKnowing the definition of an arithmetic mean, we could write:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (x_i -\\bar{x}) &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n\\bar{x} & \\text{Distribute Summation}\\\\\n              &= \\sum_{i=1}^n x_i - n\\bar{x} & \\text{Summing a constant, } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - n\\times \\left ( \\frac{1}{n} \\sum_{i=1}^n{x_i}\\right ) & \\text{Definition of } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n{x_i} & n \\times \\frac{1}{n}=1\\\\\n              &= 0             \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical-2",
    "href": "slides/05-slides.html#mean-theoretical-2",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nWhy do we care?\n\nShowing the deviations sum to 0 is another way of saying the mean is a balancing point.\nThis turns out to be a useful property of means that will reappear throughout the course\nIf I asked you to make a prediction, \\(\\hat{x}\\) of a random person’s height in this class, the mean would have the lowest mean squared error (MSE \\(=\\frac{1}{n}\\sum (x_i - \\hat{x_i})^2)\\)"
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical-3",
    "href": "slides/05-slides.html#mean-theoretical-3",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nOccasionally, you’ll read or here me say say things like:\n\nThe sample mean is an unbiased estimator of the population mean\n\nIn a statistics class, we would take time to prove this."
  },
  {
    "objectID": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean",
    "href": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean",
    "title": "POLS 1600",
    "section": "The sample mean is an unbiased estimator of the population mean",
    "text": "The sample mean is an unbiased estimator of the population mean\nClaim:\nLet \\(x_1, x_2, \\dots x_n\\) from a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\nThen:\n\\[\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n x_i\\right )\n\\]\nis an unbiased estimator of \\(\\mu\\)\n\\[\nE[\\bar{x}] = \\mu\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean-1",
    "href": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean-1",
    "title": "POLS 1600",
    "section": "The sample mean is an unbiased estimator of the population mean",
    "text": "The sample mean is an unbiased estimator of the population mean\nProof:\n\\[\n\\begin{aligned}\nE\\left [\\bar{x} \\right] &= E\\left [\\frac{1}{n}\\left (\\sum_{i=1}^n x_i \\right) \\right] & \\text{Definition of } \\bar{x} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^nE\\left [ x_i \\right]  & \\text{Linearity of Expectations} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu  & E[x_i] = \\mu \\\\\n&= \\frac{n}{n}  \\mu  & \\sum_{i=1}^n \\mu = n\\mu \\\\\n&= \\mu  & \\blacksquare \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#levels-of-understanding-1",
    "href": "slides/05-slides.html#levels-of-understanding-1",
    "title": "POLS 1600",
    "section": "Levels of understanding",
    "text": "Levels of understanding\n\nIn this course, we tend to emphasize the\n\nConceptual\nPractical\n\nOver\n\nDefinitional\nTheoretical\n\nIn an intro statistics class, the ordering might be reversed.\nTrade offs:\n\n\nPro: We actually get to work with data and do empirical research much sooner\nCons: We substitute intuitive understandings for more rigorous proofs"
  },
  {
    "objectID": "slides/05-slides.html#data-visualization-1",
    "href": "slides/05-slides.html#data-visualization-1",
    "title": "POLS 1600",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nThe grammar of graphics\nAt minimum you need:\n\ndata\naesthetic mappings\ngeometries\n\nTake a sad plot and make it better by:\n\nlabels\nthemes\nstatistics\ncooridnates\nfacets\ntransforming your data before plotting"
  },
  {
    "objectID": "slides/05-slides.html#you-are-about-to-be-reincarnated-hlo",
    "href": "slides/05-slides.html#you-are-about-to-be-reincarnated-hlo",
    "title": "POLS 1600",
    "section": "You are about to be reincarnated: HLO",
    "text": "You are about to be reincarnated: HLO\n\ndf$reincarnation\n\n&lt;labelled&lt;double&gt;[10]&gt;: You're about to be re-incarnated. Do you want to come back as a:\n [1] 1 1 1 7 6 1 6 6 7 1\n\nLabels:\n value                  label\n     1    Animal/land dweller\n     6       Bird/air dweller\n     7     Fish/water dweller\n     2                 Insect\n     9                  Plant\n    10 Single-celled organism\n\ntable(df$reincarnation)\n\n\n1 6 7 \n5 3 2"
  },
  {
    "objectID": "slides/05-slides.html#basic-plot",
    "href": "slides/05-slides.html#basic-plot",
    "title": "POLS 1600",
    "section": "Basic Plot",
    "text": "Basic Plot\n\nCodeFigure\n\n\n\ndf %&gt;%\n  ggplot(aes(x = reincarnation, \n             fill = reincarnation))+\n  geom_bar(\n    stat = \"count\"\n  )"
  },
  {
    "objectID": "slides/05-slides.html#use-a-factor-to-label-and-order-responses",
    "href": "slides/05-slides.html#use-a-factor-to-label-and-order-responses",
    "title": "POLS 1600",
    "section": "Use a factor to label and order responses",
    "text": "Use a factor to label and order responses\n\nRecodeCheck recoding\n\n\n\ndf %&gt;%\n  mutate(\n    # Turn numeric values into factor labels \n    Reincarnation = forcats::as_factor(reincarnation),\n    # Order factor in decreasing frequency of levels\n    Reincarnation = forcats::fct_infreq(Reincarnation),\n    # Reverse order so levels are increasing in frequency\n    Reincarnation = forcats::fct_rev(Reincarnation),\n    # Rename explanations\n    Why = reincarnation_why\n  ) -&gt; df\n\n\n\n\ntable(recode= df$Reincarnation, original = df$reincarnation)\n\n                        original\nrecode                   1 6 7\n  Single-celled organism 0 0 0\n  Plant                  0 0 0\n  Insect                 0 0 0\n  Fish/water dweller     0 0 2\n  Bird/air dweller       0 3 0\n  Animal/land dweller    5 0 0"
  },
  {
    "objectID": "slides/05-slides.html#revised-figure",
    "href": "slides/05-slides.html#revised-figure",
    "title": "POLS 1600",
    "section": "Revised figure",
    "text": "Revised figure\n\nCodeRevised Figure\n\n\n\ndf %&gt;% # Data\n  # Aesthetics\n  ggplot(aes(x = Reincarnation, \n             fill = Reincarnation))+\n  # Geometry\n  geom_bar(stat = \"count\")+ # Statistic\n  ## Include levels of Reincarnation w/ no values\n  scale_x_discrete(drop=FALSE)+\n  # Don't include a legend\n  scale_fill_discrete(drop=FALSE, guide=\"none\")+\n  # Flip x and y\n  coord_flip()+\n  # Remove lines\n  theme_classic() -&gt; fig1"
  },
  {
    "objectID": "slides/05-slides.html#section",
    "href": "slides/05-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "What creature and why?"
  },
  {
    "objectID": "slides/05-slides.html#adding-labelled-values",
    "href": "slides/05-slides.html#adding-labelled-values",
    "title": "POLS 1600",
    "section": "Adding labelled values",
    "text": "Adding labelled values\n\nRecodesRecode OutputAggregate Data\n\n\n\ndf %&gt;%\n  mutate(\n    # Create numeric id\n    id = 1:n(),\n    # Create a label with 3 answers and NA elsewhere\n    Label = case_when(\n      id == 10 ~ str_wrap(reincarnation_why[10],30),\n      id == 4 ~ str_wrap(reincarnation_why[4],30),\n      id == 7 ~ str_wrap(reincarnation_why[7],30),\n      TRUE ~ NA_character_\n\n    )\n\n  ) -&gt; df\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate totals before calling ggplot\nplot_df &lt;- df %&gt;%\n  group_by(Reincarnation)%&gt;% \n  summarise( \n    Count = n(), \n    Why = na.omit(unique(Label)) \n  )"
  },
  {
    "objectID": "slides/05-slides.html#youre-about-to-be-reincarnated",
    "href": "slides/05-slides.html#youre-about-to-be-reincarnated",
    "title": "POLS 1600",
    "section": "You’re about to be reincarnated:",
    "text": "You’re about to be reincarnated:\n\nAggregate dfRevised Figure CodeLabelled Figure\n\n\n\n\n\n\n\n\n\n\n\nplot_df %&gt;%\n  ggplot(aes(x = Reincarnation, \n             y = Count,\n             fill = Reincarnation, \n             label=Why))+\n  geom_bar(stat = \"identity\")+ #&lt;&lt;\n  ## Include levels of Reincarnation w/ no values\n  scale_x_discrete(drop=FALSE)+\n  # Don't include a legend\n  scale_fill_discrete(drop=FALSE, guide=\"none\")+\n  coord_flip()+\n  labs(x = \"\",y=\"\",title=\"You're about to be reincarnated.\\nWhat do you want to come back as?\")+\n  theme_classic()+\n  ggrepel::geom_label_repel(\n    fill=\"white\",\n    nudge_y = 1, \n    hjust = \"left\",\n    size=3,\n    arrow = arrow(length = unit(0.015, \"npc\"))\n    )+ \n  scale_y_continuous(\n    breaks = c(0,2,4,6,8,10,12),\n    expand = expansion(add =c(0,6))\n    ) -&gt; fig1"
  },
  {
    "objectID": "slides/05-slides.html#data-visualization-is-an-iterative-process",
    "href": "slides/05-slides.html#data-visualization-is-an-iterative-process",
    "title": "POLS 1600",
    "section": "Data visualization is an iterative process",
    "text": "Data visualization is an iterative process\n\nData visualization is an iterative process\nGood data viz requires lots of data transformations\nStart with a minimum working example and build from there\nDon’t let the perfect be the enemy of the good enough."
  },
  {
    "objectID": "slides/05-slides.html#new-packages",
    "href": "slides/05-slides.html#new-packages",
    "title": "POLS 1600",
    "section": "New packages",
    "text": "New packages\nThis week’s lab we’ll be using the dataverse package to download data on presidential elections\nNext week’s lab, we’ll be using the tidycensus package to download census data.\nWe’ll also need to install a census API to get the data.\nHere’s a detailed guide of what we’ll do in class right now."
  },
  {
    "objectID": "slides/05-slides.html#install-new-packages",
    "href": "slides/05-slides.html#install-new-packages",
    "title": "POLS 1600",
    "section": "Install new packages",
    "text": "Install new packages\nThese packages are easier to install live:\n\ninstall.packages(\"dataverse\")\ninstall.packages(\"tidycensus\")\ninstall.packages(\"easystats\")\ninstall.packages(\"DeclareDesign\")"
  },
  {
    "objectID": "slides/05-slides.html#census-api",
    "href": "slides/05-slides.html#census-api",
    "title": "POLS 1600",
    "section": "Census API",
    "text": "Census API\nPlease follow these steps so you can download data directly from the U.S. Census here:\n\nInstall the tidycensus package\nLoad the installed package\nRequest an API key from the Census\nCheck your email\nActivate your key\nInstall your API key in R\nCheck that everything worked"
  },
  {
    "objectID": "slides/05-slides.html#packages-for-today",
    "href": "slides/05-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/05-slides.html#red-covid",
    "href": "slides/05-slides.html#red-covid",
    "title": "POLS 1600",
    "section": "Red Covid",
    "text": "Red Covid\n\n\n\nRed Covid New York Times, 27 September, 2021\n\n Red Covid, an Update New York Times, 18 February, 2022"
  },
  {
    "objectID": "slides/05-slides.html#preview-of-the-lab",
    "href": "slides/05-slides.html#preview-of-the-lab",
    "title": "POLS 1600",
    "section": "Preview of the Lab",
    "text": "Preview of the Lab\nPlease download Thursday’s lab here\n\nConceptually, this lab is designed to help reinforce the relationship between linear models like \\(y=\\beta_0 + \\beta_1x\\) and the conditional expectation function \\(E[Y|X]\\).\nSubstantively, we will explore whether David Leonhardt’s claims about Red Covid the political polarization of vaccines and its consequences"
  },
  {
    "objectID": "slides/05-slides.html#lab-questions-1-5-review",
    "href": "slides/05-slides.html#lab-questions-1-5-review",
    "title": "POLS 1600",
    "section": "Lab: Questions 1-5: Review",
    "text": "Lab: Questions 1-5: Review\nQuestions 1-5 are designed to reinforce your data wrangling skills. In particular, you will get practice:\n\nCreating and recoding variables using mutate()\nCalculating a moving average or rolling mean using the rollmean() function from the zoo package\nTransforming the data on presidential elections so that it can be merged with the data on Covid-19 using the pivot_wider() function.\nMerging data together using the left_join() function."
  },
  {
    "objectID": "slides/05-slides.html#lab-questions-6-10-simple-linear-regression",
    "href": "slides/05-slides.html#lab-questions-6-10-simple-linear-regression",
    "title": "POLS 1600",
    "section": "Lab: Questions 6-10: Simple Linear Regression",
    "text": "Lab: Questions 6-10: Simple Linear Regression\n\nIn question 6, you will see how calculating conditional means provides a simple test of “Red Covid” claim.\nIn question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)\nIn question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)\nIn question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.\nQuestion 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week’s lab."
  },
  {
    "objectID": "slides/05-slides.html#before-thursday",
    "href": "slides/05-slides.html#before-thursday",
    "title": "POLS 1600",
    "section": "Before Thursday",
    "text": "Before Thursday\nThe following slides provide detailed explanations of all the code you’ll need for each question.\n\nPlease run this code before class on Thursday\nWe will review this material together at the start of class, but you will spend most of our time on the Questions 6-10"
  },
  {
    "objectID": "slides/05-slides.html#q1-setup-your-workspace",
    "href": "slides/05-slides.html#q1-setup-your-workspace",
    "title": "POLS 1600",
    "section": "Q1: Setup your workspace",
    "text": "Q1: Setup your workspace\n\nTaskCode for Q1\n\n\nQ1 asks you to setup your workspace\nThis means loading and, if needed, installing the packages you will use.\n\n\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)"
  },
  {
    "objectID": "slides/05-slides.html#q2-load-the-data",
    "href": "slides/05-slides.html#q2-load-the-data",
    "title": "POLS 1600",
    "section": "Q2 Load the data",
    "text": "Q2 Load the data\nTo explore Leonhardt’s claims about Red Covid, we’ll need data on:\n\nCovid-19\nThe 2020 Presidential Election"
  },
  {
    "objectID": "slides/05-slides.html#q2.1-load-the-covid-19-data",
    "href": "slides/05-slides.html#q2.1-load-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Q2.1 Load the Covid-19 Data",
    "text": "Q2.1 Load the Covid-19 Data\nTo load data on Covid-19 just run this\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/05-slides.html#q2.2-load-election-data",
    "href": "slides/05-slides.html#q2.2-load-election-data",
    "title": "POLS 1600",
    "section": "Q2.2 Load Election Data",
    "text": "Q2.2 Load Election Data\n\nTaskCode for Q2.2\n\n\nQ2.2. asks you to write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse\n\nOnce you’ve installed the dataverse package you should be able to do this:\n\n\n\n\n# Try this code first\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- dataverse::get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# If the code above fails, comment out and uncomment the code below:\n\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))"
  },
  {
    "objectID": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset",
    "href": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset",
    "title": "POLS 1600",
    "section": "Q3 Describe the structure of each dataset",
    "text": "Q3 Describe the structure of each dataset\nQuestion 3 asks you to describe the structure of each dataset.\n\nSpecifically, it asks you to get a high level overview of covid and pres_df and describe the unit of analysis in each dataset:\n\nDescribe substantively what specific, observation each row in the dataset corresponds to\nIn covid covid dataset, the unit of analysis is a state-date"
  },
  {
    "objectID": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset-1",
    "href": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset-1",
    "title": "POLS 1600",
    "section": "Q3 Describe the structure of each dataset",
    "text": "Q3 Describe the structure of each dataset\nHere’s some possible code you could use to get a quick HLO of each dataset:\n\nHLO covidHLO pres_df\n\n\n\n# check names in `covid`\nnames(covid)\n\n# take a quick look values of each variable\n\nglimpse(covid)\n\n# Look at first few observations for:\n# date, administrative_area_level_2, \n\ncovid %&gt;% \n  select(date, administrative_area_level_2) %&gt;%\n  head()\n\n# Summarize data to get a better sense of the unit of observastion\n\ncovid %&gt;% \n  group_by(administrative_area_level_2) %&gt;%\n  summarise(\n    n = n(), # Number of observations for each state\n    start_date = min(date, na.rm = T),\n    end_date = max(date, na.rm=T)\n  ) -&gt; hlo_covid_df\n\nhlo_covid_df\n\n\n# How many unique values of date and state are their:\n\nn_dates &lt;- length(unique(covid$date))\nn_states &lt;- length(unique(covid$administrative_area_level_2))\nn_dates\nn_states\n\n# If we had observations for every state on every date then the number of rows \n# in the data \ndim(covid)[1]\n# Should equal\ndim(covid)[1] == n_dates * n_states\n\n# This is what economists would call an unbalanced panel\n\n\n\n\n# check names in `pres_df`\nnames(pres_df)\n\n# take a quick look values of each variable\n\nglimpse(pres_df)\n\n# Unit of analysis is a year-state-candidate\npres_df %&gt;% \n  select(year, state_po, candidate) %&gt;%\n  head()\n\n# How many states?\nlength(unique(pres_df$state_po))\n\n\n\n# How many candidates and parties on the ballot in a given election year\npres_df %&gt;% \n  group_by(year) %&gt;%\n  summarise(\n    n_candidates = length(unique(candidate)),\n    # Look at both party_detailed and party_simplified\n    n_parties_detailed = length(unique(party_detailed)),\n    n_parties_simplified = length(unique(party_simplified))\n  ) -&gt; hlo_pres_df\nhlo_pres_df\n\n# Look at 2020\n# pres_df$candidate[pres_df$year == \"2020\"]"
  },
  {
    "objectID": "slides/05-slides.html#q4-recode-the-data-for-analysis",
    "href": "slides/05-slides.html#q4-recode-the-data-for-analysis",
    "title": "POLS 1600",
    "section": "Q4 Recode the data for analysis",
    "text": "Q4 Recode the data for analysis\nUsing our understanding of the structure of the data, Q4 asks you to:\n\nRecode the Covid-19 data like we’ve done before plus\nCalculate rolling means, 7 and 14 day averages\nReshape, recode, and filter the presidential election data"
  },
  {
    "objectID": "slides/05-slides.html#q4.1-recode-the-covid-19",
    "href": "slides/05-slides.html#q4.1-recode-the-covid-19",
    "title": "POLS 1600",
    "section": "Q4.1 Recode the Covid-19",
    "text": "Q4.1 Recode the Covid-19\n\nTaskCode for Q4.1Template Code for Q4.2\n\n\nThis is the same code we’ve used before to create covid_us from covid with the addition of code to calculate a rolling mean or moving average of the number of new cases\n\n\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\n\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/05-slides.html#q4.2-calculate-rolling-means-of-covid-deaths",
    "href": "slides/05-slides.html#q4.2-calculate-rolling-means-of-covid-deaths",
    "title": "POLS 1600",
    "section": "Q4.2 Calculate Rolling Means of Covid Deaths",
    "text": "Q4.2 Calculate Rolling Means of Covid Deaths\n\nTaskCode for Q4.2\n\n\nQ4.2 asks you to create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nIt encourages you to use the code new_cases_pc_7da as a template\nTo build your coding skills, try writing this yourself, then comparing it to the code in the next tab:\n\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/05-slides.html#rolling-averages",
    "href": "slides/05-slides.html#rolling-averages",
    "title": "POLS 1600",
    "section": "Rolling Averages",
    "text": "Rolling Averages\nThe next slides aren’t necessary for the lab but are designed to illustrate:\n\nthe concept of a rolling mean\nwhat the code does\nwhy might prefer rolling averages over daily values"
  },
  {
    "objectID": "slides/05-slides.html#look-at-the-output-of-zoorollmean",
    "href": "slides/05-slides.html#look-at-the-output-of-zoorollmean",
    "title": "POLS 1600",
    "section": "Look at the output of zoo::rollmean()",
    "text": "Look at the output of zoo::rollmean()\n\ncovid_us %&gt;%\n  filter(date &gt; \"2020-03-05\") %&gt;%\n  select(date,new_cases_pc,new_cases_pc_7day)\n\n# A tibble: 52,580 × 4\n# Groups:   state [51]\n   state     date       new_cases_pc new_cases_pc_7day\n   &lt;chr&gt;     &lt;date&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Minnesota 2020-03-06      NA                NA     \n 2 Minnesota 2020-03-07       0                NA     \n 3 Minnesota 2020-03-08       0.0177           NA     \n 4 Minnesota 2020-03-09       0                NA     \n 5 Minnesota 2020-03-10       0.0177           NA     \n 6 Minnesota 2020-03-11       0.0355           NA     \n 7 Minnesota 2020-03-12       0.0709           NA     \n 8 Minnesota 2020-03-13       0.0887            0.0329\n 9 Minnesota 2020-03-14       0.124             0.0507\n10 Minnesota 2020-03-15       0.248             0.0836\n# ℹ 52,570 more rows"
  },
  {
    "objectID": "slides/05-slides.html#comparing-daily-cases-to-rolling-average",
    "href": "slides/05-slides.html#comparing-daily-cases-to-rolling-average",
    "title": "POLS 1600",
    "section": "Comparing Daily Cases to Rolling Average",
    "text": "Comparing Daily Cases to Rolling Average\nThe following code illustrates how a 7-day rolling mean smooths (new_cases_pc_7da) over the noisiness of the daily measure\n\nCodeFigure\n\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2020-03-05\", \n         state == \"Minnesota\") %&gt;%\n  select(date,\n         new_cases_pc,\n         new_cases_pc_7day)%&gt;%\n  ggplot(aes(date,new_cases_pc ))+\n  geom_line(aes(col=\"Daily\"))+\n  # set y aesthetic for second line of rolling average\n  geom_line(aes(y = new_cases_pc_7day,\n                col = \"7-day average\")\n            ) +\n  theme(legend.position=\"bottom\")+\n    labs( col = \"Measure\",\n    y = \"New Cases Per 100k\", x = \"\",\n    title = \"Minnesota\"\n  ) -&gt; fig_covid_mn"
  },
  {
    "objectID": "slides/05-slides.html#q4.3-recode-presidential-data",
    "href": "slides/05-slides.html#q4.3-recode-presidential-data",
    "title": "POLS 1600",
    "section": "Q4.3 Recode Presidential data",
    "text": "Q4.3 Recode Presidential data\n\nTaskCode for Q4.3\n\n\nQ4.3 Gives you a long list of steps to recode, reshape, and filter pres_df to produce pres_df2020\nMost of this is review but it can seem like a lot.\nWalk through the provided code and see if you can map each conceptual step in Q4.3 to its implementation in the code\n\n\n\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes *100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df\n\n# Check Output:\n\nglimpse(pres2020_df)\n\nRows: 51\nColumns: 9\n$ state         &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\"…\n$ state_po      &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"F…\n$ year_election &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 20…\n$ totalvotes    &lt;dbl&gt; 2323282, 359530, 3387326, 1219069, 17500881, 3279980, 18…\n$ DEMOCRAT      &lt;dbl&gt; 849624, 153778, 1672143, 423932, 11110250, 1804352, 1080…\n$ REPUBLICAN    &lt;dbl&gt; 1441170, 189951, 1661686, 760647, 6006429, 1364607, 7147…\n$ dem_voteshare &lt;dbl&gt; 36.56999, 42.77195, 49.36469, 34.77506, 63.48395, 55.011…\n$ rep_voteshare &lt;dbl&gt; 62.031643, 52.833143, 49.055981, 62.395730, 34.320724, 4…\n$ winner        &lt;fct&gt; Trump, Trump, Biden, Trump, Biden, Biden, Biden, Biden, …"
  },
  {
    "objectID": "slides/05-slides.html#q5-merging-data",
    "href": "slides/05-slides.html#q5-merging-data",
    "title": "POLS 1600",
    "section": "Q5 merging data",
    "text": "Q5 merging data\n\nTaskMerge election data into Covid data\n\n\nQ5 asks you to merge the 2020 election data from pres2020_df into covid_us using the common state variable in each dataset using the function left_join()\n\n\n\ndim(covid_us)\n\n[1] 53678    61\n\ndim(pres2020_df)\n\n[1] 51  9\n\ncovid_us &lt;- covid_us %&gt;% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us) \n\n[1] 53678    69"
  },
  {
    "objectID": "slides/05-slides.html#advice-for-merging",
    "href": "slides/05-slides.html#advice-for-merging",
    "title": "POLS 1600",
    "section": "Advice for merging",
    "text": "Advice for merging\n\nAdviceIllustration\n\n\nWhen merging datasets:\n\nCheck the matches in your joining variables\n\nMake sure the values state are the same in each dataset\nCheck for differences in spelling, punctuation, etc.\n\nCheck the dimensions of output of your left_join()\n\nIf there is a 1-1 match the number of rows should be the same before after\n\n\n\n\n\n# Should be 51 states and DC in each\nsum(unique(pres_df$state) %in% covid_us$state)\n\n[1] 0\n\n# Look at each state variable\n## With [] index\npres_df$state[1:5]\n\n[1] \"ALABAMA\" \"ALABAMA\" \"ALABAMA\" \"ALABAMA\" \"ALABAMA\"\n\ncovid_us$state[1:5]\n\n[1] \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\"\n\n# Matching is case sensitive \n\n# make pres_df$state title case\n\n## Base R:\npres_df$state &lt;- str_to_title(pres_df$state )\n## Tidy R:\npres_df %&gt;% \n  mutate(\n    state = str_to_title(state )\n  ) -&gt; pres_df\n\n# Should be 51\nsum(unique(pres_df$state) %in% covid_us$state)\n\n[1] 50\n\n# Find the mismatch:\nunique(pres_df$state[!pres_df$state %in% covid_us$state])\n\n[1] \"District Of Columbia\"\n\n# Two equivalent ways to fix this mismatch\n## Base R: Quick fix to change spelling of DC\npres_df$state[pres2020_df$state == \"District Of Columbia\"] &lt;- \"District of Columbia\"\n\n## Tidy R: Quick fix to change spelling of DC\n\npres_df %&gt;% \n  mutate(\n    state = ifelse(test = state == \"District Of Columbia\",\n                   yes = \"District of Columbia\",\n                   no = state\n                   )\n  ) -&gt; pres_df\n\n\n# Problem Solved\nsum(unique(pres2020_df$state) %in% covid_us$state)\n\n[1] 51"
  },
  {
    "objectID": "slides/05-slides.html#causal-inference-is-about-counterfactual-comparisons",
    "href": "slides/05-slides.html#causal-inference-is-about-counterfactual-comparisons",
    "title": "POLS 1600",
    "section": "Causal inference is about counterfactual comparisons",
    "text": "Causal inference is about counterfactual comparisons\n\nCausal inference is about counterfactual comparisons\n\nWhat would have happened if some aspect of the world either had or had not been present"
  },
  {
    "objectID": "slides/05-slides.html#causal-identification",
    "href": "slides/05-slides.html#causal-identification",
    "title": "POLS 1600",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCasual Identification refers to “the assumptions needed for statistical estimates to be given a causal interpretation” Keele (2015)]\n\nWhat do we need to assume to make our claims about cause and effect credible\n\nExperimental Designs rely on randomization of treatment to justify their causal claims\nObservational Designs require additional assumptions and knowledge to make causal claims"
  },
  {
    "objectID": "slides/05-slides.html#experimental-designs",
    "href": "slides/05-slides.html#experimental-designs",
    "title": "POLS 1600",
    "section": "Experimental Designs",
    "text": "Experimental Designs\n\nExperimental designs are studies in which a causal variable of interest, the treatement, is manipulated by the researcher to examine its causal effects on some outcome of interest\nRandom assignment is the key to causal identification in experiments because it creates statistical independence between treatment and potential outcomes any potential confounding factors\n\n\n\\[\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#randomization-creates-credible-counterfactual-comparisons",
    "href": "slides/05-slides.html#randomization-creates-credible-counterfactual-comparisons",
    "title": "POLS 1600",
    "section": "Randomization creates credible counterfactual comparisons",
    "text": "Randomization creates credible counterfactual comparisons\nIf treatment has been randomly assigned, then:\n\nThe only thing that differs between treatment and control is that one group got the treatment, and another did not.\nWe can estimate the Average Treatment Effect (ATE) using the difference of sample means\n\n\n\\[\n\\begin{aligned}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#observational-designs",
    "href": "slides/05-slides.html#observational-designs",
    "title": "POLS 1600",
    "section": "Observational Designs",
    "text": "Observational Designs\n\nObservational designs are studies in which a causal variable of interest is determined by someone/thing other than the researcher (nature, governments, people, etc.)\nSince treatment has not been randomly assigned, observational studies typically require stronger assumptions to make causal claims.\nGenerally speaking, these assumptions amount to a claim about conditional independence\n\n\n\\[\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i | K_i\n\\]\n\n\nWhere after conditioning on \\(K_i\\), some knowledge about the world and how the data were generated, our treatment is as good as (as-if) randomly assigned (hence conditionally independent)\n\nEconomists often call this assumption of selection on observables"
  },
  {
    "objectID": "slides/05-slides.html#causal-inference-in-observational-studies",
    "href": "slides/05-slides.html#causal-inference-in-observational-studies",
    "title": "POLS 1600",
    "section": "Causal Inference in Observational Studies",
    "text": "Causal Inference in Observational Studies\nTo understand how to make causal claims in observational studies we will:\n\nIntroduce the concept of Directed Acyclic Graphs to describe causal relationships\nDiscuss three approaches to covariate adjustment\n\nSubclassification\nMatching\nLinear Regression\n\nThree research designs for observational data\n\nDifferences-in-Differences\nRegression Discontinuity Designs\nInstrumental Variables"
  },
  {
    "objectID": "slides/05-slides.html#two-ways-to-describe-causal-claims",
    "href": "slides/05-slides.html#two-ways-to-describe-causal-claims",
    "title": "POLS 1600",
    "section": "Two Ways to Describe Causal Claims",
    "text": "Two Ways to Describe Causal Claims\nIn this course, we will use two forms of notation to describe our causal claims.\n\nPotential Outcomes Notation (last lecture)\n\nIllustrates the fundamental problem of causal inference\n\nDirected Acyclic Graphs (DAGs)\n\nIllustrates potential bias from confounders and colliders"
  },
  {
    "objectID": "slides/05-slides.html#directed-acyclic-graphs-1",
    "href": "slides/05-slides.html#directed-acyclic-graphs-1",
    "title": "POLS 1600",
    "section": "Directed Acyclic Graphs",
    "text": "Directed Acyclic Graphs\n\nDirected Acyclic Graphs provide a way of encoding assumptions about casual relationships\n\nDirected Arrows \\(\\to\\) describe a direct causal effect\nArrow from \\(D\\to Y\\) means \\(Y_i(d) \\neq Y_i(d^\\prime)\\) “The outcome ( \\(Y\\)) for person \\(i\\) when D happens ( \\(Y_i(d)\\) ) is different than the the outcome when \\(D\\) doesn’t happen ( \\(Y_i(d^\\prime)\\) )\nNo arrow = no effect ( \\(Y_i(d) = Y_i(d^\\prime)\\) )\nAcyclic: No cycles. A variable can’t cause itself"
  },
  {
    "objectID": "slides/05-slides.html#types-of-variables-in-a-dag",
    "href": "slides/05-slides.html#types-of-variables-in-a-dag",
    "title": "POLS 1600",
    "section": "Types of variables in a DAG",
    "text": "Types of variables in a DAG\n\nDAGVariables\n\n\n\nBlair, Coppock, and Humphreys (2023) (Chap. 6.2)\n\n\n\nCausal Explanations Involve:\n\nY our outcome\nD A possible cause of Y\nM A mediator or mechanism through which D effects Y\nZ An instrument that can help us isolate the effects of D on `Y\nX2 a covariate that may moderate the effect of D on Y\n\nThreats to causal claims/Sources of bias:\n\nX1 an observed confounder that is a common cause of both D & Y\nU an unobserved confounder a common cause of both D & Y\nK a collider that is a common consequence of both D & Y"
  },
  {
    "objectID": "slides/05-slides.html#dags-illustrate-two-sources-of-bias",
    "href": "slides/05-slides.html#dags-illustrate-two-sources-of-bias",
    "title": "POLS 1600",
    "section": "DAGs illustrate two sources of bias:",
    "text": "DAGs illustrate two sources of bias:\n\n\nConfounder bias: Failing to control for a common cause of D and Y (aka Omitted Variable Bias)\nCollider bias: Controlling for a common consequence (aka Selection Bias1)\n\n\nNote in practice there’s some slippage/debate/disagreement around this nomenclature"
  },
  {
    "objectID": "slides/05-slides.html#section-1",
    "href": "slides/05-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Confounding Bias: The Coffee Example\n\nConfounding BiasCoffee and CancerAdjusting for Smoking\n\n\n\n\nDrinking coffee doesn’t cause lung cancer we might find correlation between them because they share a common cause: smoking.\nSmoking is a [confounding] variable, that if omitted will bias our results producing a spurious relationsip\n[Adjusting] for [confounders] removes this source of bias"
  },
  {
    "objectID": "slides/05-slides.html#section-2",
    "href": "slides/05-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Collider Bias: The Dating Example\n\nCollider biasSelection biasNo relationship in population\n\n\n\n\nWhy are attractive people such jerks?\nSuppose dating is a function of looks and personality\nDating is a common consequences of looks and personality\nBasing our claim off of who we date is an example of selection bias created by controlling for collider"
  },
  {
    "objectID": "slides/05-slides.html#when-to-control-for-a-variable",
    "href": "slides/05-slides.html#when-to-control-for-a-variable",
    "title": "POLS 1600",
    "section": "When to control for a variable:",
    "text": "When to control for a variable:\n\n(Blair, Coppock, and Humphreys 2023) (Chap. 6.2)"
  },
  {
    "objectID": "slides/05-slides.html#covariate-adjustment-1",
    "href": "slides/05-slides.html#covariate-adjustment-1",
    "title": "POLS 1600",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment\nCovariate adjustment refers a broad class of procedures that try to make a comparison more credible or meaningful by adjusting for some other potentially confounding factor."
  },
  {
    "objectID": "slides/05-slides.html#covariate-adjustment-2",
    "href": "slides/05-slides.html#covariate-adjustment-2",
    "title": "POLS 1600",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment\nWhen you hear people talk about\n\nControlling for age\nConditional on income\nHolding age and income constant\nCeteris paribus (All else equal)\n\nThey are typically talking about some sort of covariate adjustment."
  },
  {
    "objectID": "slides/05-slides.html#three-approaches-to-covariate-adjustment",
    "href": "slides/05-slides.html#three-approaches-to-covariate-adjustment",
    "title": "POLS 1600",
    "section": "Three approaches to covariate adjustment",
    "text": "Three approaches to covariate adjustment\n\nSubclassification\n\n👍: Easy to implement and interpret\n👎: Curse of dimensionality, Selection on observables\n\nMatching\n\n👍: Balance on multiple covariates, Mirrors logic of experimental design\n👎: Selection on observables, Only provides balance on observed variables, Lot’s of technical details…\n\nRegression\n\n👍: Easy to implement, control for many factors (good and bad)\n👎: Selection on observables, easy to fit “bad” models"
  },
  {
    "objectID": "slides/05-slides.html#understanding-linear-regression",
    "href": "slides/05-slides.html#understanding-linear-regression",
    "title": "POLS 1600",
    "section": "Understanding Linear Regression",
    "text": "Understanding Linear Regression\n\n\nConceptual\n\nSimple linear regression estimates “a line of best fit” that summarizes relationships between two variables\n\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n\\]\n\nPractical\n\nWe estimate linear models in R using the lm() function\n\n\n\nlm(y ~ x, data = df)"
  },
  {
    "objectID": "slides/05-slides.html#understanding-linear-regression-1",
    "href": "slides/05-slides.html#understanding-linear-regression-1",
    "title": "POLS 1600",
    "section": "Understanding Linear Regression",
    "text": "Understanding Linear Regression\n\n\nTechnical/Definitional\n\nLinear regression chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]\n\nTheoretical\n\nLinear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\)"
  },
  {
    "objectID": "slides/05-slides.html#conceptual-linear-regression-1",
    "href": "slides/05-slides.html#conceptual-linear-regression-1",
    "title": "POLS 1600",
    "section": "Conceptual: Linear Regression",
    "text": "Conceptual: Linear Regression\n\nRegression is a tool for describing relationships.\n\nHow does some outcome we’re interested in tend to change as some predictor of that outcome changes?\nHow does economic development vary with democracy?\nHow does economic development vary with democracy, adjusting for natural resources like oil and gas"
  },
  {
    "objectID": "slides/05-slides.html#conceptual-linear-regression-2",
    "href": "slides/05-slides.html#conceptual-linear-regression-2",
    "title": "POLS 1600",
    "section": "Conceptual: Linear Regression",
    "text": "Conceptual: Linear Regression\n\nMore formally:\n\\[\ny_i = f(x_i) + \\epsilon\n\\]\n\nY is a function of X plus some error, \\(\\epsilon\\)\nLinear regression assumes that relationship between an outcome and a predictor can be by a linear function\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-and-the-line-of-best-fit",
    "href": "slides/05-slides.html#linear-regression-and-the-line-of-best-fit",
    "title": "POLS 1600",
    "section": "Linear Regression and the Line of Best Fit",
    "text": "Linear Regression and the Line of Best Fit\n\n\nThe goal of linear regression is to choose coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to summarizes the relationship between \\(y\\) and \\(x\\)\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\n\nTo accomplish this we need some sort of criteria.\nFor linear regression, that criteria is minimizing the error between what our model predicts \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\) and what we actually observed \\((y_i)\\)\nMore on this to come. But first…"
  },
  {
    "objectID": "slides/05-slides.html#regression-notation",
    "href": "slides/05-slides.html#regression-notation",
    "title": "POLS 1600",
    "section": "Regression Notation",
    "text": "Regression Notation\n\n\\(y_i\\) an outcome variable or thing we’re trying to explain\n\nAKA: The dependent variable, The response Variable, The left hand side of the model\n\n\\(x_i\\) a predictor variables or things we think explain variation in our outcome\n\nAKA: The independent variable, covariates, the right hand side of the model.\nCap or No Cap: I’ll use \\(X\\) (should be \\(\\mathbf{X}\\)) to denote a set (matrix) of predictor variables. \\(y\\) vs \\(Y\\) can also have technical distinctions (Sample vs Population, observed value vs Random Variable, …)\n\n\\(\\beta\\) a set of unknown parameters that describe the relationship between our outcome \\(y_i\\) and our predictors \\(x_i\\)\n\\(\\epsilon\\) the error term representing variation in \\(y_i\\) not explained by our model."
  },
  {
    "objectID": "slides/05-slides.html#linear-regression",
    "href": "slides/05-slides.html#linear-regression",
    "title": "POLS 1600",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\nWe call this a bivariate regression, because there are only two variables\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\n\nWe call this a linear regression, because \\(y_i = \\beta_0 + \\beta_1 x_i\\) is the equation for a line, where:\n\n\\(\\beta_0\\) corresponds to the \\(y\\) intercept, or the model’s prediction when \\(x = 0\\).\n\\(\\beta_1\\) corresponds to the slope, or how \\(y\\) is predicted to change as \\(x\\) changes."
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-1",
    "href": "slides/05-slides.html#linear-regression-1",
    "title": "POLS 1600",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\nIf you find this notation confusing, try plugging in substantive concepts for what \\(y\\) and \\(x\\) represent\nSay we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.\n\nThe generic linear model\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon\\]\nReflects:\n\\[\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\]"
  },
  {
    "objectID": "slides/05-slides.html#practical-estimating-a-linear-regression-1",
    "href": "slides/05-slides.html#practical-estimating-a-linear-regression-1",
    "title": "POLS 1600",
    "section": "Practical: Estimating a Linear Regression",
    "text": "Practical: Estimating a Linear Regression\n\n\nWe estimate linear regressions in R using the lm() function.\nlm() requires two arguments:\n\na formula argument of the general form y ~ x read as “Y modeled by X” or below “Transgender Feeling Thermometer (y) modeled by (~) Age (x)\na data argument telling R where to find the variables in the formula\n\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))\nm1 &lt;- lm(therm_trans_t0 ~ vf_age, data = df)\nm1\n\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031"
  },
  {
    "objectID": "slides/05-slides.html#the-lm-function",
    "href": "slides/05-slides.html#the-lm-function",
    "title": "POLS 1600",
    "section": "The lm() function",
    "text": "The lm() function\n\nThe coefficients from lm() are saved in object called m1\n\nm1\n\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n\n\nm1 actually contains a lot of information\n\nnames(m1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\nm1$coefficients\n\n(Intercept)      vf_age \n 62.8195994  -0.2030711"
  },
  {
    "objectID": "slides/05-slides.html#practical-interpreting-a-linear-regression",
    "href": "slides/05-slides.html#practical-interpreting-a-linear-regression",
    "title": "POLS 1600",
    "section": "Practical: Interpreting a Linear Regression",
    "text": "Practical: Interpreting a Linear Regression\nWe can extract the intercept and slope from this simple bivariate model, using the coef() function\n\n# All the coefficients\ncoef(m1)\n\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n\n# Just the intercept\ncoef(m1)[1]\n\n(Intercept) \n    62.8196 \n\n# Just the slope\ncoef(m1)[2]\n\n    vf_age \n-0.2030711"
  },
  {
    "objectID": "slides/05-slides.html#practical-interpreting-a-linear-regression-1",
    "href": "slides/05-slides.html#practical-interpreting-a-linear-regression-1",
    "title": "POLS 1600",
    "section": "Practical: Interpreting a Linear Regression",
    "text": "Practical: Interpreting a Linear Regression\nThe two coefficients from m1 define a line of best fit, summarizing how feelings toward transgender individuals change with age\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon\\]\n\\[\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\]\n\\[\\text{Transgender Feeling Thermometer}_i = 62.82 + -0.2 \\text{Age}_i + \\epsilon_i\\]"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\n\n\nOften it’s useful for interpretation to obtain predicted values from a regression.\nTo obtain predicted vales \\((\\hat{y})\\), we simply plug in a value for \\(x\\) (In this case, \\(Age\\)) and evaluate our equation.\nFor example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?\n\n\\[\\hat{FT}_{x=18} = 62.82 + -0.2 \\times 18  = 59.16\\] \\[\\hat{FT}_{x=65} = 62.82 + -0.2 \\times 68  = 49.01\\]"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-1",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-1",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\nWe could do this by hand\n\ncoef(m1)[1] + coef(m1)[2] * 18\n\n(Intercept) \n   59.16432 \n\ncoef(m1)[1] + coef(m1)[2] * 68\n\n(Intercept) \n   49.01076"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-2",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-2",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\nMore often we will:\n\nMake a prediction data frame (called pred_df below) with the values of interests\nUse the predict() function with our linear model (m1) and pred_df\nSave the predicted values to our new column in our prediction data frame"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-3",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-3",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\n\n# Make prediction data frame\npred_df &lt;- data.frame(\n  vf_age = c(18, 68)\n)\n# Predict FT for 18 and 68 year-olds\npredict(m1, newdata = pred_df)\n\n       1        2 \n59.16432 49.01076 \n\n# Save predictions to data frame\npred_df$ft_trans_hat &lt;- predict(m1, newdata = pred_df)\npred_df\n\n  vf_age ft_trans_hat\n1     18     59.16432\n2     68     49.01076"
  },
  {
    "objectID": "slides/05-slides.html#section-3",
    "href": "slides/05-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Practical: Visualizing Linear Regression\n\nConceptCodeInterceptSlopeErrors\n\n\nWe can visualize simple regression by:\n\nplotting a scatter plot of the outcome (y-axis) and predictors (x-axis)\noverlaying the line defined by lm()\n\n\n\n\nfig_lm &lt;- df %&gt;%\n  ggplot(aes(vf_age,therm_trans_t0))+\n  geom_point(size=.5, alpha=.5)+\n  geom_abline(intercept = coef(m1)[1],\n              slope = coef(m1)[2],\n              col = \"blue\"\n              )+\n  geom_vline(xintercept = 0,linetype = 2)+\n  xlim(0,100)+\n  annotate(\"point\",\n           x = 0, y = coef(m1)[1],\n           col= \"red\",\n           )+\n  annotate(\"text\",\n           label = expression(paste(beta[0],\"= 62.81\" )),\n           x = 1, y = coef(m1)[1]+5,\n           hjust = \"left\",\n           )+\n  labs(\n    x = \"Age\",\n    y = \"Feeling Thermometer toward\\nTransgender People\"\n  )+\n  theme_classic() -&gt; fig_lm"
  },
  {
    "objectID": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1",
    "href": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1",
    "title": "POLS 1600",
    "section": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\nP: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression\nQ: Ok, that’s not really that helpful…\n\nWhat’s an error?\nWhy would we square and sum them\nHow do we minimize them.\n\n\nP: Good questions!"
  },
  {
    "objectID": "slides/05-slides.html#whats-an-error",
    "href": "slides/05-slides.html#whats-an-error",
    "title": "POLS 1600",
    "section": "What’s an error?",
    "text": "What’s an error?\nAn error, \\(\\epsilon_i\\) is simply the difference between the observed value of \\(y_i\\) and what our model would predict, \\(\\hat{y_i}\\) given some value of \\(x_i\\). So for a model:\n\\[y_i=\\beta_0+\\beta_1 x_{i} + \\epsilon_i\\]\nWe simply subtract our model’s prediction \\(\\beta_0+\\beta_1 x_{i}\\) from the the observed value, \\(y_i\\)\n\\[\\hat{\\epsilon_i}=y_i-\\hat{y_i}=(Y_i-(\\beta_0+\\beta_1 x_{i}))\\]\nTo get \\(\\epsilon_i\\)"
  },
  {
    "objectID": "slides/05-slides.html#why-are-we-squaring-and-summing-epsilon",
    "href": "slides/05-slides.html#why-are-we-squaring-and-summing-epsilon",
    "title": "POLS 1600",
    "section": "Why are we squaring and summing \\(\\epsilon\\)",
    "text": "Why are we squaring and summing \\(\\epsilon\\)\n\nThere are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)\n\nSquaring \\(\\epsilon\\) treats positive and negative residuals equally.\nSumming produces single value summarizing our models overall performance.\n\nThere are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties"
  },
  {
    "objectID": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2",
    "href": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2",
    "title": "POLS 1600",
    "section": "How do we minimize \\(\\sum \\epsilon^2\\)",
    "text": "How do we minimize \\(\\sum \\epsilon^2\\)\nOLS chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize \\(\\sum \\epsilon^2\\), the Sum of Squared Residuals (SSR)\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]"
  },
  {
    "objectID": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1-1",
    "href": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1-1",
    "title": "POLS 1600",
    "section": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)\nIn an intro stats course, we would walk through the process of finding\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\] Which involves a little bit of calculus. The big payoff is that\n\\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\\] And\n\\[ \\beta_1 = \\frac{Cov(x,y)}{Var(x)}\\] Which is never quite the epiphany, I think we think it is…\nThe following slides walk you through the mechanics of this exercise. We’re gonna skip through them in class, but they’re there for your reference"
  },
  {
    "objectID": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2-1",
    "href": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2-1",
    "title": "POLS 1600",
    "section": "How do we minimize \\(\\sum \\epsilon^2\\)",
    "text": "How do we minimize \\(\\sum \\epsilon^2\\)\nTo understand what’s going on under the hood, you need a broad understanding of some basic calculus.\nThe next few slides provide a brief review of derivatives and differential calculus."
  },
  {
    "objectID": "slides/05-slides.html#derivatives",
    "href": "slides/05-slides.html#derivatives",
    "title": "POLS 1600",
    "section": "Derivatives",
    "text": "Derivatives\n\nThe derivative of \\(f\\) at \\(x\\) is its rate of change at \\(x\\)\n\nFor a line: the slope\nFor a curve: the slope of a line tangent to the curve\n\nYou’ll see two notations for derivatives:\n\nLeibniz notation:\n\n\\[\n\\frac{df}{dx}(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{(x+h)-x}\n\\]\n\nLagrange: \\(f^{\\prime}(x)\\)"
  },
  {
    "objectID": "slides/05-slides.html#some-useful-facts-about-derivatives",
    "href": "slides/05-slides.html#some-useful-facts-about-derivatives",
    "title": "POLS 1600",
    "section": "Some useful Facts about Derivatives",
    "text": "Some useful Facts about Derivatives\nDerivative of a constant\n\\[\nf^{\\prime}(c)=0\n\\]\nDerivative of a line f(x)=2x\n\\[\nf^{\\prime}(2x)=2\n\\]\nDerivative of \\(f(x)=x^2\\)\n\\[\nf^{\\prime}(x^2)=2x\n\\]\nChain rule: y= f(g(x)). The derivative of y with respect to x is\n\\[\n\\frac{d}{dx}(f(g(x)))=f^{\\prime}(g(x))g^{\\prime}(x)\n\\]\nThe derivative of the “outside” times the derivative of the “inside,” remembering that the derivative of the outside function is evaluated at the value of the inside function."
  },
  {
    "objectID": "slides/05-slides.html#finding-a-local-minimums",
    "href": "slides/05-slides.html#finding-a-local-minimums",
    "title": "POLS 1600",
    "section": "Finding a Local Minimums",
    "text": "Finding a Local Minimums\nLocal minimum:\n\\[\nf^{\\prime}(x)=0 \\text{ and } f^{\\prime\\prime}(x)&gt;0\n\\]\n\nSource"
  },
  {
    "objectID": "slides/05-slides.html#partial-derivatives",
    "href": "slides/05-slides.html#partial-derivatives",
    "title": "POLS 1600",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\nLet \\(f\\) be a function of the variables \\((x, \\dots, X_n)\\). The partial derivative of \\(f\\) with respect to \\(X_i\\) is\n\\[\\begin{align*}\n\\frac{\\partial f(x, \\dots, X_n)}{\\partial X_i}=\\lim_{h\\to0}\\frac{f(x, \\dots X_i+h \\dots, X_n)-f(x, \\dots X_i \\dots, X_n)}{h}\n\\end{align*}\\]\n\nSource"
  },
  {
    "objectID": "slides/05-slides.html#minimizing-the-sum-of-squared-errors",
    "href": "slides/05-slides.html#minimizing-the-sum-of-squared-errors",
    "title": "POLS 1600",
    "section": "Minimizing the sum of squared errors",
    "text": "Minimizing the sum of squared errors\nOur model\n\\[y_i =\\beta_0+\\beta_1x_{i}+\\epsilon_i\\]\nFinds coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to to minimize the sum of squared residuals, \\(\\hat{\\epsilon}_i\\):\n\\[\\begin{aligned}\n\\sum \\hat{\\epsilon_i}^2 &= \\sum (y_i-\\beta_0-\\beta_1 x_{i})^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#minimizing-the-sum-of-squared-errors-1",
    "href": "slides/05-slides.html#minimizing-the-sum-of-squared-errors-1",
    "title": "POLS 1600",
    "section": "Minimizing the sum of squared errors",
    "text": "Minimizing the sum of squared errors\nWe solve for \\(\\beta_0\\) and \\(\\beta_1\\), by taking the partial derivatives with respect to \\(\\beta_0\\) and \\(\\beta_1\\), and setting them equal to zero\n\\[\\begin{aligned}\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial \\beta_0} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})=0 & f'(-x^2) = -2x\\\\\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial\\beta_1} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})x_{i}=0 & \\text{chain rule}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_0",
    "href": "slides/05-slides.html#solving-for-beta_0",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_0\\)",
    "text": "Solving for \\(\\beta_0\\)\nFirst, we’ll solve for \\(\\beta_0\\), by multiplying both sides by -1/2 and distributing the \\(\\sum\\):\n\\[\\begin{aligned}\n0 &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})\\\\\n\\sum \\beta_0 &= \\sum y_i - \\sum \\beta_1 x_{i}\\\\\nN \\beta_0 &= \\sum y_i -\\sum \\beta_1 x_{i}\\\\\n\\beta_0 &= \\frac{\\sum y_i}{N} - \\frac{\\beta_1 \\sum x_{i}}{N}\\\\\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_1",
    "href": "slides/05-slides.html#solving-for-beta_1",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_1\\)",
    "text": "Solving for \\(\\beta_1\\)\nNow, we can solve for \\(\\beta_1\\) plugging in \\(\\beta_0\\).\n\\[\\begin{aligned}\n0 &= -2\\sum [(y_i-\\beta_0-\\beta_1 x_{i})x_{i}]\\\\\n0 &= \\sum [y_ix_i-(\\bar{y} - \\beta_1 \\bar{x})x_{i}-\\beta_1 x_{i}^2]\\\\\n0 &= \\sum [y_ix_i-\\bar{y}x_{i} + \\beta_1 \\bar{x}x_{i}-\\beta_1 x_{i}^2]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_1-1",
    "href": "slides/05-slides.html#solving-for-beta_1-1",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_1\\)",
    "text": "Solving for \\(\\beta_1\\)\nNow we’ll rearrange some terms and pull out an \\(x_{i}\\) to get\n\\[\\begin{aligned}\n0 &= \\sum [(y_i -\\bar{y} + \\beta_1 \\bar{x}-\\beta_1 x_{i})x_{i}]\n\\end{aligned}\\]\nDividing both sides by \\(x_{i}\\) and distributing the summation, we can isolate \\(\\beta_1\\)\n\\[\\begin{aligned}\n\\beta_1 \\sum (x_{i}-\\bar{x}) &= \\sum (y_i -\\bar{y})\n\\end{aligned}\\]\nDividing by \\(\\sum (x_{i}-\\bar{x})\\) to get\n\\[\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})}{\\sum (x_{i}-\\bar{x})}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_1-2",
    "href": "slides/05-slides.html#solving-for-beta_1-2",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_1\\)",
    "text": "Solving for \\(\\beta_1\\)\nFinally, by multiplying by \\(\\frac{(x_{i}-\\bar{x})}{(x_{i}-\\bar{x})}\\) we get\n\\[\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})(x_{i}-\\bar{x})}{\\sum (\\bar{x}-x_{i})^2}\n\\end{aligned}\\]\nWhich has a nice interpretation:\n\\[\\begin{aligned}\n\\beta_1 &= \\frac{Cov(x,y)}{Var(x)}\n\\end{aligned}\\]\nSo the coefficient in a simple linear regression of \\(Y\\) on \\(X\\) is simply the ratio of the covariance between \\(X\\) and \\(Y\\) over the variance of \\(X\\). Neat!"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing",
    "href": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing",
    "title": "POLS 1600",
    "section": "Linear Regression is a many splendored thing",
    "text": "Linear Regression is a many splendored thing\nTimothy Lin provides a great overview of the various interpretations/motivations for linear regression.\n\nA least squares estimator\nA linear projection of \\(y\\) on the subspace spanned by \\(X\\beta\\)\nA method of moments estimator\nA maximum likelihood estimator\nA singular vector decomposition\nA linear approximation of the conditional expectation function"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing-1",
    "href": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing-1",
    "title": "POLS 1600",
    "section": "Linear Regression is a many splendored thing",
    "text": "Linear Regression is a many splendored thing\n\nTimothy Lin provides a great overview of various interpretations/motivations for linear regression.\n\nA least squares estimator\nA linear projection of \\(y\\) on the subspace spanned by \\(X\\beta\\)\nA method of moments estimator\nA maximum likelihood estimator\nA singular vector decomposition\nA linear approximation of the conditional expectation function"
  },
  {
    "objectID": "slides/05-slides.html#the-conditional-expectation-function",
    "href": "slides/05-slides.html#the-conditional-expectation-function",
    "title": "POLS 1600",
    "section": "The Conditional Expectation Function",
    "text": "The Conditional Expectation Function\nOf all the functions we could choose to describe the relationship between \\(Y\\) and \\(X\\),\n\\[\nY_i = f(X_i) + \\epsilon_i\n\\]\nthe conditional expectation of \\(Y\\) given \\(X\\) \\((E[Y|X])\\), has some appealing properties\n\\[\nY_i = E[Y_i|X_i] + \\epsilon\n\\]\nThe error, by definition, is uncorrelated with X and \\(E[\\epsilon|X]=0\\)\n\\[\nE[\\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0\n\\]\nOf all the possible functions \\(g(X)\\), we can show that \\(E[Y_i|X_i]\\) is the best predictor in terms of minimizing mean squared error\n\\[\nE[ (Y - g(Y))^2] \\geq E[(Y - E[Y|X])^2]\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#section-4",
    "href": "slides/05-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Linear Approximations to the Conditional Expectation Function\n\nConceptCEFOLS\n\n\n\n\nWe can then show (in a different class) that linear regression provides the best linear predictor of the CEF\n\nChapter 3, of Mostly Harmless Econometrics\nChapter 4 of Foundations of Agnostic Statistics\n\nFurthermore, when the CEF is linear, it’s equal exactly to OLS regression"
  },
  {
    "objectID": "slides/05-slides.html#what-you-need-to-know-about-regression",
    "href": "slides/05-slides.html#what-you-need-to-know-about-regression",
    "title": "POLS 1600",
    "section": "What you need to know about Regression",
    "text": "What you need to know about Regression\n\n\nConceptual\n\nSimple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n\\]\n\nPractical\n\nWe estimate linear models in R using the lm() function\n\n\n\nlm(y ~ x, data = df)"
  },
  {
    "objectID": "slides/05-slides.html#what-you-need-to-know-about-regression-1",
    "href": "slides/05-slides.html#what-you-need-to-know-about-regression-1",
    "title": "POLS 1600",
    "section": "What you need to know about Regression",
    "text": "What you need to know about Regression\n\n\nTechnical/Definitional\n\nLinear regression chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]\n\nTheoretical\n\nLinear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\)"
  },
  {
    "objectID": "slides/05-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/05-slides.html#motivating-example-what-causes-cholera",
    "title": "POLS 1600",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/05-slides.html#notation",
    "href": "slides/05-slides.html#notation",
    "title": "POLS 1600",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/05-slides.html#causal-effects",
    "href": "slides/05-slides.html#causal-effects",
    "title": "POLS 1600",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/05-slides.html#average-treatment-on-treated",
    "href": "slides/05-slides.html#average-treatment-on-treated",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/05-slides.html#average-treatment-on-treated-1",
    "href": "slides/05-slides.html#average-treatment-on-treated-1",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n\n\n\n\n\n\n\n\n\nPre-Period (T=0)\nPost-Period (T=1)\n\n\n\n\nTreated \\(D_{i}=1\\)\n\\(E[Y_{0i}(0)\\vert D_i = 1]\\)\n\\(E[Y_{1i}(1)\\vert D_i = 1]\\)\n\n\nControl \\(D_i=0\\)\n\\(E[Y_{0i}(0)\\vert D_i = 0]\\)\n\\(E[Y_{0i}(1)\\vert D_i = 0]\\)"
  },
  {
    "objectID": "slides/05-slides.html#data",
    "href": "slides/05-slides.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/05-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/05-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "POLS 1600",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/05-slides.html#before-vs-after-comparisons",
    "href": "slides/05-slides.html#before-vs-after-comparisons",
    "title": "POLS 1600",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/05-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/05-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "POLS 1600",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/05-slides.html#difference-in-differences-1",
    "href": "slides/05-slides.html#difference-in-differences-1",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period"
  },
  {
    "objectID": "slides/05-slides.html#difference-in-differences-2",
    "href": "slides/05-slides.html#difference-in-differences-2",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\] Is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/05-slides.html#difference-in-differences-3",
    "href": "slides/05-slides.html#difference-in-differences-3",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/05-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "POLS 1600",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V"
  },
  {
    "objectID": "slides/05-slides.html#parralel-trends",
    "href": "slides/05-slides.html#parralel-trends",
    "title": "POLS 1600",
    "section": "Parralel Trends",
    "text": "Parralel Trends"
  },
  {
    "objectID": "slides/05-slides.html#summary",
    "href": "slides/05-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\n\nTaking the pre-post difference removes any fixed differences between the units\nThen taking the difference between treated and control differences removes any common differences over time\n\nThe key identifying assumption of a DiD design is the “assumption of parallel trends”\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to test"
  },
  {
    "objectID": "slides/05-slides.html#extensions-and-limitations",
    "href": "slides/05-slides.html#extensions-and-limitations",
    "title": "POLS 1600",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nDiff-in-Diff easy to estimate with linear regression\nGeneralizes to multiple periods and treatment interventions\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\n\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/05-slides.html#applications",
    "href": "slides/05-slides.html#applications",
    "title": "POLS 1600",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?"
  },
  {
    "objectID": "slides/05-slides.html#references",
    "href": "slides/05-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nBlair, Graeme, Alexander Coppock, and Macartan Humphreys. 2023. Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign. Princeton University Press."
  },
  {
    "objectID": "slides/00-slides-template.html#class-plan",
    "href": "slides/00-slides-template.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/00-slides-template.html#annoucements",
    "href": "slides/00-slides-template.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/00-slides-template.html#feedback",
    "href": "slides/00-slides-template.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/00-slides-template.html#concepts",
    "href": "slides/00-slides-template.html#concepts",
    "title": "POLS 1600",
    "section": " Concepts",
    "text": "Concepts"
  },
  {
    "objectID": "slides/00-slides-template.html#code",
    "href": "slides/00-slides-template.html#code",
    "title": "POLS 1600",
    "section": " Code",
    "text": "Code"
  },
  {
    "objectID": "slides/00-slides-template.html#review-1",
    "href": "slides/00-slides-template.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "slides/00-slides-template.html#concept-1",
    "href": "slides/00-slides-template.html#concept-1",
    "title": "POLS 1600",
    "section": "Concept",
    "text": "Concept"
  },
  {
    "objectID": "slides/00-slides-template.html#code-2",
    "href": "slides/00-slides-template.html#code-2",
    "title": "POLS 1600",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "slides/00-slides-template.html#summary-1",
    "href": "slides/00-slides-template.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/00-slides-template.html#references",
    "href": "slides/00-slides-template.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "POLS 1600",
    "section": "",
    "text": "This course provides a foundation in the principles and practice of quantitative social science, with a focus on building tools to make descriptive, causal, and predictive inferences.\n\nLogistics\nWe meet twice a week, alternating lectures on Tuesdays and labs on Thursday. Both sessions require laptops that run R and RStudio\n\n\n\n\nLecture\nTuesdays 10:30-11:50 am\n\n\n\n\n\nLabs\nThursdays 10:30-11:50 am\n\n\n\n\n\nAssignments\nDue on Canvas\n\n\n\n\n\nLocation\nSmith-Buonanno Hall G01\n\n\n\n\n\nZoom\n\n\n\n\n\nOffice Hours\n111 Thayer St Rm 339\n\n\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\n\n\nLecture\nLabs\nSolutions\nAssignments\n\n\n\n\nWeek 0\n\n\n\n\n\n\nIntroductions\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nData & Measurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\nCausation -- Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\n\nCausation -- Observational Studies\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\nBivariate Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\n\nMultiple Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\nRegression Extensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\n\nProbability - Random Variables & Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n\n\nProbability - Limit Theorems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\n\n\nInference -- Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\n\n\nInference -- Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\n\n\nPresentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructors\n\n\nPaul Testa\nAssistant Professor\nDepartment of Political Science\nBrown University &lt;paul_testa@brown.edu&gt;\n\n\n\n\n\nMaya Nunez\nTeaching Assistant\nDepartment of Political Science\nBrown University &lt;maya_nunez@brown.edu&gt;"
  },
  {
    "objectID": "resources/04-packages.html",
    "href": "resources/04-packages.html",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "",
    "text": "This document provides instructions for installing the following packages:\n\ndataverse a package to download and read files from dataverses like Harvard’s Dataverse\nDeclareDesign a set of packages useful for describing the properities of experimetnal and observational design\ntidycensus set of functions of that allow us to download data from the US Census’ API\neasystats a set of packages like the tidyverse but for statistics.\n\nThese are useful packages, but in the past, I’ve found they don’t play nicely with the simple the_packages ipak(the_packages) approach we take in class.\nAdditionally, for tidycensus, each of you will need to request an API key from the Census and install it locally to your computers."
  },
  {
    "objectID": "resources/04-packages.html#request-an-api-key-from-the-census",
    "href": "resources/04-packages.html#request-an-api-key-from-the-census",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.1 Request an API key from the Census",
    "text": "3.1 Request an API key from the Census\nClick on this link: https://api.census.gov/data/key_signup.html\nAnd fill in the following information\n\nOrganization: “Brown University”\nEmail: firstname_lastname@brown.edu\nAgree to terms of service\nSubmit request\n\n\nknitr::include_graphics(\"images/census1.png\")"
  },
  {
    "objectID": "resources/04-packages.html#check-email",
    "href": "resources/04-packages.html#check-email",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.2 Check Email",
    "text": "3.2 Check Email\nYou should receive an email like this:\n\n\n\n\n\n\n\n\n\n\nClick on the link to activate your API key"
  },
  {
    "objectID": "resources/04-packages.html#activate-api-key",
    "href": "resources/04-packages.html#activate-api-key",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.3 Activate API key",
    "text": "3.3 Activate API key\n\nClicking the link should take you to a page that looks like this"
  },
  {
    "objectID": "resources/04-packages.html#save-api-key-in-r",
    "href": "resources/04-packages.html#save-api-key-in-r",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.4 Save API key in R",
    "text": "3.4 Save API key in R\nGo back to the email from the census\n\nCopy the string of letters and digits from the email (blocked out in red in the image above)\nThis is your unique census API key\nPaste that string in between the quotation marks below and run census_api_key()\n\n\ncensus_api_key(\"YOUR API KEY GOES HERE\")"
  },
  {
    "objectID": "resources/04-packages.html#check-that-everything-worked",
    "href": "resources/04-packages.html#check-that-everything-worked",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.5 Check that everything worked",
    "text": "3.5 Check that everything worked\n\ncensus_api_key() should save your unique API to your .Renviron file which tidycensus will use whenever you make ask the Census to Download data.\nIf everything worked as planned, running Sys.getenv(\"CENSUS_API_KEY\") should display your long API key\n\n\nSys.getenv(\"CENSUS_API_KEY\")\n\n[1] \"cad56d0c712406cfe825878e3bd0de256d19f2aa\"\n\n\nAnd you should be able to use functions from tidycensus to download census data:\n\nage10 &lt;- tidycensus::get_decennial(geography = \"state\", \n                       variables = \"P013001\", \n                       year = 2010)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\nhead(age10)\n\n# A tibble: 6 × 4\n  GEOID NAME       variable value\n  &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n1 01    Alabama    P013001   37.9\n2 02    Alaska     P013001   33.8\n3 04    Arizona    P013001   35.9\n4 05    Arkansas   P013001   37.4\n5 06    California P013001   35.2\n6 22    Louisiana  P013001   35.8\n\n\nYou can read more about what tidycensus can do here"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html",
    "href": "assignments/A1_Research_Questions.html",
    "title": "POLS 2580 Assignment 1",
    "section": "",
    "text": "Asking a good research question is one of the most important skills you will develop in your academic careers. It’s also one of the hardest.\nWe often think we’re asking one question, when in fact the study we conduct really addresses a related but distinct question. When a priest asked Willie Sutton why he robbed banks, he replied the “Well, that’s where the money is”. The priest’s question was about why rob at all, while Sutton answered the different question “Given one robs, why rob banks?” Similarly, Medieval philosophers might ask why objects stay in motion, while Newton suggests what really need is not an explanation of motion itself but of changes in motion.\nThe object of our question shapes the form of our explanation.\nIn this assignment, I would like your group to craft three potential research questions that we might explore in our research project for this class. Each question, should be a single sentence, with a few sentences answering the following questions (More details below):\n\nWhy do we care about the answer to this research question?\nWhat’s would a hypothetical “ideal experiment” to answer this question look like?\nWhat would a study with observational data look like?\nA published study that relates to this question\nHow feasible would it be to do a study like this for the course\n\nYou may use this Rmd file as a template (click here to download) or create your own file. Please submit your responses to Canvas.\nYou might start by writing down several questions of different forms about the same topic:\n\nWhy do people vote?\nWhy do people not vote?\nWhy do the rich vote at higher rates than the poor?\nWhen might people who don’t vote, be motivated to vote?\nWhat is the effect of encouraging someone to vote via a phone call?\nAre phone calls more or less effective than in-person contact for get-out the vote efforts?\n\nEach of these questions addresses a general topic that political scientists seem to think is important. Each carries some suppositions and assumptions that in turn influence the type of explanation we might find convincing. Why do people vote feels a bit broad to me. People probably vote for many reasons. How can we hope to adjudicate between all the possible reasons for voting? (Further are these the same reasons for not voting or do we need another set of explanations altogether?)\nWhether phone calls are more or less effective than in-person contacts for GOTV efforts seems more tractable, but also perhaps to narrow. Do we really care? If we’re confident we can identify an effect or difference in one study, are we sure we’ll see similar effects in a different study conducted under different circumstances?\nIn crafting your research questions, you want to strike a balance between things we actually care about (why do people vote) and things we can actually assess (what’s effect of a particularly type of encouragement to vote). A few thoughts on this process:\n\n“Why” questions tend to be more compelling than “What” or “How” or “Do” questions, I think in part because “why” questions often imply a theory and suggest a counterfactual (why this and not that), while other ways of asking questions feel more descriptive. For example, why do the rich vote at higher rates than the poor. Well, one explanation may be that their relative social and economic status means they are more likely to be targets of mobilization efforts by campaigns (among many things). So a natural follow up to this larger question might be, what’s the effect of providing similar mobilization efforts to the poor. Would they vote at similar rates to the rich? If so, then we’ve learned something about how mobilization explains class differences in participation.\nThinking about questions in terms of puzzles is another useful trick. Why do parties exist when politicians’ ideological preferences can explain the vast majority of their legislative behavior? Note this type of question contains a lot of presuppositions (how do we measure ideological preferences? Do they really explain legislative behavior? Is that what we care about?), but as point of departure for a study these type arguments can be useful\nTry to be simple and clear. Don’t worry about asking the perfect question right away. Your questions can and should evolve over time, and I suspect some of you will write a paper that has nothing to do with the questions you posed here.\n\nFor each question, please discuss the following:\n\nWhy do we care? Why we should care about the answer to this question. A strong justification is often that existing theories yield conflicting predictions and so your study will offer some insight into how to adjudicate betweeen these theories. A less strong justification is that no one has ever studied this before. Even if this is true (and it’s often not) it may be true for good reason. No need for formal citations, but if there are specific theories or claims your addressing feel free to name names.\nThe ideal experiment Please describe an “ideal” experiment that you could run that would give you some purchase on your question. Note the key feature of an experiment, is that you the researcher are able to manipulate (through random assignment) some facet of the world. Assume money, resources, physics, and even ethics are not an object. If you could randomly assign anything, what would you manipulate. At what level of analysis would your manipulation occur (i.e. are your units of analysis individuals or countries or something else). How would you measure your outcome, again assuming you were all power and all-seeing. If that manipulation isn’t feasible, what does that say about the ability to make a causal claim about your question?\nThe observational study Finally, considering some of the potential limitations that might prevent you from implementing your ideal experiment (it’s hard to randomly assign democratic government), what is one way you might address your research question with observational data. Would your study use cross-sectional or longitudinal data. What are some of the concerns (selection on observables) that arise in this setting. Is there a natural experiment or some sort of discontinuity you might leverage to approximate this experimental ideal.\n\nAgain, each paragraph should be brief and to the point. No need to specify a full research design–just give me the broad strokes. You’re writing for each question should not exceed a page.\nAfter you’ve thought through how you might go about answering your question, please find\n\nA published study that relates to this question. It need not be exactly your question as posed, but it should be in a similar area. Include a full citation, and link to the study. Then in a paragraph sentences try to summarize:\n\n\nThe study’s research question\nEmprical design\nCore findings.\n\nFinally on a scale of 1 (least feasible) to 10 (most feasible), please evaluate how likely you think it is you could write an empirical paper on this question for this course.\nDon’t worry about getting everything right. Your final projects can, will, and probably should change. The point of this exercise is to get some practice thinking about questions that interest you in the language of causal inference and potential outcomes."
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study",
    "href": "assignments/A1_Research_Questions.html#the-observational-study",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study",
    "href": "assignments/A1_Research_Questions.html#a-published-study",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care-1",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care-1",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment-1",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment-1",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study-1",
    "href": "assignments/A1_Research_Questions.html#the-observational-study-1",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study-1",
    "href": "assignments/A1_Research_Questions.html#a-published-study-1",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10-1",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10-1",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care-2",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care-2",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment-2",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment-2",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study-2",
    "href": "assignments/A1_Research_Questions.html#the-observational-study-2",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study-2",
    "href": "assignments/A1_Research_Questions.html#a-published-study-2",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10-2",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10-2",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/a5.html",
    "href": "assignments/a5.html",
    "title": "A5: Presentation",
    "section": "",
    "text": "On Thursday you will present the initial results of your group projects.\nHere is a template which you can:  download . Please upload the rendered to Canvas before class on Thursday.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A5: Presentations"
    ]
  },
  {
    "objectID": "assignments/final.html",
    "href": "assignments/final.html",
    "title": "A5: Presentation",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "Final Paper"
    ]
  },
  {
    "objectID": "assignments/a3.html",
    "href": "assignments/a3.html",
    "title": "A3: Initial Analyses",
    "section": "",
    "text": "Overview\nAssignment 3 will also serve as your lab for week 11.\nThe broad goal for this assignment is to ensure that you’re making progress on your final project.\nThe final project that you will submit for this class consists of the following:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)\n\nThe specific expectations for each section are discussed in further detail in Assignment 4\nOur focus this week will be on the logistical aspects of your final project\n\nDownloading the template for your final paper\nMaking sure you’re set up for collaboration with GoogleDrive\nFinish wrangling your data in the code of your final paper\n\nLoading the data into R\nInspect the data\nCompleting recoding and if necessary, merging of the data\n\nBegin describing the data\nBegin outlining the other sections of your document (Intro, Theory, Data & Design, etc.)\n\nNext week, we will focus on estimating, interpreting and presenting your results.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A3: Exploratory Analysis"
    ]
  },
  {
    "objectID": "assignments/pols1600_paper_template.html",
    "href": "assignments/pols1600_paper_template.html",
    "title": "POLS 1600: Final Paper Template",
    "section": "",
    "text": "This document provides a template for the structure of your final paper for POLS 1600.\nReplace this markdown text with the introduction to your paper that:\n\nClearly articulates your group’s research question\nLays out the theoretical framework that motivates your inquiry\nDescribes your empirical strategy\nProvides an outline of the rest of the paper and previews your results."
  },
  {
    "objectID": "assignments/pols1600_paper_template.html#references",
    "href": "assignments/pols1600_paper_template.html#references",
    "title": "POLS 1600: Final Paper Template",
    "section": "6.1 References",
    "text": "6.1 References\nI’d use a bulleted list to format your references. Aim for at least three, academic citations.\n\nCitrin, J. (1974). Comment: The political relevance of trust in government. American Political Science Review, 68(3), 973-988.\nLerman, Amy E., and Vesla M. Weaver. (2014) Arresting Citizenship. University of Chicago Press, 2014.\nJeong, J., & Han, S. (2020). Trust in police as an influencing factor on trust in government: 2SLS analysis using perception of safety. Policing: An International Journal."
  },
  {
    "objectID": "assignments/pols1600_paper_template.html#code-book",
    "href": "assignments/pols1600_paper_template.html#code-book",
    "title": "POLS 1600: Final Paper Template",
    "section": "7.1 Code book",
    "text": "7.1 Code book\nYour code book should describe be organized conceptually by variable type:\n\nOutcome\nKey Predictors\nCovariates\n\nFor each variable, provide:\n\nvariable_name_used_in_code | Conceptual name\n\nDescription of variable/Survey Question Wording\nOriginal name in raw data.\nSummary of any recoding, transformations (e.g. Collapsing categories, or reverse coding endpoints of scales)\nSummary of range of values (e.g. 1= Strong Democrat … 7 = Strong Republican)\n\ntrust_in_gov | Trust in Government\n\nQuestion: “How often can you trust the federal government in Washington to do what is right?”\nOriginal variable: V201233\nRecoding: V201233 reverse coded so that in trust_in_gov, 0 = “Never”, 1 = “Some of the Time”, 2 = “About half the time”, 3 = “Most of the time” and 4 = “Always”\nRange: 0 (“Never) to 4 (”Always)"
  },
  {
    "objectID": "assignments/pols1600_paper_template.html#code-appendix",
    "href": "assignments/pols1600_paper_template.html#code-appendix",
    "title": "POLS 1600: Final Paper Template",
    "section": "7.2 Code Appendix",
    "text": "7.2 Code Appendix\nFinally, in your code appendix you will display the all the code from the previous code chunks in one single code chunk by:\n\nsetting echo = T, eval = F in the code chunk’s header\ntyping the &lt;&lt;chunk_label&gt;&gt; for each of your code chunks sequentially where chunk_label is replaced with the of the corresponding code chunk (e.g. `&lt;&gt;)\n\n\n# Pacakges used in analysis\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\", \"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"boot\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n# Uncomment to install | Recomment after installing\n# remotes::install_github(\"jamesmartherus/anesr\")\nlibrary(anesr)\n\n# Load 2020 NES\ndata(\"timeseries_2020\", package =\"anesr\")\n\n# rename timeseries_2020 to df\ndf &lt;- timeseries_2020\n\n\ndf %&gt;%\n  mutate(\n    # ---- OUTCOMES ----\n    \n    ## V201233 PRE: HOW OFTEN TRUST GOVERNMENT IN WASHINGTON TO DO WHAT IS RIGHT\n    ### Reverse code so 0 = Never, 4 = Always\n    trust_gov = ifelse(V201233 &lt; 0, NA, (V201233-5)*-1),\n    \n    # ---- KEY PREDICTORs ----\n    \n    ## V202457 POST: HAS R EVER BEEN ARRESTED\n    been_arrested = case_when(\n      V202457 == 2 ~ 0,\n      V202457 == 1 ~ 1,\n      T ~ NA_real_\n    ),\n    ## V202456: DURING PAST 12 MONTHS, R OR ANY FAMILY MEMBERS STOPPED BY POLICE\n    police_stop_12mo = case_when(\n      V202456 == 2 ~ 0,\n      V202456 == 1 ~ 1,\n      T ~ NA_real_\n    ),\n    \n    # ---- COVARIATES ----\n    \n    ## V201549x  PRE: SUMMARY: R self‐identified race/ethnicity\n    race_f = ifelse(\n      V201549x &lt; 0, NA,\n      # Remove numbers from race variable labels\n      gsub(\"^[[:graph:]]* \", \"\", as_factor(V201549x))\n    ),\n    # Relevel race_f so white is reference category\n    race_f = forcats::fct_relevel(race_f,\"White, non-Hispanic\"),\n    # Create label variable with line breaks for plotting\n    race_l = stringr::str_wrap(race_f, width = 20),\n    is_white = ifelse(race_f == \"White, non-Hispanic\", 1, 0),\n    is_black = ifelse(race_f == \"Black, non-Hispanic\", 1, 0),\n    is_asian = ifelse(race_f == \"Asian or Native Hawaiian/other Pacific Islander, non-Hispanic alone\", 1, 0),\n    is_hispanic = ifelse(race_f == \"Hispanic\", 1, 0),\n    is_multiracial = ifelse(race_f == \"Multiple races, non-Hispanic\", 1, 0),\n    is_NA_AN_other = ifelse(race_f == \"Native American/Alaska Native or other race, non-Hispanic alone\", 1, 0),\n    is_nonwhite = ifelse(is_white == 1, 0, 1),\n    ## V201617x PRE: SUMMARY: Total (family) income\n    income  = ifelse(V201617x &lt; 0, NA, V201617x),\n    income_class = case_when(\n      income &lt;= 6 ~ \"Low income\",\n      income &gt; 6 & income &lt; 17 ~ \"Middle income\",\n      income &gt;= 17 ~ \"High income\",\n      T ~ NA_character_\n    ) %&gt;% factor(., levels = c(\"Middle income\",\"Low income\",\"High income\")) # Make middle income reference category\n    \n  ) -&gt; df\n\n\n\n# ---- Check Recodes ----\n\n## Trust\ntable(df$V201233, df$trust_gov, useNA = \"ifany\")\n## CJS Contact\n\n### Respondented Arrested Ever\ntable(df$V202457, df$been_arrested, useNA = \"ifany\")\n\n### Respondent or peers/family been stopped by police in past year\ntable(df$V202456, df$police_stop_12mo, useNA = \"ifany\")\n\n## Race\ntable(df$race_f, df$V201549x, useNA = \"ifany\")\n### White indicator\ntable(df$race_f, df$is_white, useNA = \"ifany\")\n\n### Non-White indicator\ntable(df$race_f, df$is_nonwhite, useNA = \"ifany\")\n\n## Income\ntable(df$income, df$income_class)\n\n# ---- Descriptive Statistics ----\n\n# Variables for table of descriptive statistics:\nthe_vars &lt;- c(\"trust_gov\",\n              \"been_arrested\", \"police_stop_12mo\",\n              \"income\", df%&gt;%select(starts_with(\"is_\"))%&gt;%names())\n\n# Create table of summary statistics\n\ndf %&gt;%\n  select(all_of(the_vars))%&gt;%\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) %&gt;%\n  mutate(\n    Variable = case_when(\n      Variable == \"been_arrested\" ~ \"Ever Arrested\",\n      Variable == \"income\" ~ \"Income\",\n      Variable == \"police_stop_12mo\" ~ \"Stopped by Police in Past Year\",\n      Variable == \"is_nonwhite\" ~ \"Non-White\",\n      Variable == \"is_white\" ~ \"White\",\n      Variable == \"is_black\" ~ \"Black\",\n      Variable == \"is_hispanic\" ~ \"Hispanic\",\n      Variable == \"is_asian\" ~ \"Asian\",\n      Variable == \"is_NA_AN_other\" ~ \"Native American, Alaskan Native, or Other\",\n      Variable == \"is_multiracial\" ~ \"Multiracial\",\n      Variable == \"trust_gov\" ~ \"Trust in Government\"\n    )\n  ) -&gt; sum_df\n\n\n\n# ---- Descriptive Figures ----\n\n\n## Distribution of trust in Government\nfig1 &lt;- df %&gt;%\n  ggplot(aes(trust_gov))+\n  geom_histogram()+\n  labs(\n    x = \"Trust in Government\"\n  )\n\n## Average proportion of respendents reporting Police Stops by Race\n\nfig2 &lt;- df %&gt;%\n  filter(!is.na(race_l))%&gt;%\n  group_by(race_l)%&gt;%\n  summarise(\n    stop = mean(police_stop_12mo, na.rm=T)\n  )%&gt;%\n  mutate(\n    race_l = fct_reorder(race_l,stop)\n  )%&gt;%\n  ggplot(aes(race_l, stop, \n             fill = race_l,\n             label = scales::percent(stop),\n             ))+\n  geom_bar(stat = \"identity\")+\n  scale_y_continuous(labels = scales::percent,\n                     expand = expansion(mult = c(0,0.5))\n                     )+\n  labs(\n    y = \"% Experiencing Police Stop\\nin Past Year\",\n    x= \"\"\n  )+\n  geom_text_repel(direction = \"x\", hjust = 1)+\n  coord_flip()+\n  guides(fill = \"none\")+\n\n  theme_bw()\n\n## Average trust by Arrest Status\nfig3 &lt;- df %&gt;%\n  mutate(\n    `Ever Arrested` = ifelse(been_arrested == 1, \"Yes\", \"No\")\n  ) %&gt;%\n  filter(!is.na(`Ever Arrested`))%&gt;%\n  ggplot(aes(`Ever Arrested`,trust_gov, \n             fill = `Ever Arrested`,\n             group = `Ever Arrested`))+\n  stat_summary()+\n  labs(\n    y = \"Trust in Government\\n(0 = Never, 4 = Always)\"\n  )\n\n\n# Table of descriptive statistics\nknitr::kable(sum_df,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =3) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 4, end_row =11)\n\n\nfig1\n\nfig2\n\n# Figure 3\nfig3\n\n# ---- Regression Models ----\n\n\n# Bivariate\nm1 &lt;- lm(trust_gov ~ been_arrested, df)\nm2 &lt;- lm(trust_gov ~ police_stop_12mo, df)\nm3 &lt;- lm(trust_gov ~ race_f, df)\nm4 &lt;- lm(trust_gov ~ income_class, df)\n\n# Mutliple regression\nm5 &lt;- lm(trust_gov ~ been_arrested + police_stop_12mo + race_f + income_class, df)\n\n\n\ntexreg::htmlreg(\n  list(m1, m2, m3,m4),\n  custom.coef.names = c(\"(Intercept)\",\n                        \"Ever Arrested\", \"Police Stopped in Past Yr\", \n                        \"Asian\",\n                        \"Black\",\n                        \"Hispanic\",\n                        \"Multiracial\",\n                        \"NA, AN, or Other\",\n                        \"Low Income\",\n                        \"High Income\"),\n  custom.model.names = c(\"Arrests\",\"Stops\",\"Race\",\"Class\"),\n  custom.header = list(\"Outcome: Trust in Government\" = 1:4),\n  caption = \"Baseline Models\"\n  \n)\n\ntexreg::htmlreg(\n  list(m5),\n  custom.coef.names = c(\"(Intercept)\",\n                        \"Ever Arrested\", \"Police Stopped in Past Yr\", \n                        \"Asian\",\n                        \"Black\",\n                        \"Hispanic\",\n                        \"Multiracial\",\n                        \"NA, AN, or Other\",\n                        \"Low Income\",\n                        \"High Income\"),\n  ci.force = T,\n  caption = \"Multiple regression\"\n  \n)\n\n\n# Figure 4\nfig4"
  },
  {
    "objectID": "labs/07-lab.html",
    "href": "labs/07-lab.html",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we will begin the process of replicating Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages and set the working directory to where this file is saved. (5 minutes)\nSummarize the study in terms of it’s research question, theory, design, and results. (10 minutes)\nDownload the replication files and save them in the same folder as this lab (5 minutes)\nLoad the data from your computers into R (5 minutes)\nGet a quick HLO of the data (10 minutes)\nMerge data on election policy into data on voting (5 minutes, together),\nRecode the covariates, key predictors, and outcome for the study (10 minutes, partly together)\nRecreate Figure 1 (15 minutes)\nRecreate Figure 2 (15 minutes)\n\nFinally, we’ll take the weekly survey which should be a fun one\nOne of these 8 tasks will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/07-lab.html#please-render-this-.qmd-file",
    "href": "labs/07-lab.html#please-render-this-.qmd-file",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/07-lab.html#load-packages",
    "href": "labs/07-lab.html#load-packages",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nWe will also want to set our working directory to where your lab is saved."
  },
  {
    "objectID": "labs/07-lab.html#important-set-your-working-directory",
    "href": "labs/07-lab.html#important-set-your-working-directory",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "IMPORTANT: Set your working directory",
    "text": "IMPORTANT: Set your working directory\n\nOn the top panel of R Studio click:\n\n\nSession Session &gt; Set working directory &gt; Source file location\n\n\nPaste the output that shows up in your console into the code chunk below\n\n\n# Set working directory\n# Session &gt; Set working directory &gt; Source file location\n# paste output here:\n\nAll right, now let’s summarize the study"
  },
  {
    "objectID": "labs/07-lab.html#recode-covariates",
    "href": "labs/07-lab.html#recode-covariates",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.1 Recode covariates",
    "text": "6.1 Recode covariates\nThe CPS are messy data. Please run the code below to recode the covariates in the spirit of (i.e. with minor changes) what Grumbach and Hill did. 2\n\n\n\n\n\n\nNote\n\n\n\nThe file cps_00021.cbk.txt contains the codebook for the data, telling us what numeric values of each variable correspond to substantively. So if you’re wondering how I know what should be recoded to what specific values, it comes from reading the codebook, looking at Grumbach and Hill’s code, looking at the raw variable with a table, and the using case_when() to judiciously code the data. You’ll get practice doing this in your final projects, but I don’t want to spend too much time on this this lab, which is why you’re only recoding the outcome voted\n\n\n\n# # Recode covariates\n# cps %&gt;% \n#   mutate(\n#     # Useful for plotting figure 2\n#     SDR = ifelse(sdr == 1, \"SDR\", \"non-SDR\"),\n#     education = case_when(\n#       educ == 1 ~ NA, #Blank\n#       educ &lt; 40 ~ 1, # No high school\n#       educ &gt;= 40 & educ &lt; 73 ~ 2, # Some high school\n#       educ == 73 ~ 3, # High school degree\n#       educ &gt;= 80 & educ &lt;= 110 ~ 4, # Some college\n#       educ &gt;= 111 & educ &lt;123 ~ 5, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n#       educ &gt;= 123 & educ &lt;=125 ~ 6, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n#       educ == 999 ~ NA # Missing/unknown\n#     ),\n#     race_f = case_when(\n#       race == 999 ~ NA,\n#       T ~ factor(race)\n#     ),\n#     is_white = case_when(\n#       race == 100 ~ 1,\n#       race == 999 ~ NA,\n#       T ~ 0\n#     ),\n#     is_black = case_when(\n#       race == 200 ~ 1,\n#       race == 999 ~ NA,\n#       T ~ 0\n#     ),\n#     is_aapi = case_when(\n#       race == 650 ~ 1,\n#       race == 651 ~ 1,\n#       race == 652 ~ 1,\n#       race == 999 ~ NA,\n#       T ~ 0\n#     ),\n#     is_other = case_when(\n#       is_white == 1 ~ 0,\n#       is_black == 1 ~ 0,\n#       is_aapi ==  1 ~ 0,\n#       race == 999 ~ NA,\n#       T ~ 1\n#     ),\n#     income = case_when(\n#       faminc &gt; 843 ~ NA, # Remove Missing/Refused\n#       T ~ as.numeric(factor(faminc))\n#     ),\n#     is_female = case_when(\n#       sex == 2 ~ 1,\n#       sex == 1 ~ 0,\n#       T ~ NA # recode Not in Universe as NA\n#     )\n#     \n#   ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab.html#create-age_group-and-age_group_xx_xx-indicators",
    "href": "labs/07-lab.html#create-age_group-and-age_group_xx_xx-indicators",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.2 Create age_group and age_group_XX_XX indicators",
    "text": "6.2 Create age_group and age_group_XX_XX indicators\nNext we’ll create an age_group variable and binary indicators for each age cohort of the form age_group_XX_XX.\nPlease uncomment and run the code below\n\n# # Create age variables\n# cps %&gt;% \n#   mutate(\n#     age_group = case_when(\n#       age &gt; 18 & age &lt;= 24 ~ \"18-24\",\n#       age &gt; 24 & age &lt;= 34  ~ \"25-34\",\n#       age &gt; 34 & age &lt;= 44  ~ \"35-44\",\n#       age &gt; 44 & age &lt;= 54  ~ \"45-54\",\n#       age &gt; 54 & age &lt;= 64  ~ \"55-64\",\n#       age &gt; 64 ~ \"65+\",\n#       T ~ NA\n# \n#     ),\n#     age_group_18_24 = ifelse(age_group == \"18-24\", 1, 0),\n#     age_group_25_34 = ifelse(age_group == \"25-34\", 1, 0),\n#     age_group_35_44 = ifelse(age_group == \"35-24\", 1, 0),\n#     age_group_45_54 = ifelse(age_group == \"45-24\", 1, 0),\n#     age_group_55_64 = ifelse(age_group == \"55-24\", 1, 0),\n#     age_group_65plus = ifelse(age_group == \"65+\", 1, 0)\n#   ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab.html#check-age-recodes",
    "href": "labs/07-lab.html#check-age-recodes",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.3 Check age recodes",
    "text": "6.3 Check age recodes\nIt’s good practice when recoding, to check the output. Please use the table() to create a cross-tab of age_group and age_group_18_24.\n\n#|label: checkage\n\n# Compare age_group to age_group_18_24 using table()\n\nExplain in words how the variable age_group_18_24 relates to the variable age_group"
  },
  {
    "objectID": "labs/07-lab.html#recode-the-outcome",
    "href": "labs/07-lab.html#recode-the-outcome",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.4 Recode the outcome",
    "text": "6.4 Recode the outcome\nNow it’s your turn. Please do the following:\n\nLook at the variable voted using the table() function\n\n1 corresponds to Did not vote\n`2 corresponds to Voted\n96,97,98 to people who didn’t provide and answer, or didn’t remember\n99 corresponds to people who shouldn’t be in the sample (“Not in universe”)\n\nCreate a new variable called dv_voted using case_when() inside of mutate() that is:\n\n1 when voted == 2\n0 when voted == 1,\n0 when voted &gt; 2 & voted &lt;99\nNA when voted == 99\n\n\n\n# Look at distribution of voted using table()\n\n\n# Create variable dv_voted using mutate(), case_when(), and voted variable"
  },
  {
    "objectID": "labs/07-lab.html#save-the-recoded-data",
    "href": "labs/07-lab.html#save-the-recoded-data",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.5 Save the recoded data",
    "text": "6.5 Save the recoded data\nFinally, let’s save our recoded data to file called cps_clean.rda that we can use for next week’s lab\nUncomment and run the following:\n\n# save(cps, file = \"cps_clean.rda\")"
  },
  {
    "objectID": "labs/07-lab.html#write-down-aesthetic-mappings-from-the-figure",
    "href": "labs/07-lab.html#write-down-aesthetic-mappings-from-the-figure",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.1 Write down aesthetic mappings from the figure:",
    "text": "7.1 Write down aesthetic mappings from the figure:\nBefore we create this figure, think about the information conveyed by the figure’s aesthetics (the x axis, the y axis, the color of the squares), and the corresponding columns from policy_data that contain this information.\n\nx-axis:\ny-axis:\ncol:"
  },
  {
    "objectID": "labs/07-lab.html#create-a-variable-called-sdr",
    "href": "labs/07-lab.html#create-a-variable-called-sdr",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.2 Create a variable called SDR",
    "text": "7.2 Create a variable called SDR\nIt will be helpful to have a variable called SDR in policy_data that takes the value of “SDR” when sdr == 1 and “non-SDR” when sdr == 0\nPlease use case_when() or ifelse() to create SDR in policy_data\n\n# Create a variable called SDR in policy_data"
  },
  {
    "objectID": "labs/07-lab.html#recreate-figure-1-1",
    "href": "labs/07-lab.html#recreate-figure-1-1",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.3 Recreate Figure 1",
    "text": "7.3 Recreate Figure 1\nRecall, we need three things to make a figure:\n\ndata\naesthetics\ngeometries\n\nUsing data from policy_data starting in 1978 (hint add a filter()) and the aesthetic mappings identified above use ggplot() with the geom_point() geometry to make a version of Figure 1 from paper.\n\n# Recreate Figure 1"
  },
  {
    "objectID": "labs/07-lab.html#interpret-figure-1.",
    "href": "labs/07-lab.html#interpret-figure-1.",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.4 Interpret Figure 1.",
    "text": "7.4 Interpret Figure 1.\nPlease answer the following questions:\n\nHow many states had Same Day Registration at some point in time? XX states\nHow many states had Same Day Registration in 2018? XX states had SDR in 2018\nDid any states get rid of Same Day Registration? When did they get rid of this policy?\nWhat’s up with North Dakota?\n\nUse this code chunk to write any code that might help you answer these questions\n\n# Write code to help you answer the questions above (if needed)"
  },
  {
    "objectID": "labs/07-lab.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "href": "labs/07-lab.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.1 Calculate the proption voting by age group and SDR",
    "text": "8.1 Calculate the proption voting by age group and SDR\nWith the cps data, use group_by() and summarize to calculate the proportion of people voting by age group in states that did and did not have same day registration in the code chunk below.\nSave the results to a new object called fig2_df\n\n#  Calculate the proportion of voting by age group and SDR"
  },
  {
    "objectID": "labs/07-lab.html#recreate-figure-2-1",
    "href": "labs/07-lab.html#recreate-figure-2-1",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.2 Recreate Figure 2",
    "text": "8.2 Recreate Figure 2\nUsing fig2_df recreate a Figure 2 from the paper:\n\nfilter out values of age_group that are NA\nset the appropriate aesthetic mappings in ggplot()\nuse geom_bar(stat = \"identity\", position = \"dodge\")\n\n\n#Recreate Figure 2"
  },
  {
    "objectID": "labs/07-lab.html#interpret-figure-2",
    "href": "labs/07-lab.html#interpret-figure-2",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.3 Interpret Figure 2",
    "text": "8.3 Interpret Figure 2\nWhat does Figure 2 tell us? Figure 2 provides …"
  },
  {
    "objectID": "labs/07-lab.html#footnotes",
    "href": "labs/07-lab.html#footnotes",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe CPS coding on this is not great and there’s no measure of ethnicity in these data. Forgive the crude indicators, but their necessary to recreate some of Grumbach and Hill’s analysis next week. ↩︎\nNote the way the recoding is described in the appendix to the paper is not how it is actually implemented in the replication code in rock_the_reg_replication_code.R. For example, the appendix describes income as ranging from 1 (Under $10k) to 16 ($500k and above), when their code, implemented above produces 32 unique values, in part because the way the CPS asked and coded the income question changed overtime. We’re going to roll with it for now…↩︎"
  },
  {
    "objectID": "labs/06-lab.html",
    "href": "labs/06-lab.html",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "",
    "text": "Today we will explore the critiques and alternative explanations for the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nRecall the core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\n\nA skeptic of this claim might argue that relationship between electoral and epidemelogical outcomes is spurious, saying somthing like:\n\nThere are lots of ways that Red States differ from Blue States — demographics, economics, geography, culture, and so on – and it is these differences that explain the phenomena of Red Covid. If we were to control for these omitted variables the relationship between a state’s partisan leanings and Covid-19 would go away.\n\nIn this lab, we will see how we can explore these claims using multiple regression to control for competing explanations.\nTo accomplish this we will:\n\nGet set up to work (10 minutes)\n\nThen we will estimate and interpret a series of regression models:\n\nA baseline Red Covid model using simple bivariate regression using the Republican vote share of states to predict the 14-day average of per capita Covid-19 deaths on September 23, 2021 (10 Minutes)\nA multiple regression model controlling for Republican vote share the median age (15 minutes)\nA model controlling for Republican vote share, the median age and median income (15 minutes)\nA model controlling for Republican vote share, the median age median income and vaccination rates (15 minutes)\nA model using Republican vote share, the median age median income to predict vaccination rates (15 minutes)\n\nFinally, we’ll take the weekly survey which will serve as a mid semester check in.\nOne of these 6 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/06-lab.html#load-packages",
    "href": "labs/06-lab.html#load-packages",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\nThere’s one new package, htmltools which we’ll use to display regression tables while we work.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "labs/06-lab.html#load-the-data",
    "href": "labs/06-lab.html#load-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the data that we created in class on Tuesday which provides a snapshot of the state of Covid-19 on September 23, 2021 in the U.S.\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nAfter running this code, the data frame covid_lab should appear in your environment pane in R Studio"
  },
  {
    "objectID": "labs/06-lab.html#describe-the-data",
    "href": "labs/06-lab.html#describe-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nIn the code chunk below, please write some code get an high level overview of the data:\n\n# High level overview\n# Number of observations and variables\n\n\n# Names of variables\n\n\n# Glimpse of data\n\n\n# Summary of data\n\n\n# Calculate standard deviations\n\nPlease use this HLO to answer the following questions:\n\nHow many observations are there:\nWhat is an observation (i.e. what is the unit of analysis):\nWhat is the primary outcome variable for today:\nWhat are the four main predictors we’ll be using:\nWill we be using the the raw values of these predictors or their standardized values?\nWhat is the standard deviation of our outcome and predictor variables:\n\nCovid-19 deaths:\nRepublican vote share:\nMedian age:\nMedian income:\nVaccination Rate:"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model",
    "href": "labs/06-lab.html#fit-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.1 Fit the model",
    "text": "2.1 Fit the model\n\nm1 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results",
    "href": "labs/06-lab.html#summarize-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.2 Summarize the results",
    "text": "2.2 Summarize the results\nNow we apply the summary() function to our model m1\n\nsummary(m1)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63271 -0.22488 -0.03769  0.13746  1.00634 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04817  11.741 7.51e-16 ***\nrep_voteshare_std  0.22682    0.04865   4.662 2.44e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.344 on 49 degrees of freedom\nMultiple R-squared:  0.3073,    Adjusted R-squared:  0.2931 \nF-statistic: 21.73 on 1 and 49 DF,  p-value: 2.44e-05\n\n\nWe see that m1 returns two coefficients, which define a line of best fit predicting Covid-19 deaths with the Republican vote share of the 2020 Presidential election:\n\n\\(\\beta_0\\) corresponds to the intercept. This is model’s prediction for a state where Trump got 0 percent of the vote. This is typically not something we care about.\n\\(\\beta_1\\) corresponds to the slope. Because we used a standardized measure of vote share, we would say that a 1-standard deviation (about 10 percentage points) increase in Republican vote share is associated with a 0.23 increase the average number of new Covid-19 deaths. Given that this per-capita measure has a standard deviation of 0.4, this is a fairly sizable association.\nFinally, note that last column of summary(m1) Pr(&gt;|t|) both the coefficients for the intercept \\((\\beta_0)\\) and rep_voteshare_std (\\((\\beta_1)\\)) are statistically significant (ie have an * next to them)."
  },
  {
    "objectID": "labs/06-lab.html#display-the-model-as-a-regression-table",
    "href": "labs/06-lab.html#display-the-model-as-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.3 Display the model as a regression table",
    "text": "2.3 Display the model as a regression table\nNext we’ll format the results of summary(m1) into a regression table using the htmlreg() function.\nRegression tables are a the standard way of concisely presenting the results of regression models.\n\nEach named row corresponds to the coefficients form the model\nIf there is an asterisks next to a coefficient, that coefficient is statistically significant with a p value below a certain threshold.\nThe numbers in parentheses below each coefficient correspond to the standard error of the coefficient (more on that later)2\nThe bottom of the table contains summary statistics of of our model, which we’ll ignore for today.\n\nThe code after htmlreg(m1) allows you to see what output of the table will look like in the html document while you’re working in the qmd file.\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n0.57***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab.html#visualize-the-model",
    "href": "labs/06-lab.html#visualize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.4 Visualize the model",
    "text": "2.4 Visualize the model\nNow let’s visualize the results of our m1 with a scatter plot.\nIn the code chunk below, I’ve written some comments to help you get started. You can also refer to last week’s lab for help\n\n# 1. Tell ggplot what data to use\n\n# 2. Set the aesthetic mappings of our figure\n \n# 3. Draw points with x values corresponding to Rep vote share and y values corresponding to Covid deaths. \n\n# 4. Add labels using `label=state_po` aesthetic (set in aes()) above\n\n# 5. Plot the regression model using geom_smooth(method = \"lm\")\n\n# 6. Change the axis labels"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results",
    "href": "labs/06-lab.html#interpret-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.5 Interpret the results",
    "text": "2.5 Interpret the results\nIn a sentence our two, summarize the results of your analysis in this section\nYou words here!"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-1",
    "href": "labs/06-lab.html#fit-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.1 Fit the model",
    "text": "3.1 Fit the model\nNow let’s test our skeptics’ claims by fitting a model m2 that controls for Age (med_age_std).\n\nRemember the first argument in lm() is formula of the form outcome variable ~ predictor1 + predictor2 + ...\n\n\n# m2"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-model",
    "href": "labs/06-lab.html#summarize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.2 Summarize the model",
    "text": "3.2 Summarize the model\nNow let’s print out a statistical summary of m2\n\n# summary of m2"
  },
  {
    "objectID": "labs/06-lab.html#display-the-model-as-a-regression-table-1",
    "href": "labs/06-lab.html#display-the-model-as-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.3 Display the model as a regression table",
    "text": "3.3 Display the model as a regression table\nNext, let’s create a regression table that displays m1 in the first column and m2 in the second column.\n\nTo do this, change list(m1) from the code above to list(m1, m2)"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results-1",
    "href": "labs/06-lab.html#interpret-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.4 Interpret the results",
    "text": "3.4 Interpret the results\nIn a few sentences, explain whether the results from m2 support the skeptics criticisms or not?"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-2",
    "href": "labs/06-lab.html#fit-the-model-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.1 Fit the Model",
    "text": "4.1 Fit the Model\nPlease fit a model called m3 implied by the skeptic’s revised claims\n\n# m3"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-model-1",
    "href": "labs/06-lab.html#summarize-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.2 Summarize the model",
    "text": "4.2 Summarize the model\nSummarize the model m3 using summary()\n\n# summary m3"
  },
  {
    "objectID": "labs/06-lab.html#display-the-models-in-a-regression-table",
    "href": "labs/06-lab.html#display-the-models-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.3 Display the models in a regression table",
    "text": "4.3 Display the models in a regression table\nAnd then display the results of models m1, m2, and m3.\n\n# regression table of m1, m2, m3"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-skeptics-claims",
    "href": "labs/06-lab.html#interpret-the-skeptics-claims",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.4 Interpret the skeptic’s claims",
    "text": "4.4 Interpret the skeptic’s claims\nIn a few sentences, explain whether the results from m3 support the skeptics criticisms or not?\nControlling for median age and income, the coefficient on Republican sote share decreases in size by more than half and is no longer statistically significant. The coefficient on median income is statistically significant and substantively suggests that states with higher median incomes tended to have fewer Covid-19 deaths on September 23, 2021."
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-3",
    "href": "labs/06-lab.html#fit-the-model-3",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.1 Fit the model",
    "text": "5.1 Fit the model\nYou know the drill.\n\n# m4"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results-1",
    "href": "labs/06-lab.html#summarize-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.2 Summarize the results",
    "text": "5.2 Summarize the results\nAgain, let’s get a quick summary of our results\n\n# summary of m4"
  },
  {
    "objectID": "labs/06-lab.html#display-the-models-in-a-regression-table-1",
    "href": "labs/06-lab.html#display-the-models-in-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.3 Display the models in a regression table",
    "text": "5.3 Display the models in a regression table\nAnd add m4 to list of models in our regression table"
  },
  {
    "objectID": "labs/06-lab.html#interpet-the-results",
    "href": "labs/06-lab.html#interpet-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.4 Interpet the results",
    "text": "5.4 Interpet the results\nBriefly interpret the results of m4"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-4",
    "href": "labs/06-lab.html#fit-the-model-4",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.1 Fit the model",
    "text": "6.1 Fit the model\nNow let’s fit the model. For ease of interpretation, let’s use the unstandardized measure of vaccination rates, percent_vaccinated as our outcome variable.\n\n# m5"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results-2",
    "href": "labs/06-lab.html#summarize-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.2 Summarize the results",
    "text": "6.2 Summarize the results\nAnd summarize the results\n\n# summary of m5"
  },
  {
    "objectID": "labs/06-lab.html#display-the-results-in-a-regression-table",
    "href": "labs/06-lab.html#display-the-results-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.3 Display the results in a regression table",
    "text": "6.3 Display the results in a regression table\nDisplay them in a regression table"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results-2",
    "href": "labs/06-lab.html#interpret-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.4 Interpret the results",
    "text": "6.4 Interpret the results\nSummarize the results of m5 and offer some broader discussion of what we’ve learned today"
  },
  {
    "objectID": "labs/06-lab.html#footnotes",
    "href": "labs/06-lab.html#footnotes",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn short, these * correspond to \\(p-values\\) below different thresholds. One * typically means \\(p &lt; 0.05\\). A p-value is a conditional probability that arises from a hypothesis test summarizing the likelihood of observing a particular test statistic (here a regression coefficient, or more specifically, a t-statistic which is the regression coefficient divided by its standard error) given a paritcular hypothesis (typically, but not allows a null hypothesis that the true coefficient is 0). In sum, a p-value assess the likelihood of seeing what we did, if in fact, there was no relationship. If that likelihood is small (p&lt;0.05), we reject the claim of no relationship. We remain uncertain about the true value of the coefficient, but we are pretty confident it’s not 0.↩︎\nA standard error is another one of those things that in the cart we’re putting before horse today. Briefly, it is an estimate of the standard deviation of the sampling distribution of a coefficient and describes how much our coefficient might vary had we had a different sample…↩︎"
  },
  {
    "objectID": "labs/04-lab.html",
    "href": "labs/04-lab.html",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "In this assignment, we’ll walk through the logic and design of Ferwerda and Miller (2014).\n\n\nConceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/04-lab.html#goals",
    "href": "labs/04-lab.html#goals",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "Conceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/02-lab-comments.html",
    "href": "labs/02-lab-comments.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to first, reproduce this figure:\n\n\n\n\n\n\n\n\n\nAnd then adapt and improve this figure (or other figures) to explore questions we have about the data\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides and the comments to last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nHow do I produce a figure that shows the average rate of new cases pe month for states with a particular type of face mask policy?\n\nWell first, I’ll need to load some packages to work with and visualize data.\nThen, I’ll need to get the data. And then…\n\n\nThink and write programmatically\n\nIn this .qmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll be working through this collectively\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nYou shouldn’t have to write much new code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!\n.html file{.unnumbered}\nFor every lab:\n\nDownload the file\nSave it in your course folder\nRender the document\nOpen the html file in your browser (Easier to read)\nCheck Render on Save and render the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "href": "labs/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.1 Create an object listing all the packages I will use today",
    "text": "2.1 Create an object listing all the packages I will use today\nThis code creates a object called the_packages which contains a vector of character strings corresponding to the names of the packages I want to use today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)"
  },
  {
    "objectID": "labs/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "href": "labs/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.2 Define a function to install and load packages",
    "text": "2.2 Define a function to install and load packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "labs/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "href": "labs/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.3 Use the ipak function to load the necessary packages",
    "text": "2.3 Use the ipak function to load the necessary packages\nNow I run the ipak() giving it the object the_packages as an input. It sorts through the packages, checks to see if they’re installed, if not installs them, and then loads all of the packages so I can use them.\n\nipak(the_packages)\n\n   tinytex kableExtra  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally    COVID19 \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n      maps    mapdata         DT \n      TRUE       TRUE       TRUE"
  },
  {
    "objectID": "labs/02-lab-comments.html#filter-out-u.s.-territories",
    "href": "labs/02-lab-comments.html#filter-out-u.s.-territories",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.1 Filter out U.S. Territories",
    "text": "5.1 Filter out U.S. Territories\nFor simplicity, (and practice filtering observations), I’ve asked us to remove observations from U.S. territories.\nThe code below\n\nCreates an object called us_territories.\nUse this object to filter out observations that are US territories\nCreates a new data frame that is just observations from the 50 U.S. states. and D.C.\nChecks that this recoding seems to have worked\n\n\n# U.S. Territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out U.S. Territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\n# Check to make sure covid_us contains only 50 states and D.C.\ndim(covid)\n\n[1] 58809    47\n\ndim(covid_us)\n\n[1] 53678    47\n\nlength(unique(covid$administrative_area_level_2)) \n\n[1] 56\n\nlength(unique(covid_us$administrative_area_level_2)) == 51\n\n[1] TRUE"
  },
  {
    "objectID": "labs/02-lab-comments.html#create-a-state-variable",
    "href": "labs/02-lab-comments.html#create-a-state-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.2 Create a state variable",
    "text": "5.2 Create a state variable\nThis is purely for convenience, because typing administrative_area_level_2 is annoying. The code copies the values of this variable into a new variable called state using the mutate() function.\nMutate returns the original data frame plus the new column. We have to save this output for our our changes to persist (i.e. we have to assign the output of mutate() back into covid_us)\nIn last week’s lab, I just piped the output to the next command, did some more recoding with mutate, and then finally saved the output back into covid_us. In this lab, I’ll save the output after each step.\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) -&gt; covid_us"
  },
  {
    "objectID": "labs/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "href": "labs/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.3 Group by the state variable to calculate new Covid-19 cases",
    "text": "5.3 Group by the state variable to calculate new Covid-19 cases\nNow I use this shorter variable state to calculate the number of new cases (new_cases) in a given state on a given date, and rescale this variable so that it’s expressed in terms of new cases per 100,000 residents.\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population * 100000\n    ) -&gt; covid_us\n\nThe slides from Tuesday, helped demonstrate what this code was doing, and why we wanted to group by state.\nHere’s an example for a subset of the data from April 1, 2020 to April 7, 2020\nWe see that the lag() function simply moves the observation of a variable “up” one row so that we can take the difference between the total number of cases in a state on one date and the total number of cases on the date before, to calculate the number of new cases\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  group_by(state) %&gt;%\n  select(state, date, confirmed) %&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\n# A tibble: 306 × 5\n# Groups:   state [51]\n   state      date       confirmed confirmed_lag1 new_cases\n   &lt;chr&gt;      &lt;date&gt;         &lt;int&gt;          &lt;int&gt;     &lt;int&gt;\n 1 Minnesota  2020-04-01       689             NA        NA\n 2 Minnesota  2020-04-02       742            689        53\n 3 Minnesota  2020-04-03       789            742        47\n 4 Minnesota  2020-04-04       865            789        76\n 5 Minnesota  2020-04-05       935            865        70\n 6 Minnesota  2020-04-06       986            935        51\n 7 California 2020-04-01      9857             NA        NA\n 8 California 2020-04-02     11190           9857      1333\n 9 California 2020-04-03     12569          11190      1379\n10 California 2020-04-04     13796          12569      1227\n# ℹ 296 more rows\n\n\nIf we hadn’t grouped by state, then when we lagged the confirmed variable, R thinks the number of confirmed cases in California before April 1, 2020, is 986 which is actually the number of cases in Minnesota on April 7, 2020\n\n# No group_by would create errors where the last observation from\n# one state becomes the first lagged observation for the next state\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;%\n  ungroup() %&gt;%\n  select(state, date, confirmed) %&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\n# A tibble: 306 × 5\n   state      date       confirmed confirmed_lag1 new_cases\n   &lt;chr&gt;      &lt;date&gt;         &lt;int&gt;          &lt;int&gt;     &lt;int&gt;\n 1 Minnesota  2020-04-01       689             NA        NA\n 2 Minnesota  2020-04-02       742            689        53\n 3 Minnesota  2020-04-03       789            742        47\n 4 Minnesota  2020-04-04       865            789        76\n 5 Minnesota  2020-04-05       935            865        70\n 6 Minnesota  2020-04-06       986            935        51\n 7 California 2020-04-01      9857            986      8871\n 8 California 2020-04-02     11190           9857      1333\n 9 California 2020-04-03     12569          11190      1379\n10 California 2020-04-04     13796          12569      1227\n# ℹ 296 more rows"
  },
  {
    "objectID": "labs/02-lab-comments.html#recode-the-facial_coverings-variable",
    "href": "labs/02-lab-comments.html#recode-the-facial_coverings-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.4 Recode the facial_coverings variable",
    "text": "5.4 Recode the facial_coverings variable\nNext we use the case_when() function inside the mutate() function to create a variable called face_masks based on the values of the facial_coverings variable in the data.\ncase_when() when uses R’s ability to make logical comparisons. When the variable facial_coverings equals 0, R will input the character string \"No policy\" into the face_masks variable.\nWhen the absolute value of facial_coverings equals 1 (i.e.facial_coverings equals 1 or -1 ), R will input the character string \"Recommended\" into the face_masks variable. And so on.\nWe use the abs() function to take the absolute value of the facial_coverings variable because codebook for these data implied:\n\nIn short: positive integers identify policies applied to the entire administrative area. Negative integers are used to identify policies that represent a best guess of the policy in force, but may not represent the real status of the given area. The negative sign is used solely to distinguish the two cases, it should not be treated as a real negative value.\n\nWe know from last weeks lab, that negative values in the U.S. typically seem to be cases where a city had a more stringent policy than the state (e.g. Chicago adopts more stringent face mask policies than Illinois).\nFinally, we put a %&gt;% after the output of case_when() and pass it’s output to the factor() function.\nThe . acts as sort of placeholder, factor() expects some input here (like a variable from a data frame), . tells R to use the output of case_when().\nThe levels = then transforms the character data produced by case_when() into a factor with an implicit ordering of levels (i.e. “No policy” &lt; “Recommended”&lt; “Some requirements” &lt;“Required shared places” &lt;“Required all times”) which turns out to be useful trick for organizing how data are plotted and visualized.\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "href": "labs/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.5 Create a variable capturing the year and month of the observation",
    "text": "5.5 Create a variable capturing the year and month of the observation\nFinally we create some variables that extract components of an observation’s date:\n\nyear = year(date) returns just the year from a variable of class Date\nmonth = month(date) returns just the month from a variable of class Date\nyear_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\") pastes these to variables together.\nstr_pad(month, width = 2, pad=0) adds a leading 0 to any month with only 1 digit, to ensure that all the months have 2 characters.\n\nThe code from your lab also calculates the percent of a states population that is vaccinated, which isn’t strictly needed for today.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nCreating separte year and month variables aren’t strictly necessary,\nWe could have written something like:\n\ncovid_us %&gt;%\n  mutate(\n    year_month = paste(year(date), str_pad(month(date), width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nBut that year_month line was already feeling kind of clunky, and maybe we’ll want the year and month variables later."
  },
  {
    "objectID": "labs/02-lab-comments.html#adding-meaningful-labels-and-title",
    "href": "labs/02-lab-comments.html#adding-meaningful-labels-and-title",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.1 Adding meaningful labels and title",
    "text": "8.1 Adding meaningful labels and title\nBecause we saved the output of our ggplot to an object called fig1 we can add additional commands to this object using the + without having to rewrite all the code.\nFirst let’s add better labels to the graph.\n\nNote that even though we flipped the coordinates, the aes aesthetic mappings stay the same. So to change the label of the figures y-axis to “Date” we change the label of x = \"Date\"\nggplot automatically generates a legend for aesthetic mappings like color We can add a line break using the the special character \\n in our code\n\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\n Policy\"\n  )\n\n\n\n\n\n\n\n\nNote the code above didn’t update fig1\n\nfig1\n\n\n\n\n\n\n\n\nWe have to save the output (if we like it) for our changes to persist.\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\nPolicy\"\n  ) -&gt; fig1\n\nfig1"
  },
  {
    "objectID": "labs/02-lab-comments.html#changing-the-theme-of-the-plot",
    "href": "labs/02-lab-comments.html#changing-the-theme-of-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.2 Changing the theme of the plot",
    "text": "8.2 Changing the theme of the plot\nHere’s an example of some different themes\n\n# Black and white\nfig1 +\n  theme_bw()\n\n\n\n\n\n\n\n# Minimal\nfig1 +\n  theme_minimal()\n\n\n\n\n\n\n\n# Classic\nfig1 +\n  theme_classic()\n\n\n\n\n\n\n\n\nThis is pretty personal, and depends of the figure itself. I like a white background and some guide lines:\n\nfig1 +\n  theme_bw() -&gt; fig1\n\nfig1"
  },
  {
    "objectID": "labs/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "href": "labs/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.3 Make the size of the dots reflect the number of states with this policy",
    "text": "8.3 Make the size of the dots reflect the number of states with this policy\nIn the cases_by_month_and_policy we have a column called n which is the number of states which had a given policy in a given month.\nWe can add an aesthetic to our plot that varies the size of the points by the number of states.\n\nfig1 +\n  aes(size = n) -&gt; fig1\n\nWe call this type of plot a bubble plot{target=“_blank”\nI have mixed feelings about multiple legends. We can remove the legend for size using the scale_size() function. I had to Google how to do this for the millionth time.\n\nfig1 +\n  scale_size(guide = \"none\") -&gt; fig1"
  },
  {
    "objectID": "labs/02-lab-comments.html#facet-the-plot",
    "href": "labs/02-lab-comments.html#facet-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.4 Facet the plot",
    "text": "8.4 Facet the plot\nVarying the size of the dots by the number of states conveys more information. But makes the chart a little harder to read. Dots overlap.\nThe facet_wrap command will produce separate bubble plots for each level of the “facetting” variable, in this case `face_masks\n\nfig1 +\n  facet_wrap(~face_masks) -&gt; fig1\nfig1\n\n\n\n\n\n\n\n\nNow I think also want a second legend for the number of states\n\nfig1 +\n  scale_size(guide = \"legend\")+\n  labs(\n    size = \"# of States\\nwith Policy\"\n  )-&gt; fig1\nfig1\n\n\n\n\n\n\n\n\nThis seems pretty good if our goal was to show in general terms\n\nIt shows the average number new cases for states with a given face mask policy over time.\nIt shows how the mix of types of face mask policies states have adopted has changed over time\n\nIf our goal was to make comparisons across face mask policies over a given time period, I’m might still prefer something closer to our original graph."
  },
  {
    "objectID": "labs/09-lab-comments.html",
    "href": "labs/09-lab-comments.html",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "",
    "text": "In this lab, we’ll explore data from the National Election Studies 2024 Pilot Study. You find an outcome of interest and a variable you think predicts interesting variation in that outcome. You’ll figure out what recoding you need to do, do that recoding and describe the data. I’ll do the same, so can some template code to compare your work to.\nEverything we’ll do today is something we’ve done before and is also something you’ll likely have to do a version of in your final project.\n\nSet up your work space (5 minutes)\nDownload and load data from the NES into R (10 minutes)\nExplore the codebook for the 2024 Pilot Study (5-10 minutes)\nGet a high level overview of the data to figure out what recoding needs to be done (5-10 minutes)\nRecode the data (15 minutes)\nDescribe the data (15-20 minutes)\nFormulate a set of research questions (10 minutes)\nSave the data (5 minutes)\n\nOne of these 8 tasks will be randomly selected as the graded question for the lab.\n\nset.seed(342024)\ngraded_question &lt;- sample(1:8,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 6 is the graded question for this week\""
  },
  {
    "objectID": "labs/09-lab-comments.html#please-render-this-.qmd-file",
    "href": "labs/09-lab-comments.html#please-render-this-.qmd-file",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/09-lab-comments.html#load-packages",
    "href": "labs/09-lab-comments.html#load-packages",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\n\n# Libraries\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(kableExtra)\nlibrary(ggpubr)\nlibrary(DeclareDesign)"
  },
  {
    "objectID": "labs/09-lab-comments.html#set-your-working-directory-in-r-studio",
    "href": "labs/09-lab-comments.html#set-your-working-directory-in-r-studio",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "1.2 Set your working directory in R Studio",
    "text": "1.2 Set your working directory in R Studio\nIn R studio set your working directory to the folder where this lab is saved by clicking &gt; Session &gt; Set Working Directory &gt; To Source File Location\n\nAfter doing so uncomment getwd() Should print out something like\n\n“~/Desktop/pols1600/labs/”\n\nDepending on where your lab is saved\n\n# In the Top Panel of RStudio Click\n# Session &gt; Set Working Directory &gt; Source File Location\n\n# Uncomment to Check Where Your File is Saved \n# getwd()\n\n\n\n\n\n\n\nNote\n\n\n\nIf getwd() says something like ‘~/Downloads/’ click: “File &gt; Save As” and save this lab in your course folder. Then close the version 09-lab.qmd that was opened from your Downloads folder and open the version of 09-lab.qmd that now exists in your course folder."
  },
  {
    "objectID": "labs/09-lab-comments.html#identify-a-potential-outcome-of-interest",
    "href": "labs/09-lab-comments.html#identify-a-potential-outcome-of-interest",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "3.1 Identify a potential outcome of interest",
    "text": "3.1 Identify a potential outcome of interest\nIn this and next week’s lab, I’ll be exploring factors that explain variation in the following outcome variables:\nA measure\n\nvchoice_rematch “Vote Trump or Biden in 2024”\n\n1 = Donald Trump\n2 = Joe Biden\n-7 = No Answer\n-1 = Inapplicable\n\n\nAnd five measures of political participation in the 2020 campaign\n\nmobil_talk “2020 campaign - Talk to others about candidates”\nmobil_online “2020 campaign - Participate in online rallies”\nmobil_rally “2020 campaign - Attend in person rallies”\nmobil_button “2020 campaign - Wear a button or campaign sticker”\nmobil_work “2020 campaign - Any other work to support candidates”\n\n1 = Yes\n2 = No\n-1 = Inapplicable\n\n\nPlease find a variable that describes some outcome of interest to you and fill in the following\n\noutcome_variable_name Question topic\n\nvariable values"
  },
  {
    "objectID": "labs/09-lab-comments.html#identify-at-least-one-variable-that-might-predict-variation-in-your-outcome.",
    "href": "labs/09-lab-comments.html#identify-at-least-one-variable-that-might-predict-variation-in-your-outcome.",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "3.2 Identify at least one variable that might predict variation in your outcome.",
    "text": "3.2 Identify at least one variable that might predict variation in your outcome.\nFrom a quick skim, I’ve selected the following potential predictors, which I will recode below:\n\nage Age\neduc Education\nfaminc_new Income\nrace Race\npid7 7 point party identification\n\nPlease find at least one more predictor which you think might explain variation in your outcome of interest and fill in the following\n\npredictor_variable_name Question topic\n\nvariable values\n\n\nFor example, perhaps you’re interested in differences by gender, or ideology, or social media use. See if you can find variables that measure these concepts.\nYou only needed to identify one, but you can choose to explore more if you want. Don’t choose 50, unless you really like recoding data."
  },
  {
    "objectID": "labs/09-lab-comments.html#examine-the-distributions-and-values-of-your-outcome-variable",
    "href": "labs/09-lab-comments.html#examine-the-distributions-and-values-of-your-outcome-variable",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.1 Examine the distributions and values of your outcome variable",
    "text": "4.1 Examine the distributions and values of your outcome variable\nPlease uncomment and run the code below\n\n# Vote Choice\nget_variable_labels(df$vchoice_rematch)\n\n[1] \"Vote Trump or Biden in 2024\"\n\nget_value_labels(df$vchoice_rematch)\n\n                    No Answer inapplicable, legitimate skip \n                           -7                            -1 \n                 Donald Trump                     Joe Biden \n                            1                             2 \n                                                            \n                            8                             9 \n\ntable(df$vchoice_rematch,useNA = \"ifany\")\n\n\n -7  -1   1   2 \n 23 151 869 866 \n\n# Acts of Participation\n# All variables start with mobil_ prefix\ndf %&gt;% select(starts_with(\"mobil\")) %&gt;% names()\n\n [1] \"mobil_talk\"             \"mobil_online\"           \"mobil_rally\"           \n [4] \"mobil_button\"           \"mobil_work\"             \"mobil_talk_skp\"        \n [7] \"mobil_online_skp\"       \"mobil_rally_skp\"        \"mobil_button_skp\"      \n[10] \"mobil_work_skp\"         \"mobil_talk_pg_timing\"   \"mobil_online_pg_timing\"\n[13] \"mobil_rally_pg_timing\"  \"mobil_button_pg_timing\" \"mobil_work_pg_timing\"  \n\n# Political Talk\nget_variable_labels(df$mobil_talk)\n\n[1] \"2020 campaign - Talk to others about candidates\"\n\nget_value_labels(df$mobil_talk)\n\n                    No Answer inapplicable, legitimate skip \n                           -7                            -1 \n                          Yes                            No \n                            1                             2 \n                                                            \n                            8                             9 \n\ntable(df$mobil_talk,useNA = \"ifany\")\n\n\n  -7   -1    1    2 \n   1  150  704 1054 \n\n# Save the names all variables related to acts of participation in 2020\nthe_participation_vars &lt;- df %&gt;% select(starts_with(\"mobil\")) %&gt;% names()\n# Only keep the variables that measure participation and not survey timing\nthe_participation_vars &lt;- the_participation_vars[1:5]\n\nFrom quickly looking at my outcome variables, I know that I will want to:\n\nRecode vchoice_rematch to dv_vote_trump2024 which\n\nequals 1 if vchoice_rematch == 1\nequals 0 if vchoice_rematch == 2\nequals NA if vchoice_rematch &lt; 0\n\nRecode variables that start with mobil_* to variables that start with polpart_* and:\n\nequal 1 if mobil_* == 1\nequal 0 if mobil_* == 2\nequals NA if mobil_* &lt; 0`\n\nCreate dv_participation* which is the sum of respondents’ five responses to the recoded polpart_* variables"
  },
  {
    "objectID": "labs/09-lab-comments.html#describe-any-recoding-of-your-outcome-variable",
    "href": "labs/09-lab-comments.html#describe-any-recoding-of-your-outcome-variable",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.2 Describe any recoding of your outcome variable",
    "text": "4.2 Describe any recoding of your outcome variable\nIn the code chunk below, please repeat this process for the outcome variable you selected in the previous section:\n\n# Get a HLO of your outcome variable\n\n\nRecode YOUR OUTCOME VARIABLE HERE to NAME FOR RECODED VARIABLE\n\nDescribe what values need to be recoded in the new variable"
  },
  {
    "objectID": "labs/09-lab-comments.html#examine-the-distributions-and-values-of-your-predictor-variable",
    "href": "labs/09-lab-comments.html#examine-the-distributions-and-values-of-your-predictor-variable",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.3 Examine the distributions and values of your predictor variable",
    "text": "4.3 Examine the distributions and values of your predictor variable\nNow I’ll repeat this process for my potential predictor variables.\nPlease uncomment and run the code below\n\n# Age\nget_variable_labels(df$age)\n\n[1] \"Profile variable: Age\"\n\nget_value_labels(df$age)\n\nnot asked \n       -9 \n\nsummary(df$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -9.00   33.00   51.00   48.43   63.00   94.00 \n\n# Education\nget_variable_labels(df$educ)\n\n[1] \"Profile variable: Education\"\n\nget_value_labels(df$educ)\n\n    No HS credential High school graduate         Some college \n                   1                    2                    3 \n       2-year degree        4-year degree            Post-grad \n                   4                    5                    6 \n\ntable(df$educ)\n\n\n  1   2   3   4   5   6 \n 84 584 381 210 426 224 \n\nsummary(df$educ)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   3.514   5.000   6.000 \n\n# Income\nget_variable_labels(df$faminc_new)\n\n[1] \"Profile variable: Family income\"\n\nget_value_labels(df$faminc_new)\n\n                    No Answer inapplicable, legitimate skip \n                           -7                            -1 \n            Less than $10,000             $10,000 - $19,999 \n                            1                             2 \n            $20,000 - $29,999             $30,000 - $39,999 \n                            3                             4 \n            $40,000 - $49,999             $50,000 - $59,999 \n                            5                             6 \n            $60,000 - $69,999             $70,000 - $79,999 \n                            7                             8 \n            $80,000 - $99,999           $100,000 - $119,999 \n                            9                            10 \n          $120,000 - $149,999           $150,000 - $199,999 \n                           11                            12 \n          $200,000 - $249,999           $250,000 - $349,999 \n                           13                            14 \n          $350,000 - $499,999              $500,000 or more \n                           15                            16 \n            Prefer not to say                               \n                           97                           998 \n                              \n                          999 \n\ntable(df$faminc_new)\n\n\n -7   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  97 \n 38 119 126 222 142 141 140 122 149 144 120 109  88  31  20  10   4 184 \n\nsummary(df$faminc_new)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -7.00    3.00    7.00   14.89   10.00   97.00 \n\n# Race\nget_variable_labels(df$race)\n\n[1] \"Race\"\n\nget_value_labels(df$race)\n\n                    No Answer inapplicable, legitimate skip \n                           -7                            -1 \n                        White                         Black \n                            1                             2 \n                     Hispanic                         Asian \n                            3                             4 \n              Native American             Two or more races \n                            5                             6 \n                        Other                Middle Eastern \n                            7                             8 \n                                                            \n                           98                            99 \n\nsummary(df$race)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.751   2.000   8.000 \n\ntable(df$race,useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8 \n1270  242  239   44   17   69   27    1 \n\n# Partisanship\nget_variable_labels(df$pid7)\n\n[1] \"Profile variable: 7 point party identification\"\n\nget_value_labels(df$pid7)\n\n           Strong Democrat   Not very strong Democrat \n                         1                          2 \n             Lean Democrat                Independent \n                         3                          4 \n           Lean Republican Not very strong Republican \n                         5                          6 \n         Strong Republican                   Not sure \n                         7                          8 \n                Don't know \n                         9 \n\nsummary(df$pid7)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   4.000   4.023   6.000   8.000      58 \n\ntable(df$pid7)\n\n\n  1   2   3   4   5   6   7   8 \n389 228 173 295 172 184 342  68 \n\n\nAfter taking a quick look at each variable, I know that I’ll want to do the following recoding:\n\nage to age1\n\nrecode -9s to NA\n\neduc no recoding needed\n\ncreate indicator has_college_degree which equals 1 is educ &gt; 4 and 0 otherwise\n\nfaminc_new to income\n\nrecode -7 and 97 to NA\n\nrace to race_5cat\n\nrace == 1 ~ \"White\"\nrace == 2 ~ \"Black\"\nrace == 3 ~ \"Hispanic\"\nrace == 4 ~ \"Asian\"\nT ~ \"Other\" (Collapse other racial categories)\n\nrace to is_* binary indicators:\n\nis_white = 1 if race==1, 0 otherwise\n\npid7 to partyid\n\nrecode pid7 == 8 to 4 (Classify Don't Knows as Independents)\n\npid7 to is_*: binary indicators:\n\nis_dem = 1 if partyid &lt; 4, 0 otherwise\nis_rep = 1 if partyid &gt; 4, 0 otherwise\nis_ind = 1 if partyid == 4, 0 otherwise"
  },
  {
    "objectID": "labs/09-lab-comments.html#describe-any-recoding-of-your-predictors",
    "href": "labs/09-lab-comments.html#describe-any-recoding-of-your-predictors",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.4 Describe any recoding of your predictor(s)",
    "text": "4.4 Describe any recoding of your predictor(s)\nIn the code chunk below, please repeat this process for the additional predictor(s) you selected in the previous section:\n\n# Get a HLO of your outcome variable\n\n\nRecode YOUR OUTCOME VARIABLE HERE to NAME FOR RECODED VARIABLE\n\nDescribe what values need to be recoded in the new variable"
  },
  {
    "objectID": "labs/09-lab-comments.html#recode-the-outcome-variables",
    "href": "labs/09-lab-comments.html#recode-the-outcome-variables",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "5.1 Recode the outcome variables",
    "text": "5.1 Recode the outcome variables\nPlease uncomment and run the following code chunk:\n\ndf %&gt;%\n  # Recode 2024 Vote Choice\n  mutate(\n    dv_vote_trump2024 = case_when(\n      vchoice_rematch == 1 ~ 1,\n      vchoice_rematch == 2 ~ 0,\n      T ~ NA\n    )\n  ) %&gt;% \n  # Recode Individual Acts of Participation\n  mutate(across(all_of(the_participation_vars), \n                \\(x) case_when(\n                  x == 1 ~ 1,\n                  x == 2 ~ 0,\n                  T ~ NA\n                ),\n                .names = \"polpart_{.col}\"\n                )\n                \n                ) %&gt;% \n  # Create Additive Measure of Participation\n  mutate(\n    dv_participation = rowSums(\n      select(.,starts_with(\"polpart\")),\n      na.rm = T)\n    \n  ) -&gt; df\n\nPlease recode your outcome of interest as needed\nRemember to save the output of your recode back into the dataframe df\n\n# Recode your outcome of interest"
  },
  {
    "objectID": "labs/09-lab-comments.html#check-the-recoding-of-your-outcome",
    "href": "labs/09-lab-comments.html#check-the-recoding-of-your-outcome",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "5.2 Check the recoding of your outcome",
    "text": "5.2 Check the recoding of your outcome\nIt’s a good habit to compare your recoded variables to their original values, to make sure your code did what you thought it did.\nPlease uncomment and run the following:\n\n# Check recodes\n\ntable(\n  recode = df$dv_vote_trump2024,\n  original = df$vchoice_rematch, \n  useNA = \"ifany\"\n  )\n\n      original\nrecode  -7  -1   1   2\n  0      0   0   0 866\n  1      0   0 869   0\n  &lt;NA&gt;  23 151   0   0\n\ntable(\n  recode = df$polpart_mobil_button,\n  original = df$mobil_button, \n  useNA = \"ifany\"\n  )\n\n      original\nrecode   -1    1    2\n  0       0    0 1386\n  1       0  373    0\n  &lt;NA&gt;  150    0    0\n\ntable(\n  total = df$dv_participation,\n  item = df$polpart_mobil_button,\n  useNA = \"ifany\"\n  )\n\n     item\ntotal   0   1 &lt;NA&gt;\n    0 884   0  149\n    1 364  62    1\n    2  89 109    0\n    3  38  71    0\n    4  11  83    0\n    5   0  48    0\n\n\nSo everything looks in order. I could have probably checked the dv_participation variable against all of its constituent items, but based off comparing it to polpart_mobil_button everything looks in order since: - there are no cases where polpart_mobil_button is 1 but dv_participation is 0 - there are no cases where dv_participation is at it’s max but polpart_mobil_button is 0. - dv_participation has the correct theoretical range from 0 acts to 5 acts.\nNow do the same for your outcome variable.\nPlease check your recoded outcome against its original values\n\n# Check recodes"
  },
  {
    "objectID": "labs/09-lab-comments.html#recode-your-predictor-variables",
    "href": "labs/09-lab-comments.html#recode-your-predictor-variables",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "5.3 Recode your predictor variables",
    "text": "5.3 Recode your predictor variables\nAgain, I’ve provide some demonstration code to recode the predictors listed above.\nPlease uncomment and run the following\n\ndf %&gt;% \n  mutate(\n    # Age\n    age = ifelse(age &lt; 0, NA, age),\n    # Education\n    education = educ,\n    educ_f = to_factor(educ), #Convert to Factor for Plotting\n    is_college_grad = ifelse(educ &gt; 4,1,0),\n    # Income\n    income = case_when(\n      faminc_new &lt; 0 ~ NA,\n      faminc_new &gt; 0 & faminc_new &gt;16 ~ NA,\n      T ~ faminc_new\n    ),\n    # Race\n    race_5cat = case_when(\n      race &lt; 5 ~ to_factor(race),\n      T ~ \"Other\"\n    ) %&gt;% factor(., levels = c(\"White\",\"Black\",\"Hispanic\",\"Asian\",\"Other\")),\n    is_white = ifelse(race == 1, 1, 0),\n    is_black = ifelse(race == 2, 1, 0),\n    is_hispanic = ifelse(race == 3, 1, 0),\n    is_asian = ifelse(race == 4, 1, 0),\n    is_other = ifelse(race == 5, 1, 0),\n    # Partisanship\n    partyid = case_when(\n      pid7 == 8 ~ 4,\n      T ~ pid7\n    ),\n    is_dem = ifelse(partyid  &lt; 4, 1, 0),\n    is_rep = ifelse(partyid  &gt; 4, 1, 0),\n    is_ind = ifelse(partyid  == 4, 1, 0),\n  ) -&gt; df\n\nPlease recode your additional predictor(s) as needed\n\n# Recode your additional predictor(s)\n\nIt’s a good habit to check your all your recoding – particularly if you’re doing something like summing over multiple columns – but for this lab, we’ll live dangerously."
  },
  {
    "objectID": "labs/09-lab-comments.html#create-a-table-of-summary-statistics",
    "href": "labs/09-lab-comments.html#create-a-table-of-summary-statistics",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.1 Create a table of summary statistics",
    "text": "6.1 Create a table of summary statistics\nPlease uncomment and run the code below which demonstrates how to produce a nicely formatted table of summary statistics\n\n# Vector of numeric variables to summarize\nthe_vars &lt;- c(\n  \"dv_vote_trump2024\", \n  \"dv_participation\",\n  \"age\",\"education\",\"income\",\n  \"is_white\",\"is_black\",\"is_hispanic\",\"is_asian\",\"is_other\",\n  \"partyid\",\"is_dem\",\"is_rep\",\"is_ind\"\n)\n\n# Vector of nicely formatted labels for variables\nthe_labels &lt;- c(\n  \"Vote for Trump in '24\",\n  \"Acts of Participation in `20\",\n  \"Age\",\"Education\", \"Income\",\n  \"White\", \"Black\",\"Hispanic\",\"Asian\",\"Other\",\n  \"Party ID\", \"Democrat\",\"Republican\",\"Independent\"\n)\n\ndf_summary &lt;- df %&gt;% \n  select(all_of(the_vars)) %&gt;%\n  rename_with(~the_labels) %&gt;% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"Variable\"\n  ) %&gt;% \n  mutate(\n    Variable = factor(Variable, levels = the_labels)\n  ) %&gt;% \n  group_by(Variable) %&gt;% \n  summarise(\n    Min = min(value,na.rm = T),\n    p25 = quantile(value, prob = .25,na.rm = T),\n    Median = quantile(value, prob = .5,na.rm = T),\n    Mean = mean(value, na.rm = T),\n    p75 = quantile(value, prob = .75,na.rm = T),\n    Max = max(value,na.rm = T),\n    `N missing` = sum(is.na(value))\n  )\n\n# Look at results\ndf_summary\n\n# A tibble: 14 × 8\n   Variable             Min        p25 Median    Mean   p75 Max      `N missing`\n   &lt;fct&gt;                &lt;dbl+lb&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lb&gt;       &lt;int&gt;\n 1 Vote for Trump in '…  0           0      1 5.01e-1   1    1 [No …         174\n 2 Acts of Participati…  0           0      0 9.25e-1   1    5 [4-y…           0\n 3 Age                  18          33     51 4.94e+1  63.8 94                31\n 4 Education             1 [No …     2      3 3.51e+0   5    6 [Pos…           0\n 5 Income                1 [No …     3      6 6.43e+0   9   16 [$50…         222\n 6 White                 0           0      1 6.65e-1   1    1 [No …           0\n 7 Black                 0           0      0 1.27e-1   0    1 [No …           0\n 8 Hispanic              0           0      0 1.25e-1   0    1 [No …           0\n 9 Asian                 0           0      0 2.30e-2   0    1 [No …           0\n10 Other                 0           0      0 8.91e-3   0    1 [No …           0\n11 Party ID              1 [No …     2      4 3.88e+0   6    7 [$60…          58\n12 Democrat              0           0      0 4.27e-1   1    1 [No …          58\n13 Republican            0           0      0 3.77e-1   1    1 [No …          58\n14 Independent           0           0      0 1.96e-1   0    1 [No …          58\n\n\nWe can then format df_summary as table using knitr() and styling options from the kableExtra package:\n\nkable(df_summary,\n      digits = 2) %&gt;% \n  kable_styling() %&gt;% \n  pack_rows(\"Outcomes\", start_row = 1, end_row = 2) %&gt;% \n  pack_rows(\"Demographic Predictors\", 3, 10) %&gt;% \n  pack_rows(\"Political Predictors\", 11, 14)\n\n\n\n\nVariable\nMin\np25\nMedian\nMean\np75\nMax\nN missing\n\n\n\n\nOutcomes\n\n\nVote for Trump in '24\n0\n0\n1\n0.50\n1.00\n1\n174\n\n\nActs of Participation in `20\n0\n0\n0\n0.93\n1.00\n5\n0\n\n\nDemographic Predictors\n\n\nAge\n18\n33\n51\n49.37\n63.75\n94\n31\n\n\nEducation\n1\n2\n3\n3.51\n5.00\n6\n0\n\n\nIncome\n1\n3\n6\n6.43\n9.00\n16\n222\n\n\nWhite\n0\n0\n1\n0.67\n1.00\n1\n0\n\n\nBlack\n0\n0\n0\n0.13\n0.00\n1\n0\n\n\nHispanic\n0\n0\n0\n0.13\n0.00\n1\n0\n\n\nAsian\n0\n0\n0\n0.02\n0.00\n1\n0\n\n\nOther\n0\n0\n0\n0.01\n0.00\n1\n0\n\n\nPolitical Predictors\n\n\nParty ID\n1\n2\n4\n3.88\n6.00\n7\n58\n\n\nDemocrat\n0\n0\n0\n0.43\n1.00\n1\n58\n\n\nRepublican\n0\n0\n0\n0.38\n1.00\n1\n58\n\n\nIndependent\n0\n0\n0\n0.20\n0.00\n1\n58"
  },
  {
    "objectID": "labs/09-lab-comments.html#modify-the-table-to-include-your-predictors",
    "href": "labs/09-lab-comments.html#modify-the-table-to-include-your-predictors",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.2 Modify the table to include your predictors",
    "text": "6.2 Modify the table to include your predictors\nUsing the two previous code chunks as a template, update the code so that the table includes your chosen outcome and predictors.\n\n# Copy, paste, and update code from datasummary_me to include your variables"
  },
  {
    "objectID": "labs/09-lab-comments.html#interpret-the-results-of-your-table.",
    "href": "labs/09-lab-comments.html#interpret-the-results-of-your-table.",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.3 Interpret the results of your table.",
    "text": "6.3 Interpret the results of your table.\nPlease write a few sentences that provide a substantive interpretation of your table of descriptive statistics\nYour reader should come away with an understanding of the characteristics of the respondents to this sample.\nThe National Election Study’s 2024 Pilot Study contains responses from 1909 individuals2. The typical respondent in the data was just under 50 years old, with some college, with an income in the range of $50k-$59k. Approximately two-thirds of the sample identified as white, with 13 percent of respondents identifying as Black, 13 percent as Hispanic, 2 percent as Asian. Forty-three percent of respondents identified as Democrats, 38 percent as Republicans, and 20 percent as Independents. The respondents were evenly split in who they would vote for 2024, with 50 percent saying they would Vote for Donald Trump, and 50 percent saying Joe Biden. In the 2020 campaign, the average respondent reported engaging in about 1 act of political participation.\n\nNote, that technically, 024 Pilot Study contains three types of respondents described by sample_type:\n\n1,500 respondents who are part of a weighted sample that generalizes to the U.S. population\n113 respondents who completed the survey, but were not included in the panel-matching procedure used to construct construct a representative sample\n296 respondents who didn’t complete the whole survey.\n\nSo, technically speaking3, if we wanted to draw inferences about the proportion of American’s planning to to vote for Trump or Biden in 2024, we should only look at the 1,500 respondents in the weighted sample, and we should calculate that proportion using the sampling weights provided by weights.\nSurvey weights are complicated things, but the basically idea is that each observation is the sample is representative of observations in the population. Some types of observations will be over-represented in our sample – these are given smaller weights – while others are under-represented – these are given greater weights.\nQuick look at the highest and lowest survey weights seems to suggest to me that weighting procedure is giving more weight to respondents with lower levels of education, and income, and less weight to higher incomes and education levels.\n\ndf %&gt;% filter(\n weight &gt; quantile(weight, .99, na.rm=T)\n ) %&gt;% \n  select(weight, gender, age, race_5cat, education, income, partyid)\n\n# A tibble: 15 × 7\n   weight gender       age race_5cat education                income    partyid \n    &lt;dbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;fct&gt;     &lt;dbl+lbl&gt;                &lt;dbl+lbl&gt; &lt;dbl+lb&gt;\n 1   7.00 1 [Male]      23 Hispanic  3 [Some college]          4 [$30,…  3 [Lea…\n 2   3.33 1 [Male]      31 Other     2 [High school graduate] NA         2 [Not…\n 3   2.96 2 [Female]    33 Hispanic  1 [No HS credential]     NA         4 [Ind…\n 4   3.36 1 [Male]      68 Black     3 [Some college]         NA         4 [Ind…\n 5   2.94 1 [Male]      67 Hispanic  1 [No HS credential]      2 [$10,…  2 [Not…\n 6   4.13 1 [Male]      22 Hispanic  3 [Some college]         11 [$120…  4 [Ind…\n 7   3.64 2 [Female]    58 Black     5 [4-year degree]         6 [$50,… NA      \n 8   3.30 1 [Male]      43 Hispanic  5 [4-year degree]         7 [$60,…  4 [Ind…\n 9   3.51 1 [Male]      69 White     2 [High school graduate]  4 [$30,…  7 [Str…\n10   3.32 1 [Male]      74 White     2 [High school graduate]  6 [$50,…  3 [Lea…\n11   3.32 1 [Male]      69 White     2 [High school graduate]  3 [$20,…  4 [Ind…\n12   3.51 1 [Male]      75 White     2 [High school graduate]  2 [$10,…  7 [Str…\n13   2.93 1 [Male]      41 Black     4 [2-year degree]         4 [$30,…  2 [Not…\n14   4.20 2 [Female]    63 Black     6 [Post-grad]             3 [$20,…  6 [Not…\n15   3.32 1 [Male]      67 White     2 [High school graduate]  5 [$40,…  3 [Lea…\n\ndf %&gt;% filter(\n  weight &lt; quantile(weight, .01, na.rm=T)\n) %&gt;% \n  select(weight, gender, age, race_5cat, education, income, partyid)\n\n# A tibble: 12 × 7\n   weight gender       age race_5cat education                income     partyid\n    &lt;dbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt; &lt;fct&gt;     &lt;dbl+lbl&gt;                &lt;dbl+lbl&gt;  &lt;dbl+l&gt;\n 1  0.314 1 [Male]      77 Black     6 [Post-grad]            10 [$100,… 1 [Str…\n 2  0.409 2 [Female]    29 Black     6 [Post-grad]            14 [$250,… 1 [Str…\n 3  0.382 1 [Male]      23 Hispanic  5 [4-year degree]        10 [$100,… 5 [Lea…\n 4  0.439 2 [Female]    26 Asian     5 [4-year degree]        12 [$150,… 2 [Not…\n 5  0.421 2 [Female]    57 Hispanic  6 [Post-grad]             6 [$50,0… 5 [Lea…\n 6  0.415 2 [Female]    52 Hispanic  6 [Post-grad]             6 [$50,0… 3 [Lea…\n 7  0.455 1 [Male]      72 Other     6 [Post-grad]             7 [$60,0… 7 [Str…\n 8  0.405 1 [Male]      32 Hispanic  5 [4-year degree]        11 [$120,… 4 [Ind…\n 9  0.362 2 [Female]    33 Black     5 [4-year degree]         1 [Less … 1 [Str…\n10  0.411 1 [Male]      42 Black     2 [High school graduate]  4 [$30,0… 4 [Ind…\n11  0.362 2 [Female]    33 Black     5 [4-year degree]         1 [Less … 1 [Str…\n12  0.346 1 [Male]      71 Black     6 [Post-grad]             2 [$10,0… 1 [Str…\n\n\nSampling weights are a can worms we won’t deal with in this class. Check out these resources on the survey and srvyr packages for how to incorporate survey weights into your analysis. In the code below we see that distributions of race and ethnicity are pretty similar in the weighted and full samples, but using survey weights seems to suggest that Biden’s support as actually higher than Trumps, although the confidence intervals (Next week!) for both overlap 0.5 suggesting the race is essentially tied.\n\n# install.packages(\"survey\")\n# install.packages(\"srvyr\")\nlibrary(survey)\nlibrary(srvyr)\n\n# Format as survey_design object\ndf_s &lt;- as_survey_design(df %&gt;% filter(sample_type == 1),weight = weight)\n\n# Calculate weighted totals and proportions\ndf_s %&gt;% \n  group_by(race) %&gt;% \n  summarise(\n    total = survey_total(),\n    proption = survey_mean()\n  )\n\n# A tibble: 7 × 5\n  race                   total total_se proption proption_se\n  &lt;dbl+lbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 1 [White]             1005.     21.6   0.670       0.0139 \n2 2 [Black]              193.     15.7   0.129       0.0102 \n3 3 [Hispanic]           189.     15.9   0.126       0.0102 \n4 4 [Asian]               35.6     6.36  0.0237      0.00423\n5 5 [Native American]     11.8     3.26  0.00789     0.00218\n6 6 [Two or more races]   46.8     7.52  0.0312      0.00499\n7 7 [Other]               17.8     5.53  0.0119      0.00367\n\n# Compare to unweighted estimates\ndf %&gt;%\n  group_by(race) %&gt;% \n  summarise(\n    total = n()\n  ) %&gt;% \n  mutate(\n    propotion = total/sum(total)\n  )\n\n# A tibble: 8 × 3\n  race                  total propotion\n  &lt;dbl+lbl&gt;             &lt;int&gt;     &lt;dbl&gt;\n1 1 [White]              1270  0.665   \n2 2 [Black]               242  0.127   \n3 3 [Hispanic]            239  0.125   \n4 4 [Asian]                44  0.0230  \n5 5 [Native American]      17  0.00891 \n6 6 [Two or more races]    69  0.0361  \n7 7 [Other]                27  0.0141  \n8 8 [Middle Eastern]        1  0.000524\n\n# Calculated weighted proportions of support\ndf_s %&gt;% \n  group_by(dv_vote_trump2024) %&gt;%\n  summarise(\n    proportion = survey_mean()\n  ) %&gt;% select(dv_vote_trump2024,proportion, proportion_se)\n\n# A tibble: 3 × 3\n  dv_vote_trump2024 proportion proportion_se\n              &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1                 0     0.510        0.0142 \n2                 1     0.476        0.0142 \n3                NA     0.0139       0.00363\n\n# Compare to unweighted proportions just among those in weighted sample\ndf %&gt;%\n  filter(sample_type == 1) %&gt;% \n  group_by(dv_vote_trump2024) %&gt;% \n  summarise(\n    total = n()\n  ) %&gt;% \n  mutate(\n    propotion = total/sum(total)\n  )\n\n# A tibble: 3 × 3\n  dv_vote_trump2024 total propotion\n              &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1                 0   755     0.503\n2                 1   727     0.485\n3                NA    18     0.012\n\n# And overall\ndf %&gt;%\n  group_by(dv_vote_trump2024) %&gt;% \n  summarise(\n    total = n()\n  ) %&gt;% \n  mutate(\n    propotion = total/sum(total)\n  )\n\n# A tibble: 3 × 3\n  dv_vote_trump2024 total propotion\n              &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1                 0   866    0.454 \n2                 1   869    0.455 \n3                NA   174    0.0911\n\n# Formally test whether trumps support is above 50 percent\ndf_s %&gt;% \nsvyttest((dv_vote_trump2024==1) - 0 ~ 0,design = .,na.rm=T)\n\n\n    Design-based one-sample t-test\n\ndata:  (dv_vote_trump2024 == 1) - 0 ~ 0\nt = 33.732, df = 1498, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.4548575 0.5110241\nsample estimates:\n     mean \n0.4829408"
  },
  {
    "objectID": "labs/09-lab-comments.html#visualize-the-data",
    "href": "labs/09-lab-comments.html#visualize-the-data",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.4 Visualize the Data",
    "text": "6.4 Visualize the Data\nPlease a choose a variable or variables whose distribution or relationship you think may be substantively interesting to the potential story you want to tell.\nIn the code below, I visualize:\n\nthe distribution of age by race using geom_boxplot()\naverage partisanship by race using stat_summary()\nhow partisanship changes with age by race using geom_smooth()\n\nAnd combine these 3 plots into a single figure using two calls to the ggarrange() function from the ggpubr package\n\nfig_age_race &lt;- df %&gt;% \n  ggplot(aes(race_5cat,age,\n             col = race_5cat))+\n  geom_boxplot()+\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    col = \"Race\",\n    title = \"Distribution of Age by Race\"\n  )+\n  theme_minimal()\n\nfig_pid_race &lt;- df %&gt;% \n  ggplot(aes(race_5cat,partyid,\n             col = race_5cat))+\n  stat_summary(position = position_dodge(width=.5))+\n  labs(\n    x = \"Race\",\n    y = \"Partisanship\",\n    col = \"Race\",\n    title = \"Average Partisanship by Race\"\n  )+\n  theme_minimal()\n\nfig_age_race_pid &lt;- df %&gt;% \n  mutate(\n    Race = race_5cat\n  ) %&gt;% \n  ggplot(aes(age,partyid,\n             col = race_5cat\n             ))+\n  geom_smooth(se = F) +\n  geom_jitter(size=.5, alpha=.3) +\n  labs(\n    x = \"age\",\n    y = \"Partisanship\",\n    col = \"Race\",\n    title = \"Distribution of Partisanship by Age and Race in 2024 NES Pilot Study\"\n  )+\n  theme_minimal()\n\nfig_desc &lt;- ggarrange(\n  # Top Row, two columns\n  ggarrange(\n    fig_age_race, fig_pid_race,\n    ncol =2,\n    legend = \"none\"\n    ), \n  # Bottom row, 1 column\n  fig_age_race_pid, \n  nrow=2,\n  common.legend = T,\n  legend = \"bottom\",\n  heights = c(1,1.5)\n  )\n\nfig_desc\n\n\n\n\n\n\n\n\nFrom the figure, we see that whites in sample have the highest median age (55 years) followed by Blacks (46 years). Hispanic and Asian respondents have the youngest median age of (40 years). Whites are also tend to lean more Republican in their partisanship than other racial minority groups. This is particularly true for older whites in the sample. Interestingly, average partisanship stays roughly constant with age for Asians and Hispanics, but older Black respondents are more likely to identify as Democrats than younger Blacks, whose partisan identification is more independent.\nPlease produce your own figure and provide a similar interpretation of what it conveys."
  },
  {
    "objectID": "labs/09-lab-comments.html#footnotes",
    "href": "labs/09-lab-comments.html#footnotes",
    "title": "Comments for Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn general, I try not to recode the original variables, but instead create new columns, with different names. But, like this footnote, it seems overly verbose to create something like age_recoded, so I’ll break my general rule↩︎\nSee note below↩︎\nWhich is the best kind of speaking↩︎\nIn general, I’m often ambivalent on this kind of “What’s the effect of x on y” phrasing. I think it’s fine for framing, but the real question I’m interested in is something about the interaction and relative effects of income and education on participation, hence the subsequent clarification of research question.↩︎"
  },
  {
    "objectID": "labs/05-lab-comments.html",
    "href": "labs/05-lab-comments.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3032022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\n\nGrading Questin 9: Basically, if you made any changes to fig_m5 100 percent. If you simply recreated fig_m5 80 percent. If you didn’t create figure fig_m5 0 percent. Sorry! But don’t fret, remember your 3 lowest lab scores are dropped from your lab grade.\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, 1, 2, 3, 4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/05-lab-comments.html#load-covid-19-data",
    "href": "labs/05-lab-comments.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid data\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "labs/05-lab-comments.html#load-election-data",
    "href": "labs/05-lab-comments.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- dataverse::get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# Backup\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/05-lab-comments.html#recode-the-covid-19-data",
    "href": "labs/05-lab-comments.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide: here\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, change the new_deaths_pc_7day to new_deaths_pc_14day and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df"
  },
  {
    "objectID": "labs/05-lab-comments.html#for-all-the-observations",
    "href": "labs/05-lab-comments.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7day)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7day\n  &lt;fct&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 Trump        19.3              0.340\n2 Biden        22.0              0.287"
  },
  {
    "objectID": "labs/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7day\n  &lt;fct&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 Trump        22.8              0.400\n2 Biden        30.6              0.380"
  },
  {
    "objectID": "labs/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7day\n  &lt;fct&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 Trump        17.1              0.302\n2 Biden        16.3              0.226\n\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that states which Biden won had about 27 new deaths compared to 23.8 new deaths in states which Trump one.\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that rates tend to be higher in Red States (0.415 deaths per 100k) than Blue States (0.349 deaths per 100k). This difference reflects the fact that Biden tended to win more populous states than trump, so simply looking at the average number of new deaths is bit misleading. Comparing 7-day averages per 100,000 residents adjusts for differences in population between Red and Blue States.\nWhen we limit our analysis, to just observations before April 19, 2021, the difference in the 7-day average rate of new Covid-19 deaths per 100,000 residents is relatively small (0.02 more deaths per 100,000 residents in Red States)\nWhen we look at observations after the vaccine became widely available the difference is more than 6 times as big (0.125 more deaths per 100,000 residents in Red States)"
  },
  {
    "objectID": "labs/05-lab-comments.html#recreating-the-nyt-figures",
    "href": "labs/05-lab-comments.html#recreating-the-nyt-figures",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "9.1 Recreating the NYT Figures",
    "text": "9.1 Recreating the NYT Figures\nThis turned out to be more annoying than I thought, but if you really wanted to recreate the figures from the articles, this was as close as I could get:\n\n# Vector containing labeled states\nthe_labs &lt;- c(\"WV\",\"WY\",\"MS\",\"KY\",\"TX\",\"FL\",\"GA\",\"IL\",\"NY\",\"VT\",\"MD\",\"CA\")\n\ncovid_us %&gt;%\n  # Only include labels for states in the the_labs\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  # Subset data\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics, flipping vax to % unvaxxed\n  ggplot(aes(x = rep_voteshare,\n             y = (100-percent_vaccinated),\n             label = nyt_labs\n             ))+\n  # points coloreded by vote share\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # color gradient\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # add simple regression line\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  # add labels\n  geom_text_repel()+\n  # futz with limits\n  ylim(15,60)+\n  # add grid lines by hand\n  geom_hline(yintercept = 60, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 40, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 20, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=20, yend = 60), col = \"lightgrey\", size = .25)+\n  # Add arrows\n  geom_segment(aes(x = 34, xend = 32, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add biden text\n  annotate(\"text\",x = 34.5, y=18.5 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=17.1 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.7, y=17.1 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  # Add trump arrow\n  geom_segment(aes(x = 70, xend = 72, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add trump text\n  annotate(\"text\",x = 69.5, y=18.5 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.1, y=17.1 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=17.1 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  # Label y-axis\n  annotate(\"text\",x = 30, y=20 ,label = \"20%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=40 ,label = \"40%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=60 ,label = \"60% of residents not fully vaccinated\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  # get rid of default theme\n  theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values (`geom_text_repel()`).\n\n\n\n\n\n\n\n\n\n\n# Same as above, but now modeling deaths with rep vote share\n\ncovid_us %&gt;%\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14day,\n             label = nyt_labs\n             ))+\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  geom_text_repel()+\n  # theme_void()+\n  ylim(-.2,2.2)+\n  geom_hline(yintercept = 0, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = .5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1.5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 2, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=0, yend = 2), col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 34, xend = 32, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.1 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.2 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.82, y=-.2 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  geom_segment(aes(x = 70, xend = 72, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.1 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.09, y=-.2 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.2 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  annotate(\"text\",x = 30, y=0.5 ,label = \"0.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1 ,label = \"1\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1.5 ,label = \"1.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=2 ,label = \"2 deaths per 100,000 residents\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  theme_void()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values (`geom_text_repel()`)."
  },
  {
    "objectID": "labs/05-lab-comments.html#footnotes",
    "href": "labs/05-lab-comments.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/08-lab.html",
    "href": "labs/08-lab.html",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we continue our replication of Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages. (5 minutes)\nSet working directory and load the data from last class. (5 minutes)\nDo some additional recoding (5 minutes)\nDescribe variation in voting by state, year, policy, and age (20 minutes)\nEstimate four regression models to understand fixed effects and cluster robust standard errors (20 minutes)\nReplicate two regression models from Grumbach and Hill (2021) interacting sdr with age_group (10 minutes)\nRecreate a portion of Figure 3 showing the marginal effect of sdr by age group. (15 minutes)\n\nFinally, we’ll take the survey for this week\nOne of these 6 tasks will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/08-lab.html#please-render-this-.qmd-file",
    "href": "labs/08-lab.html#please-render-this-.qmd-file",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab.html#load-packages",
    "href": "labs/08-lab.html#load-packages",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\",\"margins\",\n  \"modelsummary\", \"ggeffects\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo       margins \n         TRUE          TRUE          TRUE          TRUE          TRUE \n modelsummary     ggeffects \n         TRUE          TRUE"
  },
  {
    "objectID": "labs/08-lab.html#variation-by-state",
    "href": "labs/08-lab.html#variation-by-state",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.1 Variation by state",
    "text": "3.1 Variation by state\nCreate a figure that shows how average turnout varies across state in the cps data\n\n# Create dataframe of average voting rates by state\n\n\n# Use data frame of state averages to produce figure"
  },
  {
    "objectID": "labs/08-lab.html#variation-over-time",
    "href": "labs/08-lab.html#variation-over-time",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.2 Variation over time",
    "text": "3.2 Variation over time\nCreate a figure that shows how turnout varies across time.\nCalculate the yearly averages separately by election_type. Facet your plot by election_type using facet_wrap(~election_type)\n\n# Calculate yearly averages in turnout by election type\n\n\n# Display variation in turnout by year for Presidential And Midterm Elections"
  },
  {
    "objectID": "labs/08-lab.html#across-policy-and-age-for-single-state",
    "href": "labs/08-lab.html#across-policy-and-age-for-single-state",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.3 Across policy and age for single state",
    "text": "3.3 Across policy and age for single state\nFor a single state (you pick) that implemented SDR registration at some point between 1978 and 2018, plot the average turnout by age_group before and after the SDR\n\n\n\n\n\n\nTip\n\n\n\nThis is a tricky one that is designed to test your data-wrangling skills.\n\nFigure out which states adopted sdr\nCreate a summary dataframe of average turnout by age for a single state using filter(), group_by() and summarise()\nPipe this dataframe to ggplot() setting the appropriate aesthetics using geom_point() and facet_grid()\n\n\n\n\n# For a chosen state, create a dataframe showing how turnout varies by year, age_group, and SDR\n\n\n# Figure: Turnout by age group before and after SDR"
  },
  {
    "objectID": "labs/08-lab.html#describe-your-results",
    "href": "labs/08-lab.html#describe-your-results",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.4 Describe your results",
    "text": "3.4 Describe your results\nIn a few sentences, explain to your reader what these figures tell us about the data. I’ll get you started:\nFirst there is considerable variation in average rates of turnout across states. For example …\nSecond, there is also considerable variation in turnout across election years …\nAs for variation in turnout by age before and after SDR, in …"
  },
  {
    "objectID": "labs/08-lab.html#estimate-the-models",
    "href": "labs/08-lab.html#estimate-the-models",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "4.1 Estimate the models",
    "text": "4.1 Estimate the models\nI’ve gotten you started with the code for m1 Use that code as a template to estimate m2, m3, and m4\n\n# # ---- m1: Simple OLS regression ----\n# m1 &lt;- lm_robust(dv_voted ~ sdr,\n#                 data = cps,\n#                 se_type = \"classical\",\n#                 try_cholesky = T)\n\n# ---- m2: Simple OLS with robust standard errors ----\n\n\n# ---- m3: Two-way Fixed Effects for State and Year ----\n\n\n# ---- m4: TWFE for State and Year and cluster robust SEs ----"
  },
  {
    "objectID": "labs/08-lab.html#present-and-interpret-the-results",
    "href": "labs/08-lab.html#present-and-interpret-the-results",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "4.2 Present and interpret the results",
    "text": "4.2 Present and interpret the results\nWhen you’ve completed the previous section, you should be able uncomment and run the following code\n\n# htmlreg(l = list(m1,m2,m3, m4),\n#         digits = 5,\n#         include.ci = F,\n#         ) %&gt;% HTML() %&gt;% browsable()\n\nPlease write a few sentences explaining how the coefficient on sdr and it’s standard error changes across the four models. I’ll get you started:\nThe table presents the results of four regression models. The outcome in each model is a binary indicator of whether respondents to the CPS voted in a given election. The key predictor of interest in each model is the coefficient for sdr which corresponds the model’s predicted difference in turnout in states that had same day registration compared to states that did not.\nModel 1 presents the results from …\nModel 2 presents the same specification, but uses …\nModel 3 includes …\nFinally, model 4 presents the results of a TWFE regression with fixed effects for state and year with …\nIn sum, once we account for fixed differences across states and between time periods, and allow correlated errors between observations from the same state, variation in the presence of same day registration laws seems to explain …"
  },
  {
    "objectID": "labs/08-lab.html#estimate-the-models-1",
    "href": "labs/08-lab.html#estimate-the-models-1",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "5.1 Estimate the models",
    "text": "5.1 Estimate the models\n\n# ---- m1gh ----\n\n\n# ---- m2gh ----"
  },
  {
    "objectID": "labs/08-lab.html#present-the-results",
    "href": "labs/08-lab.html#present-the-results",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "5.2 Present the results",
    "text": "5.2 Present the results\nPlease present the results in a regression table.\nOnce you’ve estimated the models, you can just uncomment the code below:\n\n# htmlreg(list(m1gh, m2gh), \n#         digits = 4,\n#         include.ci = F) %&gt;% HTML() %&gt;% browsable()"
  },
  {
    "objectID": "labs/08-lab.html#calculate-marginal-effects-of-interactions",
    "href": "labs/08-lab.html#calculate-marginal-effects-of-interactions",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "6.1 Calculate Marginal Effects of Interactions",
    "text": "6.1 Calculate Marginal Effects of Interactions\nIn the code below, I’ve written a custom function called me_fn() to calculate the marginal effects of Same Day Registration, a specific age cohort1\nThe function returns a \\(1\\times 5\\) data frame with following columns\n\nAge the Age for which we are evaluating the marginal effect of sdr\nEffect the marginal effect in percentage points of predicted turnout of sdr conditional on age_group equaling the age cohort in Age\nSE the standard error of this marginal effect\nll the lower limit of a 95 percent confidence interval for our estimate\nul the upper limit of a 95 percent confidence interval for our estimate\n\nWe will talk in more detail about what a confidence interval is in two weeks. For now, you can think of this interval as a range of equally plausible values for the marginal effect of SDR at a given age cohort.\n\n\n\n\n\n\nNote\n\n\n\nThe heuristic for interpreting a confidence interval as a measure of statistical significance, is to ask:\n\nIs zero within the upper and lower limits of the confidence interval. If so, then the true estimate could be negative, or it could be positive, in which case, we conclude the estimate is not statistically significant.\n\n\n\nPlease run the code below:\n\n# ---- Function to calculate marginal effect and SE of interactions ----\n\nme_fn &lt;- function(mod, cohort, ci=0.95){\n  # Confidence Level for CI\n  alpha &lt;- 1-ci\n  z &lt;- qnorm(1-alpha/2)\n  \n  # Age (Always one for indicator of specific cohort)\n  age &lt;- 1\n  \n  # Variance Covariance Matrix from Model\n  cov &lt;- vcov(mod)\n  \n  # coefficient for SDR (Marginal Effect for reference category: 65+)\n  b1 &lt;- coef(mod)[\"sdr\"]\n  \n  # If age is one of the interactions\n  if(cohort %in% c(\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\")){\n    # get the name of the specific interaction\n    the_int &lt;- paste(\"sdr:age_group\",cohort,sep=\"\")\n    # the coefficient on the interaction\n    b2 &lt;- coef(mod)[the_int]\n    # Calculate marginal effect for age cohort\n    me &lt;- b1 + b2*age\n    me_se &lt;- sqrt(cov[\"sdr\",\"sdr\"] + age^2*cov[the_int,the_int] + 2*age*cov[\"sdr\",the_int])\n    ll &lt;- me - z*me_se\n    ul &lt;- me + z*me_se\n  }\n  if(!cohort %in% c(\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\")){\n    me &lt;- b1 \n    me_se &lt;- mod$std.error[\"sdr\"]\n    ll &lt;- mod$conf.low[\"sdr\"]\n    ul &lt;- mod$conf.high[\"sdr\"]\n  }\n\n  # scale results to be percentage points\n  res &lt;- tibble(\n    Age = cohort,\n    Effect = me*100,\n    SE = me_se*100,\n    ll = ll*100,\n    ul = ul*100\n  )\n  return(res)\n\n\n}\n\nThen uncomment the code below to create the data frame to produce a version of the coefficient plots from Grumbach and Hill’s Figure 3.\n\n# ## List of age cohorts\n# the_age_groups &lt;- levels(cps$age_group)\n# \n# ## Model 1: No controls\n# ## Estimate Marginal effect for each age cohort\n# the_age_groups %&gt;% \n#   purrr::map_df(~me_fn(m1gh, cohort=.)) %&gt;% \n#   # Add labels for plotting\n#   mutate(\n#     Age = factor(Age),\n#     Model = \"No controls\"\n#   ) -&gt; fig3_no_controls\n# \n# ## Model 3: Controls for Education, Income, Race,and Sex \n# ## Estimate Marginal effect for each age cohort\n# the_age_groups %&gt;% \n#   purrr::map_df(~me_fn(m2gh, cohort=.)) %&gt;% \n#   # Add labels for plotting\n#   mutate(\n#     Age = factor(Age),\n#     Model = \"With controls\"\n#   ) -&gt; fig3_controls\n#   \n# ## Combine estimates into data frame for plotting\n# fig3_df &lt;- fig3_no_controls %&gt;% bind_rows(fig3_controls)\n# \n# ## Display results\n# fig3_df"
  },
  {
    "objectID": "labs/08-lab.html#recreate-figure-3-1",
    "href": "labs/08-lab.html#recreate-figure-3-1",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "6.2 Recreate Figure 3",
    "text": "6.2 Recreate Figure 3\nNow we can recreate the first and second panels in the first column of Grumbach and Hill’s Figure 3.\nIn the code chunk below, pipe fig3_df into ggplot() and:\n\nSet the x aesthetic to Age\nSet the y aesthetic to Effect\nAdd geom_point()\nAdd geom_linerange(aes(ymin = ll, ymax =ul))\nAdd geom_hline(yintercept = 0)\nAdd a facet_wrap(~Model)\nSet the theme if you like\n\n\n# --- Figure 3 ----"
  },
  {
    "objectID": "labs/08-lab.html#compare-your-figure-to-figure-3-from-the-text",
    "href": "labs/08-lab.html#compare-your-figure-to-figure-3-from-the-text",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "6.3 Compare your figure to Figure 3 from the text",
    "text": "6.3 Compare your figure to Figure 3 from the text\nFinally, write a few sentences comparing your results to those presented in Figure 3 of Grumbach and Hill.\nHere’s a snapshot of the relevant panels of Figure 3 for reference:\n\nPlease comment on the following:\n\nHow does the size (magnitude) of the marginal effects for 18-24 year olds compare to those reported in Figure 3?\nHow would you interpret this marginal effect substantively?\nAre any of the marginal effects in the model with no controls statistically significant?\n\nAnd if you’re interested, check out the comments for some further discussion about the differences between our replication and the published results."
  },
  {
    "objectID": "labs/08-lab.html#footnotes",
    "href": "labs/08-lab.html#footnotes",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not a robust function, but one designed to work specifically for these data and models↩︎"
  },
  {
    "objectID": "labs/pols1600-final-exam.html",
    "href": "labs/pols1600-final-exam.html",
    "title": "POLS 1600 Take Home Final",
    "section": "",
    "text": "This document provides your final exam for POLS 1600.\nThe exam consists of four parts corresponding to core concepts and skills from POLS 1600:\n\nCausal Inference (20 Points)\nInterpreting Linear Models (20 points)\nQuantifying Uncertainty through Statistical Inference (20 points)\nConducting applied empirical analysis (40 points)\n\nEach section contains a series of conceptual questions, and or coding tasks for your to accomplish.\nYou will write your answers to these questions and tasks in this Rmd file.\nYou will submit the html file produced by this .Rmd file on on Canvas by May 12, 11:59 pm.\nExams submitted after May 12 will automatically lose 10 points for each late day.\nIf you submit the qmd file instead of the html file you will lose 5 points from your exam grade.\nIf you like, I will meet with you by Zoom or in person for 30 minutes to answer general questions about material from the course. I will not answer specific questions from the exam.\nThis exam is designed to test your knowledge and understanding of concepts and skills from the course. You can and should make use of material from lectures and labs, but if I find evidence of plagiarism (copying and pasting answers verbatim from Google or ChatGPT), I will initiate plagiarism procedures described in Brown’s Academic Code"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#what-is-the-fundamental-problem-of-causal-inference-3-points",
    "href": "labs/pols1600-final-exam.html#what-is-the-fundamental-problem-of-causal-inference-3-points",
    "title": "POLS 1600 Take Home Final",
    "section": "1.1 What is the fundamental problem of causal inference? (3 points)",
    "text": "1.1 What is the fundamental problem of causal inference? (3 points)\n\nPlease frame your answer in terms of potential outcomes and counterfactual comparisons"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#how-does-random-assignment-help-solve-this-fundamental-problem-3-points",
    "href": "labs/pols1600-final-exam.html#how-does-random-assignment-help-solve-this-fundamental-problem-3-points",
    "title": "POLS 1600 Take Home Final",
    "section": "1.2 How does random assignment help solve this fundamental problem? (3 points)",
    "text": "1.2 How does random assignment help solve this fundamental problem? (3 points)\n\nPlease discuss the concepts of statistical independence and average treatment effects."
  },
  {
    "objectID": "labs/pols1600-final-exam.html#whats-the-difference-between-an-observational-and-experimental-design-4-points",
    "href": "labs/pols1600-final-exam.html#whats-the-difference-between-an-observational-and-experimental-design-4-points",
    "title": "POLS 1600 Take Home Final",
    "section": "1.3 What’s the difference between an observational and experimental design? (4 points)",
    "text": "1.3 What’s the difference between an observational and experimental design? (4 points)\n\nWho or what determines the presence of some causal factor\nWhich the strengths and weaknesses of each design"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#explain-the-logic-of-a-difference-in-differences-design-10-points",
    "href": "labs/pols1600-final-exam.html#explain-the-logic-of-a-difference-in-differences-design-10-points",
    "title": "POLS 1600 Take Home Final",
    "section": "1.4 Explain the logic of a difference-in-differences design? (10 points)",
    "text": "1.4 Explain the logic of a difference-in-differences design? (10 points)\n\nDescribe the basic logic of the design\nExplain the key identifying assumptions of the design\nDiscuss some testable implications of these assumptions"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#what-is-a-linear-regression-and-why-is-it-useful-5-points",
    "href": "labs/pols1600-final-exam.html#what-is-a-linear-regression-and-why-is-it-useful-5-points",
    "title": "POLS 1600 Take Home Final",
    "section": "2.1 What is a linear regression and why is it useful? (5 points)",
    "text": "2.1 What is a linear regression and why is it useful? (5 points)\nIn your answer, please discuss:\n\nThe basic goals and intuitions that motivate linear regression\nThe mechanics of ordinary least squares regression\nThe relationship between linear regression and the Conditional Expectation Function"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#what-does-it-mean-to-control-for-a-variable-in-a-multiple-regression-5-points",
    "href": "labs/pols1600-final-exam.html#what-does-it-mean-to-control-for-a-variable-in-a-multiple-regression-5-points",
    "title": "POLS 1600 Take Home Final",
    "section": "2.2 What does it mean to control for a variable in a multiple regression? (5 points)",
    "text": "2.2 What does it mean to control for a variable in a multiple regression? (5 points)\nIn your answer, please explain:\n\nWhy would we want to control for covariates?\nHow linear regression accomplishes this\nWhat covariates we want to control for\nWhat covariates we shouldn’t control for"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#interpreting-regression-models-10-points",
    "href": "labs/pols1600-final-exam.html#interpreting-regression-models-10-points",
    "title": "POLS 1600 Take Home Final",
    "section": "2.3 Interpreting regression models (10 points)",
    "text": "2.3 Interpreting regression models (10 points)\nSuppose we are interested in understanding partisan differences in vaccination rates.\nOur outcome is a binary indicator which takes a value of 1 if someone was vaccinated, and 0 otherwise.\nOur key predictor of interest people’s self-reported partisanship, measured with a 7-point scale where (1 = Strong Democrat, 2 = Democrat, 3 Independent who leans Democratic, 4 = Independent who leans Republican, 5 = Republican, and 7 = Strong Republican)\nPlease answer the following:\n\nSuppose we expect Republicans are less likely to be vaccinated.\n\n\nSpecify a bivariate model predicting vaccination status with partisanship, measured by a 7-point scale to test this expectation (ie fill in the ??? in the equation below):\n\n\\[\\text{???} = \\beta_0 + \\beta_1 \\times \\text{???} + \\epsilon\\]\n\nIf your expectations are correct, what is the expected sign and significance of the coefficient on \\(\\beta_1\\)\nHow should we interpret \\(\\beta_0\\) in this model?\n\n\nSuppose we find the expected relationship in our bivariate model. Suppose a skeptic sees your results and says:\n“You claim differences in vaccination rates are a function of partisanship, I think this is really just a matter of geography. Republicans are more likely to live in rural areas. Vaccines were less available in rural areas and the pandemic was less severe.”\n\nTo address the skeptics concerns, you estimate a second model “controlling for geography” with a binary indicator of whether someone lives in a rural setting (rural = 1) or not (rural = 0)\n\nSpecify a multiple regression model predicting vaccination status with partisanship and an indicator of whether someone lives in a rural setting.\n\n\\[\\text{???} = \\beta_0 + \\beta_1 \\times \\text{???} + \\beta_2 \\times \\text{???} + \\epsilon\\]\n\nAssuming there is some truth behind the skeptic claims, what is the expected sign and significance of the coefficient on rural status in this model?\nIf the skeptic is right, and differences in vaccination rates are entirely a function of geography, what should happen to the sign and significance of the coefficient on partisanship in this model compared to the bivariate model?\nIf the skeptic is only partly right, and only part of the relationship between partisanship and vaccination is actually a function of geographic differences, what should happen to the sign and significance of the coefficient on partisanship in this model compared to the bivariate model?\nIf the skeptic is completely wrong, and geography has no relationship to vaccination rates, what should we expect in terms of the size and significance of the coefficient on rural status? How will the coefficient on partisanship in this model change?"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#what-is-a-confidence-interval-10-points",
    "href": "labs/pols1600-final-exam.html#what-is-a-confidence-interval-10-points",
    "title": "POLS 1600 Take Home Final",
    "section": "3.1 What is a confidence interval? (10 points)",
    "text": "3.1 What is a confidence interval? (10 points)\nPlease provide a one-two paragraph explanation of what a confidence interval is, how we construct confidence intervals, and how confidence intervals can be used to conduct statistical inference.\nIn your answer, please be sure to discuss and explain the following concepts:\n\nPopulations and samples\nSampling distributions\nStandard errors\nSignificance thresholds and critical values\nFor a 95% confidence interval, what are we 95% percent confident about?"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#what-is-a-hypothesis-test-10-points",
    "href": "labs/pols1600-final-exam.html#what-is-a-hypothesis-test-10-points",
    "title": "POLS 1600 Take Home Final",
    "section": "3.2 What is a hypothesis test? (10 points)",
    "text": "3.2 What is a hypothesis test? (10 points)\nPlease provide a one-two paragraph explanation of what a hypothesis test is, how we conduct hypothesis tests, and how hypothesis tests can be used to conduct statistical inference.\nIn your answer, please be sure to explain the following:\n\nWhat is a null hypothesis?\nWhat is a test stastitic?\nWhat is a distribution of a test statistic under the null?\nWhat is a p-value?\nWhat’s the difference between a Type I and Type II error?"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#look-at-and-recode-the-data-10-points",
    "href": "labs/pols1600-final-exam.html#look-at-and-recode-the-data-10-points",
    "title": "POLS 1600 Take Home Final",
    "section": "4.1 Look at and recode the data (10 points)",
    "text": "4.1 Look at and recode the data (10 points)\nFirst write some code to give yourself a high level overivew the data.\n\n# High Level Overview of the Data\n\n\nCode Advice I would use head(), table() and summary() to accomplish this\n\nBased on this overview of the data, recode the raw variables to create the following columns in the dataset:\n\nself_censor from V201627 with negative values in V201627 recoded to be NA\npid from V201511x with negative values in V201511x recoded to be NA\npid_3cat from pid.\n\nWhen pid corresponds to a Democratic respondent, pid_3cat should take the value of “Democrat”.\nWhen pid corresponds to a Republican respondent, pid_3cat should take the value of “Republican”.\nWhen pid corresponds to an Independent respondent, pid_3cat should take the value of “Independent”.\nWhen pid is NA, pid_3cat should be NA\n\nracial_attitudes from V202300 with negative values recoded to be NA and higher values corresponding to greater levels of agreement with statement (e.g. where V202300 equals 1 (Strongly Agree), racial_attitudes should equal 5, V202300 equals 5 (Strongly Disagree Agree), racial_attitudes should equal 1 )\n\n\n# Recode data\n\n\nCode Advice I would use %&gt;%, mutate(), ifelse() and case_when() to accomplish this.\n\nIf your coding is correct, the following code should all return TRUE\n\n# # 7 total columns in the data\n# dim(df)[2] == 7\n# \n# # Recodes of V201627\n# median(df$self_censor, na.rm=T) == 2\n# sum(is.na(df$self_censor)) == 135\n# \n# # Recodes of V201511x\n# median(df$pid, na.rm=T) == 4\n# sum(is.na(df$pid)) ==35\n# table(df$pid_3cat) == c(3835,968,3442)\n# sum(is.na(df$pid_3cat)) ==35\n# \n# # Recodes of V202300\n# median(df$racial_attitudes, na.rm=T) == 3\n# sum(is.na(df$racial_attitudes)) == 907"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#produce-a-table-of-descriptive-statistics-10-points",
    "href": "labs/pols1600-final-exam.html#produce-a-table-of-descriptive-statistics-10-points",
    "title": "POLS 1600 Take Home Final",
    "section": "4.2 Produce a table of descriptive statistics (10 points)",
    "text": "4.2 Produce a table of descriptive statistics (10 points)\nPlease calculate the following descriptive statistics for self_censor, pid, and racial_resentment describing the:\n\nMinimum\n25th percentile\nMedian\nMean\n75th percentile\nMaximum\nNumber of Missing Observations\n\nIdeally, present this information like we did in class as a table of descriptive statistics. Use this information to describe the distributions of your data.\n\n# Calculate Summary Statistics\n\n\nCode Advice Look at the final paper template\n\n\n# Table of Descriptive Statistics"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#estimtate-three-models-5-points",
    "href": "labs/pols1600-final-exam.html#estimtate-three-models-5-points",
    "title": "POLS 1600 Take Home Final",
    "section": "4.3 Estimtate three models (5 points)",
    "text": "4.3 Estimtate three models (5 points)\nPlease estimate the following models:\n\nA simple model testing whether Republicans and Independents report higher or lower rates of self censorship compared to Democrats (hint, use pid_3cat as your predictor)\n\n\\[\\text{Self Censoring} = \\beta_0 + \\beta_1 \\text{Republican} + \\beta_2 \\text{Independent} + \\epsilon\\]\n\nA model which explores how these partisan differences change when controlling for variation in respondents’ attitudes about race.\n\n\\[\\text{SC} = \\beta_0 + \\beta_1 \\text{Rep} + \\beta_2 \\text{Ind} + \\beta_3 \\text{Racial Attitudes} +\\epsilon\\]\n\nAn interaction model which allows the relationship between racial attitudes and self-censorship to vary by partisanship.\n\nexplores how these partisan differences change when controlling for variation in respondents’ attitudes about race.\n\\[\\begin{align}\\text{SC} &= \\beta_0 + \\beta_1 \\text{Rep} + \\beta_2 \\text{Ind} + \\beta_3 \\text{RA} + \\\\\n&\\beta_4 \\text{Rep} \\times \\text{RA} + \\beta_5 \\text{Ind} \\times \\text{RA} +  \\epsilon\\end{align}\\]\n\n# Estimate models"
  },
  {
    "objectID": "labs/pols1600-final-exam.html#present-and-interperet-your-results-15-points",
    "href": "labs/pols1600-final-exam.html#present-and-interperet-your-results-15-points",
    "title": "POLS 1600 Take Home Final",
    "section": "4.4 Present and interperet your results (15 points)",
    "text": "4.4 Present and interperet your results (15 points)\nFinally, present and interpret the substantive and statistical signficance of your results.\nPlease include:\n\nA regression table containing the coefficients, standard errors, p-values for each model\nA plot of the predicted values from your third model (the one interacting the categorical measure of partisanship with the numeric measure of racial attitudes).\nA interpretation of the substantive and statistical significance of your three models with clear references to your table and figure.\n\n# Use htmlreg to present your models as a regression table\n\n# Create a prediction dataframe and produce predicted values \n# for you interaction model\n\n\n# Plot the predicted values (ideally with confidence intervals)\n# using ggplot\n\nHere’s an example of the kind of final figure you could produce for the interaction model\n\n\nCode Advice Again the final paper template has lots of code that should be useful for you."
  },
  {
    "objectID": "labs/10-lab-comments.html",
    "href": "labs/10-lab-comments.html",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "",
    "text": "In this lab, we’ll continue exploring data from the National Election Studies 2024 Pilot Study.\nFrom last week’s lab you should have identified some variables of interest in the data, recoded them as needed, and begun to formulate some research questions.\nIn this week’s lab, we’ll pick up where we left off:\n\nSet up your work space (5 minutes)\nLoad the data from last week (5 minutes)\nQuickly summarize the data (15)\nRevisit and revise your research question and expectations (15 minutes)\nEstimate models to explore your question (10 minutes)\nInterpret the results of your model (30 minutes)\n\nOne of these 6 tasks will be randomly selected as the graded question for the lab.\n\nset.seed(11042024)\ngraded_question &lt;- sample(1:6,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 6 is the graded question for this week\""
  },
  {
    "objectID": "labs/10-lab-comments.html#please-render-this-.qmd-file",
    "href": "labs/10-lab-comments.html#please-render-this-.qmd-file",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/10-lab-comments.html#load-packages",
    "href": "labs/10-lab-comments.html#load-packages",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\n\n# Libraries\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(kableExtra)\nlibrary(texreg)\nlibrary(ggpubr)\nlibrary(patchwork)\nlibrary(DeclareDesign)"
  },
  {
    "objectID": "labs/10-lab-comments.html#set-your-working-directory-in-r-studio",
    "href": "labs/10-lab-comments.html#set-your-working-directory-in-r-studio",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "1.2 Set your working directory in R Studio",
    "text": "1.2 Set your working directory in R Studio\nIn R studio set your working directory to the folder where this lab is saved by clicking &gt; Session &gt; Set Working Directory &gt; To Source File Location\n\nAfter doing so uncomment getwd() Should print out something like\n\n“~/Desktop/pols1600/labs/”\n\nDepending on where your lab is saved\n\n# In the Top Panel of RStudio Click\n# Session &gt; Set Working Directory &gt; Source File Location\n\n# Uncomment to Check Where Your File is Saved \n# getwd()\n\n\n\n\n\n\n\nNote\n\n\n\nIf getwd() says something like ‘~/Downloads/’ click: “File &gt; Save As” and save this lab in your course folder. Then close the version 09-lab.qmd that was opened from your Downloads folder and open the version of 09-lab.qmd that now exists in your course folder."
  },
  {
    "objectID": "labs/10-lab-comments.html#caclulate-summary-statistics-for-recoded-variables",
    "href": "labs/10-lab-comments.html#caclulate-summary-statistics-for-recoded-variables",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "3.1 Caclulate summary statistics for recoded variables",
    "text": "3.1 Caclulate summary statistics for recoded variables\n\n# Vector of numeric variables to summarize\nthe_vars &lt;- c(\n  \"dv_vote_trump2024\", \n  \"dv_participation\",\n  \"age\",\"education\",\"income\",\n  \"is_white\",\"is_black\",\"is_hispanic\",\"is_asian\",\"is_other\",\n  \"partyid\",\"is_dem\",\"is_rep\",\"is_ind\"\n)\n\n# Vector of nicely formatted labels for variables\nthe_labels &lt;- c(\n  \"Vote for Trump in '24\",\n  \"Acts of Participation in `20\",\n  \"Age\",\"Education\", \"Income\",\n  \"White\", \"Black\",\"Hispanic\",\"Asian\",\"Other\",\n  \"Party ID\", \"Democrat\",\"Republican\",\"Independent\"\n)\n\ndf_summary &lt;- df %&gt;% \n  select(all_of(the_vars)) %&gt;%\n  rename_with(~the_labels) %&gt;% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"Variable\"\n  ) %&gt;% \n  mutate(\n    Variable = factor(Variable, levels = the_labels)\n  ) %&gt;% \n  group_by(Variable) %&gt;% \n  summarise(\n    Min = min(value,na.rm = T),\n    p25 = quantile(value, prob = .25,na.rm = T),\n    Median = quantile(value, prob = .5,na.rm = T),\n    Mean = mean(value, na.rm = T),\n    p75 = quantile(value, prob = .75,na.rm = T),\n    Max = max(value,na.rm = T),\n    SD = sd(value, na.rm=T),\n    `N missing` = sum(is.na(value))\n  )"
  },
  {
    "objectID": "labs/10-lab-comments.html#display-the-results-as-a-formated-table",
    "href": "labs/10-lab-comments.html#display-the-results-as-a-formated-table",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "3.2 Display the results as a formated table",
    "text": "3.2 Display the results as a formated table\n\nkable(df_summary,\n      digits = 2) %&gt;% \n  kable_styling() %&gt;% \n  pack_rows(\"Outcomes\", start_row = 1, end_row = 2) %&gt;% \n  pack_rows(\"Demographic Predictors\", 3, 10) %&gt;% \n  pack_rows(\"Political Predictors\", 11, 14)\n\n\n\n\nVariable\nMin\np25\nMedian\nMean\np75\nMax\nSD\nN missing\n\n\n\n\nOutcomes\n\n\nVote for Trump in '24\n0\n0\n1\n0.50\n1.00\n1\n0.50\n174\n\n\nActs of Participation in `20\n0\n0\n0\n0.93\n1.00\n5\n1.31\n0\n\n\nDemographic Predictors\n\n\nAge\n18\n33\n51\n49.37\n63.75\n94\n17.74\n31\n\n\nEducation\n1\n2\n3\n3.51\n5.00\n6\n1.51\n0\n\n\nIncome\n1\n3\n6\n6.43\n9.00\n16\n3.55\n222\n\n\nWhite\n0\n0\n1\n0.67\n1.00\n1\n0.47\n0\n\n\nBlack\n0\n0\n0\n0.13\n0.00\n1\n0.33\n0\n\n\nHispanic\n0\n0\n0\n0.13\n0.00\n1\n0.33\n0\n\n\nAsian\n0\n0\n0\n0.02\n0.00\n1\n0.15\n0\n\n\nOther\n0\n0\n0\n0.01\n0.00\n1\n0.09\n0\n\n\nPolitical Predictors\n\n\nParty ID\n1\n2\n4\n3.88\n6.00\n7\n2.15\n58\n\n\nDemocrat\n0\n0\n0\n0.43\n1.00\n1\n0.49\n58\n\n\nRepublican\n0\n0\n0\n0.38\n1.00\n1\n0.48\n58\n\n\nIndependent\n0\n0\n0\n0.20\n0.00\n1\n0.40\n58"
  },
  {
    "objectID": "labs/10-lab-comments.html#interpret-the-results",
    "href": "labs/10-lab-comments.html#interpret-the-results",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "3.3 Interpret the results",
    "text": "3.3 Interpret the results\nPlease use these tables to describe a typical respondent to the 2024 NES Pilot Study:\nThe National Election Study’s 2024 Pilot Study contains responses from 1909 individuals. The typical respondent in the data was just under 50 years old, with some college, with an income in the range of $50k-$59k. Approximately two-thirds of the sample identified as white, with 13 percent of respondents identifying as Black, 13 percent as Hispanic, 2 percent as Asian. Forty-three percent of respondents identified as Democrats, 38 percent as Republicans, and 20 percent as Independents. The respondents were evenly split in who they would vote for 2024, with 50 percent saying they would Vote for Donald Trump, and 50 percent saying Joe Biden. In the 2020 campaign, the average respondent reported engaging in about 1 act of political participation."
  },
  {
    "objectID": "labs/10-lab-comments.html#research-question",
    "href": "labs/10-lab-comments.html#research-question",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "4.1 Research question",
    "text": "4.1 Research question\nME: How does support for Trump in the 2024 election vary with age and race?\nYOU:"
  },
  {
    "objectID": "labs/10-lab-comments.html#expectations",
    "href": "labs/10-lab-comments.html#expectations",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "4.2 Expectations",
    "text": "4.2 Expectations\nME: On average, I expect that older voters will be more supportive of Trump, but suspect that this trend varies by race. I expect it will be particularly true among White voters, but less so among people of color. Since, partisanship is likely to be strong predictor of vote choice, I will explore whether these specific relationships hold, once we control for variations in partisan identification which we know varies both by age and race.\nYOU:"
  },
  {
    "objectID": "labs/10-lab-comments.html#linear-model",
    "href": "labs/10-lab-comments.html#linear-model",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "4.3 Linear Model",
    "text": "4.3 Linear Model\nME:\nI will estimate the following models:\n\\[\ny = \\beta_0 + \\beta_1 age + \\sum\\beta_k race_k + \\epsilon\n\\] If my expectations hold, I expect the coefficient on age in this model to be positive, indicating older voters are more likely to vote for Trump. White respondents are the reference category in this model1 and so the coefficients on the racial indicators (\\(\\beta_k race\\)) correspond to how members of each racial group differ from white respondents in their propensity to vote for Trump. I expect all of these coefficients to be negative.\nTo explore whether the relationship between age and vote choice varies by race, I will fit an interaction model:\n\\[\ny = \\beta_0 + \\beta_1 age + \\sum\\beta_k race_k + \\epsilon + \\sum\\beta_{jk} age \\times race_k  + \\epsilon\n\\] This model allows the relationship between age and vote choice to vary by race. For white respondents, the relationship is described by \\(\\beta_1\\). Again I expect it to be positive, suggesting older white voters are more likely to vote for trump. For racial minorities, the marginal effect of age for the racial or ethnic group \\(k\\) is described by \\(\\beta_1 + \\beta_{jk}\\). In general, I expect that the coefficients on \\(\\beta_{jk}\\) to be negative such that older racial minorities are less likely to support Trump than older white respondents.\nFinally, I will estimate a model that controls for partisanship. If the relationships between age, and race and vote choice are simply a reflection of differences in partisan identification, then coefficients on these predictors should no longer be statistically significant, while the coefficient on partisanship should positive and statistically significant.\n\\[\ny = \\beta_0 + \\beta_1 pid + \\beta_2 age + \\sum\\beta_k race_k + \\epsilon + \\sum\\beta_{jk} age \\times race_k \\epsilon\n\\]\nYou:\nWe will estimate the following models:\n\\[\ny = \\beta_0 + \\beta_1 ...\n\\]\n\\[\ny = \\beta_0 + \\beta_1 ...\n\\]"
  },
  {
    "objectID": "labs/10-lab-comments.html#regression-table",
    "href": "labs/10-lab-comments.html#regression-table",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "6.1 Regression Table",
    "text": "6.1 Regression Table\nhtmlreg(list(m1, m2, m3),\n        custom.model.names = c(\n          \"Baseline\", \"Interaction\", \"Alternative\"\n        ),\n        caption = \"Support for Trump in 2024\",\n        caption.above = T,\n        custom.coef.names = c(\n          \"(Intercept)\",\n          \"Age\",\n          \"Black\",\n          \"Hispanic\",\n          \"Asian\",\n          \"Other\",\n          \"Age:Black\",\n          \"Age:Hispanic\",\n          \"Age:Asian\",\n          \"Age:Other\",\n          \"Party ID\"\n        ) ,\n        include.ci = F,\n        digits = 3)\n\n\nSupport for Trump in 2024\n\n\n\n\n \n\n\nBaseline\n\n\nInteraction\n\n\nAlternative\n\n\n\n\n\n\n(Intercept)\n\n\n0.449***\n\n\n0.361***\n\n\n-0.160***\n\n\n\n\n \n\n\n(0.038)\n\n\n(0.045)\n\n\n(0.032)\n\n\n\n\nAge\n\n\n0.002**\n\n\n0.004***\n\n\n-0.000\n\n\n\n\n \n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n\n\nBlack\n\n\n-0.280***\n\n\n0.213*\n\n\n-0.038\n\n\n\n\n \n\n\n(0.034)\n\n\n(0.100)\n\n\n(0.083)\n\n\n\n\nHispanic\n\n\n-0.089*\n\n\n0.045\n\n\n-0.052\n\n\n\n\n \n\n\n(0.038)\n\n\n(0.101)\n\n\n(0.073)\n\n\n\n\nAsian\n\n\n-0.238**\n\n\n0.007\n\n\n-0.233\n\n\n\n\n \n\n\n(0.076)\n\n\n(0.236)\n\n\n(0.136)\n\n\n\n\nOther\n\n\n-0.031\n\n\n-0.035\n\n\n0.125\n\n\n\n\n \n\n\n(0.052)\n\n\n(0.152)\n\n\n(0.108)\n\n\n\n\nAge:Black\n\n\n \n\n\n-0.011***\n\n\n-0.000\n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\nAge:Hispanic\n\n\n \n\n\n-0.003\n\n\n0.001\n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n(0.001)\n\n\n\n\nAge:Asian\n\n\n \n\n\n-0.005\n\n\n0.003\n\n\n\n\n \n\n\n \n\n\n(0.005)\n\n\n(0.003)\n\n\n\n\nAge:Other\n\n\n \n\n\n0.000\n\n\n-0.002\n\n\n\n\n \n\n\n \n\n\n(0.003)\n\n\n(0.002)\n\n\n\n\nParty ID\n\n\n \n\n\n \n\n\n0.173***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.003)\n\n\n\n\nR2\n\n\n0.047\n\n\n0.061\n\n\n0.569\n\n\n\n\nAdj. R2\n\n\n0.044\n\n\n0.056\n\n\n0.566\n\n\n\n\nNum. obs.\n\n\n1735\n\n\n1735\n\n\n1691\n\n\n\n\nRMSE\n\n\n0.489\n\n\n0.486\n\n\n0.329\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/10-lab-comments.html#visualize-predicted-values",
    "href": "labs/10-lab-comments.html#visualize-predicted-values",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "6.2 Visualize predicted values",
    "text": "6.2 Visualize predicted values\n\n# Predictors for m2\npred_dfm2 &lt;- expand_grid(\n  age = seq(min(df$age, na.rm =T), max(df$age, na.rm = T)),\n  race_5cat = sort(unique(df$race_5cat))\n)\n\n# Predictions with confidence intervals\npred_dfm2 &lt;- cbind(\n  pred_dfm2,\n  predict(m2, newdata = pred_dfm2, interval = \"confidence\")$fit\n)\n\n# Plot predictions from m2\n\nfig_m2 &lt;- pred_dfm2 %&gt;% \n  ggplot(aes(age, fit, col = race_5cat))+\n  geom_ribbon(aes(ymin = lwr, ymax = upr, \n    fill = race_5cat\n    ),\n    alpha = .5)+\n  geom_line()+\n  facet_wrap( ~ race_5cat)+\n  theme_minimal()+\n  guides(\n    col = \"none\",\n    fill = \"none\"\n  )+\n  labs(\n    y = \"Predicted Vote Choice\",\n    title = \"Support for Trump by Age and Race\"\n  )\n\n# Display figure\nfig_m2\n\n\n\n\n\n\n\n# Predictors for m3\npred_dfm3 &lt;- expand_grid(\n  age = seq(min(df$age, na.rm =T), max(df$age, na.rm = T)),\n  race_5cat = sort(unique(df$race_5cat)),\n  partyid = mean(df$partyid, na.rm=T)\n)\n\n# Predictions with confidence intervals\npred_dfm3 &lt;- cbind(\n  pred_dfm3,\n  predict(m3, newdata = pred_dfm3, interval = \"confidence\")$fit\n)\n\n# Plot predictions from m2\n\nfig_m3 &lt;- pred_dfm3 %&gt;% \n  ggplot(aes(age, fit, col = race_5cat))+\n  geom_ribbon(aes(ymin = lwr, ymax = upr, \n    fill = race_5cat\n    ),\n    alpha = .5)+\n  geom_line()+\n  facet_wrap( ~ race_5cat)+\n  theme_minimal()+\n  guides(\n    col = \"none\",\n    fill = \"none\"\n  )+\n  labs(\n    y = \"Predicted Vote Choice\",\n    title = \"Support for Trump by Age and Race Controlling for Partisanship\"\n  )\n\nfig_m3"
  },
  {
    "objectID": "labs/10-lab-comments.html#interperet-results",
    "href": "labs/10-lab-comments.html#interperet-results",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "6.3 Interperet results",
    "text": "6.3 Interperet results\nME: Our baseline model confirms our initial expectations. Controlling for race, older respondents have higher predicted levels of support for Trump by 0.002 percentage points. Controlling for race, the model predicts that a 60 year old respondent is about 6.3 percentage points more likely to vote for Trump than a 30 year-old respondent. The test statistic for this coefficient is 3.10 corresponding to a p-value &lt; 0.05, suggesting that if there were no relationship between age and vote choice in this model, it would be very unlikely that we observed a test statistic of this magnitude. Similarly the confidence interval for this estimate has suggests that coefficients as small as 0.0008 and as large as 0.0034 are plausible values for the relationship between age and vote choice in these data. Similarly, controlling for age, Black, Hispanic, and Asian respondents report significantly lower levels of support for Trump than white respondents (p &lt; 0.05).\nTurning to the interaction model, we see that the magnitude of the coefficient on age (which corresponds to the marginal effect of age for white respondents) increases, while the coefficients on the interactions between age and racial indicators are generally negative, suggesting that the relationship between age and support for Trump is less strong for these racial and ethnic groups. Figure 1 helps clarify these marginal effects, as we see that slope for age is clearly positive for white respondents, clearly negative for Black respondents. The confidence intervals for the predicted values of the other racial and ethnic groups are generally wide, and consisent with positive, negative, or no relationship between age and vote choice.\nFinally, looking at the model controlling for partisanship, we see that none of the relationships between age, race, and vote choice, remain statistically significant once we account for the relationships between partisanship and vote choice, and the relationships between partisanship and age and race. In sum, apparent demographic differences in support for Trump appear driven by the differences in partisan identification across racial groups and within age groups.\nYOU:"
  },
  {
    "objectID": "labs/10-lab-comments.html#footnotes",
    "href": "labs/10-lab-comments.html#footnotes",
    "title": "Comments for Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBecause white is first level the factor variable race↩︎"
  },
  {
    "objectID": "labs/02-lab.html",
    "href": "labs/02-lab.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to first, reproduce this figure:\n\n\n\n\n\n\n\n\n\nAnd then adapt and improve this figure (or other figures) to explore questions we have about the data\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides and the comments to last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nHow do I produce a figure that shows the average rate of new cases pe month for states with a particular type of face mask policy?\n\nWell first, I’ll need to load some packages to work with and visualize data.\nThen, I’ll need to get the data. And then…\n\n\nThink and write programmatically\n\nIn this .qmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll be working through this collectively\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nYou shouldn’t have to write much new code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!"
  },
  {
    "objectID": "labs/02-lab.html#step-2.1",
    "href": "labs/02-lab.html#step-2.1",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "3.1 Step 2.1",
    "text": "3.1 Step 2.1\nDescribe briefly (in a sentence or two or a couple of bullet points) what this section does\n\n# Write the code for Step 2.1 here"
  },
  {
    "objectID": "labs/02-lab.html#step-2.2",
    "href": "labs/02-lab.html#step-2.2",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "3.2 Step 2.2",
    "text": "3.2 Step 2.2\nDescribe briefly (in a sentence or two or a couple of bullet points) what this section does\n\n# Write the code for Step 2.2 here"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html",
    "href": "labs/comments/02-lab-comments.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to reproduce this figure:\n\n\n\n\n\n\n\n\n\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides (week 1 here, week 2 here) and last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nLearn how to take a complex problem (“How do I produce a figure that shows the average rate of new cases per month for states with a particular type of face mask policy”) which you may think you have no idea how to do and break this challenge down into concrete tasks which you do know how do (“Well first, I’ll need to load some packages to work with and visualize data. Then, I’ll need to get the data. And then…”)\n\nThink and write programmatically\n\nIn this .Rmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll produce a collective outline of what we need to do before getting to the actual coding\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nAgain you shouldn’t have to write much code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\nBut shouldn’t we be writing our own code?! Yes. Sure. Eventually.\nThe tutorials give you practice writing single commands, and by the end of the class you should be able write this code like this for to accomplish similar tasks\nBut even then, you will not be writing code from memory. I still have to Google functions, and often search my old code to find a clever solution to task.\nEveryone starts learning to code by copying and pasting other people’s code.\nThis will help minimize (but not eliminate) syntactic errors, while over time we get better writing code from scratch and fixing errors as the develop.\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nFor fun, let’s say that the first group that successfully recreates this figure gets to choose one of the following non-monetary prizes:\n\nI’ll tell them a joke\nOne AMA I will answer truthfully\nOne question to be asked on the weekly class survey\n0.00001% extra credit added to their final grade for the course.\n\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "href": "labs/comments/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.1 Create an object listing all the packages I will use today",
    "text": "2.1 Create an object listing all the packages I will use today\nThis code creates a object called the_packages which contains a vector of character strings corresponding to the names of the packages I want to use today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "href": "labs/comments/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.2 Define a function to install and load packages",
    "text": "2.2 Define a function to install and load packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "href": "labs/comments/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.3 Use the ipak function to load the necessary packages",
    "text": "2.3 Use the ipak function to load the necessary packages\nNow I run the ipak() giving it the object the_packages as an input. It sorts through the packages, checks to see if they’re installed, if not installs them, and then loads all of the packages so I can use them.\n\nipak(the_packages)\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#filter-out-u.s.-territories",
    "href": "labs/comments/02-lab-comments.html#filter-out-u.s.-territories",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.1 Filter out U.S. Territories",
    "text": "5.1 Filter out U.S. Territories\nFor simplicity, (and practice filtering observations), I’ve asked us to remove observations from U.S. territories.\nThe code below\n\nCreates an object called us_territories.\nUse this object to filter out observations that are US territories\nCreates a new data frame that is just observations from the 50 U.S. states. and D.C.\nChecks that this recoding seems to have worked\n\n\n# U.S. Territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out U.S. Territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\nError in covid %&gt;% filter(!administrative_area_level_2 %in% territories): could not find function \"%&gt;%\"\n\n# Check to make sure covid_us contains only 50 states and D.C.\ndim(covid)\n\n[1] 80156    47\n\ndim(covid_us)\n\nError in eval(expr, envir, enclos): object 'covid_us' not found\n\nlength(unique(covid$administrative_area_level_2)) \n\n[1] 56\n\nlength(unique(covid_us$administrative_area_level_2)) == 51\n\nError in eval(expr, envir, enclos): object 'covid_us' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-a-state-variable",
    "href": "labs/comments/02-lab-comments.html#create-a-state-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.2 Create a state variable",
    "text": "5.2 Create a state variable\nThis is purely for convenience, because typing administrative_area_level_2 is annoying. The code copies the values of this variable into a new variable called state using the mutate() function.\nMutate returns the original data frame plus the new column. We have to save this output for our our changes to persist (i.e. we have to assign the output of mutate() back into covid_us)\nIn last week’s lab, I just piped the output to the next command, did some more recoding with mutate, and then finally saved the output back into covid_us. In this lab, I’ll save the output after each step.\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(state = administrative_area_level_2, ): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "href": "labs/comments/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.3 Group by the state variable to calculate new Covid-19 cases",
    "text": "5.3 Group by the state variable to calculate new Covid-19 cases\nNow I use this shorter variable state to calculate the number of new cases (new_cases) in a given state on a given date, and rescale this variable so that it’s expressed in terms of new cases per 100,000 residents.\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000\n    ) -&gt; covid_us\n\nError in covid_us %&gt;% dplyr::group_by(state) %&gt;% mutate(new_cases = confirmed - : could not find function \"%&gt;%\"\n\n\nThe slides from Tuesday, helped demonstrate what this code was doing, and why we wanted to group by state.\nHere’s an example for a subset of the data from April 1, 2020 to April 7, 2020\nWe see that the lag() function simply moves the observation of a variable “up” one row so that we can take the difference between the total number of cases in a state on one date and the total number of cases on the date before, to calculate the number of new cases\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  group_by(state) %&gt;%\n  select(state, date, confirmed)%&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\nError in covid_us %&gt;% filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;% : could not find function \"%&gt;%\"\n\n\nIf we hadn’t grouped by state, then when we lagged the confirmed variable, R thinks the number of confirmed cases in California before April 1, 2020, is 986 which is actually the number of cases in Minnesota on April 7, 2020\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  ungroup() %&gt;%\n  select(state, date, confirmed)%&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\nError in covid_us %&gt;% filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;% : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#recode-the-facial_coverings-variable",
    "href": "labs/comments/02-lab-comments.html#recode-the-facial_coverings-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.4 Recode the facial_coverings variable",
    "text": "5.4 Recode the facial_coverings variable\nNext we use the case_when() function inside the mutate() function to create a variable called face_masks based on the values of the facial_coverings variable in the data.\ncase_when() when uses R’s ability to make logical comparisons. When the variable facial_coverings equals 0, R will input the character string \"No policy\" into the face_masks variable.\nWhen the absolute value of facial_coverings equals 1 (i.e.facial_coverings equals 1 or -1 ), R will input the character string \"Recommended\" into the face_masks variable. And so on.\nWe use the abs() function to take the absolute value of the facial_coverings variable because codebook for these data implied:\n\nIn short: positive integers identify policies applied to the entire administrative area. Negative integers are used to identify policies that represent a best guess of the policy in force, but may not represent the real status of the given area. The negative sign is used solely to distinguish the two cases, it should not be treated as a real negative value.\n\nWe know from last weeks lab, that negative values in the U.S. typically seem to be cases where a city had a more stringent policy than the state (e.g. Chicago adopts more stringent face mask policies than Illinois).\nFinally, we put a %&gt;% after the output of case_when() and pass it’s output to the factor() function.\nThe . acts as sort of placeholder, factor() expects some input here (like a variable from a data frame), . tells R to use the output of case_when().\nThe levels = then transforms the character data produced by case_when() into a factor with an implicit ordering of levels (i.e. “No policy” &lt; “Recommended”&lt; “Some requirements” &lt;“Required shared places” &lt;“Required all times”) which turns out to be useful trick for organizing how data are plotted and visualized.\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(face_masks = case_when(facial_coverings == : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "href": "labs/comments/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.5 Create a variable capturing the year and month of the observation",
    "text": "5.5 Create a variable capturing the year and month of the observation\nFinally we create some variables that extract components of an observation’s date:\n\nyear = year(date) returns just the year from a variable of class Date\nmonth = month(date) returns just the month from a variable of class Date\nyear_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\") pastes these to variables together.\nstr_pad(month, width = 2, pad=0) adds a leading 0 to any month with only 1 digit, to ensure that all the months have 2 characters.\n\nThe code from your lab also calculates the percent of a states population that is vaccinated, which isn’t strictly needed for today.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(year = year(date), month = month(date), year_month = paste(year, : could not find function \"%&gt;%\"\n\n\nCreating separte year and month variables aren’t strictly necessary,\nWe could have written something like:\n\ncovid_us %&gt;%\n  mutate(\n    year_month = paste(year(date), str_pad(month(date), width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nBut that year_month line was already feeling kind of clunky, and maybe we’ll want the year and month variables later."
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#adding-meaningful-labels-and-title",
    "href": "labs/comments/02-lab-comments.html#adding-meaningful-labels-and-title",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.1 Adding meaningful labels and title",
    "text": "8.1 Adding meaningful labels and title\nBecause we saved the output of our ggplot to an object called fig1 we can add additional commands to this object using the + without having to rewrite all the code.\nFirst let’s add better labels to the graph.\n\nNote that even though we flipped the coordinates, the aes aesthetic mappings stay the same. So to change the label of the figures y-axis to “Date” we change the label of x = \"Date\"\nggplot automatically generates a legend for aesthetic mappings like color We can add a line break using the the special character \\n in our code\n\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\n Policy\"\n  )\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nNote the code above didn’t update fig1\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nWe have to save the output (if we like it) for our changes to persist.\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\nPolicy\"\n  ) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#changing-the-theme-of-the-plot",
    "href": "labs/comments/02-lab-comments.html#changing-the-theme-of-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.2 Changing the theme of the plot",
    "text": "8.2 Changing the theme of the plot\nHere’s an example of some different themes\n\n# Black and white\nfig1 +\n  theme_bw()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n# Minimal\nfig1 +\n  theme_minimal()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n# Classic\nfig1 +\n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nThis is pretty personal, and depends of the figure itself. I like a white background and some guide lines:\n\nfig1 +\n  theme_bw() -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "href": "labs/comments/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.3 Make the size of the dots reflect the number of states with this policy",
    "text": "8.3 Make the size of the dots reflect the number of states with this policy\nIn the cases_by_month_and_policy we have a column called n which is the number of states which had a given policy in a given month.\nWe can add an aesthetic to our plot that varies the size of the points by the number of states.\n\nfig1 +\n  aes(size = n) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nWe call this type of plot a bubble plot{target=“_blank”\nI have mixed feelings about multiple legends. We can remove the legend for size using the scale_size() function. I had to Google how to do this for the millionth time.\n\nfig1 +\n  scale_size(guide = \"none\") -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#facet-the-plot",
    "href": "labs/comments/02-lab-comments.html#facet-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.4 Facet the plot",
    "text": "8.4 Facet the plot\nVarying the size of the dots by the number of states conveys more information. But makes the chart a little harder to read. Dots overlap.\nThe facet_wrap command will produce separate bubble plots for each level of the “facetting” variable, in this case `face_masks\n\nfig1 +\n  facet_wrap(~face_masks) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nNow I think also want a second legend for the number of states\n\nfig1 +\n  scale_size(guide = \"legend\")+\n  labs(\n    size = \"# of States\\nwith Policy\"\n  )-&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nThis seems pretty good if our goal was to show in general terms\n\nIt shows the average number new cases for states with a given face mask policy over time.\nIt shows how the mix of types of face mask policies states have adopted has changed over time\n\nIf our goal was to make comparisons across face mask policies over a given time period, I’m might still prefer something closer to our original graph."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html",
    "href": "labs/comments/01-lab-comments.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#uncomment-and-run-the-following-code",
    "href": "labs/comments/01-lab-comments.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#uncomment-and-run-the-code-below",
    "href": "labs/comments/01-lab-comments.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\ncovid_us %&gt;%\n  filter(facial_coverings == -4) %&gt;%\n  select(date, state) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    n = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n  )%&gt;%\n  arrange(earliest_date)\n\n# A tibble: 4 × 4\n  state              n earliest_date latest_date\n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;        &lt;date&gt;     \n1 Illinois         156 2020-10-01    2021-05-15 \n2 Massachusetts     35 2020-10-02    2020-11-05 \n3 South Carolina    61 2020-10-13    2020-12-12 \n4 Maryland         158 2020-11-06    2021-04-12"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "href": "labs/comments/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explain in words your best understanding of what each line of code is doing:",
    "text": "6.2 Please explain in words your best understanding of what each line of code is doing:\n\ncovid_us %&gt;% tells R that every line of code after will use covid_us dataframe\nfilter(facial_coverings == -4) %&gt;% tells R to filter out only the rows where the facial coverings variable equals -4\nselect(date, state) %&gt;% tells R to select the columns named date and state\ngroup_by(state) %&gt;% tells R that subsequent commands should be done separately for each unique value of state\nsummarize( tells R we want to summarize the output of susequent commands\nn = n(), tells R to count the number of observations (state-dates) for each state that had a value of -4 on the facial_coverings variable\nearliest_date = min(date), tells R to report the earliest date that each state had a value of -4\nlatest_date = max(date), tells R to report the last date that each state had a value of -4\n)%&gt;% tells R we’re finished with the summarize() function\narrange(earliest_date) arranges the data in asscending order from earliest to latest start date\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\nSubstantively, what does the previous chunk of code tell us?\n\nSo there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. Illinois was the first state where this code appears, and it appears present in 156 observations while Montana was the last adopting a policy code -4 on March 25, 2021\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#please-run-the-following-code",
    "href": "labs/comments/01-lab-comments.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  )%&gt;%\n  filter(RegionName == \"Illinois\", \n         date &gt; \"2020-08-01\", \n         date &lt; \"2021-01-01\",\n         !is.na(H6_Notes)) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\n# A tibble: 8 × 4\n  date       `H6_Facial Coverings` H6_Flag H6_Notes                             \n  &lt;date&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                \n1 2020-08-21                     2       1 \"In Executive Order 2020-52, Executi…\n2 2020-08-26                     2       1 \"Effective from 26 August 2020, the …\n3 2020-09-18                     2       1 \"On 18 September, in Executive Order…\n4 2020-10-01                     4       0 \"Originally coded a 3T, but looking …\n5 2020-10-16                     4       0 \"In Executive Order (EO) 2020-59, Go…\n6 2020-11-13                     4       0 \"Noting that Executive Order 2020-71…\n7 2020-11-20                     4       0 \"Executive Order 2020-73 requires pe…\n8 2020-12-01                     3       1 \"Chicago seems to have changed its g…"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/comments/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;% Tells R to use the Oxford policy data\nmutate(date = ymd(Date))%&gt;% Creates a date variable of class date from the original Date variable (which was class numeric)\nfilter(RegionName == \"Illinois\", subsets the data to just Illinois\ndate &gt; \"2020-08-01\", filters out dates before August 1, 2020\ndate &lt; \"2021-01-01\", filters out observations with dates after January 1,2021\n!is.na(H6_Notes)) %&gt;% filters out observations without notes (which appear in the data when policy changes)\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks Selects just the date and notes variables and saves them to an object called il_facemasks\nil_facemasks prints the obejct in the console\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\n[1] \"On 18 September, in Executive Order 2020-55, the Governor reissued most executive orders, extending a majority of the provisions through 17 October 2020. This includes mask requirements.      https://web.archive.org/web/20200922144918/https://www2.illinois.gov/Pages/Executive-Orders/ExecutiveOrder2020-55.aspx\"\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\n# il_facemasks$H6_Notes[???]"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/comments/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy\nIn Illinois, the -4’s seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of facial_coverings to construct our face_mask variable, we’re probably over stating the extent the extensiveness of policies in effect.\nSo we should be cautious in how we interpret our collapsed variable, face_mask. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-central-tendency",
    "href": "labs/comments/01-lab-comments.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n[ {x}=_{i=1}^n x_i ]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\n\nmean(covid_us$new_cases_pc, na.rm=T)\n\n[1] 28.11486\n\n\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nFormally, you’ll often see this written in terms of Expected Values: Something like\n[ E[Y|X=x] ]\nOr to make it more concrete:\n[ E[ | ] ]\nIn code, we could accomplish this manually, using the index operator:\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n[1] 10.26168\n\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommend”\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"Recommended\"], na.rm=T)\n\n[1] 16.61408\n\n\nBy using group_by() with summarise() we can accomplish this more quickly:\n\ncovid_us%&gt;%\n  group_by(face_masks)%&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  )\n\n# A tibble: 6 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2\n6 &lt;NA&gt;                           11.8\n\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n[ M_x = X_i : ^{x_i} f_x(X)dx=^f_x(X)dx=1/2 ]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\nmedian(covid_us$new_cases_pc, na.rm=T)\n\n[1] 10.52355\n\n\nInteresting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the new_cases_pc has a “long tail” or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.\n\n\nMedians are less influenced by outliers than means\n\n\n\nhist(covid_us$new_cases_pc, breaks = 100)\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\nsort(table(covid_us$face_masks))\n\n\n    Required all times              No policy            Recommended \n                  1032                   3893                   8879 \nRequired shared places      Some requirements \n                 15088                  24786 \n\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:\n\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Visualisation_mode_median_mean.svg/240px-Visualisation_mode_median_mean.svg.png\")"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-dispersion",
    "href": "labs/comments/01-lab-comments.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\nrange(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916 1531.8669\n\nmin(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916\n\nmax(covid_us$new_cases_pc,na.rm = T)\n\n[1] 1531.867\n\n\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\ncovid_us %&gt;%\n  filter(\n    new_cases_pc &lt; -188 |\n    new_cases_pc &gt; 1500\n  )%&gt;%\n  select(state, date,new_cases_pc)\n\n# A tibble: 5 × 3\n# Groups:   state [5]\n  state        date       new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;            &lt;dbl&gt;\n1 Florida      2021-06-04        -189.\n2 Rhode Island 2022-01-04        1532.\n3 Tennessee    2023-01-01        -267.\n4 Nebraska     2022-10-28        -276.\n5 Kentucky     2022-10-11        -198.\n\n\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n[ p_x = X_i : ^{x_i} f_x(X)dx= p; ^f_x(X)dx=1-p ]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\n\nquantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)\n\n     25%      75% \n 0.00000 32.14152 \n\n\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n[ ^2_x=_{i=1}n(x_i-{x})2 ]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\nUse the var() function to calculate the variance of the new_cases_pc variable.\n\nvar(covid_us$new_cases_pc,na.rm=T)\n\n[1] 3402.718\n\n# Calculate by hand\n\nsum(\n  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, \n  na.rm=T\n  )/(sum(!is.na(covid_us$new_cases_pc))-1)\n\n[1] 3402.718\n\n\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n[ _x== ]\nStandard deviations are easier to interpet because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\nAgain, let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable\n\nsd(covid_us$new_cases_pc,na.rm=T)\n\n[1] 58.33282"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-association",
    "href": "labs/comments/01-lab-comments.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n[ cov(x,y)=_{i=1}^n(x_i-{x})(y_i-{y}) ]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\nvar(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)\n\n[1] -19.96569\n\n\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n[ _{x,y} = ]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\n\ncor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = \"complete.obs\")\n\n[1] -0.01369243\n\n\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\ncovid_us %&gt;%\n  filter(year &gt; 2020)%&gt;%\n  ungroup() %&gt;%\n  group_by(year,month)%&gt;%\n  summarise(\n    mn_per_vax = mean(percent_vaccinated, na.rm=T),\n    cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n  )\n\nError in `summarise()`:\nℹ In argument: `cor = cor(new_cases_pc, percent_vaccinated, use =\n  \"complete.obs\")`.\nℹ In group 28: `year = 2023` and `month = 4`.\nCaused by error in `cor()`:\n! no complete element pairs"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "href": "labs/comments/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?\nProbably not that much. Different Face mask policies are implemented at different times in the pandemic. For example, by 2021, almost all states have some requirements. Comparing the average for new cases in states with no policy to states with full requirements, is comparing the state of world in early 2020 to the state of the world in late 2020 to mid 2021. But lots of things differ between these periods. Other policies are also going into effect, new variants are emerging.\nIn short, those simple conditional means across the full data don’t really provide an apples to apples comparison.\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,n,fill=face_masks))+\n  geom_bar(stat=\"identity\")\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    new_cases = sum(new_cases)\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,new_cases))+\n  geom_smooth()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 322 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/comments/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\nSo when we compare states in September 2020, we see that rates of new cases tend to be much higher in states with only recommend face mask policies compared to states with at least some requirements.\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  filter(year_month == \"2020-09\")%&gt;%\n  group_by(face_masks) %&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc,na.rm=T)\n  )\n\n# A tibble: 4 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Recommended                    43.9\n2 Some requirements              13.5\n3 Required shared places         13.0\n4 Required all times             10.1\n\n\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(year_month,face_masks) %&gt;%\n  summarise(\n    n = length(unique(state)),\n    new_cases_pc = round(mean(new_cases_pc,na.rm=T)),\n    total_cases = round(mean(confirmed,na.rm=T))\n  ) -&gt; cases_by_month_and_policy\n\n`summarise()` has grouped output by 'year_month'. You can override using the\n`.groups` argument.\n\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\nDT::datatable(cases_by_month_and_policy,\n              filter = \"top\")\n\n\n\n\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\ncases_by_month_and_policy %&gt;%\n  ggplot(aes(\n    x= year_month,\n    y = new_cases_pc,\n    col=face_masks))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#footnotes",
    "href": "labs/comments/01-lab-comments.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html",
    "href": "labs/comments/10-lab-comments.html",
    "title": "Lab 10 The final lab!!",
    "section": "",
    "text": "In our final lab, you will apply concepts and skills from this course to explore data from the 2020 American National Election Study. Specifically you will\n\nIdentify an outcome of interest (5-10 minutes)\nIdentify key predictors and covariates (5-10 minutes)\nRecode your data (20 minutes)\nDescribe your data (20 minutes)\nDescribe your question, expectations, and models (10 minutes)\nEstimate, present, and interpret your models (20 minutes, Graded Question)\n\nIdeally, each group will pursue a question that interests them. I will also complete these tasks live, so, if you’re not feeling confident, you can follow along with me and submit the code I demo in class as your lab for a grade of 85.."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#summary-statistics",
    "href": "labs/comments/10-lab-comments.html#summary-statistics",
    "title": "Lab 10 The final lab!!",
    "section": "4.1 Summary statistics:",
    "text": "4.1 Summary statistics:\nProducing a table of summary statistics requires a little foresight.\nEssentially you want to make a data frame where each row is a (numeric) variable, and each column is a statistic (minimum, 25th percentile, median, mean, 75th percentile, max, Number of missing).\nTo do this, I would:\n\ncreate a object called the_vars which contains the names (in quotation marks) of the variables you want to summarize.\nSelect these variables from your data set. using df%&gt;%select(all_of(the_vars))\nUse %&gt;%pivot_longer() specifying cols=select(all_of(the_vars)), and names_to equals \"Variable\" and values_to = \"value\" to transform this wide dataset into a long dataset\nThen use %&gt;%group_by(Variable)%&gt;% and summarise() to calculate the statistics for each variable of interest (e.g. %&gt;%summarise(Mean = mean(value, na.rm=T))))\nSave the output to an object called something like sum_df\nIn a new chunk use knitr::kable(sum_df) %&gt;% kableExtra::kable_styling() to format your table. Set echo=F in the code chunk head\n\n\n# Summarise data\n\nthe_vars &lt;- c(\"ft_police\",\n              \"is_white\",\n              \"been_arrested\",\n              \"income\")\n\ndf%&gt;%\n  select(all_of(the_vars)) %&gt;%\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\",\n    values_to = \"value\"\n  )%&gt;%group_by(Variable)%&gt;%\n  summarise(\n    Min = min(value, na.rm=T),\n    p25 = quantile(value, probs = .25,na.rm=T),\n    Median = median(value, na.rm=T),\n    Mean = mean(value, na.rm=T),\n    p75 = quantile(value, probs = .75,na.rm=T),\n    Max = max(value, na.rm=T)\n\n            ) %&gt;% \n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  ) %&gt;% \n  arrange(Variable) -&gt; sum_df\n\nError in UseMethod(\"select\"): no applicable method for 'select' applied to an object of class \"function\"\n\n\n# Display results\nknitr::kable(sum_df, digits = 2) %&gt;% kableExtra::kable_styling()\nError in eval(expr, envir, enclos): object 'sum_df' not found"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#descriptive-figures",
    "href": "labs/comments/10-lab-comments.html#descriptive-figures",
    "title": "Lab 10 The final lab!!",
    "section": "4.2 Descriptive Figures",
    "text": "4.2 Descriptive Figures\nTo create a figure, you’ll need to specificy the following\n\ndata (e.g. df %&gt;%)\naesthetic mappings, ggplot(aes(x = predictor, y = outcome))\ngeometries\n\nUnivariate: geom_density(), geom_boxplot() geom_histogram()\nBivariate: geom_point() (for a scatterplot), geom_line() for a trend.\n\n\nOnce you have a minimal working example, play around with other grammars of graphics:\n\nlabs() for custom labels\ntheme_XXX for custom themes\nfacet_wrap(~group) to produce the same plot facetted by some categorical grouping variable\n\nWhen you’re happy with your figure, save it as object in R (e.g. fig1 &lt;- df %&gt;% ggplot(aes(predictor, outcome))+geom_point()). Put that object in its own chunk to display it in your document.\nDon’t let the perfect be the enemy of the good.\n\n# Descriptive figures"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#descrptive-interpretation",
    "href": "labs/comments/10-lab-comments.html#descrptive-interpretation",
    "title": "Lab 10 The final lab!!",
    "section": "4.3 Descrptive Interpretation:",
    "text": "4.3 Descrptive Interpretation:\nPlease provide an overview of the data (source, number of observations, unit of analysis).\nDescribe a typical observation, making reference to the statistics in your summary table.\nOffer a substantive interpretation of your descriptive figure(s). What do they tell us about the distribution of a key variable, or the relationship between two variables."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#fit-the-models",
    "href": "labs/comments/10-lab-comments.html#fit-the-models",
    "title": "Lab 10 The final lab!!",
    "section": "6.1 Fit the models",
    "text": "6.1 Fit the models\n\n# Model 1: Bivariate Model\n\n# Model 2: Multiple Regression"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#display-the-models-in-a-regression-table",
    "href": "labs/comments/10-lab-comments.html#display-the-models-in-a-regression-table",
    "title": "Lab 10 The final lab!!",
    "section": "6.2 Display the models in a regression table",
    "text": "6.2 Display the models in a regression table\n\n# Regression table"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#interpet-your-models",
    "href": "labs/comments/10-lab-comments.html#interpet-your-models",
    "title": "Lab 10 The final lab!!",
    "section": "6.3 Interpet your models",
    "text": "6.3 Interpet your models\nPlease write a 1 paragraph summary interpreting your results in terms of both their statistical and substantive significance. Assume your audience is smart, but has never taken POLS 1600. Explain to them what a regression model is, what a standard error, p-value, and/or confidence interval is. How should they interepret the substantive findings of your model. How should they assess the statistical uncertainty around these results?\nPerhaps you might reade create a plot of predicted values from a model to help facilitate the substantive interpretation of your results. If so, here’s a code chunk for you:\n\n# Additional code chunk to facilitate interpretation of models"
  },
  {
    "objectID": "labs/comments/04-lab-comments.html",
    "href": "labs/comments/04-lab-comments.html",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "set.seed(20231005)\n  the.questions&lt;-1:12\n  graded&lt;-sample(the.questions,1)\n  graded\n\n[1] 3\nQuestion 3 is the graded question for this assignment"
  },
  {
    "objectID": "labs/comments/04-lab-comments.html#goals",
    "href": "labs/comments/04-lab-comments.html#goals",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "0.1 Goals",
    "text": "0.1 Goals\nConceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html",
    "href": "labs/comments/05-lab-comments.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3032022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\n\nGrading Questin 9: Basically, if you made any changes to fig_m5 100 percent. If you simply recreated fig_m5 80 percent. If you didn’t create figure fig_m5 0 percent. Sorry! But don’t fret, remember your 3 lowest lab scores are dropped from your lab grade.\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, 1, 2, 3, 4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#load-covid-19-data",
    "href": "labs/comments/05-lab-comments.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid data\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  level = 2,\n  verbose = F\n)"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#load-election-data",
    "href": "labs/comments/05-lab-comments.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\n# This joyously stopped working last night...\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# Backup\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#recode-the-covid-19-data",
    "href": "labs/comments/05-lab-comments.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide, starting here and ending here\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/comments/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, change the new_deaths_pc_7da to new_deaths_pc_14da and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7da = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14da = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/comments/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-all-the-observations",
    "href": "labs/comments/05-lab-comments.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7da)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7da\n  &lt;fct&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 Trump        18.4             0.324\n2 Biden        21.1             0.276"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/comments/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7da\n  &lt;fct&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 Trump        22.8             0.400\n2 Biden        30.6             0.380"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/comments/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7da\n  &lt;fct&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 Trump        15.9             0.281\n2 Biden        15.5             0.216\n\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that states which Biden won had about 27 new deaths compared to 23.8 new deaths in states which Trump one.\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that rates tend to be higher in Red States (0.415 deaths per 100k) than Blue States (0.349 deaths per 100k). This difference reflects the fact that Biden tended to win more populous states than trump, so simply looking at the average number of new deaths is bit misleading. Comparing 7-day averages per 100,000 residents adjusts for differences in population between Red and Blue States.\nWhen we limit our analysis, to just observations before April 19, 2021, the difference in the 7-day average rate of new Covid-19 deaths per 100,000 residents is relatively small (0.02 more deaths per 100,000 residents in Red States)\nWhen we look at observations after the vaccine became widely available the difference is more than 6 times as big (0.125 more deaths per 100,000 residents in Red States)"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#recreating-the-nyt-figures",
    "href": "labs/comments/05-lab-comments.html#recreating-the-nyt-figures",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "9.1 Recreating the NYT Figures",
    "text": "9.1 Recreating the NYT Figures\nThis turned out to be more annoying than I thought, but if you really wanted to recreate the figures from the articles, this was as close as I could get:\n\n# Vector containing labeled states\nthe_labs &lt;- c(\"WV\",\"WY\",\"MS\",\"KY\",\"TX\",\"FL\",\"GA\",\"IL\",\"NY\",\"VT\",\"MD\",\"CA\")\n\ncovid_us %&gt;%\n  # Only include labels for states in the the_labs\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  # Subset data\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics, flipping vax to % unvaxxed\n  ggplot(aes(x = rep_voteshare,\n             y = (100-percent_vaccinated),\n             label = nyt_labs\n             ))+\n  # points coloreded by vote share\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # color gradient\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # add simple regression line\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  # add labels\n  geom_text_repel()+\n  # futz with limits\n  ylim(15,60)+\n  # add grid lines by hand\n  geom_hline(yintercept = 60, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 40, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 20, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=20, yend = 60), col = \"lightgrey\", size = .25)+\n  # Add arrows\n  geom_segment(aes(x = 34, xend = 32, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add biden text\n  annotate(\"text\",x = 34.5, y=18.5 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=17.1 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.7, y=17.1 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  # Add trump arrow\n  geom_segment(aes(x = 70, xend = 72, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add trump text\n  annotate(\"text\",x = 69.5, y=18.5 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.1, y=17.1 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=17.1 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  # Label y-axis\n  annotate(\"text\",x = 30, y=20 ,label = \"20%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=40 ,label = \"40%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=60 ,label = \"60% of residents not fully vaccinated\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  # get rid of default theme\n  theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values or values outside the scale range\n(`geom_text_repel()`).\n\n\n\n\n\n\n\n\n\n\n# Same as above, but now modeling deaths with rep vote share\n\ncovid_us %&gt;%\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14da,\n             label = nyt_labs\n             ))+\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  geom_text_repel()+\n  # theme_void()+\n  ylim(-.2,2.2)+\n  geom_hline(yintercept = 0, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = .5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1.5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 2, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=0, yend = 2), col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 34, xend = 32, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.1 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.2 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.82, y=-.2 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  geom_segment(aes(x = 70, xend = 72, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.1 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.09, y=-.2 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.2 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  annotate(\"text\",x = 30, y=0.5 ,label = \"0.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1 ,label = \"1\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1.5 ,label = \"1.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=2 ,label = \"2 deaths per 100,000 residents\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  theme_void()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values or values outside the scale range\n(`geom_text_repel()`)."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#footnotes",
    "href": "labs/comments/05-lab-comments.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/03-lab.html",
    "href": "labs/03-lab.html",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "",
    "text": "Today we will explore the logic and design of Broockman and Kalla’s 2016 study, “Durably reducing transphobia: A field experiment on door-to-door canvassing”, from the recruitment of subjects for the study to the delivery of their interventions. Then we will explore whether the intervention had any effect on respondents’ feelings toward transgender individuals.\nTo accomplish this we will:\n\nSummarize the study (5 Minutes)\nSet up our work space (2-3 Minutes)\nLoad a portion of the replication data (1-2 Minutes)\nGet a high level overview of the data (5 minutes)\nDescribe the distribution of covariates in the full dataset (5 minutes)\nExamine the difference in covariates between those who did and did not complete the survey (10 minutes)\nExamine the difference in covariates between those assigned to each treatment condition in the study. (10 minutes)\nEstimate the average treatment effect of the intervention (10 minutes)\nPlot the results and comment on the study (10 minutes)\nTake the weekly survey (3-5 minutes)\n\nOne of these 9 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/03-lab.html#footnotes",
    "href": "labs/03-lab.html#footnotes",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find the full set of replication files here↩︎\nThe actual study contains a number of measures about transgender attitudes and policies which are scaled together to produce a single measure of subjects latent tolerance. For simplicity, we’ll focus on this single survey item.↩︎\nRecall that only some people who completed the baseline and were assigned to receive the treatment actually answered the door when canvassers came knocking.↩︎"
  },
  {
    "objectID": "labs/01-lab-comments.html",
    "href": "labs/01-lab-comments.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/01-lab-comments.html#uncomment-and-run-the-following-code",
    "href": "labs/01-lab-comments.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/01-lab-comments.html#uncomment-and-run-the-code-below",
    "href": "labs/01-lab-comments.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\ncovid_us %&gt;%\n  filter(facial_coverings == -4) %&gt;%\n  select(date, state) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    n = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n  )%&gt;%\n  arrange(earliest_date)\n\n# A tibble: 4 × 4\n  state              n earliest_date latest_date\n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;        &lt;date&gt;     \n1 Illinois         156 2020-10-01    2021-05-15 \n2 Massachusetts     35 2020-10-02    2020-11-05 \n3 South Carolina    61 2020-10-13    2020-12-12 \n4 Maryland         158 2020-11-06    2021-04-12"
  },
  {
    "objectID": "labs/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "href": "labs/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explain in words your best understanding of what each line of code is doing:",
    "text": "6.2 Please explain in words your best understanding of what each line of code is doing:\n\ncovid_us %&gt;% tells R that every line of code after will use covid_us dataframe\nfilter(facial_coverings == -4) %&gt;% tells R to filter out only the rows where the facial coverings variable equals -4\nselect(date, state) %&gt;% tells R to select the columns named date and state\ngroup_by(state) %&gt;% tells R that subsequent commands should be done separately for each unique value of state\nsummarize( tells R we want to summarize the output of susequent commands\nn = n(), tells R to count the number of observations (state-dates) for each state that had a value of -4 on the facial_coverings variable\nearliest_date = min(date), tells R to report the earliest date that each state had a value of -4\nlatest_date = max(date), tells R to report the last date that each state had a value of -4\n)%&gt;% tells R we’re finished with the summarize() function\narrange(earliest_date) arranges the data in asscending order from earliest to latest start date\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\nSubstantively, what does the previous chunk of code tell us?\n\nSo there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. Illinois was the first state where this code appears, and it appears present in 156 observations while Montana was the last adopting a policy code -4 on March 25, 2021\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/01-lab-comments.html#please-run-the-following-code",
    "href": "labs/01-lab-comments.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  )%&gt;%\n  filter(RegionName == \"Illinois\", \n         date &gt; \"2020-08-01\", \n         date &lt; \"2021-01-01\",\n         !is.na(H6_Notes)) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\n# A tibble: 8 × 4\n  date       `H6_Facial Coverings` H6_Flag H6_Notes                             \n  &lt;date&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                \n1 2020-08-21                     2       1 \"In Executive Order 2020-52, Executi…\n2 2020-08-26                     2       1 \"Effective from 26 August 2020, the …\n3 2020-09-18                     2       1 \"On 18 September, in Executive Order…\n4 2020-10-01                     4       0 \"Originally coded a 3T, but looking …\n5 2020-10-16                     4       0 \"In Executive Order (EO) 2020-59, Go…\n6 2020-11-13                     4       0 \"Noting that Executive Order 2020-71…\n7 2020-11-20                     4       0 \"Executive Order 2020-73 requires pe…\n8 2020-12-01                     3       1 \"Chicago seems to have changed its g…"
  },
  {
    "objectID": "labs/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;% Tells R to use the Oxford policy data\nmutate(date = ymd(Date))%&gt;% Creates a date variable of class date from the original Date variable (which was class numeric)\nfilter(RegionName == \"Illinois\", subsets the data to just Illinois\ndate &gt; \"2020-08-01\", filters out dates before August 1, 2020\ndate &lt; \"2021-01-01\", filters out observations with dates after January 1,2021\n!is.na(H6_Notes)) %&gt;% filters out observations without notes (which appear in the data when policy changes)\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks Selects just the date and notes variables and saves them to an object called il_facemasks\nil_facemasks prints the obejct in the console\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\n[1] \"On 18 September, in Executive Order 2020-55, the Governor reissued most executive orders, extending a majority of the provisions through 17 October 2020. This includes mask requirements.      https://web.archive.org/web/20200922144918/https://www2.illinois.gov/Pages/Executive-Orders/ExecutiveOrder2020-55.aspx\"\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\nil_facemasks$H6_Notes[il_facemasks$date == \"2020-10-01\"]\n\n[1] \"Originally coded a 3T, but looking at the below description, which includes even residential buildings, it is hard to conceive of a time outside the home when a Chicago resident would not be required to wear a mask. The Phase IV \\\"Gradually Resume\\\" guidelines seem not to provide any significant exemption (https://archive.fo/dOyY9). Hence code moves up to 4T.    Effective October 1, 2020, residents of Chicago are required to wear masks in all public places.     “Any individual who is over age two and able to medically tolerate a mask shall be required to wear a mask when in a public place, which for purposes of this Order includes any common or shared space in: (1) a residential multi-unit building or (2) any non-residential building, unless otherwise provided for in the Phase IV: Gradually Resume guidelines promulgated by the Office of the Mayor (\\\"Gradually Resume Guidelines\\\")”    Additionally, but separately, “Individuals must, at all times and as much as reasonably possible, maintain Social Distancing from any other person who does not live with them.”    https://web.archive.org/web/20201116163255/https://www.chicago.gov/content/dam/city/sites/covid/health-orders/CDPH%20Order%202020-9%20-%205th%20Amended%20FINAL%209.30.20_AAsigned.pdf\""
  },
  {
    "objectID": "labs/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy\nIn Illinois, the -4’s seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of facial_coverings to construct our face_mask variable, we’re probably over stating the extent the extensiveness of policies in effect.\nSo we should be cautious in how we interpret our collapsed variable, face_mask. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy."
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-central-tendency",
    "href": "labs/01-lab-comments.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n[ {x}=_{i=1}^n x_i ]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\n\nmean(covid_us$new_cases_pc, na.rm=T)\n\n[1] 28.11486\n\n\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nFormally, you’ll often see this written in terms of Expected Values: Something like\n[ E[Y|X=x] ]\nOr to make it more concrete:\n[ E[ | ] ]\nIn code, we could accomplish this manually, using the index operator:\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n[1] 10.26168\n\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommend”\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"Recommended\"], na.rm=T)\n\n[1] 16.61408\n\n\nBy using group_by() with summarise() we can accomplish this more quickly:\n\ncovid_us%&gt;%\n  group_by(face_masks)%&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  )\n\n# A tibble: 6 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2\n6 &lt;NA&gt;                           11.8\n\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n[ M_x = X_i : ^{x_i} f_x(X)dx=^f_x(X)dx=1/2 ]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\nmedian(covid_us$new_cases_pc, na.rm=T)\n\n[1] 10.52355\n\n\nInteresting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the new_cases_pc has a “long tail” or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.\n\n\nMedians are less influenced by outliers than means\n\n\n\nhist(covid_us$new_cases_pc, breaks = 100)\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\nsort(table(covid_us$face_masks))\n\n\n    Required all times              No policy            Recommended \n                  1032                   3893                   8879 \nRequired shared places      Some requirements \n                 15088                  24786 \n\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:"
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-dispersion",
    "href": "labs/01-lab-comments.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\nrange(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916 1531.8669\n\nmin(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916\n\nmax(covid_us$new_cases_pc,na.rm = T)\n\n[1] 1531.867\n\n\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\ncovid_us %&gt;%\n  filter(\n    new_cases_pc &lt; -188 |\n    new_cases_pc &gt; 1500\n  )%&gt;%\n  select(state, date,new_cases_pc)\n\n# A tibble: 5 × 3\n# Groups:   state [5]\n  state        date       new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;            &lt;dbl&gt;\n1 Florida      2021-06-04        -189.\n2 Rhode Island 2022-01-04        1532.\n3 Tennessee    2023-01-01        -267.\n4 Nebraska     2022-10-28        -276.\n5 Kentucky     2022-10-11        -198.\n\n\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n[ p_x = X_i : ^{x_i} f_x(X)dx= p; ^f_x(X)dx=1-p ]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\n\nquantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)\n\n     25%      75% \n 0.00000 32.14152 \n\n\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n[ ^2_x=_{i=1}n(x_i-{x})2 ]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\nUse the var() function to calculate the variance of the new_cases_pc variable.\n\nvar(covid_us$new_cases_pc,na.rm=T)\n\n[1] 3402.718\n\n# Calculate by hand\n\nsum(\n  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, \n  na.rm=T\n  )/(sum(!is.na(covid_us$new_cases_pc))-1)\n\n[1] 3402.718\n\n\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n[ _x== ]\nStandard deviations are easier to interpet because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\nAgain, let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable\n\nsd(covid_us$new_cases_pc,na.rm=T)\n\n[1] 58.33282"
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-association",
    "href": "labs/01-lab-comments.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n[ cov(x,y)=_{i=1}^n(x_i-{x})(y_i-{y}) ]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\nvar(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)\n\n[1] -19.96569\n\n\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n[ _{x,y} = ]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\n\ncor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = \"complete.obs\")\n\n[1] -0.01369243\n\n\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\ncovid_us %&gt;%\n  filter(year &gt; 2020)%&gt;%\n  ungroup() %&gt;%\n  group_by(year,month)%&gt;%\n  summarise(\n    mn_per_vax = mean(percent_vaccinated, na.rm=T),\n    cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n  )\n\nError in `summarise()`:\nℹ In argument: `cor = cor(new_cases_pc, percent_vaccinated, use =\n  \"complete.obs\")`.\nℹ In group 28: `year = 2023` and `month = 4`.\nCaused by error in `cor()`:\n! no complete element pairs"
  },
  {
    "objectID": "labs/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "href": "labs/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?\nProbably not that much. Different Face mask policies are implemented at different times in the pandemic. For example, by 2021, almost all states have some requirements. Comparing the average for new cases in states with no policy to states with full requirements, is comparing the state of world in early 2020 to the state of the world in late 2020 to mid 2021. But lots of things differ between these periods. Other policies are also going into effect, new variants are emerging.\nIn short, those simple conditional means across the full data don’t really provide an apples to apples comparison.\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,n,fill=face_masks))+\n  geom_bar(stat=\"identity\")\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    new_cases = sum(new_cases)\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,new_cases))+\n  geom_smooth()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 328 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "labs/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\nSo when we compare states in September 2020, we see that rates of new cases tend to be much higher in states with only recommend face mask policies compared to states with at least some requirements.\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  filter(year_month == \"2020-09\")%&gt;%\n  group_by(face_masks) %&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc,na.rm=T)\n  )\n\n# A tibble: 4 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Recommended                    43.9\n2 Some requirements              13.5\n3 Required shared places         13.0\n4 Required all times             10.1\n\n\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(year_month,face_masks) %&gt;%\n  summarise(\n    n = length(unique(state)),\n    new_cases_pc = round(mean(new_cases_pc,na.rm=T)),\n    total_cases = round(mean(confirmed,na.rm=T))\n  ) -&gt; cases_by_month_and_policy\n\n`summarise()` has grouped output by 'year_month'. You can override using the\n`.groups` argument.\n\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\nDT::datatable(cases_by_month_and_policy,\n              filter = \"top\")\n\n\n\n\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\ncases_by_month_and_policy %&gt;%\n  ggplot(aes(\n    x= year_month,\n    y = new_cases_pc,\n    col=face_masks))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/01-lab-comments.html#footnotes",
    "href": "labs/01-lab-comments.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/01-lab.html",
    "href": "labs/01-lab.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/01-lab.html#uncomment-and-run-the-following-code",
    "href": "labs/01-lab.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\n# covid_us %&gt;%\n#   mutate(\n#     year = year(date),\n#     month = month(date),\n#     year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n#     percent_vaccinated = people_fully_vaccinated/population*100  \n#     ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/01-lab.html#uncomment-and-run-the-code-below",
    "href": "labs/01-lab.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\n# covid_us %&gt;%\n#   filter(facial_coverings == -4) %&gt;%\n#   select(date, state) %&gt;%\n#   group_by(state) %&gt;%\n#   summarize(\n#     n = n(),\n#     earliest_date = min(date),\n#     latest_date = max(date),\n#   )%&gt;%\n#   arrange(earliest_date)"
  },
  {
    "objectID": "labs/01-lab.html#please-explainwhat-each-line-of-code-is-doing",
    "href": "labs/01-lab.html#please-explainwhat-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explainwhat each line of code is doing:",
    "text": "6.2 Please explainwhat each line of code is doing:\n\ncovid_us %&gt;% Write your explanation here\nfilter(facial_coverings == -4) %&gt;%\nselect(date, state) %&gt;%\ngroup_by(state) %&gt;%\nsummarize(\nn = n(),\nearliest_date = min(date),\nlatest_date = max(date),\n)%&gt;%\narrange(earliest_date)\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\n\n6.2.1 Substantively, what does the previous chunk of code tell us?\n\n\n\n\n\n\nNote\n\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/01-lab.html#please-run-the-following-code",
    "href": "labs/01-lab.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  ) %&gt;%\n  filter(\n    RegionName == \"Illinois\",\n    date &gt; \"2020-08-01\", \n    date &lt; \"2021-01-01\",\n    !is.na(H6_Notes)\n    ) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\n\nError in oxford_us %&gt;% mutate(date = ymd(Date)) %&gt;% filter(RegionName == : could not find function \"%&gt;%\"\n\nil_facemasks\n\nError in eval(expr, envir, enclos): object 'il_facemasks' not found"
  },
  {
    "objectID": "labs/01-lab.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/01-lab.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;%\nmutate(date = ymd(Date))%&gt;%\nfilter(RegionName == \"Illinois\",\ndate &gt; \"2020-08-01\",\ndate &lt; \"2021-01-01\",\n!is.na(H6_Notes)) %&gt;%\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\nError in eval(expr, envir, enclos): object 'il_facemasks' not found\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\n# il_facemasks$H6_Notes[???]"
  },
  {
    "objectID": "labs/01-lab.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/01-lab.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-central-tendency",
    "href": "labs/01-lab.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nLater in the course we’ll talk about how we can use something like a mean to estimate an Expected Value: Something like\n\\[ E[Y|X=x] \\]\nOr to make it more concrete:\n\\[ E[\\text{New Cases} | \\text{Policy = \"recommended\"}] \\]\nIn code, we could accomplish this manually, using the index operator:\n\n# mean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommended”\nBy using group_by() with summarise() we can accomplish this more quickly:\n\n# covid_us%&gt;%\n#   group_by(face_masks)%&gt;%\n#   summarise(\n#     new_cases_pc = mean(new_cases_pc, na.rm=T)\n#   )\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n\\[\nM_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx=\\int_{x_i}^\\infty f_x(X)dx=1/2\n\\]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\n\n\n\n\n\nNote\n\n\n\n\nMedians are less influenced by outliers than means\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\n# sort(table(covid_us$face_masks))\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-dispersion",
    "href": "labs/01-lab.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n\\[\np_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx= p; \\int_{x_i}^\\infty f_x(X)dx=1-p\n\\]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n\\[\n\\sigma^2_x=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2\n\\]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\n\n8.2.3.1 Use the var() function to calculate the variance of the new_cases_pc variable.\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n\\[\n\\sigma_x=\\sqrt{\\sigma^2_x}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nStandard deviations are easier to interpret because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\n\n8.2.4.1 let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-association",
    "href": "labs/01-lab.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n\\[\ncov(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n\\[\n\\rho_{x,y} = \\frac{cov(x_y)}{\\sigma_x,\\sigma_y}\n\\]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\n# covid_us %&gt;%\n#   filter(year &gt; 2020)%&gt;%\n#   ungroup() %&gt;%\n#   group_by(year,month)%&gt;%\n#   summarise(\n#     mn_per_vax = mean(percent_vaccinated, na.rm=T),\n#     cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n#   )"
  },
  {
    "objectID": "labs/01-lab.html#what-do-these-averages-really-tell-us",
    "href": "labs/01-lab.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?"
  },
  {
    "objectID": "labs/01-lab.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/01-lab.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\n# DT::datatable(cases_by_month_and_policy,\n#               filter = \"top\")\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\n# cases_by_month_and_policy %&gt;%\n#   ggplot(aes(\n#     x= year_month,\n#     y = new_cases_pc,\n#     col=face_masks))+\n#   geom_point()+coord_flip()\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/01-lab.html#footnotes",
    "href": "labs/01-lab.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/09-lab.html",
    "href": "labs/09-lab.html",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "",
    "text": "In this lab, we’ll explore data from the National Election Studies 2024 Pilot Study. You find an outcome of interest and a variable you think predicts interesting variation in that outcome. You’ll figure out what recoding you need to do, do that recoding and describe the data. I’ll do the same, so can some template code to compare your work to.\nEverything we’ll do today is something we’ve done before and is also something you’ll likely have to do a version of in your final project. Here’s the plan\n\nSet up your work space (5 minutes)\nDownload and load data from the NES into R (10 minutes)\nExplore the codebook for the 2024 Pilot Study (5-10 minutes)\nGet a high level overview of the data to figure out what recoding needs to be done (5-10 minutes)\nRecode the data (15 minutes)\nDescribe the data (15-20 minutes)\nFormulate a set of research questions (10 minutes)\nSave the data (5 minutes)\n\nFinally, as always please take the class survey."
  },
  {
    "objectID": "labs/09-lab.html#please-render-this-.qmd-file",
    "href": "labs/09-lab.html#please-render-this-.qmd-file",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/09-lab.html#load-packages",
    "href": "labs/09-lab.html#load-packages",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\n\n# Libraries\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(kableExtra)\nlibrary(ggpubr)\nlibrary(DeclareDesign)\n## ADD DeclareDesign"
  },
  {
    "objectID": "labs/09-lab.html#set-your-working-directory-in-r-studio",
    "href": "labs/09-lab.html#set-your-working-directory-in-r-studio",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "1.2 Set your working directory in R Studio",
    "text": "1.2 Set your working directory in R Studio\nIn R studio set your working directory to the folder where this lab is saved by clicking &gt; Session &gt; Set Working Directory &gt; To Source File Location\n\nAfter doing so uncomment getwd() Should print out something like\n\n“~/Desktop/pols1600/labs/”\n\nDepending on where your lab is saved\n\n# In the Top Panel of RStudio Click\n# Session &gt; Set Working Directory &gt; Source File Location\n\n# Uncomment to Check Where Your File is Saved \n# getwd()\n\n\n\n\n\n\n\nNote\n\n\n\nIf getwd() says something like ‘~/Downloads/’ click: “File &gt; Save As” and save this lab in your course folder. Then close the version 09-lab.qmd that was opened from your Downloads folder and open the version of 09-lab.qmd that now exists in your course folder."
  },
  {
    "objectID": "labs/09-lab.html#identify-a-potential-outcome-of-interest",
    "href": "labs/09-lab.html#identify-a-potential-outcome-of-interest",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "3.1 Identify a potential outcome of interest",
    "text": "3.1 Identify a potential outcome of interest\nIn this and next week’s lab, I’ll be exploring factors that explain variation in the following outcome variables:\nA measure\n\nvchoice_rematch “Vote Trump or Biden in 2024”\n\n1 = Donald Trump\n2 = Joe Biden\n-7 = No Answer\n-1 = Inapplicable\n\n\nAnd five measures of political participation in the 2020 campaign\n\nmobil_talk “2020 campaign - Talk to others about candidates”\nmobil_online “2020 campaign - Participate in online rallies”\nmobil_rally “2020 campaign - Attend in person rallies”\nmobil_button “2020 campaign - Wear a button or campaign sticker”\nmobil_work “2020 campaign - Any other work to support candidates”\n\n1 = Yes\n2 = No\n-1 = Inapplicable\n\n\nPlease find a variable that describes some outcome of interest to you and fill in the following\n\noutcome_variable_name Question topic\n\nvariable values"
  },
  {
    "objectID": "labs/09-lab.html#identify-at-least-one-variable-that-might-predict-variation-in-your-outcome.",
    "href": "labs/09-lab.html#identify-at-least-one-variable-that-might-predict-variation-in-your-outcome.",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "3.2 Identify at least one variable that might predict variation in your outcome.",
    "text": "3.2 Identify at least one variable that might predict variation in your outcome.\nFrom a quick skim, I’ve selected the following potential predictors, which I will recode below:\n\nage Age\neduc Education\nfaminc_new Income\nrace Race\npid7 7 point party identification\n\nPlease find at least one more predictor which you think might explain variation in your outcome of interest and fill in the following\n\npredictor_variable_name Question topic\n\nvariable values\n\n\nFor example, perhaps you’re interested in differences by gender, or ideology, or social media use. See if you can find variables that measure these concepts.\nYou only needed to identify one, but you can choose to explore more if you want. Don’t choose 50, unless you really like recoding data."
  },
  {
    "objectID": "labs/09-lab.html#examine-the-distributions-and-values-of-your-outcome-variable",
    "href": "labs/09-lab.html#examine-the-distributions-and-values-of-your-outcome-variable",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.1 Examine the distributions and values of your outcome variable",
    "text": "4.1 Examine the distributions and values of your outcome variable\nPlease uncomment and run the code below\n\n# # Vote Choice\n# get_variable_labels(df$vchoice_rematch)\n# get_value_labels(df$vchoice_rematch)\n# table(df$vchoice_rematch,useNA = \"ifany\")\n# \n# # Acts of Participation\n# # All variables start with mobil_ prefix\n# df %&gt;% select(starts_with(\"mobil\")) %&gt;% names()\n# \n# # Political Talk\n# get_variable_labels(df$mobil_talk)\n# get_value_labels(df$mobil_talk)\n# table(df$mobil_talk,useNA = \"ifany\")\n# \n# # Save the names all variables related to acts of participation in 2020\n# the_participation_vars &lt;- df %&gt;% select(starts_with(\"mobil\")) %&gt;% names()\n# # Only keep the variables that measure participation and not survey timing\n# the_participation_vars &lt;- the_participation_vars[1:5]\n\nFrom quickly looking at my outcome variables, I know that I will want to:\n\nRecode vchoice_rematch to dv_vote_trump2024 which\n\nequals 1 if vchoice_rematch == 1\nequals 0 if vchoice_rematch == 2\nequals NA if vchoice_rematch &lt; 0\n\nRecode variables that start with mobil_* to variables that start with polpart_* and:\n\nequal 1 if mobil_* == 1\nequal 0 if mobil_* == 2\nequals NA if mobil_* &lt; 0`\n\nCreate dv_participation* which is the sum of respondents’ five responses to the recoded polpart_* variables"
  },
  {
    "objectID": "labs/09-lab.html#describe-any-recoding-of-your-outcome-variable",
    "href": "labs/09-lab.html#describe-any-recoding-of-your-outcome-variable",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.2 Describe any recoding of your outcome variable",
    "text": "4.2 Describe any recoding of your outcome variable\nIn the code chunk below, please repeat this process for the outcome variable you selected in the previous section:\n\n# Get a HLO of your outcome variable\n\n\nRecode YOUR OUTCOME VARIABLE HERE to NAME FOR RECODED VARIABLE\n\nDescribe what values need to be recoded in the new variable"
  },
  {
    "objectID": "labs/09-lab.html#examine-the-distributions-and-values-of-your-predictor-variable",
    "href": "labs/09-lab.html#examine-the-distributions-and-values-of-your-predictor-variable",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.3 Examine the distributions and values of your predictor variable",
    "text": "4.3 Examine the distributions and values of your predictor variable\nNow I’ll repeat this process for my potential predictor variables.\nPlease uncomment and run the code below\n\n# # Age\n# get_variable_labels(df$age)\n# get_value_labels(df$age)\n# summary(df$age)\n# \n# \n# # Education\n# get_variable_labels(df$educ)\n# get_value_labels(df$educ)\n# table(df$educ)\n# summary(df$educ)\n# \n# \n# # Income\n# get_variable_labels(df$faminc_new)\n# get_value_labels(df$faminc_new)\n# table(df$faminc_new)\n# summary(df$faminc_new)\n# \n# \n# # Race\n# get_variable_labels(df$race)\n# get_value_labels(df$race)\n# summary(df$race)\n# table(df$race,useNA = \"ifany\")\n# \n# \n# # Partisanship\n# get_variable_labels(df$pid7)\n# get_value_labels(df$pid7)\n# summary(df$pid7)\n# table(df$pid7)\n\nAfter taking a quick look at each variable, I know that I’ll want to do the following recoding:\n\nage to age1\n\nrecode -9s to NA\n\neduc no recoding needed\n\ncreate indicator has_college_degree which equals 1 is educ &gt; 4 and 0 otherwise\n\nfaminc_new to income\n\nrecode -7 and 97 to NA\n\nrace to race_5cat\n\nrace == 1 ~ \"White\"\nrace == 2 ~ \"Black\"\nrace == 3 ~ \"Hispanic\"\nrace == 4 ~ \"Asian\"\nT ~ \"Other\" (Collapse other racial categories)\n\nrace to is_* binary indicators:\n\nis_white = 1 if race==1, 0 otherwise\n\npid7 to partyid\n\nrecode pid7 == 8 to 4 (Classify Don't Knows as Independents)\n\npid7 to is_*: binary indicators:\n\nis_dem = 1 if partyid &lt; 4, 0 otherwise\nis_rep = 1 if partyid &gt; 4, 0 otherwise\nis_ind = 1 if partyid == 4, 0 otherwise"
  },
  {
    "objectID": "labs/09-lab.html#describe-any-recoding-of-your-predictors",
    "href": "labs/09-lab.html#describe-any-recoding-of-your-predictors",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "4.4 Describe any recoding of your predictor(s)",
    "text": "4.4 Describe any recoding of your predictor(s)\nIn the code chunk below, please repeat this process for the additional predictor(s) you selected in the previous section:\n\n# Get a HLO of your outcome variable\n\n\nRecode YOUR OUTCOME VARIABLE HERE to NAME FOR RECODED VARIABLE\n\nDescribe what values need to be recoded in the new variable"
  },
  {
    "objectID": "labs/09-lab.html#recode-the-outcome-variables",
    "href": "labs/09-lab.html#recode-the-outcome-variables",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "5.1 Recode the outcome variables",
    "text": "5.1 Recode the outcome variables\nPlease uncomment and run the following code chunk:\n\n# df %&gt;%\n#   # Recode 2024 Vote Choice\n#   mutate(\n#     dv_vote_trump2024 = case_when(\n#       vchoice_rematch == 1 ~ 1,\n#       vchoice_rematch == 2 ~ 0,\n#       T ~ NA\n#     )\n#   ) %&gt;% \n#   # Recode Individual Acts of Participation\n#   mutate(across(all_of(the_participation_vars), \n#                 \\(x) case_when(\n#                   x == 1 ~ 1,\n#                   x == 2 ~ 0,\n#                   T ~ NA\n#                 ),\n#                 .names = \"polpart_{.col}\"\n#                 )\n#                 \n#                 ) %&gt;% \n#   # Create Additive Measure of Participation\n#   mutate(\n#     dv_participation = rowSums(\n#       select(.,starts_with(\"polpart\")),\n#       na.rm = T)\n#     \n#   ) -&gt; df\n\nPlease recode your outcome of interest as needed\nRemember to save the output of your recode back into the dataframe df\n\n# Recode your outcome of interest"
  },
  {
    "objectID": "labs/09-lab.html#check-the-recoding-of-your-outcome",
    "href": "labs/09-lab.html#check-the-recoding-of-your-outcome",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "5.2 Check the recoding of your outcome",
    "text": "5.2 Check the recoding of your outcome\nIt’s a good habit to compare your recoded variables to their original values, to make sure your code did what you thought it did.\nPlease uncomment and run the following:\n\n# Check recodes\n\n# table(\n#   recode = df$dv_vote_trump2024,\n#   original = df$vchoice_rematch, \n#   useNA = \"ifany\"\n#   )\n# \n# table(\n#   recode = df$polpart_mobil_button,\n#   original = df$mobil_button, \n#   useNA = \"ifany\"\n#   )\n# table(\n#   total = df$dv_participation,\n#   item = df$polpart_mobil_button,\n#   useNA = \"ifany\"\n#   )\n\nSo everything looks in order. I could have probably checked the dv_participation variable against all of its constituent items, but based off comparing it to polpart_mobil_button everything looks in order since: - there are no cases where polpart_mobil_button is 1 but dv_participation is 0 - there are no cases where dv_participation is at it’s max but polpart_mobil_button is 0. - dv_participation has the correct theoretical range from 0 acts to 5 acts.\nNow do the same for your outcome variable.\nPlease check your recoded outcome against its original values\n\n# Check recodes"
  },
  {
    "objectID": "labs/09-lab.html#recode-your-predictor-variables",
    "href": "labs/09-lab.html#recode-your-predictor-variables",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "5.3 Recode your predictor variables",
    "text": "5.3 Recode your predictor variables\nAgain, I’ve provide some demonstration code to recode the predictors listed above.\nPlease uncomment and run the following\n\n# df %&gt;% \n#   mutate(\n#     # Age\n#     age = ifelse(age &lt; 0, NA, age),\n#     # Education\n#     education = educ,\n#     educ_f = to_factor(educ), #Convert to Factor for Plotting\n#     is_college_grad = ifelse(educ &gt; 4,1,0),\n#     # Income\n#     income = case_when(\n#       faminc_new &lt; 0 ~ NA,\n#       faminc_new &gt; 0 & faminc_new &gt;16 ~ NA,\n#       T ~ faminc_new\n#     ),\n#     # Race\n#     race_5cat = case_when(\n#       race &lt; 5 ~ to_factor(race),\n#       T ~ \"Other\"\n#     ) %&gt;% factor(., levels = c(\"White\",\"Black\",\"Hispanic\",\"Asian\",\"Other\")),\n#     is_white = ifelse(race == 1, 1, 0),\n#     is_black = ifelse(race == 2, 1, 0),\n#     is_hispanic = ifelse(race == 3, 1, 0),\n#     is_asian = ifelse(race == 4, 1, 0),\n#     is_other = ifelse(race == 5, 1, 0),\n#     # Partisanship\n#     partyid = case_when(\n#       pid7 == 8 ~ 4,\n#       T ~ pid7\n#     ),\n#     is_dem = ifelse(partyid  &lt; 4, 1, 0),\n#     is_rep = ifelse(partyid  &gt; 4, 1, 0),\n#     is_ind = ifelse(partyid  == 4, 1, 0),\n#   ) -&gt; df\n\nPlease recode your additional predictor(s) as needed\n\n# Recode your additional predictor(s)\n\nIt’s a good habit to check your all your recoding – particularly if you’re doing something like summing over multiple columns – but for this lab, we’ll live dangerously."
  },
  {
    "objectID": "labs/09-lab.html#create-a-table-of-summary-statistics",
    "href": "labs/09-lab.html#create-a-table-of-summary-statistics",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.1 Create a table of summary statistics",
    "text": "6.1 Create a table of summary statistics\nPlease uncomment and run the code below which demonstrates how to produce a nicely formatted table of summary statistics\n\n# # Vector of numeric variables to summarize\n# the_vars &lt;- c(\n#   \"dv_vote_trump2024\", \n#   \"dv_participation\",\n#   \"age\",\"education\",\"income\",\n#   \"is_white\",\"is_black\",\"is_hispanic\",\"is_asian\",\"is_other\",\n#   \"partyid\",\"is_dem\",\"is_rep\",\"is_ind\"\n# )\n# \n# # Vector of nicely formatted labels for variables\n# the_labels &lt;- c(\n#   \"Vote for Trump in '24\",\n#   \"Acts of Participation in `20\",\n#   \"Age\",\"Education\", \"Income\",\n#   \"White\", \"Black\",\"Hispanic\",\"Asian\",\"Other\",\n#   \"Party ID\", \"Democrat\",\"Republican\",\"Independent\"\n# )\n# \n# df_summary &lt;- df %&gt;% \n#   select(all_of(the_vars)) %&gt;%\n#   rename_with(~the_labels) %&gt;% \n#   pivot_longer(\n#     cols = everything(),\n#     names_to = \"Variable\"\n#   ) %&gt;% \n#   mutate(\n#     Variable = factor(Variable, levels = the_labels)\n#   ) %&gt;% \n#   group_by(Variable) %&gt;% \n#   summarise(\n#     Min = min(value,na.rm = T),\n#     p25 = quantile(value, prob = .25,na.rm = T),\n#     Median = quantile(value, prob = .5,na.rm = T),\n#     Mean = mean(value, na.rm = T),\n#     p75 = quantile(value, prob = .75,na.rm = T),\n#     Max = max(value,na.rm = T),\n#     `N missing` = sum(is.na(value))\n#   )\n# \n# # Look at results\n# df_summary\n\nWe can then format df_summary as table using knitr() and styling options from the kableExtra package:"
  },
  {
    "objectID": "labs/09-lab.html#modify-the-table-to-include-your-predictors",
    "href": "labs/09-lab.html#modify-the-table-to-include-your-predictors",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.2 Modify the table to include your predictors",
    "text": "6.2 Modify the table to include your predictors\nUsing the two previous code chunks as a template, update the code so that the table includes your chosen outcome and predictors.\n\n# Copy, paste, and update code from datasummary_me to include your variables"
  },
  {
    "objectID": "labs/09-lab.html#interpret-the-results-of-your-table.",
    "href": "labs/09-lab.html#interpret-the-results-of-your-table.",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.3 Interpret the results of your table.",
    "text": "6.3 Interpret the results of your table.\nPlease write a few sentences that provide a substantive interpretation of your table of descriptive statistics\nYour reader should come away with an understanding of the characteristics of the respondents to this sample.\nThe National Election Study’s 2024 Pilot Study contains responses from 1909 individuals2. The typical respondent in the data was just under 50 years old, with some college, with an income in the range of $50k-$59k. Approximately two-thirds of the sample identified as white, with 13 percent of respondents identifying as Black, 13 percent as Hispanic, 2 percent as Asian. Forty-three percent of respondents identified as Democrats, 38 percent as Republicans, and 20 percent as Independents. The respondents were evenly split in who they would vote for 2024, with 50 percent saying they would Vote for Donald Trump, and 50 percent saying Joe Biden. In the 2020 campaign, the average respondent reported engaging in about 1 act of political participation."
  },
  {
    "objectID": "labs/09-lab.html#visualize-the-data",
    "href": "labs/09-lab.html#visualize-the-data",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "6.4 Visualize the Data",
    "text": "6.4 Visualize the Data\nPlease a choose a variable or variables whose distribution or relationship you think may be substantively interesting to the potential story you want to tell.\nIn the code below, I visualize:\n\nthe distribution of age by race using geom_boxplot()\naverage partisanship by race using stat_summary()\nhow partisanship changes with age by race using geom_smooth()\n\nAnd combine these 3 plots into a single figure using two calls to the ggarrange() function from the ggpubr package\n\n# fig_age_race &lt;- df %&gt;% \n#   ggplot(aes(race_5cat,age,\n#              col = race_5cat))+\n#   geom_boxplot()+\n#   labs(\n#     x = \"Race\",\n#     y = \"Age\",\n#     col = \"Race\",\n#     title = \"Distribution of Age by Race\"\n#   )+\n#   theme_minimal()\n# \n# fig_pid_race &lt;- df %&gt;% \n#   ggplot(aes(race_5cat,partyid,\n#              col = race_5cat))+\n#   stat_summary(position = position_dodge(width=.5))+\n#   labs(\n#     x = \"Race\",\n#     y = \"Partisanship\",\n#     col = \"Race\",\n#     title = \"Average Partisanship by Race\"\n#   )+\n#   theme_minimal()\n# \n# fig_age_race_pid &lt;- df %&gt;% \n#   mutate(\n#     Race = race_5cat\n#   ) %&gt;% \n#   ggplot(aes(age,partyid,\n#              col = race_5cat\n#              ))+\n#   geom_smooth(se = F) +\n#   geom_jitter(size=.5, alpha=.3) +\n#   labs(\n#     x = \"age\",\n#     y = \"Partisanship\",\n#     col = \"Race\",\n#     title = \"Distribution of Partisanship by Age and Race\"\n#   )+\n#   theme_minimal()\n# \n# fig_desc &lt;- ggarrange(\n#   # Top Row, two columns\n#   ggarrange(\n#     fig_age_race, fig_pid_race,\n#     ncol =2,\n#     legend = \"none\"\n#     ), \n#   # Bottom row, 1 column\n#   fig_age_race_pid, \n#   nrow=2,\n#   common.legend = T,\n#   legend = \"bottom\",\n#   heights = c(1,1.5)\n#   )\n# \n# fig_desc\n\nFrom the figure, we see that whites in sample have the highest median age (55 years) followed by Blacks (46 years). Hispanic and Asian respondents have the youngest median age of (40 years). Whites are also tend to lean more Republican in their partisanship than other racial minority groups. This is particularly true for older whites in the sample. Interestingly, average partisanship stays roughly constant with age for Asians and Hispanics, but older Black respondents are more likely to identify as Democrats than younger Blacks, whose partisan identification is more independent.\nPlease produce your own figure and provide a similar interpretation of what it conveys."
  },
  {
    "objectID": "labs/09-lab.html#footnotes",
    "href": "labs/09-lab.html#footnotes",
    "title": "Lab 09 - Exploring data in the 2020 National Elections Study",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn general, I try not to recode the original variables, but instead create new columns, with different names. But, like this footnote, it seems overly verbose to create something like age_recoded, so I’ll break my general rule↩︎\nSee note below↩︎\nIn general, I’m often ambivalent on this kind of “What’s the effect of x on y” phrasing. I think it’s fine for framing, but the real question I’m interested in is something about the interaction and relative effects of income and education on participation, hence the subsequent clarification of research question.↩︎"
  },
  {
    "objectID": "labs/10-lab.html",
    "href": "labs/10-lab.html",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "",
    "text": "In this lab, we’ll continue exploring data from the National Election Studies 2024 Pilot Study.\nFrom last week’s lab you should have identified some variables of interest in the data, recoded them as needed, and begun to formulate some research questions.\nIn this week’s lab, we’ll pick up where we left off:\n\nSet up your work space (5 minutes)\nLoad the data from last week (5 minutes)\nQuickly summarize the data (15)\nRevisit and revise your research question and expectations (15 minutes)\nEstimate models to explore your question (10 minutes)\nInterpret the results of your model (30 minutes)\n\nOne of these 6 tasks will be randomly selected as the graded question for the lab."
  },
  {
    "objectID": "labs/10-lab.html#please-render-this-.qmd-file",
    "href": "labs/10-lab.html#please-render-this-.qmd-file",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/10-lab.html#load-packages",
    "href": "labs/10-lab.html#load-packages",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nPlease load the libraries you will use in your analysis.\n\n# Libraries"
  },
  {
    "objectID": "labs/10-lab.html#set-your-working-directory-in-r-studio",
    "href": "labs/10-lab.html#set-your-working-directory-in-r-studio",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "1.2 Set your working directory in R Studio",
    "text": "1.2 Set your working directory in R Studio\nIn R studio set your working directory to the folder where this lab is saved by clicking &gt; Session &gt; Set Working Directory &gt; To Source File Location\n\nAfter doing so uncomment getwd() Should print out something like\n\n“~/Desktop/pols1600/labs/”\n\nDepending on where your lab is saved\n\n# In the Top Panel of RStudio Click\n# Session &gt; Set Working Directory &gt; Source File Location\n\n# Uncomment to Check Where Your File is Saved \n# getwd()\n\n\n\n\n\n\n\nNote\n\n\n\nIf getwd() says something like ‘~/Downloads/’ click: “File &gt; Save As” and save this lab in your course folder. Then close the version 09-lab.qmd that was opened from your Downloads folder and open the version of 09-lab.qmd that now exists in your course folder."
  },
  {
    "objectID": "labs/10-lab.html#caclulate-summary-statistics-for-recoded-variables",
    "href": "labs/10-lab.html#caclulate-summary-statistics-for-recoded-variables",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "3.1 Caclulate summary statistics for recoded variables",
    "text": "3.1 Caclulate summary statistics for recoded variables\n\n# # Vector of numeric variables to summarize\n# the_vars &lt;- c(\n#   \"dv_vote_trump2024\", \n#   \"dv_participation\",\n#   \n#   # ---- Add your outcome here ----\n#   \n#   \"age\",\"education\",\"income\",\n#   \"is_white\",\"is_black\",\"is_hispanic\",\"is_asian\",\"is_other\",\n#   \"partyid\",\"is_dem\",\"is_rep\",\"is_ind\"\n#   \n#   # ----- Add your predictor here ----\n# \n#   )\n# \n# # Vector of nicely formatted labels for variables\n# the_labels &lt;- c(\n#   \"Vote for Trump in '24\",\n#   \"Acts of Participation in `20\",\n#   \n#   # ---- Add a label for your DV here ----\n#   \n#   \"Age\",\"Education\", \"Income\",\n#   \"White\", \"Black\",\"Hispanic\",\"Asian\",\"Other\",\n#   \"Party ID\", \"Democrat\",\"Republican\",\"Independent\"\n#   \n#   # ---- Add a label for your IV here ----\n# \n#   )\n# \n# df_summary &lt;- df %&gt;% \n#   select(all_of(the_vars)) %&gt;%\n#   rename_with(~the_labels) %&gt;% \n#   pivot_longer(\n#     cols = everything(),\n#     names_to = \"Variable\"\n#   ) %&gt;% \n#   mutate(\n#     Variable = factor(Variable, levels = the_labels)\n#   ) %&gt;% \n#   group_by(Variable) %&gt;% \n#   summarise(\n#     Min = min(value,na.rm = T),\n#     p25 = quantile(value, prob = .25,na.rm = T),\n#     Median = quantile(value, prob = .5,na.rm = T),\n#     Mean = mean(value, na.rm = T),\n#     p75 = quantile(value, prob = .75,na.rm = T),\n#     Max = max(value,na.rm = T),\n#     \n#     # ---- Add sd here ----\n#     \n#     `N missing` = sum(is.na(value))\n#   )"
  },
  {
    "objectID": "labs/10-lab.html#display-the-results-as-a-formated-table",
    "href": "labs/10-lab.html#display-the-results-as-a-formated-table",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "3.2 Display the results as a formated table",
    "text": "3.2 Display the results as a formated table\nOnce you’ve created df_summary, you can uncomment the code below. Be sure to update the arguments in the pack_rows() function\n\n# kable(df_summary,\n#       digits = 2) %&gt;% \n#   kable_styling() %&gt;% \n#   pack_rows(\"Outcomes\", start_row = 1, end_row = 2) %&gt;% \n#   pack_rows(\"Demographic Predictors\", 3, 10) %&gt;% \n#   pack_rows(\"Political Predictors\", 11, 14)"
  },
  {
    "objectID": "labs/10-lab.html#interpret-the-results",
    "href": "labs/10-lab.html#interpret-the-results",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "3.3 Interpret the results",
    "text": "3.3 Interpret the results\nPlease use these tables to describe a typical respondent to the 2024 NES Pilot Study:\nThe National Election Study’s 2024 Pilot Study contains responses from 1909 individuals. The typical respondent in the data was …"
  },
  {
    "objectID": "labs/10-lab.html#research-question",
    "href": "labs/10-lab.html#research-question",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "4.1 Research question",
    "text": "4.1 Research question\nME: How does support for Trump in the 2024 election vary with age and race?\nYOU:"
  },
  {
    "objectID": "labs/10-lab.html#expectations",
    "href": "labs/10-lab.html#expectations",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "4.2 Expectations",
    "text": "4.2 Expectations\nME: On average, I expect that older voters will be more supportive of Trump, but suspect that this trend varies by race. I expect it will be particularly true among White voters, but less so among people of color. Since, partisanship is likely to be strong predictor of vote choice, I will explore whether these specific relationships hold, once we control for variations in partisan identification which we know varies both by age and race.\nYOU:"
  },
  {
    "objectID": "labs/10-lab.html#linear-model",
    "href": "labs/10-lab.html#linear-model",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "4.3 Linear Model",
    "text": "4.3 Linear Model\nME:\nI will estimate the following models:\n\\[\ny = \\beta_0 + \\beta_1 age + \\sum\\beta_k race_k + \\epsilon\n\\] If my expectations hold, I expect the coefficient on age in this model to be positive, indicating older voters are more likely to vote for Trump. White respondents are the reference category in this model1 and so the coefficients on the racial indicators (\\(\\beta_k race\\)) correspond to how members of each racial group differ from white respondents in their propensity to vote for Trump. I expect all of these coefficients to be negative.\nTo explore whether the relationship between age and vote choice varies by race, I will fit an interaction model:\n\\[\ny = \\beta_0 + \\beta_1 age + \\sum\\beta_k race_k + \\epsilon + \\sum\\beta_{jk} age \\times race_k  + \\epsilon\n\\] This model allows the relationship between age and vote choice to vary by race. For white respondents, the relationship is described by \\(\\beta_1\\). Again I expect it to be positive, suggesting older white voters are more likely to vote for trump. For racial minorities, the marginal effect of age for the racial or ethnic group \\(k\\) is described by \\(\\beta_1 + \\beta_{jk}\\). In general, I expect that the coefficients on \\(\\beta_{jk}\\) to be negative such that older racial minorities are less likely to support Trump than older white respondents.\nFinally, I will estimate a model that controls for partisanship. If the relationships between age, and race and vote choice are simply a reflection of differences in partisan identification, then coefficients on these predictors should no longer be statistically significant, while the coefficient on partisanship should positive and statistically significant.\n\\[\ny = \\beta_0 + \\beta_1 pid + \\beta_2 age + \\sum\\beta_k race_k + \\epsilon + \\sum\\beta_{jk} age \\times race_k \\epsilon\n\\]\nYou:\nWe will estimate the following models:\n\\[\ny = \\beta_0 + \\beta_1 ...\n\\]\n\\[\ny = \\beta_0 + \\beta_1 ...\n\\]"
  },
  {
    "objectID": "labs/10-lab.html#regression-table",
    "href": "labs/10-lab.html#regression-table",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "6.1 Regression Table",
    "text": "6.1 Regression Table\nHere is some template code I used, demontstrating how to update the labels of the columns and rows in the model\n# htmlreg(list(m1vc, m2vc, m3vc),\n#         custom.model.names = c(\n#           \"Baseline\", \"Interaction\", \"Alternative\"\n#         ),\n#         caption = \"Support for Trump in 2024\",\n#         caption.above = T,\n#         custom.coef.names = c(\n#           \"(Intercept)\",\n#           \"Age\",\n#           \"Black\",\n#           \"Hispanic\",\n#           \"Asian\",\n#           \"Other\",\n#           \"Age:Black\",\n#           \"Age:Hispanic\",\n#           \"Age:Asian\",\n#           \"Age:Other\",\n#           \"Party ID\"\n#         ) ,\n#         include.ci = F,\n#         digits = 3)\nPlease produce a similar table for your models"
  },
  {
    "objectID": "labs/10-lab.html#visualize-predicted-values",
    "href": "labs/10-lab.html#visualize-predicted-values",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "6.2 Visualize predicted values",
    "text": "6.2 Visualize predicted values\nSimilarly here is some code to produce predicted values for m2vc and m3vc. The basic steps are:\n\nCreate a prediction data frame varying the values of some predictors from a model, while holding others constant using expand_grid()\nProduce predicted values with confidence intervals from your regression model using the predict() function\nBind those predictions and CIs to the orginal prediction dataframe using cbind()\nProduce a figure using ggplot()\n\n\n# # Predictors for m2\n# pred_dfm2 &lt;- expand_grid(\n#   age = seq(min(df$age, na.rm =T), max(df$age, na.rm = T)),\n#   race_5cat = sort(unique(df$race_5cat))\n# )\n# \n# # Predictions with confidence intervals\n# pred_dfm2 &lt;- cbind(\n#   pred_dfm2,\n#   predict(m2, newdata = pred_dfm2, interval = \"confidence\")$fit\n# )\n# \n# # Plot predictions from m2\n# \n# fig_m2 &lt;- pred_dfm2 %&gt;% \n#   ggplot(aes(age, fit, col = race_5cat))+\n#   geom_ribbon(aes(ymin = lwr, ymax = upr, \n#     fill = race_5cat\n#     ),\n#     alpha = .5)+\n#   geom_line()+\n#   facet_wrap( ~ race_5cat)+\n#   theme_minimal()+\n#   guides(\n#     col = \"none\",\n#     fill = \"none\"\n#   )+\n#   labs(\n#     y = \"Predicted Vote Choice\",\n#     title = \"Support for Trump by Age and Race\"\n#   )\n# \n# # Display figure\n# fig_m2\n# \n# # Predictors for m3\n# pred_dfm3 &lt;- expand_grid(\n#   age = seq(min(df$age, na.rm =T), max(df$age, na.rm = T)),\n#   race_5cat = sort(unique(df$race_5cat)),\n#   partyid = mean(df$partyid, na.rm=T)\n# )\n# \n# # Predictions with confidence intervals\n# pred_dfm3 &lt;- cbind(\n#   pred_dfm3,\n#   predict(m3, newdata = pred_dfm3, interval = \"confidence\")$fit\n# )\n# \n# # Plot predictions from m2\n# \n# fig_m3 &lt;- pred_dfm3 %&gt;% \n#   ggplot(aes(age, fit, col = race_5cat))+\n#   geom_ribbon(aes(ymin = lwr, ymax = upr, \n#     fill = race_5cat\n#     ),\n#     alpha = .5)+\n#   geom_line()+\n#   facet_wrap( ~ race_5cat)+\n#   theme_minimal()+\n#   guides(\n#     col = \"none\",\n#     fill = \"none\"\n#   )+\n#   labs(\n#     y = \"Predicted Vote Choice\",\n#     title = \"Support for Trump by Age and Race Controlling for Partisanship\"\n#   )\n# \n# fig_m3\n\nPlease produce at least one figure showing predicted values from at least one of your models\n\n# Create a prediction data frame\n\n# Bind predicted values to prediction data frame\n\n# Produce Figure\n\n# Dispaly figure"
  },
  {
    "objectID": "labs/10-lab.html#interperet-results",
    "href": "labs/10-lab.html#interperet-results",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "6.3 Interperet results",
    "text": "6.3 Interperet results\nHere is my interpretation of the models:\nME: Our baseline model confirms our initial expectations. Controlling for race, older respondents have higher predicted levels of support for Trump by 0.002 percentage points. Controlling for race, the model predicts that a 60 year old respondent is about 6.3 percentage points more likely to vote for Trump than a 30 year-old respondent. The test statistic for this coefficient is 3.10 corresponding to a p-value &lt; 0.05, suggesting that if there were no relationship between age and vote choice in this model, it would be very unlikely that we observed a test statistic of this magnitude. Similarly the confidence interval for this estimate has suggests that coefficients as small as 0.0008 and as large as 0.0034 are plausible values for the relationship between age and vote choice in these data. Similarly, controlling for age, Black, Hispanic, and Asian respondents report significantly lower levels of support for Trump than white respondents (p &lt; 0.05).\nTurning to the interaction model, we see that the magnitude of the coefficient on age (which corresponds to the marginal effect of age for white respondents) increases, while the coefficients on the interactions between age and racial indicators are generally negative, suggesting that the relationship between age and support for Trump is less strong for these racial and ethnic groups. Figure 1 helps clarify these marginal effects, as we see that slope for age is clearly positive for white respondents, clearly negative for Black respondents. The confidence intervals for the predicted values of the other racial and ethnic groups are generally wide, and consisent with positive, negative, or no relationship between age and vote choice.\nFinally, looking at the model controlling for partisanship, we see that none of the relationships between age, race, and vote choice, remain statistically significant once we account for the relationships between partisanship and vote choice, and the relationships between partisanship and age and race. In sum, apparent demographic differences in support for Trump appear driven by the differences in partisan identification across racial groups and within age groups.\nYOU:"
  },
  {
    "objectID": "labs/10-lab.html#footnotes",
    "href": "labs/10-lab.html#footnotes",
    "title": "Lab 10 - Exploring Questions from the 2024 National Elections Pilot Study",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBecause white is first level the factor variable race↩︎"
  },
  {
    "objectID": "labs/05-lab.html",
    "href": "labs/05-lab.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, 1, 2, 3, 4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/05-lab.html#load-covid-19-data",
    "href": "labs/05-lab.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid-19 data"
  },
  {
    "objectID": "labs/05-lab.html#load-election-data",
    "href": "labs/05-lab.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\n# Load election data\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/05-lab.html#recode-the-covid-19-data",
    "href": "labs/05-lab.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide: here\n\n# Create a vector containing of US territories\n\n\n# Filter out Territories and create state variable\n\n\n# Calculate new cases, new cases per capita, and 7-day average\n\n\n\n# Recode facemask policy (Not strictly necessary so feel free to skip)\n\n\n# Create year-month and percent vaccinated variables"
  },
  {
    "objectID": "labs/05-lab.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/05-lab.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, copy the code for new_deaths_pc_7day change the new_deaths_pc_7day to new_deaths_pc_14day and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\n# Create the following variables:\n# new_deaths\n# new_deaths_pc\n# new_deaths_pc_7day\n# new_deaths_pc_14day"
  },
  {
    "objectID": "labs/05-lab.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/05-lab.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data"
  },
  {
    "objectID": "labs/05-lab.html#for-all-the-observations",
    "href": "labs/05-lab.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7day)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7day"
  },
  {
    "objectID": "labs/05-lab.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/05-lab.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7day before April 19, 2021"
  },
  {
    "objectID": "labs/05-lab.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/05-lab.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7day after April 19, 2021\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that …\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that …\nWhen we limit our analysis, to just observations before April 19, 2021 …\nWhen we look at observations after the vaccine became widely available …"
  },
  {
    "objectID": "labs/05-lab.html#footnotes",
    "href": "labs/05-lab.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/06-lab-comments.html",
    "href": "labs/06-lab-comments.html",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "",
    "text": "Today we will explore the critiques and alternative explanations for the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nRecall the core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\n\nA skeptic of this claim might argue that relationship between electoral and epidemelogical outcomes is spurious, saying somthing like:\n\nThere are lots of ways that Red States differ from Blue States — demographics, economics, geography, culture, and so on – and it is these differences that explain the phenomena of Red Covid. If we were to control for these omitted variables the relationship between a state’s partisan leanings and Covid-19 would go away.\n\nIn this lab, we will see how we can explore these claims using multiple regression to control for competing explanations.\nTo accomplish this we will:\n\nGet set up to work (10 minutes)\n\nThen we will estimate and interpret a series of regression models:\n\nA baseline Red Covid model using simple bivariate regression using the Republican vote share of states to predict the 14-day average of per capita Covid-19 deaths on September 23, 2021 (10 Minutes)\nA multiple regression model controlling for Republican vote share the median age (15 minutes)\nA model controlling for Republican vote share, the median age and median income (15 minutes)\nA model controlling for Republican vote share, the median age median income and vaccination rates (15 minutes)\nA model using Republican vote share, the median age median income to predict vaccination rates (15 minutes)\n\nFinally, we’ll take the weekly survey which will serve as a mid semester check in.\nOne of these 6 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3052024)\ngraded_question &lt;- sample(1:6,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 4 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/06-lab-comments.html#load-packages",
    "href": "labs/06-lab-comments.html#load-packages",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\nThere’s one new package, htmltools which we’ll use to display regression tables while we work.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "labs/06-lab-comments.html#load-the-data",
    "href": "labs/06-lab-comments.html#load-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the data that we created in class on Tuesday which provides a snapshot of the state of Covid-19 on September 23, 2021 in the U.S.\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nAfter running this code, the data frame covid_lab should appear in your environment pane in R Studio"
  },
  {
    "objectID": "labs/06-lab-comments.html#describe-the-data",
    "href": "labs/06-lab-comments.html#describe-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nIn the code chunk below, please write some code get an high level overview of the data:\n\n# High level overview\n# Number of observations and variables\ndim(covid_lab)\n\n[1] 51 14\n\n# Names of variables\nnames(covid_lab)\n\n [1] \"state\"                  \"state_po\"               \"date\"                  \n [4] \"new_deaths_pc_14day\"    \"percent_vaccinated\"     \"winner\"                \n [7] \"rep_voteshare\"          \"med_age\"                \"med_income\"            \n[10] \"population\"             \"rep_voteshare_std\"      \"med_age_std\"           \n[13] \"med_income_std\"         \"percent_vaccinated_std\"\n\n# Glimpse of data\nglimpse(covid_lab)\n\nRows: 51\nColumns: 14\n$ state                  &lt;chr&gt; \"Minnesota\", \"California\", \"Florida\", \"Wyoming\"…\n$ state_po               &lt;I&lt;chr&gt;&gt; MN, CA, FL, WY, SD, KS, NV, VA, WA, OR, WI, …\n$ date                   &lt;date&gt; 2021-09-23, 2021-09-23, 2021-09-23, 2021-09-23…\n$ new_deaths_pc_14day    &lt;dbl&gt; 0.2216457, 0.2953878, 1.6069796, 0.9379675, 0.2…\n$ percent_vaccinated     &lt;dbl&gt; 60.97676, 60.75909, 58.42173, 43.87526, 52.3523…\n$ winner                 &lt;fct&gt; Biden, Biden, Trump, Trump, Trump, Trump, Biden…\n$ rep_voteshare          &lt;dbl&gt; 45.28494, 34.32072, 51.21982, 69.49979, 61.7693…\n$ med_age                &lt;dbl&gt; 38.0, 36.5, 42.0, 37.7, 37.0, 36.7, 38.0, 38.2,…\n$ med_income             &lt;dbl&gt; 71306, 75235, 55660, 64049, 58275, 59597, 60365…\n$ population             &lt;int&gt; 5639632, 39512223, 21477737, 578759, 884659, 29…\n$ rep_voteshare_std      &lt;dbl&gt; -0.322498636, -1.235628147, 0.171773837, 1.6941…\n$ med_age_std            &lt;dbl&gt; -0.16122563, -0.78101262, 1.49153966, -0.285183…\n$ med_income_std         &lt;dbl&gt; 0.76603213, 1.13270974, -0.69414553, 0.08876578…\n$ percent_vaccinated_std &lt;dbl&gt; 0.5889289, 0.5623160, 0.2765518, -1.5018949, -0…\n\n# Summary of data\nsummary(covid_lab)\n\n    state             state_po              date            new_deaths_pc_14day\n Length:51          Length:51          Min.   :2021-09-23   Min.   :0.08097    \n Class :character   Class :AsIs        1st Qu.:2021-09-23   1st Qu.:0.25590    \n Mode  :character   Mode  :character   Median :2021-09-23   Median :0.37909    \n                                       Mean   :2021-09-23   Mean   :0.56561    \n                                       3rd Qu.:2021-09-23   3rd Qu.:0.85135    \n                                       Max.   :2021-09-23   Max.   :1.81515    \n percent_vaccinated   winner   rep_voteshare       med_age        med_income   \n Min.   :43.58      Trump:25   Min.   : 5.397   Min.   :30.80   Min.   :45081  \n 1st Qu.:49.40      Biden:26   1st Qu.:40.975   1st Qu.:36.95   1st Qu.:55560  \n Median :54.65                 Median :49.237   Median :38.20   Median :61439  \n Mean   :56.16                 Mean   :49.157   Mean   :38.39   Mean   :63098  \n 3rd Qu.:62.07                 3rd Qu.:57.866   3rd Qu.:39.50   3rd Qu.:71464  \n Max.   :72.39                 Max.   :69.500   Max.   :44.70   Max.   :86420  \n   population       rep_voteshare_std    med_age_std       med_income_std   \n Min.   :  578759   Min.   :-3.644447   Min.   :-3.13620   Min.   :-1.6814  \n 1st Qu.: 1789606   1st Qu.:-0.681443   1st Qu.:-0.59508   1st Qu.:-0.7034  \n Median : 4467673   Median : 0.006679   Median :-0.07859   Median :-0.1548  \n Mean   : 6445656   Mean   : 0.000000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 7446805   3rd Qu.: 0.725319   3rd Qu.: 0.45856   3rd Qu.: 0.7807  \n Max.   :39512223   Max.   : 1.694179   Max.   : 2.60716   Max.   : 2.1766  \n percent_vaccinated_std\n Min.   :-1.5381       \n 1st Qu.:-0.8265       \n Median :-0.1841       \n Mean   : 0.0000       \n 3rd Qu.: 0.7230       \n Max.   : 1.9845       \n\n# Variables I want to calculate sd for\nthe_vars &lt;- c(\"new_deaths_pc_14day\",\"rep_voteshare\",\"med_age\",\"med_income\",\"percent_vaccinated\")\n\n# Calculate standard deviations\ncovid_lab %&gt;%\n  select(all_of(the_vars))%&gt;%\n  summarise_all(sd)\n\n# A tibble: 1 × 5\n  new_deaths_pc_14day rep_voteshare med_age med_income percent_vaccinated\n                &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n1               0.409          12.0    2.42     10715.               8.18\n\n\nPlease use this HLO to answer the following questions:\n\nHow many observations are there: 50\nWhat is an observation (i.e. what is the unit of analysis): A U.S. State (on September 23, 2021)\nWhat is the primary outcome variable for today: The 14-day average of new Covid-19 deaths\nWhat are the four main predictors we’ll be using: We’ll be predicting Covid-19 deaths with measures of Republican Vote Share (rep_voteshare), Median Age (med_age), Median Income (med_income), and Percent Vaccintated (percent_vaccinated).\nWill we be using the the raw values of these predictors or their standardized values? We’ll be using the standardized version of these variables which all have the suffix _std\nWhat are the standard deviations of our outcome and predictor variables:\n\nCovid-19 deaths: 0.40 deaths\nRepublican vote share: 10.4 percentage points\nMedian age: 2.36 years\nMedian income: $ 10,288\nVaccination Rate: 7.98 percentage points"
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model",
    "href": "labs/06-lab-comments.html#fit-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.1 Fit the model",
    "text": "2.1 Fit the model\n\nm1 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-results",
    "href": "labs/06-lab-comments.html#summarize-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.2 Summarize the results",
    "text": "2.2 Summarize the results\nNow we apply the summary() function to our model m1\n\nsummary(m1)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63271 -0.22488 -0.03769  0.13746  1.00634 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04817  11.741 7.51e-16 ***\nrep_voteshare_std  0.22682    0.04865   4.662 2.44e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.344 on 49 degrees of freedom\nMultiple R-squared:  0.3073,    Adjusted R-squared:  0.2931 \nF-statistic: 21.73 on 1 and 49 DF,  p-value: 2.44e-05\n\n\nWe see that m1 returns two coefficients, which define a line of best fit predicting Covid-19 deaths with the Republican vote share of the 2020 Presidential election:\n\n\\(\\beta_0\\) corresponds to the intercept. This is model’s prediction for a state where Trump got 0 percent of the vote. This is typically not something we care about.\n\\(\\beta_1\\) corresponds to the slope. Because we used a standardized measure of vote share, we would say that a 1-standard deviation (about 10 percentage points) increase in Republican vote share is associated with a 0.23 increase the average number of new Covid-19 deaths. Given that this per-capita measure has a standard deviation of 0.4, this is a fairly sizable association.\nFinally, note that last column of summary(m1) Pr(&gt;|t|) both the coefficients for the intercept \\((\\beta_0)\\) and rep_voteshare_std (\\((\\beta_1)\\)) are statistically significant (ie have an * next to them)."
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-model-as-a-regression-table",
    "href": "labs/06-lab-comments.html#display-the-model-as-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.3 Display the model as a regression table",
    "text": "2.3 Display the model as a regression table\nNext we’ll format the results of summary(m1) into a regression table using the htmlreg() function.\nRegression tables are a the standard way of concisely presenting the results of regression models.\n\nEach named row corresponds to the coefficients form the model\nIf there is an asterisks next to a coefficient, that coefficient is statistically significant with a p value below a certain threshold.\nThe numbers in parentheses below each coefficient correspond to the standard error of the coefficient (more on that later)2\nThe bottom of the table contains summary statistics of of our model, which we’ll ignore for today.\n\nThe code after htmlreg(m1) allows you to see what output of the table will look like in the html document while you’re working in the Rmd file.\n\n\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n0.57***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#visualize-the-model",
    "href": "labs/06-lab-comments.html#visualize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.4 Visualize the model",
    "text": "2.4 Visualize the model\nNow let’s visualize the results of our m1 with a scatter plot.\nIn the code chunk below, I’ve written some comments to help you get started. You can also refer to last week’s lab for help\n\n# 1. Tell ggplot what data to use\ncovid_lab %&gt;%\n# 2. Set the aesthetic mappings of our figure\n  ggplot(aes(x = rep_voteshare_std,\n             y = new_deaths_pc_14day,\n             label = state_po))+\n# 3. Draw points with x values corresponding to Rep vote share and y values corresponding to Covid deaths. \n  geom_point(\n# 4. Make the points smaller in size\n    size = .5\n    )+\n# 5. Add labels using `label=state_po` aesthetic \n  geom_text_repel(\n# 6. Make the label size smaller    \n    size = 2)+\n# 7. Plot the regression model\n  geom_smooth(method = \"lm\")+\n# 8. Change the axis labels\n  labs(\n    x = \"Republican Vote Share (std)\",\n    y = \"New Covid-19 Deaths\\n(14-day ave)\"\n  )"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-results",
    "href": "labs/06-lab-comments.html#interpret-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.5 Interpret the results",
    "text": "2.5 Interpret the results\nIn a sentence our two, summarize the results of your analysis in this section\nOur model in this section provides results consistent with the phenomena of Red Covid. State’s with higher Republican vote shares tended to have higher per capita rates of death from Covid-19 on September 23, 2022."
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-1",
    "href": "labs/06-lab-comments.html#fit-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.1 Fit the model",
    "text": "3.1 Fit the model\nNow let’s test our skeptics’ claims by fitting a model m2 that controls for Age (med_age_std).\n\nRemember the first argument in lm() is formula of the form outcome variable ~ predictor1 + predictor2 + ...\n\n\nm2 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-model",
    "href": "labs/06-lab-comments.html#summarize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.2 Summarize the model",
    "text": "3.2 Summarize the model\nNow let’s print out a statistical summary of m2\n\nsummary(m2)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, \n    data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59508 -0.24816 -0.05547  0.13825  0.99419 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04847  11.670 1.27e-15 ***\nrep_voteshare_std  0.23074    0.04933   4.677 2.40e-05 ***\nmed_age_std        0.03152    0.04933   0.639    0.526    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3461 on 48 degrees of freedom\nMultiple R-squared:  0.3131,    Adjusted R-squared:  0.2845 \nF-statistic: 10.94 on 2 and 48 DF,  p-value: 0.0001218"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-model-as-a-regression-table-1",
    "href": "labs/06-lab-comments.html#display-the-model-as-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.3 Display the model as a regression table",
    "text": "3.3 Display the model as a regression table\nNext, let’s create a regression table that displays m1 in the first column and m2 in the second column.\n\nTo do this, change list(m1) from the code above to list(m1, m2)\n\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n\n\n \n(0.05)\n(0.05)\n\n\nmed_age_std\n \n0.03\n\n\n \n \n(0.05)\n\n\nR2\n0.31\n0.31\n\n\nAdj. R2\n0.29\n0.28\n\n\nNum. obs.\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-results-1",
    "href": "labs/06-lab-comments.html#interpret-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.4 Interpret the results",
    "text": "3.4 Interpret the results\nIn a few sentences, explain whether the results from m2 support the skeptics criticisms or not?\n\nThey do not. Controlling for differences in the median age of state’s population does little to change the relationship between partisanship and Covid-19 that we saw in m1. If anything the the relationship is slightly stronger, while the coefficient on age is substantively small and statistically non-significant.\n\nPart of what’s at play here, is that relationship between age and Covid-19 outcomes at the state level is pretty weak, perhaps reflecting the early focus on vaccinating the eldery.\nSimilarly, the idea that Red States tend to be older doesn’t appear to be empirically true, perhaps reflecting differences between the general population and voting population.\n\n# Fit models\nm2_death_age &lt;- lm(new_deaths_pc_14day ~ med_age_std, covid_lab)\nm2_rep_age &lt;- lm(rep_voteshare_std ~ med_age_std, covid_lab)\n\n# Summarize models\nsummary(m2_death_age)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ med_age_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4796 -0.3095 -0.1863  0.2853  1.2488 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.565611   0.057879   9.772  4.3e-13 ***\nmed_age_std 0.002763   0.058454   0.047    0.962    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4133 on 49 degrees of freedom\nMultiple R-squared:  4.56e-05,  Adjusted R-squared:  -0.02036 \nF-statistic: 0.002234 on 1 and 49 DF,  p-value: 0.9625\n\nsummary(m2_rep_age)\n\n\nCall:\nlm(formula = rep_voteshare_std ~ med_age_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8705 -0.6764  0.0464  0.6577  1.8335 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.883e-16  1.403e-01   0.000    1.000\nmed_age_std -1.246e-01  1.417e-01  -0.879    0.384\n\nResidual standard error: 1.002 on 49 degrees of freedom\nMultiple R-squared:  0.01553,   Adjusted R-squared:  -0.004562 \nF-statistic: 0.7729 on 1 and 49 DF,  p-value: 0.3836\n\n# Display as formatted regression table\nhtmlreg(list(m2_rep_age,\n             m2_death_age),\n        custom.model.names = c(\"Deaths\", \"Rep Vote\"),\n        custom.coef.names = c(\"(Intercept)\",\"Median Age (std)\"),\n        custom.header = list(\"DV\"=1:2)  )%&gt;% HTML() %&gt;% browsable()\n\n\nStatistical models\n\n\n \nDV\n\n\n \nDeaths\nRep Vote\n\n\n\n\n(Intercept)\n-0.00\n0.57***\n\n\n \n(0.14)\n(0.06)\n\n\nMedian Age (std)\n-0.12\n0.00\n\n\n \n(0.14)\n(0.06)\n\n\nR2\n0.02\n0.00\n\n\nAdj. R2\n-0.00\n-0.02\n\n\nNum. obs.\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Visualize the models\ncovid_lab %&gt;%\n  ggplot(aes(x = med_age_std,\n             y = rep_voteshare_std,\n             label = state_po))+\n  geom_point(size = .5)+\n  geom_text_repel(size = 2)+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\ncovid_lab %&gt;%\n  ggplot(aes(x = med_age_std,\n             y = new_deaths_pc_14day,\n             label = state_po))+\n  geom_point(size = .5)+\n  geom_text_repel(size = 2)+\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-2",
    "href": "labs/06-lab-comments.html#fit-the-model-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.1 Fit the Model",
    "text": "4.1 Fit the Model\nPlease fit a model called m3 implied by the skeptic’s revised claims\n\nm3 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-model-1",
    "href": "labs/06-lab-comments.html#summarize-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.2 Summarize the model",
    "text": "4.2 Summarize the model\nSummarize the model m3 using summary()\n\nsummary(m3)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50751 -0.19703 -0.06278  0.20024  0.92320 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04425  12.782  &lt; 2e-16 ***\nrep_voteshare_std  0.07140    0.06654   1.073  0.28869    \nmed_age_std       -0.01692    0.04744  -0.357  0.72296    \nmed_income_std    -0.21669    0.06660  -3.254  0.00211 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.316 on 47 degrees of freedom\nMultiple R-squared:  0.4394,    Adjusted R-squared:  0.4036 \nF-statistic: 12.28 on 3 and 47 DF,  p-value: 4.689e-06"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-models-in-a-regression-table",
    "href": "labs/06-lab-comments.html#display-the-models-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.3 Display the models in a regression table",
    "text": "4.3 Display the models in a regression table\nAnd then display the results of models m1, m2, and m3.\n\nhtmlreg(list(m1,m2,m3)) %&gt;% HTML() %&gt;% browsable()\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n0.07\n\n\n \n(0.05)\n(0.05)\n(0.07)\n\n\nmed_age_std\n \n0.03\n-0.02\n\n\n \n \n(0.05)\n(0.05)\n\n\nmed_income_std\n \n \n-0.22**\n\n\n \n \n \n(0.07)\n\n\nR2\n0.31\n0.31\n0.44\n\n\nAdj. R2\n0.29\n0.28\n0.40\n\n\nNum. obs.\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-skeptics-claims",
    "href": "labs/06-lab-comments.html#interpret-the-skeptics-claims",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.4 Interpret the skeptic’s claims",
    "text": "4.4 Interpret the skeptic’s claims\nIn a few sentences, explain whether the results from m3 support the skeptics criticisms or not?\nControlling for median age and income, the coefficient on Republican sote share decreases in size by more than half and is no longer statistically significant. The coefficient on median income is statistically significant and substantively suggests that states with higher median incomes tended to have fewer Covid-19 deaths on September 23, 2021."
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-3",
    "href": "labs/06-lab-comments.html#fit-the-model-3",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.1 Fit the model",
    "text": "5.1 Fit the model\nYou know the drill.\n\nm4 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std + percent_vaccinated_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-results-1",
    "href": "labs/06-lab-comments.html#summarize-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.2 Summarize the results",
    "text": "5.2 Summarize the results\nAgain, let’s get a quick summary of our results\n\nsummary(m4)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + \n    med_income_std + percent_vaccinated_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52997 -0.16350 -0.02941  0.12926  0.94733 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.56561    0.04091  13.825  &lt; 2e-16 ***\nrep_voteshare_std      -0.08933    0.08161  -1.095  0.27940    \nmed_age_std             0.07110    0.05278   1.347  0.18458    \nmed_income_std         -0.11888    0.06969  -1.706  0.09478 .  \npercent_vaccinated_std -0.28633    0.09554  -2.997  0.00438 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2922 on 46 degrees of freedom\nMultiple R-squared:  0.531, Adjusted R-squared:  0.4902 \nF-statistic: 13.02 on 4 and 46 DF,  p-value: 3.621e-07"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-models-in-a-regression-table-1",
    "href": "labs/06-lab-comments.html#display-the-models-in-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.3 Display the models in a regression table",
    "text": "5.3 Display the models in a regression table\nAnd add m4 to list of models in our regression table\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n(0.04)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n0.07\n-0.09\n\n\n \n(0.05)\n(0.05)\n(0.07)\n(0.08)\n\n\nmed_age_std\n \n0.03\n-0.02\n0.07\n\n\n \n \n(0.05)\n(0.05)\n(0.05)\n\n\nmed_income_std\n \n \n-0.22**\n-0.12\n\n\n \n \n \n(0.07)\n(0.07)\n\n\npercent_vaccinated_std\n \n \n \n-0.29**\n\n\n \n \n \n \n(0.10)\n\n\nR2\n0.31\n0.31\n0.44\n0.53\n\n\nAdj. R2\n0.29\n0.28\n0.40\n0.49\n\n\nNum. obs.\n51\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpet-the-results",
    "href": "labs/06-lab-comments.html#interpet-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.4 Interpet the results",
    "text": "5.4 Interpet the results\nBriefly interpret the results of m4\nControlling for vaccination rates, none of the other variables in m4 are statistically significant predictors of Covid-19 deaths."
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-4",
    "href": "labs/06-lab-comments.html#fit-the-model-4",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.1 Fit the model",
    "text": "6.1 Fit the model\nNow let’s fit the model. For ease of interpretation, let’s use the unstandardized measure of vaccination rates, percent_vaccinated as our outcome variable.\n\nm5 &lt;- lm(percent_vaccinated ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-results-2",
    "href": "labs/06-lab-comments.html#summarize-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.2 Summarize the results",
    "text": "6.2 Summarize the results\nAnd summarize the results\n\nsummary(m5)\n\n\nCall:\nlm(formula = percent_vaccinated ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7719 -2.7849  0.2386  1.7743  7.6442 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        56.1597     0.5109 109.919  &lt; 2e-16 ***\nrep_voteshare_std  -4.5916     0.7683  -5.977 2.92e-07 ***\nmed_age_std         2.5143     0.5477   4.590 3.31e-05 ***\nmed_income_std      2.7939     0.7690   3.633 0.000691 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.649 on 47 degrees of freedom\nMultiple R-squared:  0.8129,    Adjusted R-squared:  0.801 \nF-statistic: 68.09 on 3 and 47 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-results-in-a-regression-table",
    "href": "labs/06-lab-comments.html#display-the-results-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.3 Display the results in a regression table",
    "text": "6.3 Display the results in a regression table\nDisplay them in a regression table\n\n\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n56.16***\n\n\n \n(0.51)\n\n\nrep_voteshare_std\n-4.59***\n\n\n \n(0.77)\n\n\nmed_age_std\n2.51***\n\n\n \n(0.55)\n\n\nmed_income_std\n2.79***\n\n\n \n(0.77)\n\n\nR2\n0.81\n\n\nAdj. R2\n0.80\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-results-2",
    "href": "labs/06-lab-comments.html#interpret-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.4 Interpret the results",
    "text": "6.4 Interpret the results\nSummarize the results of m5 and offer some broader discussion of what we’ve learned today\nIn m5 all three predictors have strong statistically significant relationships with vaccination rates in the expected direction. States where more people voted for Trump in 2020 tend to have lower rates of vaccination. States with an older population, and a richer population tend to have higher rates of vaccination.\nThinking back to the previous models we’ve estimated, we might argue that the effects these predictors have on Covid-19 death rates is mediated through their relationship with vaccination rates.\nMore broadly, what does this analysis mean for arguments about Red Covid. I guess, I’d say it’s complicated. Vaccines are clearly effective at reducing Covid-19 deaths. In both aggregate and invidual level data, Republicans appear to be less willing to get vaccinated. But lots of other factors influence vaccination rates and public health more broadly.\nRegression is a tool for trying to explore these competing explanations, but without strong theory and clever design, it’s unlikely to resolve debates. There’s almost always a skeptic waiting to say “Yes, but have you controlled for …” We can try to address there concerns by controlling for more and more variables. But a better strategy is often to say, I don’t need to control for X because the logic of my design already accounts for X.\nStill it’s hard to think what that kind of design would be for something like for debates about Red Covid. Maybe we need different (individual data) or maybe we’d want to reframe the question or draw out further testable implications of the claim. If your group is looking for a final project, there are number of directions you could take this kind of analysis:\n\nLooking at different periods overtime\nLooking at smaller units of aggregation (counties)\nControlling for alternative factors\nLeveraging variation in the availability of vaccines over time and place."
  },
  {
    "objectID": "labs/06-lab-comments.html#footnotes",
    "href": "labs/06-lab-comments.html#footnotes",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn short, these * correspond to \\(p-values\\) below different thresholds. One * typically means \\(p &lt; 0.05\\). A p-value is a conditional probability that arises from a hypothesis test summarizing the likelihood of observing a particular test statistic (here a regression coefficient, or more specifically, a t-statistic which is the regression coefficient divided by its standard error) given a paritcular hypothesis (typically, but not allows a null hypothesis that the true coefficient is 0). In sum, a p-value assess the likelihood of seeing what we did, if in fact, there was no relationship. If that likelihood is small (p&lt;0.05), we reject the claim of no relationship. We remain uncertain about the true value of the coefficient, but we are pretty confident it’s not 0.↩︎\nA standard error is another one of those things that in the cart we’re putting before horse today. Briefly, it is an estimate of the standard deviation of the sampling distribution of a coefficient and describes how much our coefficient might vary had we had a different sample…↩︎"
  },
  {
    "objectID": "labs/08-lab-comments.html",
    "href": "labs/08-lab-comments.html",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we continue our replication of Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages. (5 minutes)\nSet working directory and load the data from last class. (5 minutes)\nDo some additional recoding (5 minutes)\nDescribe variation in voting by state, year, policy, and age (20 minutes)\nEstimate four regression models to understand fixed effects and cluster robust standard errors (20 minutes)\nReplicate two regression models from Grumbach and Hill (2021) interacting sdr with age_group (10 minutes)\nRecreate a portion of Figure 3 showing the marginal effect of sdr by age group. (15 minutes)\n\nFinally, we’ll take the survey for this week\nOne of these 6 tasks will be randomly selected as the graded question for the lab.\n\nset.seed(3212024)\ngraded_question &lt;- sample(1:6,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 2 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/08-lab-comments.html#please-render-this-.qmd-file",
    "href": "labs/08-lab-comments.html#please-render-this-.qmd-file",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab-comments.html#load-packages",
    "href": "labs/08-lab-comments.html#load-packages",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\",\"margins\",\n  \"modelsummary\", \"ggeffects\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo       margins \n         TRUE          TRUE          TRUE          TRUE          TRUE \n modelsummary     ggeffects \n         TRUE          TRUE"
  },
  {
    "objectID": "labs/08-lab-comments.html#variation-by-state",
    "href": "labs/08-lab-comments.html#variation-by-state",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.1 Variation by state",
    "text": "3.1 Variation by state\nCreate a figure that shows how average turnout varies across state in the cps data\n\n# Create dataframe of average voting rates by state\ncps %&gt;% \n  group_by(st) %&gt;% \n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) %&gt;% \n  mutate(\n    st = fct_reorder(st, turnout)\n    ) -&gt; df_state\n\n# Use data frame of state averages to produce figure\ndf_state %&gt;% \n  ggplot(aes(turnout,st,fill=turnout))+\n  geom_bar(stat = \"identity\") +\n  theme_minimal()"
  },
  {
    "objectID": "labs/08-lab-comments.html#variation-over-time",
    "href": "labs/08-lab-comments.html#variation-over-time",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.2 Variation over time",
    "text": "3.2 Variation over time\nCreate a figure that shows how turnout varies across time.\nCalculate the yearly averages separately by election_type. Facet your plot by election_type using facet_wrap(~election_type)\n\n# Calculate yearly averages in turnout by election type\ncps %&gt;% \n  group_by(year, election_type ) %&gt;%\n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) -&gt; df_year\n\n# Display variation in turnout by year for Presidential And Midterm Elections\ndf_year %&gt;% \n  ggplot(aes(year, turnout, col=election_type)) +\n  geom_line() +\n  facet_grid(~election_type)+\n  theme_minimal()+\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size =5))"
  },
  {
    "objectID": "labs/08-lab-comments.html#across-policy-and-age-for-single-state",
    "href": "labs/08-lab-comments.html#across-policy-and-age-for-single-state",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.3 Across policy and age for single state",
    "text": "3.3 Across policy and age for single state\nFor a single state (you pick) that implemented SDR registration at some point between 1978 and 2018, plot the average turnout by age_group before and after the SDR\n\n\n\n\n\n\nTip\n\n\n\nThis is a tricky one that is designed to test your data-wrangling skills.\n\nFigure out which states adopted sdr\nCreate a summary dataframe of average turnout by age for a single state using filter(), group_by() and summarise()\nPipe this dataframe to ggplot() setting the appropriate aesthetics using geom_point() and facet_grid()\n\n\n\n\n# For a chosen state, create a dataframe showing how turnout varies by year, age_group, and SDR\n\n\n# states that implement SDR\nsdr_states &lt;- unique(cps$st[cps$sdr == 1])\n\ncps %&gt;% \n  filter(st %in% sdr_states) %&gt;% \n  group_by(st, year, SDR, age_group) %&gt;% \n  summarise(\n    turnout = mean(dv_voted,na.rm=T)\n  ) -&gt; sdr_age_df\n\n# Figure: Turnout by age group before and after SDR\n\nsdr_age_df %&gt;% \n  filter(!is.na(age_group)) %&gt;% \n  filter(st == \"NH\") %&gt;% \n  mutate(\n    # Relevel age_group so 18-24 comes first for this fig\n    age_group = factor(as.character(age_group))\n  ) %&gt;% \n  ggplot(aes(year, turnout, col = SDR))+\n  geom_point()+\n  # calculate conditional means by SDR using lm\n  stat_smooth(method = \"lm\", formula = \"y~1\")+\n  facet_grid(~ age_group)+\n  theme_minimal()+ \n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size =5))"
  },
  {
    "objectID": "labs/08-lab-comments.html#describe-your-results",
    "href": "labs/08-lab-comments.html#describe-your-results",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.4 Describe your results",
    "text": "3.4 Describe your results\nIn a few sentences, explain to your reader what these figures tell us about the data:\nFirst there is considerable variation in average rates of turnout across states. For example, across the years considered, the average turnout rate in West Virginia (a state which never implemented Same Day Registration) was about 46.8 percent among CPS respondents, while the average turnout rate in Minnesota (a state which had SDR for the entire period) was more than 20 percentage points higher (68.2 percent).\nSecond, there is also considerable variation in turnout across election years. Turnout in presidential elections is much higher than turnout in midterm elections. From one presidential or midterm election to the next, turnout rates can swing by as much as 10 percentage points. There does not seem to be a clear trend in turnout over time in these data.\nAs for variation in turnout by age before and after SDR, your interpretations depend on which state you chose. In states like New Hampshire or North Carolina, turnout seems markedly higher among young folks after the states adopted SDR.1 In states like Illinois or Wyoming, there doesn’t appear to be much of a difference.\n\n# Additional code to help me describe these figures\ndf_state %&gt;% \n  filter(turnout == min(turnout) | turnout == max(turnout))\n\n# A tibble: 2 × 2\n  st    turnout\n  &lt;fct&gt;   &lt;dbl&gt;\n1 MN      0.682\n2 WV      0.468\n\ndf_year %&gt;% \n  group_by(election_type) %&gt;% \n  mutate(\n    swing = abs(turnout - lag(turnout))\n  ) %&gt;% \n  summarize(\n    sd = sd(turnout),\n    max_swing = max(swing, na.rm=T)\n  )\n\n# A tibble: 2 × 3\n  election_type     sd max_swing\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 General       0.0293    0.0945\n2 Midterm       0.0287    0.102 \n\n\nHere’s the turnout by age plot for North Carolina\n\nsdr_age_df %&gt;% \n  filter(!is.na(age_group)) %&gt;% \n  filter(st == \"NC\") %&gt;% \n  mutate(\n    age_group = factor(as.character(age_group))\n  ) %&gt;% \n  ggplot(aes(year, turnout, col = SDR))+\n  geom_point()+\n  stat_smooth(method = \"lm\", formula = \"y~1\")+\n  facet_grid(~ age_group)+\n  theme_minimal()+\n   theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size =5))\n\n\n\n\n\n\n\n\nHere’s the same plot for Illinois\n\nsdr_age_df %&gt;% \n  filter(!is.na(age_group)) %&gt;% \n  filter(st == \"IL\") %&gt;% \n  mutate(\n    age_group = factor(as.character(age_group))\n  ) %&gt;% \n  ggplot(aes(year, turnout, col = SDR))+\n  geom_point()+\n  stat_smooth(method = \"lm\", formula = \"y~1\")+\n  facet_grid(~ age_group)+\n  theme_minimal()+\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size =5))\n\n\n\n\n\n\n\n\nAnd here is a plot of the average turnout rates among 18-24 year olds over time in the 18 states that adopted SDR at some point between 1978-2018\n\nsdr_age_df %&gt;%\n  filter(age_group == \"18-24\") %&gt;% \n  ggplot(aes(year, turnout, col = SDR))+\n  geom_point()+\n  stat_smooth(method = \"lm\", formula = \"y~1\", se = F)+\n  facet_wrap(~st , ncol = 3)+\n  theme_minimal()+\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(size =5))"
  },
  {
    "objectID": "labs/08-lab-comments.html#estimate-the-models",
    "href": "labs/08-lab-comments.html#estimate-the-models",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "4.1 Estimate the models",
    "text": "4.1 Estimate the models\nI’ve gotten you started with m1 Use that code as a template to estimate m2, m3, and m4\n\n# ---- m1: Simple OLS regression ----\nm1 &lt;- lm_robust(dv_voted ~ sdr, \n                data = cps,\n                se_type = \"classical\",\n                try_cholesky = T)\n\n# ---- m2: Simple OLS with robust standard errors ----\nm2 &lt;- lm_robust(dv_voted ~ sdr, \n                data = cps,\n                se_type = \"stata\",\n                try_cholesky = T)\n\n# ---- m3: Two-way Fixed Effects for State and Year ----\nm3 &lt;- lm_robust(dv_voted ~ sdr,\n                data = cps,\n                fixed_effects = ~ st + year,\n                se_type = \"stata\",\n                try_cholesky = T)\n\n# ---- m4: TWFE for State and Year and cluster robust SEs ----\n\nm4 &lt;- lm_robust(dv_voted ~ sdr,\n                data = cps,\n                fixed_effects = ~ st + year,\n                se_type = \"stata\",\n                clusters = st,\n                try_cholesky = T)"
  },
  {
    "objectID": "labs/08-lab-comments.html#present-and-interpret-the-results",
    "href": "labs/08-lab-comments.html#present-and-interpret-the-results",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "4.2 Present and interpret the results",
    "text": "4.2 Present and interpret the results\nWhen you’ve completed the previous section, you should be able uncomment and run the following code\n\nhtmlreg(l = list(m1,m2,m3, m4),\n        digits = 5,\n        include.ci = F,\n        ) %&gt;% HTML() %&gt;% browsable()\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n0.54787***\n0.54787***\n \n \n\n\n \n(0.00037)\n(0.00038)\n \n \n\n\nsdr\n0.06159***\n0.06159***\n0.00671***\n0.00671\n\n\n \n(0.00110)\n(0.00108)\n(0.00185)\n(0.01405)\n\n\nR2\n0.00157\n0.00157\n0.02845\n0.02845\n\n\nAdj. R2\n0.00157\n0.00157\n0.02841\n0.02841\n\n\nNum. obs.\n1988501\n1988501\n1988501\n1988501\n\n\nRMSE\n0.49658\n0.49658\n0.48986\n0.48986\n\n\nN Clusters\n \n \n \n49\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\nPlease write a few sentences explaining how the coefficient on sdr and it’s standard error changes across the four models. I’ll get you started:\nThe table presents the results of four regression models. The outcome in each model is a binary indicator of whether respondents to the CPS voted in a given election. The key predictor of interest in each model is the coefficient for sdr which corresponds the model’s predicted difference in turnout in states that had same day registration compared to states that did not. Model 1 presents the results from a simple linear regression assuming homoskedastic errors (i.e. constant error variance). The coefficient on sdr of 0.062 implies that this model predicts turnout will be higher by 6.2 percentage points in states with Same Day Registration. The standard error of 0.00110 is small relative to the coefficient, which as we will see in two weeks implies a statistically significant relationship.\nModel 2 presents the same specification, but uses robust standard errors that allow for non-constant (heteroskedastic) error variance. The coefficient on sdr is exactly the same as Model 1, and the standard error is similar to the ten-thousandth decimal place. In short, simply including robust standard errors does little to change our substantive interpretation.\nModel 3 includes fixed effects of state and year. Specifically, lm_robust() uses the within-in transformation via the method of alternating projections, subtracting off the state and year level means of the predictors and outcomes before estimating the regression. This transformation is equivalent to including indicator variables to represent the various levels of each fixed effect (e.g. including year and st as factor variables in the model). That latter, sometimes called the dummy variable approach, is more computationally intensive (if there are lots of levels to the fixed effect, the matrices involved in the calculation get very large), but as the code below demonstrates, they all yield the same estimates and standard errors (although there are some additional degrees of freedom adjustments that lm_robust() performs, which we are not when we calculate the within transformations by hand)\n\n# Kludge-y method of alternating projections....\ncps %&gt;% \n  ungroup() %&gt;% \n  # Only use observations from m3\n  filter(!is.na(dv_voted), !is.na(sdr)) %&gt;% \n  dplyr::group_by(st) %&gt;% \n  # Within transformation by states\n  mutate(\n    dv_voted_within = dv_voted - mean(dv_voted, na.rm=T),\n    sdr_within = sdr - mean(sdr, na.rm=T)\n  ) %&gt;% \n  ungroup() %&gt;% \n  # Within transformation by year\n  dplyr::group_by(year) %&gt;%\n  mutate(\n    dv_voted_within = dv_voted_within - mean(dv_voted_within, na.rm=T),\n    sdr_within = sdr_within - mean(sdr_within,na.rm=T)\n  ) %&gt;% ungroup() %&gt;% \n  # Within transformation by year\n  dplyr::group_by(st) %&gt;%\n  mutate(\n    dv_voted_within = dv_voted_within - mean(dv_voted_within, na.rm=T),\n    sdr_within = sdr_within - mean(sdr_within,na.rm=T)\n  ) %&gt;% \n    dplyr::group_by(year) %&gt;%\n  mutate(\n    dv_voted_within = dv_voted_within - mean(dv_voted_within, na.rm=T),\n    sdr_within = sdr_within - mean(sdr_within,na.rm=T)\n  ) %&gt;% ungroup() %&gt;% \n  dplyr::group_by(st) %&gt;%\n  mutate(\n    dv_voted_within = dv_voted_within - mean(dv_voted_within, na.rm=T),\n    sdr_within = sdr_within - mean(sdr_within,na.rm=T)\n  ) %&gt;% \n    dplyr::group_by(year) %&gt;%\n  mutate(\n    dv_voted_within = dv_voted_within - mean(dv_voted_within, na.rm=T),\n    sdr_within = sdr_within - mean(sdr_within,na.rm=T)\n  ) %&gt;% ungroup()-&gt; cps\n\nstart_time_m3_within  &lt;- Sys.time()\nm3_within &lt;- lm_robust(dv_voted_within ~ sdr_within, \n                       cps,\n                       se_type = \"stata\",\n                       try_cholesky = T)\n\nend_time_m3_within  &lt;- Sys.time()\ntime_m3_within &lt;- round(end_time_m3_within - start_time_m3_within,5)\n\n\nstart_time_m3_indicator  &lt;- Sys.time()\nm3_indicator &lt;- lm_robust(dv_voted ~ sdr + factor(year) + factor(st),\n                          cps,\n                          se_type = \"stata\",\n                          try_cholesky = T)\nend_time_m3_indicator  &lt;- Sys.time()\ntime_m3_indicator &lt;- round(end_time_m3_indicator - start_time_m3_indicator,5)\n\ntime_m3_within\n\nTime difference of 2.45665 secs\n\ntime_m3_indicator\n\nTime difference of 9.88044 secs\n\n\n\nhtmlreg(list(m3, m3_within, m3_indicator),\n        omit.coef = \"factor|Intercept\",\n        digits = 5,\n        include.ci = F,\n        custom.model.names = c(\"lm_robust (within transformation)\",\"Within transformation by hand\",\"Indicator FE\")\n        ) %&gt;% HTML() %&gt;% browsable()\n\n\nStatistical models\n\n\n \nlm_robust (within transformation)\nWithin transformation by hand\nIndicator FE\n\n\n\n\nsdr\n0.00671***\n \n0.00671***\n\n\n \n(0.00185)\n \n(0.00185)\n\n\nsdr_within\n \n0.00671***\n \n\n\n \n \n(0.00185)\n \n\n\nR2\n0.02845\n0.00001\n0.02845\n\n\nAdj. R2\n0.02841\n0.00001\n0.02841\n\n\nNum. obs.\n1988501\n1988501\n1988501\n\n\nRMSE\n0.48986\n0.48985\n0.48986\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\nFinally, model 4 presents the results of a TWFE regression with fixed effects for state and year with robust standard errors clustered by state. The coefficient on sdr remains the same as in Model 3, predicting about 0.6 percentage points higher turnout in states with same day registration. We see, however, that once we relax the assumption that our errors are independent, and allow for correlations between observations from the same state, our uncertainty about this estimate (quantified by its standard error) increases 10-fold, and we are non longer confident that there is a statistically significant relationship between same day registration and voting in these data.\nIn sum, once we account for fixed differences across states and between time periods, and allow correlated errors between observations from the same state, variation in the presence of same day registration laws seems to explain relatively little variation in voting\n\n\n\n\n\n\nNote\n\n\n\nSo which model should we prefer? I would say the default approach of most social scientists is to prefer the most conservative estimate, which in this case corresponds to the results from model 4. We’d rather under claim than over claim. Since rarely if ever, do we know the right model , it’s generally common practice to report the results of multiple models with different specifications (as Grumbach and Hill do) as a way of demonstrating the robustness of ones’ results."
  },
  {
    "objectID": "labs/08-lab-comments.html#estimate-the-models-1",
    "href": "labs/08-lab-comments.html#estimate-the-models-1",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "5.1 Estimate the models",
    "text": "5.1 Estimate the models\n\nm1gh &lt;- lm_robust(dv_voted ~ sdr*age_group, \n                  data = cps,\n                  fixed_effects = ~ st + year,\n                  se_type = \"stata\",\n                  clusters = st,\n                  try_cholesky = T\n                  )\n\n\nm2gh &lt;- lm_robust(dv_voted ~ sdr*age_group +\n                    factor(race) + is_female + income + education, \n                  data = cps,\n                  fixed_effects = ~ st + year,\n                  se_type = \"stata\",\n                  clusters = st,\n                  try_cholesky = T\n                  )"
  },
  {
    "objectID": "labs/08-lab-comments.html#present-the-results",
    "href": "labs/08-lab-comments.html#present-the-results",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "5.2 Present the results",
    "text": "5.2 Present the results\nPlease present the results in a regression table.\nOnce you’ve estimated the models, you can just uncomment the code below:\n\nhtmlreg(list(m1gh, m2gh), \n        digits = 4,\n        include.ci = F) %&gt;% HTML() %&gt;% browsable()\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\n\n\n\n\nsdr\n0.0120\n0.0004\n\n\n \n(0.0162)\n(0.0128)\n\n\nage_group18-24\n-0.3518***\n-0.4125***\n\n\n \n(0.0061)\n(0.0051)\n\n\nage_group25-34\n-0.2149***\n-0.3198***\n\n\n \n(0.0060)\n(0.0040)\n\n\nage_group35-44\n-0.1026***\n-0.2179***\n\n\n \n(0.0063)\n(0.0038)\n\n\nage_group45-54\n-0.0431***\n-0.1450***\n\n\n \n(0.0059)\n(0.0038)\n\n\nage_group55-64\n0.0064\n-0.0642***\n\n\n \n(0.0047)\n(0.0036)\n\n\nsdr:age_group18-24\n0.0142\n0.0472***\n\n\n \n(0.0108)\n(0.0081)\n\n\nsdr:age_group25-34\n0.0003\n0.0219**\n\n\n \n(0.0137)\n(0.0076)\n\n\nsdr:age_group35-44\n-0.0142\n0.0074\n\n\n \n(0.0162)\n(0.0087)\n\n\nsdr:age_group45-54\n-0.0147\n0.0001\n\n\n \n(0.0120)\n(0.0068)\n\n\nsdr:age_group55-64\n-0.0149\n-0.0108\n\n\n \n(0.0096)\n(0.0062)\n\n\nfactor(race)200\n \n0.0518***\n\n\n \n \n(0.0081)\n\n\nfactor(race)300\n \n-0.0548***\n\n\n \n \n(0.0118)\n\n\nfactor(race)650\n \n-0.1619***\n\n\n \n \n(0.0305)\n\n\nfactor(race)651\n \n-0.1784***\n\n\n \n \n(0.0090)\n\n\nfactor(race)652\n \n-0.1277***\n\n\n \n \n(0.0188)\n\n\nfactor(race)700\n \n-0.0916***\n\n\n \n \n(0.0236)\n\n\nfactor(race)801\n \n0.0130\n\n\n \n \n(0.0110)\n\n\nfactor(race)802\n \n-0.0233*\n\n\n \n \n(0.0097)\n\n\nfactor(race)803\n \n-0.0406\n\n\n \n \n(0.0277)\n\n\nfactor(race)804\n \n-0.0566\n\n\n \n \n(0.0403)\n\n\nfactor(race)805\n \n0.0548\n\n\n \n \n(0.0291)\n\n\nfactor(race)806\n \n-0.0432\n\n\n \n \n(0.0520)\n\n\nfactor(race)807\n \n-0.0486\n\n\n \n \n(0.0574)\n\n\nfactor(race)808\n \n-0.0593\n\n\n \n \n(0.0473)\n\n\nfactor(race)809\n \n-0.1125***\n\n\n \n \n(0.0271)\n\n\nfactor(race)810\n \n0.0169\n\n\n \n \n(0.0340)\n\n\nfactor(race)811\n \n-0.0932\n\n\n \n \n(0.1396)\n\n\nfactor(race)812\n \n-0.0474\n\n\n \n \n(0.0891)\n\n\nfactor(race)813\n \n-0.0445***\n\n\n \n \n(0.0076)\n\n\nfactor(race)814\n \n-0.0316\n\n\n \n \n(0.1223)\n\n\nfactor(race)815\n \n-0.1124\n\n\n \n \n(0.0601)\n\n\nfactor(race)816\n \n0.2548\n\n\n \n \n(0.1910)\n\n\nfactor(race)817\n \n0.1245\n\n\n \n \n(0.1979)\n\n\nfactor(race)818\n \n0.8195***\n\n\n \n \n(0.0074)\n\n\nfactor(race)819\n \n-0.0831\n\n\n \n \n(0.1869)\n\n\nfactor(race)820\n \n0.0325\n\n\n \n \n(0.0592)\n\n\nfactor(race)830\n \n-0.1871**\n\n\n \n \n(0.0667)\n\n\nis_female\n \n0.0240***\n\n\n \n \n(0.0017)\n\n\nincome\n \n0.0093***\n\n\n \n \n(0.0002)\n\n\neducation\n \n0.0895***\n\n\n \n \n(0.0016)\n\n\nR2\n0.0864\n0.1707\n\n\nAdj. R2\n0.0864\n0.1706\n\n\nNum. obs.\n1980510\n1616508\n\n\nRMSE\n0.4748\n0.4515\n\n\nN Clusters\n49\n49\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/08-lab-comments.html#calculate-marginal-effects-of-interactions",
    "href": "labs/08-lab-comments.html#calculate-marginal-effects-of-interactions",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "6.1 Calculate Marginal Effects of Interactions",
    "text": "6.1 Calculate Marginal Effects of Interactions\nIn the code below, I’ve written a custom function called me_fn() to calculate the marginal effects of Same Day Registration, a specific age cohort2\nThe function returns a \\(1\\times 5\\) data frame with following columns\n\nAge the Age for which we are evaluating the marginal effect of sdr\nEffect the marginal effect in percentage points of predicted turnout of sdr conditional on age_group equaling the age cohort in Age\nSE the standard error of this marginal effect\nll the lower limit of a 95 percent confidence interval for our estimate\nul the upper limit of a 95 percent confidence interval for our estimate\n\nWe will talk in more detail about what a confidence interval is in two weeks. For now, you can think of this interval as a range of equally plausible values for the marginal effect of SDR at a given age cohort.\n\n\n\n\n\n\nNote\n\n\n\nThe heuristic for interpreting a confidence interval as a measure of statistical significance, is to ask:\n\nIs zero within the upper and lower limits of the confidence interval. If so, then the true estimate could be negative, or it could be positive, in which case, we conclude the estimate is not statistically significant.\n\n\n\nPlease run the code below:\n\n# ---- Function to calculate marginal effect and SE of interactions ----\n\nme_fn &lt;- function(mod, cohort, ci=0.95){\n  # Confidence Level for CI\n  alpha &lt;- 1-ci\n  z &lt;- qnorm(1-alpha/2)\n  \n  # Age (Always one for indicator of specific cohort)\n  age &lt;- 1\n  \n  # Variance Covariance Matrix from Model\n  cov &lt;- vcov(mod)\n  \n  # coefficient for SDR (Marginal Effect for reference category: 65+)\n  b1 &lt;- coef(mod)[\"sdr\"]\n  \n  # If age is one of the interactions\n  if(cohort %in% c(\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\")){\n    # get the name of the specific interaction\n    the_int &lt;- paste(\"sdr:age_group\",cohort,sep=\"\")\n    # the coefficient on the interaction\n    b2 &lt;- coef(mod)[the_int]\n    # Calculate marginal effect for age cohort\n    me &lt;- b1 + b2*age\n    me_se &lt;- sqrt(cov[\"sdr\",\"sdr\"] + age^2*cov[the_int,the_int] + 2*age*cov[\"sdr\",the_int])\n    ll &lt;- me - z*me_se\n    ul &lt;- me + z*me_se\n  }\n  if(!cohort %in% c(\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\")){\n    me &lt;- b1 \n    me_se &lt;- mod$std.error[\"sdr\"]\n    ll &lt;- mod$conf.low[\"sdr\"]\n    ul &lt;- mod$conf.high[\"sdr\"]\n  }\n\n  # scale results to be percentage points\n  res &lt;- tibble(\n    Age = cohort,\n    Effect = me*100,\n    SE = me_se*100,\n    ll = ll*100,\n    ul = ul*100\n  )\n  return(res)\n\n\n}\n\nThen uncomment the code below to create the data frame to produce a version of the coefficient plots from Grumbach and Hill’s Figure 3.\n\n## List of age cohorts\nthe_age_groups &lt;- levels(cps$age_group)\n\n## Model 1: No controls\n## Estimate Marginal effect for each age cohort\nthe_age_groups %&gt;% \n  purrr::map_df(~me_fn(m1gh, cohort=.)) %&gt;% \n  # Add labels for plotting\n  mutate(\n    Age = factor(Age),\n    Model = \"No controls\"\n  ) -&gt; fig3_no_controls\n\n## Model 3: Controls for Education, Income, Race,and Sex \n## Estimate Marginal effect for each age cohort\nthe_age_groups %&gt;% \n  purrr::map_df(~me_fn(m2gh, cohort=.)) %&gt;% \n  # Add labels for plotting\n  mutate(\n    Age = factor(Age),\n    Model = \"With controls\"\n  ) -&gt; fig3_controls\n  \n## Combine estimates into data frame for plotting\nfig3_df &lt;- fig3_no_controls %&gt;% bind_rows(fig3_controls)\n\n## Display results\nfig3_df\n\n# A tibble: 12 × 6\n   Age    Effect    SE     ll    ul Model        \n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n 1 65+    1.20    1.62 -2.06   4.47 No controls  \n 2 18-24  2.63    1.60 -0.500  5.75 No controls  \n 3 25-34  1.23    1.56 -1.83   4.29 No controls  \n 4 35-44 -0.218   1.59 -3.33   2.89 No controls  \n 5 45-54 -0.262   1.36 -2.93   2.40 No controls  \n 6 55-64 -0.287   1.28 -2.80   2.22 No controls  \n 7 65+    0.0382  1.28 -2.53   2.61 With controls\n 8 18-24  4.76    1.61  1.60   7.91 With controls\n 9 25-34  2.23    1.40 -0.510  4.97 With controls\n10 35-44  0.778   1.39 -1.95   3.51 With controls\n11 45-54  0.0440  1.28 -2.47   2.56 With controls\n12 55-64 -1.04    1.19 -3.37   1.28 With controls\n\n\n\nIf you’re trying to understand the code above, it’s a equivalent to writing something like:\n\nrbind(\nme_fn(m1gh, cohort=\"18-24\"),\nme_fn(m1gh, cohort=\"25-34\"),\nme_fn(m1gh, cohort=\"35-44\"),\nme_fn(m1gh, cohort=\"45-54\"),\nme_fn(m1gh, cohort=\"55-64\"),\nme_fn(m1gh, cohort=\"65+\")\n) %&gt;% \n  mutate(\n    Age = factor(Age),\n    Model = \"No controls\"\n  )\n\n# A tibble: 6 × 6\n  Age   Effect    SE     ll    ul Model      \n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1 18-24  2.63   1.60 -0.500  5.75 No controls\n2 25-34  1.23   1.56 -1.83   4.29 No controls\n3 35-44 -0.218  1.59 -3.33   2.89 No controls\n4 45-54 -0.262  1.36 -2.93   2.40 No controls\n5 55-64 -0.287  1.28 -2.80   2.22 No controls\n6 65+    1.20   1.62 -2.06   4.47 No controls\n\n\nUsing map_df() from the purrr package, simply lets us pipe the values of age_group, to the cohort argument of the me_fn() we defined above. map_df() then evaluates me_fn() for each value of the the_age_groups and returns a data frame with 6 rows corresponding to the marginal effects, standard errors, and confidence intervals for each age group."
  },
  {
    "objectID": "labs/08-lab-comments.html#recreate-figure-3-1",
    "href": "labs/08-lab-comments.html#recreate-figure-3-1",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "6.2 Recreate Figure 3",
    "text": "6.2 Recreate Figure 3\nNow we can recreate the first and second panels in the first column of Grumbach and Hill’s Figure 3.\nIn the code chunk below, pipe fig3_df into ggplot() and:\n\nSet the x aesthetic to Age\nSet the y aesthetic to Effect\nAdd geom_point()\nAdd geom_linerange(aes(ymin = ll, ymax =ul))\nAdd geom_hline(yintercept = 0)\nAdd a facet_wrap(~Model)\nSet the theme if you like\n\n\nfig3_df %&gt;% \n  ggplot(aes(Age, Effect))+\n  geom_point()+\n  geom_linerange(aes(ymin = ll, ymax =ul))+\n  geom_hline(yintercept = 0, linetype = \"dashed\")+\n  facet_wrap(~Model)+\n  theme_minimal()"
  },
  {
    "objectID": "labs/08-lab-comments.html#compare-your-figure-to-figure-3-from-the-text",
    "href": "labs/08-lab-comments.html#compare-your-figure-to-figure-3-from-the-text",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "6.3 Compare your figure to Figure 3 from the text",
    "text": "6.3 Compare your figure to Figure 3 from the text\nFinally, write a few sentences comparing your results to those presented in Figure 3 of Grumbach and Hill.\nHere’s a snapshot of the relevant panels of Figure 3 for reference:\n\nPlease comment on the following:\n\nHow does the size (magnitude) of the marginal effects for 18-24 year olds compare to those reported in Figure 3?\nHow would you interpret this marginal effect substantively?\nAre any of the marginal effects in the model with no controls statistically significant?\n\nAnd if you’re interested, check out the comments for some further discussion about the differences between our replication and the published results.\n\nSo, I would say that the general constellation of results is similar to those in reported in the first two rows of the first column of Figure 3, but with some noticeable differences.\nAs with Grumbach and Hill’s Figure 3, the size of the marginal effect of SDR is generally largest for respondents in the 18-24-year-old age cohort. The model with no controls predicts that turnout among young folks will be about 2.6 percentage points higher in states with SDR. Including demographic and socio-economic controls, that estimate rises to about 4.6 percentage points.\nHowever, none of the marginal effects appear statistically significant in the model with no controls (all the CIs include 0). In the model with controls, the marginal effect for 18-24 year olds remains statistically significant, while the confidence interval on the marginal effect for 25-34 now includes 0 (i.e. is no longer statistically significant, or maybe is only marginally significant). Further, the specific point estimates differ in our replication from the results reported in the appendix of Grumbach and Hill\nSo what’s going on?\nDifferences in recoding If you dive into the replication code for the paper, you’ll note that their are some differences in how we recoded variables. In particular, our age_group variable classifies people in the 18-24 cohort, if their reported age is 18 to 24 when they took the survey. In the original paper, 18-24 year olds are coded with the following:\n\n#| eval: false\n# cps$age_group[cps$age&lt;=24] &lt;- \"18-24\"\n\nSo young folks below the age of 18 are included in this measure, which wouldn’t alter the estimates if they didn’t report voting, but it turns out that 7,991 folks in the CPS reported voting even though their recorded age is under 18.\n\ntable(cps$dv_voted, cps$age &lt; 18)\n\n   \n      FALSE    TRUE\n  0  876904    7991\n  1 1103606       0\n\n\nThese respondents are included in G&H’s analysis, but excluded from ours.\nThere are also some differences in the way we recoded covariates, like income and education that might also contribute to some of the differences we see in the model with controls.\nDifferences in standard errors The second main difference between the paper and our replication, is how we calculate the standard error for the marginal effect of Same Day Registration for a given age group.\nIn G&H, based on the 100 or so lines of code from line 662 it looks like they’re constructing confidence intervals around the estimates of the marginal effects, using the SEs for the interaction terms. But as discussed above, the correct standard errors need to be calculated from the variances and covariances of both the coefficients on sdr and sdr’s interaction with a specific age_group indicator. Doing so generally gives us wider confidence intervals, which likely explains the difference in the model with controls for 25-34 year-olds.\nWhat does it all mean?\nI guess I would say something like the following: Errors in code happen which is why it is doing empirical research in a reproducible way – sharing your data and code – is so vital.\nIn the present case, the errors seem to me to be relatively minor (some quibbles about how variables were recoded, a different approach to calculating standard errors) – such that the core finding, SDR has a bigger impact on young folks than old folks– remains relatively unchanged. Of course, we’ve only replicated two of 9 models in figure 3, but I suspect we’d find similar results. Further, Figure 3, is only one piece of evidence in their larger argument.\nSo even though we’ve uncovered some irregularities in how data were coded and standard errors calculated, I would caution against jumping to the conclusion that we’ve disproved or invalidated the paper. We haven’t.\nBut hopefully we’ve gotten a sense of how important it is to work in a clear manner. To check and double check our results. To know our data (e.g. do folks who legally can’t vote, report voting in the data?) To make sure our code is clean, easy to follow, and does what we think it does.\nFinally, I hope that in looking at the data by state and year and age group, we’ve gained a better sense of what fixed effects regression is trying to accomplish. Broadly, we’re taking into account variation across states and time periods, and then seeing if the thing we’re interested – Same Day Registration – still explains variation voting. These models are motivated by the same logic of the difference-in-differences designs we talked about in class. The interpretation of these regressions becomes more complicated (and their similarity to the the canonical DiD more tenuous), when “treatment” is implemented at different time periods, as well as when states can go from being not treated, to treated, back to not-treated, like Ohio, not to mention the fact that these models also introduce interactions between SDR and age groups to test for heterogeneous effects."
  },
  {
    "objectID": "labs/08-lab-comments.html#footnotes",
    "href": "labs/08-lab-comments.html#footnotes",
    "title": "Comments for Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut note the bump in turnout in NC appears pretty similar across age groups↩︎\nThis is not a robust function, but one designed to work specifically for these data and models↩︎"
  },
  {
    "objectID": "labs/03-lab-comments.html",
    "href": "labs/03-lab-comments.html",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "",
    "text": "Today we will explore the logic and design of Broockman and Kalla’s 2016 study, “Durably reducing transphobia: A field experiment on door-to-door canvassing”, from the recruitment of subjects for the study to the delivery of their interventions. Then we will explore whether the intervention had any effect on respondents’ feelings toward transgender individuals.\nTo accomplish this we will:\n\nSummarize the study (5 Minutes)\nSet up our work space (2-3 Minutes)\nLoad a portion of the replication data (1-2 Minutes)\nGet a high level overview of the data (5 minutes)\nDescribe the distribution of covariates in the full dataset (5 minutes)\nExamine the difference in covariates between those who did and did not complete the survey (10 minutes)\nExamine the difference in covariates between those assigned to each treatment condition in the study. (10 minutes)\nEstimate the average treatment effect of the intervention (10 minutes)\nPlot the results and comment on the study (10 minutes)\nTake the weekly survey (3-5 minutes)\n\nOne of these 9 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(22022024)\ngraded_question &lt;- sample(1:9,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/03-lab-comments.html#footnotes",
    "href": "labs/03-lab-comments.html#footnotes",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find the full set of replication files here↩︎\nThe actual study contains a number of measures about transgender attitudes and policies which are scaled together to produce a single measure of subjects latent tolerance. For simplicity, we’ll focus on this single survey item.↩︎\nRecall that only some people who completed the baseline and were assigned to receive the treatment actually answered the door when canvassers came knocking.↩︎"
  },
  {
    "objectID": "labs/07-lab-comments.html",
    "href": "labs/07-lab-comments.html",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we will begin the process of replicating Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages and set the working directory to where this file is saved. (5 minutes)\nSummarize the study in terms of it’s research question, theory, design, and results. (10 minutes)\nDownload the replication files and save them in the same folder as this lab (5 minutes)\nLoad the data from your computers into R (5 minutes)\nGet a quick HLO of the data (10 minutes)\nMerge data on election policy into data on voting (5 minutes, together),\nRecode the covariates, key predictors, and outcome for the study (10 minutes, partly together)\nRecreate Figure 1 (15 minutes)\nRecreate Figure 2 (15 minutes)\n\nFinally, we’ll take the weekly survey which should be a fun one\nOne of these 8 tasks will be randomly selected as the graded question for the lab.\n\nset.seed(3142024)\ngraded_question &lt;- sample(1:8,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 6 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/07-lab-comments.html#please-render-this-.qmd-file",
    "href": "labs/07-lab-comments.html#please-render-this-.qmd-file",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/07-lab-comments.html#load-packages",
    "href": "labs/07-lab-comments.html#load-packages",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nWe will also want to set our working directory to where your lab is saved."
  },
  {
    "objectID": "labs/07-lab-comments.html#important-set-your-working-directory",
    "href": "labs/07-lab-comments.html#important-set-your-working-directory",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "IMPORTANT: Set your working directory",
    "text": "IMPORTANT: Set your working directory\n\nOn the top panel of R Studio click:\n\n\nSession Session &gt; Set working directory &gt; Source file location\n\n\nPaste the output that shows up in your console into the code chunk below\n\n\n# Set working directory\n# Session &gt; Set working directory &gt; Source file location\n# paste output here:\n\nAll right, now let’s summarize the study"
  },
  {
    "objectID": "labs/07-lab-comments.html#recode-covariates",
    "href": "labs/07-lab-comments.html#recode-covariates",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.1 Recode covariates",
    "text": "6.1 Recode covariates\nThe CPS are messy data. Please run the code below to recode the covariates in the spirit of (i.e. with minor changes) what Grumbach and Hill did. 3\n\n\n\n\n\n\nNote\n\n\n\nThe file cps_00021.cbk.txt contains the codebook for the data, telling us what numeric values of each variable correspond to substantively. So if you’re wondering how I know what should be recoded to what specific values, it comes from reading the codebook, looking at Grumbach and Hill’s code, looking at the raw variable with a table, and the using case_when() to judiciously code the data. You’ll get practice doing this in your final projects, but I don’t want to spend too much time on this this lab, which is why you’re only recoding the outcome voted\n\n\n\n# Recode covariates\ncps %&gt;% \n  mutate(\n    # Useful for plotting figure 2\n    SDR = ifelse(sdr == 1, \"SDR\", \"non-SDR\"),\n    education = case_when(\n      educ == 1 ~ NA, #Blank\n      educ &lt; 40 ~ 1, # No high school\n      educ &gt;= 40 & educ &lt; 73 ~ 2, # Some high school\n      educ == 73 ~ 3, # High school degree\n      educ &gt;= 80 & educ &lt;= 110 ~ 4, # Some college\n      educ &gt;= 111 & educ &lt;123 ~ 5, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n      educ &gt;= 123 & educ &lt;=125 ~ 6, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n      educ == 999 ~ NA # Missing/unknown\n    ),\n    race_f = case_when(\n      race == 999 ~ NA,\n      T ~ factor(race)\n    ),\n    is_white = case_when(\n      race == 100 ~ 1,\n      race == 999 ~ NA,\n      T ~ 0\n    ),\n    is_black = case_when(\n      race == 200 ~ 1,\n      race == 999 ~ NA,\n      T ~ 0\n    ),\n    is_aapi = case_when(\n      race == 650 ~ 1,\n      race == 651 ~ 1,\n      race == 652 ~ 1,\n      race == 999 ~ NA,\n      T ~ 0\n    ),\n    is_other = case_when(\n      is_white == 1 ~ 0,\n      is_black == 1 ~ 0,\n      is_aapi ==  1 ~ 0,\n      race == 999 ~ NA,\n      T ~ 1\n    ),\n    income = case_when(\n      faminc &gt; 843 ~ NA, # Remove Missing/Refused\n      T ~ as.numeric(factor(faminc))\n    ),\n    is_female = case_when(\n      sex == 2 ~ 1,\n      sex == 1 ~ 0,\n      T ~ NA # recode Not in Universe as NA\n    )\n    \n  ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab-comments.html#create-age_group-and-age_group_xx_xx-indicators",
    "href": "labs/07-lab-comments.html#create-age_group-and-age_group_xx_xx-indicators",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.2 Create age_group and age_group_XX_XX indicators",
    "text": "6.2 Create age_group and age_group_XX_XX indicators\nNext we’ll create an age_group variable and binary indicators for each age cohort of the form age_group_XX_XX.\nPlease uncomment and run the code below\n\n# Create age variables \ncps %&gt;% \n  mutate(\n    age_group = case_when(\n      age &gt;= 18 & age &lt;= 24 ~ \"18-24\",\n      age &gt; 24 & age &lt;= 34  ~ \"25-34\",\n      age &gt; 34 & age &lt;= 44  ~ \"35-44\",\n      age &gt; 44 & age &lt;= 54  ~ \"45-54\",\n      age &gt; 54 & age &lt;= 64  ~ \"55-64\",\n      age &gt; 64 ~ \"65+\",\n      T ~ NA\n\n    ),\n    age_group_18_24 = ifelse(age_group == \"18-24\", 1, 0),\n    age_group_25_34 = ifelse(age_group == \"25-34\", 1, 0),\n    age_group_35_44 = ifelse(age_group == \"35-24\", 1, 0),\n    age_group_45_54 = ifelse(age_group == \"45-24\", 1, 0),\n    age_group_55_64 = ifelse(age_group == \"55-24\", 1, 0),\n    age_group_65plus = ifelse(age_group == \"65+\", 1, 0)\n  ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab-comments.html#check-age-recodes",
    "href": "labs/07-lab-comments.html#check-age-recodes",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.3 Check age recodes",
    "text": "6.3 Check age recodes\nIt’s good practice when recoding, to check the output. Please use the table() to create a crosstab of age_group and age_group_18_24.\n\n#|label: checkage\n\n# Compare age_group to age_group_18_24 using table()\ntable(cps$age_group, cps$age_group_18_24)\n\n       \n             0      1\n  18-24      0 274383\n  25-34 416613      0\n  35-44 399527      0\n  45-54 353672      0\n  55-64 296342      0\n  65+   372517      0\n\n\nExplain in words how the variable age_group_18_24 relates to the variable age_group age_group is a categorical variable which describes the age cohort that respondent to the CPS belongs to. age_group_18_24 is a binary indicator variable that takes a value 1 for respondents who belong to the cohort of people 18-24 years old and 0 otherwise."
  },
  {
    "objectID": "labs/07-lab-comments.html#recode-the-outcome",
    "href": "labs/07-lab-comments.html#recode-the-outcome",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.4 Recode the outcome",
    "text": "6.4 Recode the outcome\nNow it’s your turn. Please do the following:\n\nLook at the variable voted using the table() function\n\n1 corresponds to Did not vote\n`2 corresponds to Voted\n96,97,98 to people who didn’t provide and answer, or didn’t remember\n99 corresponds to people who shouldn’t be in the sample (“Not in universe”)\n\nCreate a new variable called dv_voted using case_when() inside of mutate() that is:\n\n1 when voted == 2\n0 when voted == 1,\n0 when voted &gt; 2 & voted &lt;99\nNA when voted == 99\n\n\n\n# Look at distribution of voted using table()\ntable(cps$voted)\n\n\n      1       2      96      97      98      99 \n 704510 1103606   14257   38897  127231  881162 \n\n# Create variable dv_voted using mutate(), case_when(), and voted variable\ncps %&gt;% \n  mutate(\n    dv_voted = case_when(\n      voted == 2 ~ 1,\n      voted == 1 ~ 0,\n      voted &gt; 2 & voted &lt; 99 ~ 0,\n      voted == 99 ~ NA\n    )\n  ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab-comments.html#save-the-recoded-data",
    "href": "labs/07-lab-comments.html#save-the-recoded-data",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.5 Save the recoded data",
    "text": "6.5 Save the recoded data\nFinally, let’s save our recoded data to file called cps_clean.rda that we can use for next week’s lab\nUncomment and run the following:\n\n# save(cps, file = \"cps_clean.rda\")"
  },
  {
    "objectID": "labs/07-lab-comments.html#write-down-aesthetic-mappings-from-the-figure",
    "href": "labs/07-lab-comments.html#write-down-aesthetic-mappings-from-the-figure",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.1 Write down aesthetic mappings from the figure:",
    "text": "7.1 Write down aesthetic mappings from the figure:\nBefore we create this figure, think about the information conveyed by the figure’s aesthetics (the x axis, the y axis, the color of the squares), and the corresponding columns from policy_data that contain this information.\n\nx-axis: year\ny-axis: st\ncol: sdr"
  },
  {
    "objectID": "labs/07-lab-comments.html#create-a-variable-called-sdr",
    "href": "labs/07-lab-comments.html#create-a-variable-called-sdr",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.2 Create a variable called SDR",
    "text": "7.2 Create a variable called SDR\nIt will be helpful to have a variable called SDR in policy_data that takes the value of “SDR” when sdr == 1 and “non-SDR” when sdr == 0\nPlease use case_when() or ifelse() to create SDR in policy_data\n\n# Create a variable called SDR in policy_data\npolicy_data %&gt;% \n  mutate(\n    SDR = ifelse(sdr == 1, \"SDR\", \"non-SDR\")\n  ) -&gt; policy_data"
  },
  {
    "objectID": "labs/07-lab-comments.html#recreate-figure-1-1",
    "href": "labs/07-lab-comments.html#recreate-figure-1-1",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.3 Recreate Figure 1",
    "text": "7.3 Recreate Figure 1\nRecall, we need three things to make a figure:\n\ndata\naesthetics\ngeometries\n\nUsing data from policy_data starting in 1978 (hint add a filter()) and the aesthetic mappings identified above use ggplot() with the geom_point() geometry to make a version of Figure 1 from paper.\n\n# Recreate Figure 1\npolicy_data %&gt;% \n  filter(year &gt;= 1978) %&gt;% \n  ggplot(aes(year, st,\n             col = SDR)) +\n  geom_point() -&gt; fig1\n\nfig1"
  },
  {
    "objectID": "labs/07-lab-comments.html#interpret-figure-1.",
    "href": "labs/07-lab-comments.html#interpret-figure-1.",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.4 Interpret Figure 1.",
    "text": "7.4 Interpret Figure 1.\nPlease answer the following questions:\n\nHow many states had Same Day Registration at some point in time? 19 states\nHow many states had Same Day Registration in 2018? 18 states had SDR in 2018\nDid any states get rid of Same Day Registration? When did they get rid of this policy? Yes, Ohio in 2014\nWhat’s up with North Dakota? North Dakota doesn’t require registration to vote. As per footnote 5, they’re excluded from the main analysis.\n\nUse this code chunk to write any code that might help you answer these questions\n\n# Write code to help you answer the questions above (if needed)\n# 19 states had SDR at some point\nlength(unique(policy_data$state[policy_data$sdr==1]))\n\n[1] 19\n\nlength(unique(policy_data$state[policy_data$sdr==1 & policy_data$ year == 2018])) \n\n[1] 18\n\nwith(policy_data %&gt;% \n       filter(state==\"Ohio\"),\n     table(year,sdr))\n\n      sdr\nyear   0 1\n  1970 1 0\n  1971 1 0\n  1972 1 0\n  1973 1 0\n  1974 1 0\n  1975 1 0\n  1976 1 0\n  1977 1 0\n  1978 1 0\n  1979 1 0\n  1980 1 0\n  1981 1 0\n  1982 1 0\n  1983 1 0\n  1984 1 0\n  1985 1 0\n  1986 1 0\n  1987 1 0\n  1988 1 0\n  1989 1 0\n  1990 1 0\n  1991 1 0\n  1992 1 0\n  1993 1 0\n  1994 1 0\n  1995 1 0\n  1996 1 0\n  1997 1 0\n  1998 1 0\n  1999 1 0\n  2000 1 0\n  2001 1 0\n  2002 1 0\n  2003 1 0\n  2004 1 0\n  2005 0 1\n  2006 0 1\n  2007 0 1\n  2008 0 1\n  2009 0 1\n  2010 0 1\n  2011 0 1\n  2012 0 1\n  2013 0 1\n  2014 1 0\n  2015 1 0\n  2016 1 0\n  2017 1 0\n  2018 1 0"
  },
  {
    "objectID": "labs/07-lab-comments.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "href": "labs/07-lab-comments.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.1 Calculate the proption voting by age group and SDR",
    "text": "8.1 Calculate the proption voting by age group and SDR\nWith the cps data, use group_by() and summarize() to calculate the proportion of people voting by age group in states that did and did not have same day registration in the code chunk below.\nSave the results to a new object called fig2_df\n\n#  Calculate the proportion of voting by age group and SDR\ncps %&gt;% \n  group_by(age_group, SDR) %&gt;% \n  summarise(\n    prop_vote = mean(dv_voted, na.rm=T)\n  ) -&gt; fig2_df\nfig2_df\n\n# A tibble: 14 × 3\n# Groups:   age_group [7]\n   age_group SDR     prop_vote\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1 18-24     SDR         0.386\n 2 18-24     non-SDR     0.315\n 3 25-34     SDR         0.509\n 4 25-34     non-SDR     0.452\n 5 35-44     SDR         0.604\n 6 35-44     non-SDR     0.561\n 7 45-54     SDR         0.658\n 8 45-54     non-SDR     0.617\n 9 55-64     SDR         0.705\n10 55-64     non-SDR     0.666\n11 65+       SDR         0.715\n12 65+       non-SDR     0.660\n13 &lt;NA&gt;      SDR         0    \n14 &lt;NA&gt;      non-SDR     0"
  },
  {
    "objectID": "labs/07-lab-comments.html#recreate-figure-2-1",
    "href": "labs/07-lab-comments.html#recreate-figure-2-1",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.2 Recreate Figure 2",
    "text": "8.2 Recreate Figure 2\nUsing fig2_df recreate a Figure 2 from the paper:\n\nfilter out values of age_group that are NA\nset the appropriate aesthetic mappings in ggplot()\nuse geom_bar(stat = “identity”, position = “dodge”)\n\n\n#Recreate Figure 2\nfig2_df %&gt;% \n  filter(!is.na(age_group)) %&gt;% \n  ggplot(aes(age_group,prop_vote,fill = SDR))+\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") -&gt; fig2\n\nfig2"
  },
  {
    "objectID": "labs/07-lab-comments.html#interpret-figure-2",
    "href": "labs/07-lab-comments.html#interpret-figure-2",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.3 Interpret Figure 2",
    "text": "8.3 Interpret Figure 2\nWhat does Figure 2 tell us? Figure 2 provides initial support for Grumbach and Hill’s general argument. The proportion of people voting in states with SDR is higher than states with out SDR, and this gap is larger among younger age cohorts."
  },
  {
    "objectID": "labs/07-lab-comments.html#footnotes",
    "href": "labs/07-lab-comments.html#footnotes",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey also estimate models using both individual and aggregate data, for reasons a bit beyond the scope of this course↩︎\nThe CPS coding on this is not great and there’s no measure of ethnicity in these data. Forgive the crude indicators, but their necessary to recreate some of Grumbach and Hill’s analysis next week. ↩︎\nNote the way the recoding is described in the appendix to the paper is not how it is actually implemented in the replication code in rock_the_reg_replication_code.R. For example, the appendix describes income as ranging from 1 (Under $10k) to 16 ($500k and above), when their code, implemented above produces 32 unique values, in part because the way the CPS asked and coded the income question changed overtime. We’re going to roll with it for now…↩︎"
  },
  {
    "objectID": "assignments/a2.html",
    "href": "assignments/a2.html",
    "title": "POLS 1600: Research Topics",
    "section": "",
    "text": "For this assignment, please upload to Canvas an html file named a2_group0X_data.html produced from an Quarto Markdown file called a2_group0X_data.qmd (changing group0X to your group’s number group0X) that contains the following:\n\nA revised description of your group’s research project\nA description of a linear model implied by your question\nR code that loads some potentially relevant data to your question and at least one descriptive summary of that data.\nSome information about your group such as:\n\nA group name1\nA group color or color scheme\nA group motto, mascot, crest, etc.\nYour group’s theme song\nYour group’s astrological sign\nAnything else that you think well help you form strong ingroup bounds that facilitate collaboration\n\n\nYour files (.qmd file, .html file, and maybe a data file2) should be saved in the shared Google drive folder Group_XX for your group.\n\n\n\n\n\n\nNote\n\n\n\nGoogle is great for sharing files but not so great for version control. When working on this assignment, please Coordinate with your group so only ONE PERSON is editing the file at a given time",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#gathering-data-from-existing-data-sets",
    "href": "assignments/a2.html#gathering-data-from-existing-data-sets",
    "title": "POLS 1600: Research Topics",
    "section": "Gathering Data from Existing Data Sets",
    "text": "Gathering Data from Existing Data Sets\nBelow are some common data sets in American, Comparative and International Relations. Again your question doesn’t have to be political and you can look elsewhere for data – (the U.S. Census, Data.gov).",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#american-politics",
    "href": "assignments/a2.html#american-politics",
    "title": "POLS 1600: Research Topics",
    "section": "American Politics",
    "text": "American Politics\n\nThe American National Elections Studies (NES)\n\nhttp://www.electionstudies.org/\n\nCCES\n\nhttp://projects.iq.harvard.edu/cces/home\n\nGeneral Social Survey\n\nhttp://www3.norc.org/GSS+Website/\n\nRoper Center\n\nhttps://ropercenter.cornell.edu//\n\nPew\n\nhttp://www.pewresearch.org/data/",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#comparative-politicsinternational-relations",
    "href": "assignments/a2.html#comparative-politicsinternational-relations",
    "title": "POLS 1600: Research Topics",
    "section": "Comparative Politics/International Relations",
    "text": "Comparative Politics/International Relations\n\nWorld Values Survey\n\nhttp://www.worldvaluessurvey.org/wvs.jsp\n\nEurobarometer\n\nhttp://ec.europa.eu/public_opinion/index_en.htm\n\nLatinobarometer\n\nhttp://www.latinobarometro.org/lat.jsp\n\nOECD\n\nhttps://data.oecd.org/\n\nQuality of Government Data (I love this)\n\nhttp://qog.pol.gu.se/data\n\nUppsala Conflict Data (I hate this8)\n\nhttp://www.pcr.uu.se/research/UCDP/\n\nThreat and imposition of sanctions\n\nhttp://www.unc.edu/~bapat/TIES.htm\n\nCorrelates of War (Also hate this)\n\nhttp://www.correlatesofwar.org/",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#general-research",
    "href": "assignments/a2.html#general-research",
    "title": "POLS 1600: Research Topics",
    "section": "General Research",
    "text": "General Research\n\nHarvard Dataverse (Good for finding replication data from published studies)\n\nhttps://dataverse.harvard.edu/\n\nICPSR (Same)\n\nhttps://www.icpsr.umich.edu/icpsrweb/landing.jsp",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#footnotes",
    "href": "assignments/a2.html#footnotes",
    "title": "POLS 1600: Research Topics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re Group 01 don’t change your name to Group 4↩︎\nPlease don’t upload terabytes of data though.↩︎\nThat is, don’t worry about whether your question is causally identified.↩︎\nThis is a common explanation for socio-economic differences in political participation.↩︎\nAlso known as a dummy variable, or binary variable because it only takes values of 0 or 1↩︎\nYou’d probably also want to some clarity about how you’re treating Independents and people who say they’re Independents but when asked if they lean toward one party or the other. Typically, these partisan leaners tend to behave like partisans.↩︎\nAssuming for simplicity that we’re excluding independents from our analysis…↩︎\nBasically conflict data is real pain to work with, and it’s not always clear what statitistical inference means for these type of questions. But if your group is desperate to study these types of questions we can talk. I’d recommend trying to replicate and extend someone else’s analysis of these data, rather than starting from scratch↩︎",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a1.html",
    "href": "assignments/a1.html",
    "title": "POLS 1600: Research Topics",
    "section": "",
    "text": "Asking a good research question is one of the most important skills you will develop in your academic careers. It’s also one of the hardest.\nWe often think we’re asking one question, when in fact the study we conduct really addresses a related but distinct question. When a priest asked Willie Sutton why he robbed banks, he replied the “Well, that’s where the money is”. The priest’s question was about why rob at all, while Sutton answered the different question “Given one robs, why rob banks?” Similarly, Medieval philosophers might ask why objects stay in motion, while Newton suggests what really need is not an explanation of motion itself but of changes in motion.\nThe object of our question shapes the form of our explanation.\nIn this assignment, I would like your group to craft three potential research questions that we might explore in our research project for this class. Each question, should be a single sentence, with a few sentences answering the following questions (More details below):\n\nWhy do we care about the answer to this research question?\nWhat’s would a hypothetical “ideal experiment” to answer this question look like?\nWhat would a study with observational data look like?\nA published study that relates to this question\nHow feasible would it be to do a study like this for the course\n\nYou may use this Rmd file as a template (click here to download) or create your own file. Please submit your responses to Canvas.\nYou might start by writing down several questions of different forms about the same topic:\n\nWhy do people vote?\nWhy do people not vote?\nWhy do the rich vote at higher rates than the poor?\nWhen might people who don’t vote, be motivated to vote?\nWhat is the effect of encouraging someone to vote via a phone call?\nAre phone calls more or less effective than in-person contact for get-out the vote efforts?\n\nEach of these questions addresses a general topic that political scientists seem to think is important. Each carries some suppositions and assumptions that in turn influence the type of explanation we might find convincing. Why do people vote feels a bit broad to me. People probably vote for many reasons. How can we hope to adjudicate between all the possible reasons for voting? (Further are these the same reasons for not voting or do we need another set of explanations altogether?)\nWhether phone calls are more or less effective than in-person contacts for GOTV efforts seems more tractable, but also perhaps to narrow. Do we really care? If we’re confident we can identify an effect or difference in one study, are we sure we’ll see similar effects in a different study conducted under different circumstances?\nIn crafting your research questions, you want to strike a balance between things we actually care about (why do people vote) and things we can actually assess (what’s effect of a particularly type of encouragement to vote). A few thoughts on this process:\n\n“Why” questions tend to be more compelling than “What” or “How” or “Do” questions, I think in part because “why” questions often imply a theory and suggest a counterfactual (why this and not that), while other ways of asking questions feel more descriptive. For example, why do the rich vote at higher rates than the poor. Well, one explanation may be that their relative social and economic status means they are more likely to be targets of mobilization efforts by campaigns (among many things). So a natural follow up to this larger question might be, what’s the effect of providing similar mobilization efforts to the poor. Would they vote at similar rates to the rich? If so, then we’ve learned something about how mobilization explains class differences in participation.\nThinking about questions in terms of puzzles is another useful trick. Why do parties exist when politicians’ ideological preferences can explain the vast majority of their legislative behavior? Note this type of question contains a lot of presuppositions (how do we measure ideological preferences? Do they really explain legislative behavior? Is that what we care about?), but as point of departure for a study these type arguments can be useful\nTry to be simple and clear. Don’t worry about asking the perfect question right away. Your questions can and should evolve over time, and I suspect some of you will write a paper that has nothing to do with the questions you posed here.\n\nFor each question, please discuss the following:\n\nWhy do we care? Why we should care about the answer to this question. A strong justification is often that existing theories yield conflicting predictions and so your study will offer some insight into how to adjudicate betweeen these theories. A less strong justification is that no one has ever studied this before. Even if this is true (and it’s often not) it may be true for good reason. No need for formal citations, but if there are specific theories or claims your addressing feel free to name names.\nThe ideal experiment Please describe an “ideal” experiment that you could run that would give you some purchase on your question. Note the key feature of an experiment, is that you the researcher are able to manipulate (through random assignment) some facet of the world. Assume money, resources, physics, and even ethics are not an object. If you could randomly assign anything, what would you manipulate. At what level of analysis would your manipulation occur (i.e. are your units of analysis individuals or countries or something else). How would you measure your outcome, again assuming you were all power and all-seeing. If that manipulation isn’t feasible, what does that say about the ability to make a causal claim about your question?\nThe observational study Finally, considering some of the potential limitations that might prevent you from implementing your ideal experiment (it’s hard to randomly assign democratic government), what is one way you might address your research question with observational data. Would your study use cross-sectional or longitudinal data. What are some of the concerns (selection on observables) that arise in this setting. Is there a natural experiment or some sort of discontinuity you might leverage to approximate this experimental ideal.\n\nAgain, each paragraph should be brief and to the point. No need to specify a full research design–just give me the broad strokes. You’re writing for each question should not exceed a page.\nAfter you’ve thought through how you might go about answering your question, please find\n\nA published study that relates to this question. It need not be exactly your question as posed, but it should be in a similar area. Include a full citation, and link to the study. Then in a paragraph sentences try to summarize:\n\n\nThe study’s research question\nEmprical design\nCore findings.\n\nFinally on a scale of 1 (least feasible) to 10 (most feasible), please evaluate how likely you think it is you could write an empirical paper on this question for this course.\nDon’t worry about getting everything right. Your final projects can, will, and probably should change. The point of this exercise is to get some practice thinking about questions that interest you in the language of causal inference and potential outcomes.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care",
    "href": "assignments/a1.html#why-do-we-care",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment",
    "href": "assignments/a1.html#the-ideal-experiment",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study",
    "href": "assignments/a1.html#the-observational-study",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study",
    "href": "assignments/a1.html#a-published-study",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10",
    "href": "assignments/a1.html#feasibility-x10",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care-1",
    "href": "assignments/a1.html#why-do-we-care-1",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment-1",
    "href": "assignments/a1.html#the-ideal-experiment-1",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study-1",
    "href": "assignments/a1.html#the-observational-study-1",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study-1",
    "href": "assignments/a1.html#a-published-study-1",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10-1",
    "href": "assignments/a1.html#feasibility-x10-1",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care-2",
    "href": "assignments/a1.html#why-do-we-care-2",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment-2",
    "href": "assignments/a1.html#the-ideal-experiment-2",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study-2",
    "href": "assignments/a1.html#the-observational-study-2",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study-2",
    "href": "assignments/a1.html#a-published-study-2",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10-2",
    "href": "assignments/a1.html#feasibility-x10-2",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a4.html",
    "href": "assignments/a4.html",
    "title": "A4: Project Drafts",
    "section": "",
    "text": "You final will paper consists of seven sections. There is no minimum (or maximum) length for the final paper, but I’ve included rough estimates in parentheses of how long I think each section should be along with the relative contribution of each section to your final grade.\nSpecific guidance, expectations, and grading criteria for each section are provided below.\nYou need not have every section completely finished in this draft, but the more you have done the more feedback you will get. Remember, the data, design, and results section make up 70 percent of your grade, so while a good introduction, compelling theory, and thoughtful conclusion are important, you should budget your time accordingly.\nYour actual grade for the draft will reflect a general assessment of the status of your project\nAdditionally, I will assign points to each section as if it were you final paper to give you a sense of where to focus your efforts. This is not your actual grade for this assignment, but can be though of as a rough estimate of the grade you would receive on a section if you were to submit it, as is, for your final paper. Your grade for that section on the final paper may increase or decrease depending on the changes you make to your draft. Additionally, I will assign points to each section as if it were you final paper to give you a sense of where to focus your efforts. This is not your actual grade for this assignment, but can be though of as a rough estimate of the grade you would receive on a section if you were to submit it, as is, for your final paper. Your grade for that section on the final paper may increase or decrease depending on the changes you make to your draft.\nI have also posted a  template for the final paper and an example of a final paper from a previous version of POLS 1600.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-5-points",
    "href": "assignments/a4.html#grading-criteria-5-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (5 points)",
    "text": "Grading Criteria (5 points)\n\n5 points\n\nArticulates clear and compelling research question\nOffers a strong motivation for pursuing this question tied to existing social science theory\nDescribes a convincing empirical strategy with appropriate data and research design\nPreviews interesting results with substantive interpretation\nMakes reader excited to read the rest of the paper\nWriting throughout is clear, precise, and engaging, with correct spelling and grammar and consistent citations\n\n4 points\n\nArticulates research question\nLink between research question and social science theory could be more developed\nDescribes empirical strategy with data and research design\nPreview of results with little to no interpretation\nLeaves reader moderately interested in the rest of the paper.\nWriting throughout is clear and precise, with correct spelling and grammar and consistent citations\n\n&lt; 4 points\n\nResearch question unclear or absent\nLittle to no discussion of why this research question is of interest to social scientists and citizens\nLittle to no discussion of the data and research design used to pursue this question.\nNo discussion of empirical results\nDoes little to pique reader’s interest in the rest of the paper.\nWriting throughout is muddled and vague, with uncorrekt spellung and grammur and consistent citations",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-10-points",
    "href": "assignments/a4.html#grading-criteria-10-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (10 points)",
    "text": "Grading Criteria (10 points)\n\n9-10 points:\n\nCareful discussion of relevant theoretical, empirical, and/or substantive contexts that motivates research question.\nCites at least three relevant pieces of academic literature on research topic. Uses this literature to articulate theoretical framework for thinking about research question.\nTheoretical framework clearly defines:\nthe outcome (or outcomes) of interest\nthe factors expected to explain variation in that outcome\nsome potential alternative explanations/omitted variables, that might explain variation in both the outcome and predictors\nDevelops a clear set of expectations informed by theoretical framework for how these explanatory factors should relate to the outcome (and possibly each other)\n\n8 points:\n\nBrief discussion of theoretical, empirical, and/or substantive contexts that motivates research question.\nCites less than three relevant pieces of academic literature on research topic. Uses this literature to articulate theoretical framework for thinking about research question.\nTheoretical framework broadly defines:\nthe outcome (or outcomes) of interest\nthe factors expected to explain variation in that outcome\nsome potential alternative explanations/omitted variables, that might explain variation in both the outcome and predictors\nDevelops a set of expectations for how these explanatory factors should relate to the outcome (and possibly each other, but connection to larger theoretical framework is unclear\n\n&lt;8 points:\n\nLittle to no discussion of theoretical, empirical, and/or substantive contexts that motivates research question.\nNo citation of relevant pieces of academic literature on research topic. Broader theoretical framework absent.\nFails to clearly define:\nthe outcome (or outcomes) of interest\nthe factors expected to explain variation in that outcome\nsome potential alternative explanations/omitted variables, that might explain variation in both the outcome and predictors\nFails to articulate a clear set of expectations for how these explanatory factors should relate to the outcome",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-20-points",
    "href": "assignments/a4.html#grading-criteria-20-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (20 points)",
    "text": "Grading Criteria (20 points)\n\n18-20 points:\n\nData sources are clear and appropriate\nUnit of analysis is clear and appropriate\nOutcomes, key predictors, and covariates are clearly defined and carefully described\nDescriptive tables and figures are thoroughly discussed in text with correct interpretations\nData appear to be clean and tidy\n\n16-17 points:\n\nData sources are clear and appropriate\nUnit of analysis is unclear\nOutcomes, key predictors, and covariates are briefly defined and described\nDescriptive tables and figures are briefly discussed in text with mostly correct interpretations\nData appear to be clean and tidy\n\n&lt; 16 points:\n\nData sources are not identified\nUnit of analysis is is not discussed or incorrect\nOutcomes, key predictors, and covariates are not defined and described\nDescriptive tables and figures absent, not discussed or interpreted incorrectly\nClear errors in coding (e.g. failing to recode values as NA)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-30-points",
    "href": "assignments/a4.html#grading-criteria-30-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (30 points)",
    "text": "Grading Criteria (30 points)\n\n23-25 points:\n\nCorrectly specifies multiple models to test empirical expectations\nExpresses empirical expectations in terms of coefficients from linear models\nExplains motivation and interpretation of specified models\nConveys a clear understanding of\n\nIdentifying assumptions of empirical design\nWhat linear regression is\nWhat it means to control for variables\nHow to interpret coefficients from regression\nHow to quantify uncertainty around regression estimates, using both confidence intervals and hypothesis tests\n\n\n20-22 points:\n\nCorrectly specifies two models to test empirical expectations\nExpresses empirical expectations in terms of coefficients from linear models\nExplains motivation and interpretation of specified models\nConveys some understanding of\n\nIdentifying assumptions of empirical design\nWhat linear regression is\nWhat it means to control for variables\nHow to interpret coefficients from regression\nHow to quantify uncertainty around regression estimates, using both confidence intervals and hypothesis tests\n\n\n&lt; 20 points:\n\nIncorrectly specifies one models to test empirical expectations\nDoes not express empirical expectations in terms of coefficients from linear models\nLittle to no discussion of the motivation and interpretation of specified models\nConveys little to no understanding of\n\nIdentifying assumptions of empirical design\nWhat linear regression is\nWhat it means to control for variables\nHow to interpret coefficients from regression\nHow to quantify uncertainty around regression estimates, using both confidence intervals and hypothesis tests",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-25-points",
    "href": "assignments/a4.html#grading-criteria-25-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (25 points)",
    "text": "Grading Criteria (25 points)\n\n23-25 points:\n\nEmpirical results are\n\nclearly presented in tables and figures appropriate variable names and labels\ncorrectly interpreted in terms of substantive and statistical significance\nused to evaluate theoretical expectations\ninterpreted to demonstrate mastery of the concepts of\n\nbivariate and multiple regression\nconfidence intervals\nhypothesis testing\n\ntell a coherent and compelling story that provides insights into research question\n\n\n20-22 points:\n\nEmpirical results are\n\nroughly presented in tables and figures with awkward variable names and labels\ninterpreted mostly correctly in terms of substantive and statistical significance\nused to evaluate theoretical expectations\ninterpreted to demonstrate general understanding of the concepts of\n\nbivariate and multiple regression\nconfidence intervals\nhypothesis testing\n\ntell a coherent story that provides partial insights into research question\n\n\n&lt; 20 points:\n\nEmpirical results are\n\npoorly presented in tables and figures with awkward variable names and labels\ninterpreted mostly incorrectly in terms of substantive and statistical significance\nare not used to evaluate theoretical expectations\ninterpreted to demonstrate little to no understanding of the concepts of\n\nbivariate and multiple regression\nconfidence intervals\nhypothesis testing\n\nfail to tell a coherent story that provides little to no insights into research question",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-25-points-1",
    "href": "assignments/a4.html#grading-criteria-25-points-1",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (25 points)",
    "text": "Grading Criteria (25 points)\n\n5 points\n\nClear summary of the project’s findings\nDiscussion of the strengths and weaknesses of the project\nInteresting suggestions for further research that seem likely to add additional insights and/or address limitations of the present study.\nCorrectly formatted references/works cited\n\n4 points\n\nCursory summary of the project’s findings\nDiscussion of the strengths and weaknesses of the project\nSome suggestions for further research informed by the present project\nCorrectly formatted references/works cited\n\n&lt; 4 points\n\nLittle to no summary of the project’s findings\nLittle to no discussion of the strengths and weaknesses of the project\nNo suggestions for further research informed by the present project\nNo references/works cited",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-25-points-2",
    "href": "assignments/a4.html#grading-criteria-25-points-2",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (25 points)",
    "text": "Grading Criteria (25 points)\n\n10 points\n\nCodebook is\n\neasy to read and informative\ndescribes all the variables used in the analysis.\n\nCode appendix is\n\nnicely formatted\nwell-commented\nable to to reproduces all the analysis in text without errors\n\nNo R code, raw output, error messages appears in the main text of the paper.\n\n8-9 points\n\nCodebook is\n\ndescribes all of the variables used in the analysis.\n\nCode appendix:\n\nCould be more cleanly formatted and needs more comments\nable to reproduces all the analysis in text without errors or only minor errors\n\nNo R code, raw output, error messages appears in the main text of the paper.\n\n&lt; 8 points\n\nCodebook is absent, incomplete, and/or incorrect\nCode appendix is:\n\nPoorly formatted\nContains no explanatory comments\nunable to reproduce most of the analysis in the text.\nContains clear errors\n\nR code, raw R output, error messages appear throughout the main text of the paper.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "You have three types of assignments in this course",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#labs",
    "href": "assignments/index.html#labs",
    "title": "Assignments",
    "section": "Labs",
    "text": "Labs\n\nEach Thursday we will work in groups to complete an in-class lab\nThe labs are designed to reinforce and extend concepts from lecture using real world data.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#tutorials",
    "href": "assignments/index.html#tutorials",
    "title": "Assignments",
    "section": "Tutorials",
    "text": "Tutorials\nCoding tutorials to reinforce concepts from lecture and textbook.\nAccessed by running in the console of R Studio\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nComplete the tutorial. Save output as “LASTNAME_TutorialNumber.html”\nUpload output to Canvas by Friday by 11:59 pm",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#final-project",
    "href": "assignments/index.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\nFinally, throughout the semester you will be have periodic assignments to ensure that you’re making progress on your final projects.\n\nWeek 4:  Research Topics\nWeek 6:  Potential Data\nWeek 8:  Explortatory Analysis\nWeek 11: Draft\nWeek 12: Presentations\nWeek 13: Final Paper",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#motivating-example",
    "href": "assignments/pols1600_presentation_template.html#motivating-example",
    "title": "POLS 1600",
    "section": "Motivating Example",
    "text": "Motivating Example\nProvide an image or anecdote that can help your audience understand what they’re going to learn about"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#research-question",
    "href": "assignments/pols1600_presentation_template.html#research-question",
    "title": "POLS 1600",
    "section": "Research Question",
    "text": "Research Question\nProvide a concise summary of your project’s core question(s)"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#theory",
    "href": "assignments/pols1600_presentation_template.html#theory",
    "title": "POLS 1600",
    "section": "Theory",
    "text": "Theory\n\nUse this section to explain how we should think about how to answer your research question\nWhat explains variation in your outcome?"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#expectations",
    "href": "assignments/pols1600_presentation_template.html#expectations",
    "title": "POLS 1600",
    "section": "Expectations",
    "text": "Expectations\n\nTranslate your theory into a set of testable empirical implications"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#data",
    "href": "assignments/pols1600_presentation_template.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nSummarize your data:\n\nSource(s)\n\nUnit of analysis\nNumber of observations (periods of observation)\n\nMeasures of:\n\nOutcomes\nKey Predictors\nCovariates"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#descriptive-summary",
    "href": "assignments/pols1600_presentation_template.html#descriptive-summary",
    "title": "POLS 1600",
    "section": "Descriptive Summary",
    "text": "Descriptive Summary\n\nProvide a figure or table that provides a nice overview or insight into your data"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#design",
    "href": "assignments/pols1600_presentation_template.html#design",
    "title": "POLS 1600",
    "section": "Design",
    "text": "Design\nDescribe your empirical design\n\nPresent at least one of the models you estimate\n\n\\[\n\\text{Outcome} = \\beta_0 + \\beta_1 \\text{predictor}\n\\]\n\nExplain to your audience what your theory implies for the sign, size, and signficance of the coefficients"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#results",
    "href": "assignments/pols1600_presentation_template.html#results",
    "title": "POLS 1600",
    "section": "Results",
    "text": "Results\n\nSummaryTableFigure\n\n\n\nSummarize the results of your analysis\nInterpret the coefficients from your models in terms of the predictions of your theory\n\n\n\nPut a screen shot of a regression table here\n\nOr you could generate the table by including your paper’s code and analysis in these slides\n\n\n\nPut a figure here"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#conclusion",
    "href": "assignments/pols1600_presentation_template.html#conclusion",
    "title": "POLS 1600",
    "section": "Conclusion",
    "text": "Conclusion\n\nRestate your research question\nSummarize your answer\nOffer some discussion of:\n\nImplications\nLimitations\nDirections for future research"
  },
  {
    "objectID": "assignments/pols1600_presentation_template.html#appendix",
    "href": "assignments/pols1600_presentation_template.html#appendix",
    "title": "POLS 1600",
    "section": "Appendix",
    "text": "Appendix\n\nAdditional slides go here"
  },
  {
    "objectID": "resources/00-software-setup.html#tldr",
    "href": "resources/00-software-setup.html#tldr",
    "title": "Getting started with R",
    "section": "TLDR",
    "text": "TLDR\nIf you’re pressed for time here’s the short version of what you should do before next class.\n\nUpdate your operating system\nDownload R\nDownload R Studio\nOpen R Studio on your computer\nCreate an Quarto Markdown Document (.qmd) in R Studio\n\n\nFile &gt; New File &gt; Quarto Document\nInstall any requested packages\n\n\nRender this .Qmd into an html file.\n\n\nClick the render arrow with the needle OR:\nUse the hot keys: Mac: cmd + shift + k PC: crtl + shift + k\n\n\nInstall some additional packages for the course\n\nSpecifically copy and paste the following into the console in R Studio and hit enter:\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nYou’ll need to have the package rmarkdown installed for\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\nTo work, so make sure you’ve copied and pasted the code above into your console\n\nIf you have a question or somethings not working, don’t hesitate to ask. Please email me at paul_testa@brown.edu or come to my office at 111 Thayer St Rm 339."
  },
  {
    "objectID": "resources/00-software-setup.html#install-the-devtools-and-remotes-packages",
    "href": "resources/00-software-setup.html#install-the-devtools-and-remotes-packages",
    "title": "Getting started with R",
    "section": "7.1 Install the devtools and remotes packages",
    "text": "7.1 Install the devtools and remotes packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers.\nTo get the most out of R, you’ll need to install add-on packages, which are user-written to extend/expand your R capabilities.\nPackages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE).\nAlternatively, they may be available via the software sharing platform GitHub.\n\nTo download these packages, you first need to install the devtools and remotes packages.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n\nPlace your cursor in the console (lower left panel), and copy and paste each line of code above. After you’ve pasted a line, hit Enter/Return and R will execute (run) that line of code. So type:\n\ninstall.packages(“devtools”)\n\nHit enter.\nThen type\n\ninstall.packages(“remotes”)\n\nAnd hit enter again.\nEach time, R will likely spit out some cryptic red text as it installs the packages.\nWhen it’s done, R will you should see a line with a single &gt; in the console.\nYou should be able to see the newly installed packages by scrolling through or searching the Packages pane on the bottom left"
  },
  {
    "objectID": "resources/00-software-setup.html#install-packages-for-course",
    "href": "resources/00-software-setup.html#install-packages-for-course",
    "title": "Getting started with R",
    "section": "7.2 Install Packages for Course",
    "text": "7.2 Install Packages for Course\nNow we’ll use the intall_github() function from the remotes package, to install some packages we’ll use for this course.\nAgain, copy and paste each line of code into your console, and hit Enter/Return to run that code.\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nWe’ll go over this during our next meeting so don’t worry if this doesn’t work\nYou will likely be asked to update some existing packages\nType 1 in the console and hit enter\n\nIn particular, we’ll be using a version of Matthew Blackwell’s qsslearnr as problem sets for this course.\nYou can see the available problem sets by running the following code in your console:\n\nlearnr::run_tutorial(package = \"qsslearnr\")\n\nAnd start a tutorial by running:\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nTo try and explain in words what this code is doing:\n\nlearnr::run_tutorial( Says use the run_tutorial() from the learnr package\n\"00-intro\" tells run_tutorial() to run the \"00-intro\" tutorial\npackage = \"qsslearnr\" tells run_tutorial() to look for this tutorial in the qsslearnr package.\n\nIf you run this code, you should see the following tutorial show up in the upper right panel:"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-adding-a-tutorial-panel",
    "href": "resources/00-software-setup.html#optional-adding-a-tutorial-panel",
    "title": "Getting started with R",
    "section": "7.3 Optional: Adding a Tutorial Panel",
    "text": "7.3 Optional: Adding a Tutorial Panel\nYou can also add a “Tutorial” panel to R Studio.\n\nClick on “Tools &gt; Global Options”\n\nAlternatively you can use the hotkey combination cmd + , on a Mac cntrl + , … No Shortcut for PC :(\n\n\n\n\nSelect the Pain Layout tab. In the upper right of the four pains, check the box next to Tutorial\nYou may need to close and re-open R Studio. When you do, in the upper right tab you should see:"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-adjusting-quarto-r-markdown-display-options",
    "href": "resources/00-software-setup.html#optional-adjusting-quarto-r-markdown-display-options",
    "title": "Getting started with R",
    "section": "7.4 Optional: Adjusting Quarto / R Markdown Display Options",
    "text": "7.4 Optional: Adjusting Quarto / R Markdown Display Options\n\n2+2\n\n[1] 4\n\n\nThis is just personal perference, but while we’re changin some global options, I’d recommend\n\nClick on `RMarkdown\nSet “Show In Document Outline” to `All Sections and Chunks\nUncheck the box that says “Show Output in line for all R Markdown Documents”"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-dont-save-r-history",
    "href": "resources/00-software-setup.html#optional-dont-save-r-history",
    "title": "Getting started with R",
    "section": "7.5 Optional: Don’t Save R History",
    "text": "7.5 Optional: Don’t Save R History\nFinally, in the R General tab, I’d strongly recommend unchecking the box that says “Always save R History”\n\n** Be sure to click OK** when you’re done updating these settings."
  },
  {
    "objectID": "slides/08-slides.html#class-plan",
    "href": "slides/08-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements (5 min)\nFeedback (5 min)\nClass plan\n\nProbability (10 min)\nConditional Probability (10 min)\nProbability Distributions (10 min)\nExpected Values and Variances (10 min)\nStandard Errors (10 min)\nPreviewing Lab 8 (10 min)"
  },
  {
    "objectID": "slides/08-slides.html#annoucements-assignment-2",
    "href": "slides/08-slides.html#annoucements-assignment-2",
    "title": "POLS 1600",
    "section": "Annoucements: Assignment 2",
    "text": "Annoucements: Assignment 2\nFull prompt here\n\nA revised description of your group’s research project\nA description of a linear model implied by your question\nR code that loads some potentially relevant data to your question and at least one descriptive summary of that data.\nSome information about your group such as:\n\nA group name1\nA group color or color scheme\nA group motto, mascot, crest, etc.\nYour group’s theme song\nYour group’s astrological sign\nAnything else that you think well help you form strong ingroup bounds that facilitate collaboration\n\n\nIf you’re Group 01 don’t change your name to Group 4"
  },
  {
    "objectID": "slides/08-slides.html#setup-packages-for-today",
    "href": "slides/08-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/08-slides.html#feedback",
    "href": "slides/08-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/08-slides.html#what-did-we-like",
    "href": "slides/08-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/08-slides.html#what-did-we-dislike",
    "href": "slides/08-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/08-slides.html#our-advice-for-pols-1600",
    "href": "slides/08-slides.html#our-advice-for-pols-1600",
    "title": "POLS 1600",
    "section": "Our advice for POLS 1600",
    "text": "Our advice for POLS 1600"
  },
  {
    "objectID": "slides/08-slides.html#our-fashion-advice",
    "href": "slides/08-slides.html#our-fashion-advice",
    "title": "POLS 1600",
    "section": "Our Fashion Advice",
    "text": "Our Fashion Advice\n\n\n\n\n\nOption\nChoice\nVotes\n\n\n\n\nJacket\nTuxedo\n9\n\n\nPalette\nSpring (Warm + Light)\n10\n\n\nPant\nKhakis\n6\n\n\nPatterns\nHow 'bout a fun graphic or print\n7\n\n\nShoe\nCrocs\n9\n\n\nTie\nBow tie\n12\n\n\nTop\nSports jersey\n8"
  },
  {
    "objectID": "slides/08-slides.html#section",
    "href": "slides/08-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Naive professor in class survey: \"Help me with my fit\"Students: \"Ok, bet. Tuxedo, crocs, and a jersey\" https://t.co/NPmBZhsfen pic.twitter.com/cWlytcZ12h\n\n— Paul Testa ((ProfPaulTesta?)) March 17, 2023"
  },
  {
    "objectID": "slides/08-slides.html#probability-1",
    "href": "slides/08-slides.html#probability-1",
    "title": "POLS 1600",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event happening.\nStatistics uses probability to quantify uncertainty about estimates and hypotheses.\nTo do this, we will need to understand:\n\nDefinitions (experiment, sample space, events)\nThree rules of probability (Kolmogorov axioms)\nTwo interpretations interpreting probabilities (Frequentist and Bayesian)"
  },
  {
    "objectID": "slides/08-slides.html#experiments-sample-spaces-sets-and-events",
    "href": "slides/08-slides.html#experiments-sample-spaces-sets-and-events",
    "title": "POLS 1600",
    "section": "Experiments, sample spaces, sets, and events",
    "text": "Experiments, sample spaces, sets, and events\n\nIn probability theory, an experiment describes a repeatable process where the outcome is uncertain\n\nProcesses where the outcomes are uncertain are called non-deterministic or stochastic\n\nThe sample space of an experiment is the set \\((\\Omega\\) “omega”, or \\(S\\)) of all the possible outcomes of an experiment"
  },
  {
    "objectID": "slides/08-slides.html#experiments-sample-spaces-sets-and-events-1",
    "href": "slides/08-slides.html#experiments-sample-spaces-sets-and-events-1",
    "title": "POLS 1600",
    "section": "Experiments, sample spaces, sets, and events",
    "text": "Experiments, sample spaces, sets, and events\n\n\n\nSets can be:\n\nempty \\(( A: \\{\\emptyset\\})\\)\na single event \\(( Coin: \\{\\text{Heads}\\})\\)\nmultiple events \\(( Odd\\, \\#s: \\{\\text{1,3,5}\\})\\)\ninfinite \\((\\mathbb{R}: \\text{ The set of real numbers}\\{ -\\infty \\dots +\\infty\\}\\))\n\nAn event, \\((E\\) or \\(A)\\) is a subset of outcomes in the sample space\n\nThe sample space for a coin flip is \\(\\Omega = \\{\\text{Heads, Tails}\\}\\)\nThe event Heads is a subset of \\(\\Omega\\)"
  },
  {
    "objectID": "slides/08-slides.html#subsets",
    "href": "slides/08-slides.html#subsets",
    "title": "POLS 1600",
    "section": "Subsets",
    "text": "Subsets\n\nSubset: Let \\(D\\) be the set outcomes for a 6-side die: \\(D=\\{1,2,3,4,5,6\\}\\)\n\n\\(Primes=\\{2,3,5\\}\\)\n\\(Primes \\subset D \\iff \\forall X \\in Primes, X \\in D\\)"
  },
  {
    "objectID": "slides/08-slides.html#unions-intersections-and-complements",
    "href": "slides/08-slides.html#unions-intersections-and-complements",
    "title": "POLS 1600",
    "section": "Unions, Intersections, and Complements",
    "text": "Unions, Intersections, and Complements\n\nUnions\n\n\\(A \\cup B = \\{X:X \\in A \\lor X \\in B \\}\\)\nEither \\(A\\), \\(B\\) or both \\(A\\) and \\(B\\) occur\n\nIntersections\n\n\\(A \\cap B = \\{X:X \\in A \\land X \\in B \\}\\)\nBoth \\(A\\) and \\(B\\) occur\n\nComplements\n\n\\(A'=A^\\complement = \\{X:X\\notin A\\}\\)\n\\(A'=A^\\complement\\) means \\(A\\) does not occur\n\\(\\emptyset^\\complement=S\\) and \\(S^\\complement=\\emptyset\\)"
  },
  {
    "objectID": "slides/08-slides.html#section-1",
    "href": "slides/08-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "slides/08-slides.html#three-rules-of-probability",
    "href": "slides/08-slides.html#three-rules-of-probability",
    "title": "POLS 1600",
    "section": "Three Rules of Probability",
    "text": "Three Rules of Probability\n\n\nProbability is defined by three rules or assumptions called the Kolmogorov Axioms\n\n\nPositivity: The probability of any event \\(A\\) is nonnegative\n\n\\[Pr(A) \\geq 0 \\]\n\nCertainty: The probability that one of the outcomes in the sample space occurs is 1\n\n\\[Pr(\\Omega) = 1 \\]\n\nAdditivity: If events \\(A\\) and \\(B\\) are mutually exclusive, then:\n\n\\[Pr(A \\text{ or } B) = Pr(A) + Pr(B)\\]"
  },
  {
    "objectID": "slides/08-slides.html#the-addition-rule",
    "href": "slides/08-slides.html#the-addition-rule",
    "title": "POLS 1600",
    "section": "The Addition Rule",
    "text": "The Addition Rule\nFor events, \\(A\\) and \\(B\\), the addition rule says we can find the probability of either \\(A\\) or \\(B\\) occurring:\n\\[Pr(A \\cup B) = Pr(A \\text{ or } B) = Pr(A) + Pr(B) - \\underbrace{Pr(A \\text{ and } B)}_{\\text{aka } Pr(A \\cap B)}\\]\nIn words: The probability of either A or B occurring is the probability that A occurs plus the probability that B occurs - minus the probability that both occur (so that we’re not double counting…)"
  },
  {
    "objectID": "slides/08-slides.html#section-2",
    "href": "slides/08-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "slides/08-slides.html#the-law-of-total-probability-part-1",
    "href": "slides/08-slides.html#the-law-of-total-probability-part-1",
    "title": "POLS 1600",
    "section": "The Law of Total Probability (Part 1)",
    "text": "The Law of Total Probability (Part 1)\nFor any event two events, \\(A\\) and \\(B\\), the probability of \\(A\\) \\((Pr(A))\\) can be decomposed into the sum of the probabilities of two mutually exclusive events:\n\\[Pr(A) = Pr(A \\text{ and } B) + Pr(A \\text{ and } B^{\\complement})\\]"
  },
  {
    "objectID": "slides/08-slides.html#section-3",
    "href": "slides/08-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "slides/08-slides.html#two-interpretations-of-probablity",
    "href": "slides/08-slides.html#two-interpretations-of-probablity",
    "title": "POLS 1600",
    "section": "Two interpretations of probablity",
    "text": "Two interpretations of probablity\n\nProbabilities are defined by these three axioms\nThe are two broad ways of interpreting what probabilities mean:\n\nFrequentist\nBayesian"
  },
  {
    "objectID": "slides/08-slides.html#frequentist-interpretations-of-probability",
    "href": "slides/08-slides.html#frequentist-interpretations-of-probability",
    "title": "POLS 1600",
    "section": "Frequentist interpretations of probability",
    "text": "Frequentist interpretations of probability\n\n\nProbability describes how likely it is that some event happens.\n\nFlip a fair coin, the probability of heads is Pr(Heads) = 0.5\n\nFrequentist: view this probability as the limit of the relative frequency of an event over repeated trials.\n\n\\[Pr(E) = \\lim_{n \\to \\infty} \\frac{n_{E}}{n} \\approx \\frac{ \\text{# of Times E happened}}{\\text{Total # of Trials}}\\]\n\nThinking about probability as a relative frequency, requires us to know how to count the number of times an event occurred (see also)"
  },
  {
    "objectID": "slides/08-slides.html#frequentist-interpretations-of-probability-1",
    "href": "slides/08-slides.html#frequentist-interpretations-of-probability-1",
    "title": "POLS 1600",
    "section": "Frequentist interpretations of probability",
    "text": "Frequentist interpretations of probability\n\nProbabilities from a Frequentist perspective are defined by fixed and unknown parameters\nThe goal of statistics for a frequentist is to learn about these parameters from data.\nFrequentist statistics often ask questions like “What is the probability of observing some data \\(Y\\), given a hypothesis about the true value of parameter(s), \\(\\theta\\), that generated it."
  },
  {
    "objectID": "slides/08-slides.html#frequentist-interpretations-of-probability-2",
    "href": "slides/08-slides.html#frequentist-interpretations-of-probability-2",
    "title": "POLS 1600",
    "section": "Frequentist interpretations of probability",
    "text": "Frequentist interpretations of probability\nFor example, suppose we wanted to test whether a coin is “fair” \\((p = Pr(Heads) = .5; q = Pr(Tails) = 1-p = .5).\\) We could:\n\nFlip a fair coin 10 times. Our estimate of the \\(Pr(H)\\) is the number of heads divided by 10. It could be 0.5, but also 0 or 1, or some number in between.\nFlip a coin 100 times and our estimate will be closer to the true \\(paramter\\).\nFlip a coin an \\(\\infty\\) amount of times and the relative frequency will converge to the true parameter \\((Pr(H) = \\lim_{n \\to \\infty} \\frac{n_{H}}{n} = p = 0.5 \\text{ for a fair coin})\\)"
  },
  {
    "objectID": "slides/08-slides.html#bayesian-interpretations-of-probability",
    "href": "slides/08-slides.html#bayesian-interpretations-of-probability",
    "title": "POLS 1600",
    "section": "Bayesian interpretations of probability",
    "text": "Bayesian interpretations of probability\n\nFrequentist interpretations make sense for describing processes that we could easily repeat (e.g. Coin flips, Surveys, Experiments)\nBut feel more convoluted when trying to describe events like “the probability of that Biden wins reelection.”\nBayesian interpretations of probability view probabilities as subjective beliefs.\nThe task for a Bayesian statistics is to update these prior beliefs () based on a model of the likelihood of observing some data to form new beliefs after observing the data (called the posterior beliefs)."
  },
  {
    "objectID": "slides/08-slides.html#bayesian-updating",
    "href": "slides/08-slides.html#bayesian-updating",
    "title": "POLS 1600",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\n\nBayesians update their beliefs according to Bayes Rule, which says:\n\n\\[\\text{posterior} \\propto \\text{likelihood} \\times \\text{prior}\\] More formally:\n\\[\\underbrace{Pr(\\theta|Y)}_{\\text{Posterior}} \\propto \\underbrace{Pr(Y|\\theta)}_{\\text{Likelihood}}) \\times \\underbrace{Pr(\\theta)}_{\\text{Prior}}\\]"
  },
  {
    "objectID": "slides/08-slides.html#bayesian-vs-frequentists",
    "href": "slides/08-slides.html#bayesian-vs-frequentists",
    "title": "POLS 1600",
    "section": "Bayesian vs Frequentists",
    "text": "Bayesian vs Frequentists\n\nOur two main tools for doing statistical inference in this course\n\nHypothesis Testing\nInterval Estimation\n\nFollow largely from frequentist interpretations of probability"
  },
  {
    "objectID": "slides/08-slides.html#bayesian-vs-frequentists-1",
    "href": "slides/08-slides.html#bayesian-vs-frequentists-1",
    "title": "POLS 1600",
    "section": "Bayesian vs Frequentists",
    "text": "Bayesian vs Frequentists\n\nThe differences between Bayesian and Frequentist frameworks, are both philosophical and technical in nature\n\nIs probability a relative frequency or subjective belief? How do we form and use prior beliefs\nBayesian statistics relies heavily on algorhithms for Markov Chain Monte-Carlo simulations made possible by advances in computing.\n\nFor most of the questions in this course, these two frameworks will yield similar (even identical) conclusions.\n\nSometimes it’s helpful to think like a Bayesian, others, like a frequentist"
  },
  {
    "objectID": "slides/08-slides.html#summary-probability",
    "href": "slides/08-slides.html#summary-probability",
    "title": "POLS 1600",
    "section": "Summary: Probability",
    "text": "Summary: Probability\n\nProbability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur\nProbabilities are:\n\nNon-negative\nUnitary\nAdditive\n\nTwo different interpretations of probability:\n\nFrequentists: Probability is a long run relative frequency\nBayesians: Probability reflect subjective beliefs which we update upon observing data"
  },
  {
    "objectID": "slides/08-slides.html#conditional-probability-definition",
    "href": "slides/08-slides.html#conditional-probability-definition",
    "title": "POLS 1600",
    "section": "Conditional Probability: Definition",
    "text": "Conditional Probability: Definition\nThe conditional probability that event A occurred, given that event B occurred is written as \\(Pr(A|B)\\) (“The probability of A given B”) and defined as:\n\\[Pr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)} = \\frac{\\text{Probability of Both A and B}}{\\text{Probability of B}}\\]\n\n\\(Pr(A \\cap B)\\) is the same as \\(Pr(A \\text{ and } B)\\) is the joint probability of both events occurring\n\\(Pr(B)\\) is the marginal probability of B occuring"
  },
  {
    "objectID": "slides/08-slides.html#conditional-probability-multiplication-rule",
    "href": "slides/08-slides.html#conditional-probability-multiplication-rule",
    "title": "POLS 1600",
    "section": "Conditional Probability: Multiplication Rule",
    "text": "Conditional Probability: Multiplication Rule\nJoint probabilities are symmetrical. \\(Pr(A \\cap B) = Pr(B \\cap A)\\).\nBy rearranging terms:\n\\[Pr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)}\\] We get the multiplication rule:\n\\[Pr(A \\cap B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A)\\]"
  },
  {
    "objectID": "slides/08-slides.html#the-law-of-total-probability-part-2",
    "href": "slides/08-slides.html#the-law-of-total-probability-part-2",
    "title": "POLS 1600",
    "section": "The Law of Total Probability (Part 2)",
    "text": "The Law of Total Probability (Part 2)\nWe can use multiplication rule to derive an alternative form of the law of total probability:\n\\[Pr(A) = Pr(A|B)Pr(B) + Pr(A|B^\\complement)Pr(B^\\complement)\\]"
  },
  {
    "objectID": "slides/08-slides.html#independence",
    "href": "slides/08-slides.html#independence",
    "title": "POLS 1600",
    "section": "Independence",
    "text": "Independence\nEvents \\(A\\) and \\(B\\) are independent if\n\\[Pr(A|B) = Pr(A) \\text{ and } Pr(B|A) = Pr(B)\\]\nConceputally, If \\(A\\) and \\(B\\) are independent knowing whether \\(B\\) occurred, tells us nothing about \\(A\\), and so the conditional probability of \\(A\\) given \\(B\\), \\(Pr(A|B)\\) is equal to the unconditional, or marginal probability, \\(Pr(A)\\)"
  },
  {
    "objectID": "slides/08-slides.html#independence-1",
    "href": "slides/08-slides.html#independence-1",
    "title": "POLS 1600",
    "section": "Independence",
    "text": "Independence\nFormally, two events are statistically independent if and only if the joint probability is equal to product of the marginal probabilities\n\\[Pr(A\\text{ and }B) = Pr(A)Pr(B)\\]"
  },
  {
    "objectID": "slides/08-slides.html#conditional-independence",
    "href": "slides/08-slides.html#conditional-independence",
    "title": "POLS 1600",
    "section": "Conditional Independence",
    "text": "Conditional Independence\nWe can extend the concept of independence to situations with more than two events:\nIf events \\(A\\), \\(B\\), and \\(C\\) are jointly independent then:\n\\[Pr(A \\cap B \\cap C) = Pr(A)Pr(B)Pr(C)\\]\nJoint independence implies pairwise independence and conditional independence:\n\\[Pr(A \\cap B | C) = Pr(A|C)Pr(B|C)\\] But not the reverse."
  },
  {
    "objectID": "slides/08-slides.html#bayes-rule",
    "href": "slides/08-slides.html#bayes-rule",
    "title": "POLS 1600",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nBayes rule is theorem for how we should update our beliefs about \\(A\\) given that \\(B\\) occurred:\n\\[Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}\\] Where\n\n\\(Pr(A)\\) is called the prior probability of A (our initial belief)\n\\(Pr(A|B)\\) is called the posterior probability of A given B (our updated belief after observing B)"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-probability-you-have-covid-19-given-a-positive-test",
    "href": "slides/08-slides.html#whats-the-probability-you-have-covid-19-given-a-positive-test",
    "title": "POLS 1600",
    "section": "What’s the probability you have Covid-19 given a positive test",
    "text": "What’s the probability you have Covid-19 given a positive test\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}\\]"
  },
  {
    "objectID": "slides/08-slides.html#possible-outcomes",
    "href": "slides/08-slides.html#possible-outcomes",
    "title": "POLS 1600",
    "section": "Possible Outcomes",
    "text": "Possible Outcomes\nFour possible outcomes\n\n\n\n\n\nTest\nHave Covid\nDon't Have Covid\n\n\n\n\nPositive\nTrue Positive\nFalse Positive\n\n\nNegative\nFalse Negative\nTrue Negative"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-probability-you-have-covid-19-given-a-positive-test-1",
    "href": "slides/08-slides.html#whats-the-probability-you-have-covid-19-given-a-positive-test-1",
    "title": "POLS 1600",
    "section": "What’s the probability you have Covid-19 given a positive test",
    "text": "What’s the probability you have Covid-19 given a positive test\n\nLet’s assume:\n\n1 out 100 people have Covid-19\nOur test correctly identifies true positives 95 percent of the time (sensitivity = True Positive Rate)\nOur test correctly identifies true negatives 95 percent of the time (specificity = True Negative Rate)\n\nIn a sample of 100,000 people then:\n\n\n\n\n\nTest\nHave Covid\nDon't Have Covid\n\n\n\n\nPositive\n950\n4950\n\n\nNegative\n50\n94050"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-probability-you-have-covid-19-given-a-positive-test-2",
    "href": "slides/08-slides.html#whats-the-probability-you-have-covid-19-given-a-positive-test-2",
    "title": "POLS 1600",
    "section": "What’s the probability you have Covid-19 given a positive test",
    "text": "What’s the probability you have Covid-19 given a positive test\n\nNow we can calculate the relevant quantities for:\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}\\]\n\n\\(Pr(+|Covid) = 950/(1000) \\approx 0.95\\)\n\n\\(Pr(Covid) = 1000/100000 \\approx 0.01\\)\n\n\\(Pr(+) = Pr(+|Covid) + Pr(+|Covid)= .95*.01 + .05*.99 \\approx 0.059\\)\n\nWhich yields:\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)} = \\frac{0.95 \\times 0.01}{0.059} \\approx 0.16\\]"
  },
  {
    "objectID": "slides/08-slides.html#what-if-you-took-a-second-test",
    "href": "slides/08-slides.html#what-if-you-took-a-second-test",
    "title": "POLS 1600",
    "section": "What if you took a second test?",
    "text": "What if you took a second test?\nWe could use our updated posterior belief as our new prior:\n\\[Pr(Covid|2nd +) = \\frac{0.95 \\times 0.16}{0.16\\times0.95 + (1-0.16)\\times 0.95 } \\approx 0.783\\]\nNow we’re much more confident that we have Covid-19"
  },
  {
    "objectID": "slides/08-slides.html#random-variables",
    "href": "slides/08-slides.html#random-variables",
    "title": "POLS 1600",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables assign numeric values to each event in an experiment.\n\nMutually exclusive and exhaustive, together cover the entire sample space.\n\nDiscrete random variables take on finite, or countably infinite distinct values.\nContinuous variables can take on an uncountably infinite number of values."
  },
  {
    "objectID": "slides/08-slides.html#example-toss-two-coins",
    "href": "slides/08-slides.html#example-toss-two-coins",
    "title": "POLS 1600",
    "section": "Example: Toss Two Coins",
    "text": "Example: Toss Two Coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)"
  },
  {
    "objectID": "slides/08-slides.html#probability-distributions",
    "href": "slides/08-slides.html#probability-distributions",
    "title": "POLS 1600",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\nThe can be represented in terms of:\n\nProbability Mass/Density Functions\n\nDiscrete variables have probability mass functions (PMF)\nContinuous variables have probability density functions (PDF)\n\nCumulative Density Functions\n\nDiscrete: Summation of discrete probabilities\nContinuous: Integration over a range of values"
  },
  {
    "objectID": "slides/08-slides.html#discrete-distributions",
    "href": "slides/08-slides.html#discrete-distributions",
    "title": "POLS 1600",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nProbability Mass Function (pmf): \\(f(x)=p(X=x)\\)\nAssigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\nCumulative Distribution Function (cdf) \\(F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)\\)\n\nSum of the probability mass for events less than or equal to \\(x_j\\)"
  },
  {
    "objectID": "slides/08-slides.html#example-toss-two-coins-1",
    "href": "slides/08-slides.html#example-toss-two-coins-1",
    "title": "POLS 1600",
    "section": "Example: Toss Two coins",
    "text": "Example: Toss Two coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)\n\n\\(f(X=0)=p(X=0)=1/4\\)\n\\(f(X=1)=p(X=1)=1/2\\)\n\\(F(X\\leq 1) = p(X \\leq 1)= 3/4\\)"
  },
  {
    "objectID": "slides/08-slides.html#rolling-a-die",
    "href": "slides/08-slides.html#rolling-a-die",
    "title": "POLS 1600",
    "section": "Rolling a die",
    "text": "Rolling a die\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X&lt;=2) = 1/6 + 1/6 = 1/3"
  },
  {
    "objectID": "slides/08-slides.html#continuous-distributions",
    "href": "slides/08-slides.html#continuous-distributions",
    "title": "POLS 1600",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nProbability Density Functions (PDF): \\(f(x)\\)\n\nAssigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\nBut… since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n\nCumulative Distribution Function (CDF) \\(F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx\\)\n\nInstead of summing up to a specific value (discrete) we integrate over all possible values up to \\(x\\)\nProbability of having a value less than x"
  },
  {
    "objectID": "slides/08-slides.html#integrals",
    "href": "slides/08-slides.html#integrals",
    "title": "POLS 1600",
    "section": "Integrals",
    "text": "Integrals\nWhat’s the area of the rectangle?\n\n\\(base\\times height\\)"
  },
  {
    "objectID": "slides/08-slides.html#integrals-1",
    "href": "slides/08-slides.html#integrals-1",
    "title": "POLS 1600",
    "section": "Integrals",
    "text": "Integrals\nHow would we find the area under a curve?"
  },
  {
    "objectID": "slides/08-slides.html#integrals-2",
    "href": "slides/08-slides.html#integrals-2",
    "title": "POLS 1600",
    "section": "Integrals",
    "text": "Integrals\nWell suppose we added up the areas of a bunch of rectangles roughly whose height’s approximated the height of the curve?\n\nCan we do any better?"
  },
  {
    "objectID": "slides/08-slides.html#integrals-3",
    "href": "slides/08-slides.html#integrals-3",
    "title": "POLS 1600",
    "section": "Integrals",
    "text": "Integrals\nLet’s make the rectangles smaller\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get’s even better"
  },
  {
    "objectID": "slides/08-slides.html#link-between-pdf-and-cdf",
    "href": "slides/08-slides.html#link-between-pdf-and-cdf",
    "title": "POLS 1600",
    "section": "Link between PDF and CDF",
    "text": "Link between PDF and CDF\nIf \\[F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx \\]\nThen by the fundamental theorem of calculus\n\\[\\frac{d}{dx}F(x)=f(x)\\]\nIn words\n\nthe PDF \\((f(x))\\)is the derivative (rate of change) of the CDF \\((F(X))\\)\nthe CDF describes the area under the curve defined by f(x) up to x"
  },
  {
    "objectID": "slides/08-slides.html#properties-of-the-cdf",
    "href": "slides/08-slides.html#properties-of-the-cdf",
    "title": "POLS 1600",
    "section": "Properties of the CDF",
    "text": "Properties of the CDF\n\n\\(0\\leq F(x) \\leq 1\\)\n\\(F\\) is non-decreasing and right continuous\n\\(\\lim_{x\\to-\\infty}F(x)=0\\)\n\\(\\lim_{x\\to\\infty}F(x)=1\\)\nFor all \\(a,b \\in \\mathbb{R}\\) s.t. \\(a&lt;b\\)\n\n\n\\[p(a &lt; X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx \\]"
  },
  {
    "objectID": "slides/08-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "href": "slides/08-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "title": "POLS 1600",
    "section": "Recall the PMF and CDF of a die",
    "text": "Recall the PMF and CDF of a die"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-probability",
    "href": "slides/08-slides.html#whats-the-probability",
    "title": "POLS 1600",
    "section": "What’s the probability",
    "text": "What’s the probability\n\n\\(p(X=1)...p(X=6) = 1/6\\)\n\\(p( 2 &lt; X \\leq 5) = F(5)-F(2)=5/6-1/6=4/6=2/3\\)"
  },
  {
    "objectID": "slides/08-slides.html#what-well-use-proability-distributions-for",
    "href": "slides/08-slides.html#what-well-use-proability-distributions-for",
    "title": "POLS 1600",
    "section": "What we’ll use proability distributions for:",
    "text": "What we’ll use proability distributions for:\nIn this course, we’ll use probability distributions to\n\nModel the data generating process as a function of parameters we can estimate\nTo perform statitical inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)"
  },
  {
    "objectID": "slides/08-slides.html#section-5",
    "href": "slides/08-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "Please memorize these over Spring Break:"
  },
  {
    "objectID": "slides/08-slides.html#section-6",
    "href": "slides/08-slides.html#section-6",
    "title": "POLS 1600",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "slides/08-slides.html#expected-value",
    "href": "slides/08-slides.html#expected-value",
    "title": "POLS 1600",
    "section": "Expected Value",
    "text": "Expected Value\nA (probability) weighted average of the possible outcomes of a random variable, often labeled \\(\\mu\\)\nDiscrete:\n\\[\\mu_X=E(X)=\\sum xp(x)\\]\nContinuous\n\\[\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx\\]"
  },
  {
    "objectID": "slides/08-slides.html#condtional-expectations",
    "href": "slides/08-slides.html#condtional-expectations",
    "title": "POLS 1600",
    "section": "Condtional Expectations:",
    "text": "Condtional Expectations:\nFor a continuous variable:\n\\[\nE(X|Y=y) = \\int_{-\\infty}^{\\infty}xf_{x|y}(x|y) dx\n\\]\nWhere:\n\\[\nf_{x|y}(x|y) = \\frac{f_{x,y}(x,y)}{f_y(y)} = \\frac{\\text{Joint distribution of X and Y}}{\\text{Marginal distribution of Y}}\n\\]\nWhich follows from the law of total probability"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "href": "slides/08-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "title": "POLS 1600",
    "section": "What’s the expected value of a 1 roll of fair die?",
    "text": "What’s the expected value of a 1 roll of fair die?\n\\[\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-slides.html#properties-of-expected-values",
    "href": "slides/08-slides.html#properties-of-expected-values",
    "title": "POLS 1600",
    "section": "Properties of Expected Values",
    "text": "Properties of Expected Values\n\n\\(E(c)=c\\)\n\\(E(a+bX)=a+bE[X]\\)\n\\(E[E[X]]=X\\)\n\\(E[E[Y|X]]=E[Y]\\)\n\\(E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx\\)\n\\(E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)\\)\n\\(E[XY]=E[X]E[Y]\\) if \\(X\\) and \\(Y\\) are independent"
  },
  {
    "objectID": "slides/08-slides.html#section-7",
    "href": "slides/08-slides.html#section-7",
    "title": "POLS 1600",
    "section": "",
    "text": "How many times would you have to roll a fair die to get all six sides?\n\nWe can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success, \\(p\\). The expected value of a geometric variable is:\n\n\n\\[\\begin{align*}E(X)&=\\sum_{k=1}^{\\infty}kp(1-p)^{k-1} \\\\\n&=p\\sum_{k=1}^{\\infty}k(1-p)^{k-1} \\\\\n&=p\\left(-\\frac{d}{dp}\\sum_{k=1}^{\\infty}(1-p)^k\\right) \\text{(Chain rule)} \\\\\n&=p\\left(-\\frac{d}{dp}\\frac{1-p}{p}\\right) \\text{(Geometric Series)} \\\\\n&=p\\left(\\frac{d}{dp}\\left(1-\\frac{1}{p}\\right)\\right)=p\\left(\\frac{1}{p^2}\\right)=\\frac1p\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-slides.html#rolling-a-fair-die-to-get-all-six-sides",
    "href": "slides/08-slides.html#rolling-a-fair-die-to-get-all-six-sides",
    "title": "POLS 1600",
    "section": "Rolling a fair die to get all six sides",
    "text": "Rolling a fair die to get all six sides\nFor this question, we need to calculate the probability of success, p, after getting a side we need.\nThe probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:\n\nev &lt;- c()\nfor(i in 6:1){\n  ev[i] &lt;- 6/i\n  \n}\n# Expected rolls for each 1 through 6th side\nrev(ev)\n\n[1] 1.0 1.2 1.5 2.0 3.0 6.0\n\n# Total \nsum(ev)\n\n[1] 14.7"
  },
  {
    "objectID": "slides/08-slides.html#variance",
    "href": "slides/08-slides.html#variance",
    "title": "POLS 1600",
    "section": "Variance",
    "text": "Variance\nIf \\(X\\) has a finite mean \\(E[X]=\\mu\\), then \\(E[(X-\\mu)^2]\\) is finite and called the variance of \\(X\\) which we write as \\(\\sigma^2\\) or \\(Var[X]\\)."
  },
  {
    "objectID": "slides/08-slides.html#variance-1",
    "href": "slides/08-slides.html#variance-1",
    "title": "POLS 1600",
    "section": "Variance",
    "text": "Variance\n\\[\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}\\]\n\n“The variance of X is equal to the expected value of X-squared, minus the square of X’s expected value.”\n\\(\\sigma^2=E[X^2]-E[X]^2\\) is a useful identity in proofs and derivations"
  },
  {
    "objectID": "slides/08-slides.html#standard-deviations",
    "href": "slides/08-slides.html#standard-deviations",
    "title": "POLS 1600",
    "section": "Standard Deviations",
    "text": "Standard Deviations\nA standard deviation is just the square root of the variance\n\\[\\sigma=\\sqrt{Var[X]}\\]\nStandard deviations are useful for describing:\n\nA typical deviation from the mean/Expected value\nThe width or spread of a distribution"
  },
  {
    "objectID": "slides/08-slides.html#covariance-and-correlation",
    "href": "slides/08-slides.html#covariance-and-correlation",
    "title": "POLS 1600",
    "section": "Covariance and correlation",
    "text": "Covariance and correlation\n\nCovariance measures the degree to which two random variables vary together.\n\n\\(Cov[X,Y] \\to +\\) An increase in \\(X\\) tends to be larger than its mean when \\(Y\\) is larger than its mean\n\n\\[Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\\]\n\nThe correlation between \\(X\\) and \\(Y\\) is simply the covariance of \\(X\\) and \\(Y\\) divided by the standard deviation of each.\n\n\\[\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}\\]\n\nNormalized covariance to a scale that runs between \\([-1,1]\\)"
  },
  {
    "objectID": "slides/08-slides.html#properties-of-variance-and-covariance",
    "href": "slides/08-slides.html#properties-of-variance-and-covariance",
    "title": "POLS 1600",
    "section": "Properties of Variance and Covariance",
    "text": "Properties of Variance and Covariance\n\n\\(Cov[X,Y]=E[XY]-E[X]E[Y]\\)\n\\(Var[X]=E[X^2]-(E[X])^2\\)\n\\(Var[X|Y]=E[X^2|Y]-(E[X|Y])^2\\)\n\\(Cov[X,Y]=Cov[X,E[Y|X]]\\)\n\\(Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]\\)\n\\(Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]\\)"
  },
  {
    "objectID": "slides/08-slides.html#what-you-need-to-know-wynk",
    "href": "slides/08-slides.html#what-you-need-to-know-wynk",
    "title": "POLS 1600",
    "section": "What you need to know (WYNK)",
    "text": "What you need to know (WYNK)\nHonestly, for this class, you won’t need to know these properties.\nThey’ll show up in proofs and theorems and become important when you’re trying to evaluate properties of an estimator (isn’t unbiased, is it “efficient”, or consistent does it have minimum variance?) but that’s for another day/course."
  },
  {
    "objectID": "slides/08-slides.html#summary-random-variables-and-probability-distributions",
    "href": "slides/08-slides.html#summary-random-variables-and-probability-distributions",
    "title": "POLS 1600",
    "section": "Summary: Random Variables and Probability Distributions",
    "text": "Summary: Random Variables and Probability Distributions\n\nRandom variables assign numeric values to each event in an experiment.\nProbability distributions assign probabilities to the values that a random variable can take.\n\nDiscrete distributions are described by their pmf and cdf\nContinuous distributions by their pdf and cdf"
  },
  {
    "objectID": "slides/08-slides.html#summary-random-variables-and-probability-distributions-1",
    "href": "slides/08-slides.html#summary-random-variables-and-probability-distributions-1",
    "title": "POLS 1600",
    "section": "Summary: Random Variables and Probability Distributions",
    "text": "Summary: Random Variables and Probability Distributions\n\nProbability distributions let us describe the data generating process and encode information about the world into our models\n\nThere are lots of distributions\nDon’t worry about memorizing formulas\nDo develop intuitions about the nature of your data generating process (Is my outcome continuous or disrecte, binary or count, etc.)\n\nTwo key features of probability distributions are their:\n\nExpected values probability weighted averages\nVariances which quantify variation around expected values"
  },
  {
    "objectID": "slides/08-slides.html#interpreting-regressions",
    "href": "slides/08-slides.html#interpreting-regressions",
    "title": "POLS 1600",
    "section": "Interpreting regressions",
    "text": "Interpreting regressions\n\nRegression coefficients \\((\\beta)\\) are crucial for substantive interpretations (sign and size)\nThe standard errors of these coefficients \\((SE(\\beta))\\) are the key to evaluating the statistical significance of these coefficients"
  },
  {
    "objectID": "slides/08-slides.html#whats-a-standard-error",
    "href": "slides/08-slides.html#whats-a-standard-error",
    "title": "POLS 1600",
    "section": "What’s a standard error?",
    "text": "What’s a standard error?\n\nThe standard error of an estimate is the standard deviation of the theoretical sampling distribution\nA sampling distribution is a distribution of the estimates we would observe in repeated sampling\n\nExample: Re-run the 1978 CPS, we get different respondents, and thus different estimates.\n\nStandard errors describe the width of the sampling distribution\n\nHow much our estimates might vary from the true (population) value from sample to sample.\n\nStandard errors can be used to construct intervals and conduct tests that quantify our uncertainty about our estimate"
  },
  {
    "objectID": "slides/08-slides.html#section-8",
    "href": "slides/08-slides.html#section-8",
    "title": "POLS 1600",
    "section": "",
    "text": "Standard errors of regression coefficients\nFor a linear regression written in matrix notation:\n\\[\ny = X\\beta + \\epsilon\n\\]\nOLS yields estimates of \\(\\beta\\), \\(\\hat{\\beta}\\) by minimizing the sum of squared residuals\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]"
  },
  {
    "objectID": "slides/08-slides.html#section-9",
    "href": "slides/08-slides.html#section-9",
    "title": "POLS 1600",
    "section": "",
    "text": "Standard errors of regression coefficients\nOne can show under a set of assumptions that variance-covariance matrix of \\(\\hat{\\beta}\\) is\n\\[\n\\begin{aligned}\nE[(\\hat{\\beta} -\\beta)-(\\hat{\\beta} -\\beta)'] &= \\sigma^2(X'X)^{-1}\\\\\n& = \\begin{bmatrix}\nVar(\\hat\\beta_0) & Cov(\\hat{\\beta_0},\\hat{\\beta_1}) & \\cdots & Cov(\\hat{\\beta_0},\\hat{\\beta_k}))\\\\\nCov(\\hat\\beta_1,\\hat{\\beta_0}) & Var(\\hat{\\beta_1},) & \\cdots & Cov(\\hat{\\beta_1},\\hat{\\beta_k}))\\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nCov(\\hat\\beta_k,\\hat{\\beta_0}) & Cov(\\hat{\\beta_k},\\hat{\\beta_1}) & \\cdots & Var(\\hat{\\beta_k})\\\\\n\\end{bmatrix}\n\\end{aligned}\n\\] Where we can estimate \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2\\) the mean \\((1/n-k)\\) squared error \\((\\epsilon'\\epsilon)\\) of the regression\n\\[\n\\hat{\\sigma}^2 = \\frac{\\epsilon'\\epsilon}{n-k}\n\\]"
  },
  {
    "objectID": "slides/08-slides.html#section-10",
    "href": "slides/08-slides.html#section-10",
    "title": "POLS 1600",
    "section": "",
    "text": "Standard errors of regression coefficients\nThe standard error for the \\(k\\)th coefficient \\(\\beta_k\\) is simply the square root of the of the \\(k\\)th diagnoal element of the variance-covariance matrix\n\\[\n\\text{SE}(\\beta_k) = \\sqrt{Var(\\beta_k)}\n\\]"
  },
  {
    "objectID": "slides/08-slides.html#robust-standard-errors",
    "href": "slides/08-slides.html#robust-standard-errors",
    "title": "POLS 1600",
    "section": "Robust Standard Errors",
    "text": "Robust Standard Errors\n\\(\\sigma^2(X'X)^{-1}\\) is a good estimate of the variance of \\(\\hat{\\beta}\\) if the errors in a regression are independent and identically distributed (iid).\nThese turn out to be strong assumptions that are violated when there is:\n\nNon-constant error variance (aka heteroskedasticity)\n\nThe variance among the treated units tends to be higher than the variance among control units\n\nAutocorrelation\n\nWe observe the same unit over multiple periods (Say RI in 2016, 2018, 2020)\n\nClustering\n\nRespondents in RI are more similar to each other than respondents in MA"
  },
  {
    "objectID": "slides/08-slides.html#robust-standard-errors-1",
    "href": "slides/08-slides.html#robust-standard-errors-1",
    "title": "POLS 1600",
    "section": "Robust Standard Errors",
    "text": "Robust Standard Errors\n\nRobust standard errors are ways of calculating standard errors for regressions when we think the assumption of IID errors is unrealistic\n\nThe assumption of IID is almost always unrealistic…\n\nWe call these of estimators robust because they provide consistent estimates of the SE even when errors are not independent and identically distributed."
  },
  {
    "objectID": "slides/08-slides.html#robust-standard-errors-in-r",
    "href": "slides/08-slides.html#robust-standard-errors-in-r",
    "title": "POLS 1600",
    "section": "Robust standard errors in R",
    "text": "Robust standard errors in R"
  },
  {
    "objectID": "slides/08-slides.html#lm_robust",
    "href": "slides/08-slides.html#lm_robust",
    "title": "POLS 1600",
    "section": "lm_robust()",
    "text": "lm_robust()\nIn this weeks lab we will get practice using the lm_robust() function from the estimatr.\nAs you will see, lm_robust() provides a convenient way to:\n\ncalculate a variety of robust standard errors using the se_type = \"stata\" argument for example to get the SEs Stata uses\ninclude fixed effects using fixed_effects = ~ st + year argument\ncluster standard errors by some grouping id variable cluster=st\ngenerate estimates quickly using the Cholesky Decomposition"
  },
  {
    "objectID": "slides/08-slides.html#overview-1",
    "href": "slides/08-slides.html#overview-1",
    "title": "POLS 1600",
    "section": "Overview",
    "text": "Overview\nThe goals of this weeks lab are to:\n\nHelp develop your inuition behind the Two-way Fixed Effects Estimator\nLearn how to estimate models with fixed effects and robust clustered standard errors using lm_robust()\nInterpret the marginal effects of interaction models"
  },
  {
    "objectID": "slides/08-slides.html#recreating-figure-2",
    "href": "slides/08-slides.html#recreating-figure-2",
    "title": "POLS 1600",
    "section": "Recreating Figure 2",
    "text": "Recreating Figure 2\n\nTask Code Fig 2\n\n\nQuestion 8 from last week’s lab asked you to recreate Figure 2 from Grumbach and Hill (2022)\n\n\n\n# Load data\nload(url(\"https://pols1600.paultesta.org/files/data/cps_clean.rda\"))\n\n# Calculate turnout by age group and SDR\ncps %&gt;% \n  group_by(age_group, SDR) %&gt;% \n  summarise(\n    prop_vote = mean(dv_voted, na.rm=T)\n  ) -&gt; fig2_df\n\n# Recreate figure 2\nfig2_df %&gt;% \n  filter(!is.na(age_group)) %&gt;% \n  ggplot(aes(age_group,prop_vote,fill = SDR))+\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") -&gt; fig2"
  },
  {
    "objectID": "slides/08-slides.html#q3.1-describing-variation-across-states",
    "href": "slides/08-slides.html#q3.1-describing-variation-across-states",
    "title": "POLS 1600",
    "section": "Q3.1 Describing variation across states",
    "text": "Q3.1 Describing variation across states\n\nTask Code Fig 3.1 Fixed Effects\n\n\nQ3 will ask you to describe variation in turnout across states and years, and policy.\nLet’s get a little practice calculating turnout across states\n\n\n\n# Calculate turnout by state\ncps %&gt;% \n  group_by(st) %&gt;% \n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) %&gt;% \n  mutate(\n    st = fct_reorder(st, turnout)\n    ) -&gt; df_state\n\n\n\n# Visualize turnout by state\ndf_state %&gt;% \n  ggplot(aes(turnout,st))+\n  geom_bar(stat = \"identity\") -&gt; fig3_1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, there is considerable variation in average turnout across States\nQ3.2 will ask you to describe similar variation across years.\nQ3.3 will then ask you to look at variation across SDR policy within a single state.\nThe goal these questions is to help illustrate motivation for including fixed effects as way of generalizing the logic of a difference in differences design"
  },
  {
    "objectID": "slides/08-slides.html#references",
    "href": "slides/08-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nGrumbach, Jacob M, and Charlotte Hill. 2022. “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.” The Journal of Politics 84 (1): 405–17."
  },
  {
    "objectID": "slides/11-slides-old.html#general-plan",
    "href": "slides/11-slides-old.html#general-plan",
    "title": "Week 11:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nReview: Confidence Intervals\nLecture : Hypothesis Testing\nDemo: Final Projects"
  },
  {
    "objectID": "slides/11-slides-old.html#course-plan",
    "href": "slides/11-slides-old.html#course-plan",
    "title": "Week 11:",
    "section": "Course Plan",
    "text": "Course Plan\n\nApril 18: Lecture – Hypothesis Testing\nApril 20 Lab – Hypothesis Testing and Interval Estimation\nApril 25: Lecture – Course Review\nApril 27: Workshop:\nApril 30: Take Home Final Exam\nMay ?: Tacos or Pizza with POLS 1140?\nMay 7: Take Home Final Exam due\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/11-slides-old.html#new-packages",
    "href": "slides/11-slides-old.html#new-packages",
    "title": "Week 11:",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anser)"
  },
  {
    "objectID": "slides/11-slides-old.html#packages-for-today",
    "href": "slides/11-slides-old.html#packages-for-today",
    "title": "Week 11:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"modelr\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/11-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/11-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 11:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/11-slides-old.html#load-packages-for-today",
    "href": "slides/11-slides-old.html#load-packages-for-today",
    "title": "Week 11:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         purrr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign        modelr           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, middle, center # 🔍 # Review ## Confidence Intervals"
  },
  {
    "objectID": "slides/11-slides-old.html#confidence-intervals",
    "href": "slides/11-slides-old.html#confidence-intervals",
    "title": "Week 11:",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nStatistical inference involves quantifying uncertainty about what could have happened\nWe can describe this uncertainty in terms of a sampling distribution (what could have happened had we had a different sample)\nThe standard deviation of a sampling distribution describes its width (spread) and is called a standard error\nStandard errors decrease with by the \\(\\sqrt{N}\\) where \\(N\\) is the size of our sample\nWe can estimate standard errors via simulation or analytically.\n\nSimulations require fewer assumptions, but take more time.\nAnalytic estimates are are quick, but require more assumptions.\n\nWe use standard errors to construct confidence intervals which we interpret as describing a range of plausible values for the thing we’re trying to estimate.\nAny one 95% confidence interval may or may not contain the truth, but in repeated sampling, 95% of the intervals we construct would contain the truth."
  },
  {
    "objectID": "slides/11-slides-old.html#standard-errors-of-regression-coefficients",
    "href": "slides/11-slides-old.html#standard-errors-of-regression-coefficients",
    "title": "Week 11:",
    "section": "Standard Errors of Regression Coefficients",
    "text": "Standard Errors of Regression Coefficients\nLast class, we calculated the standard error of a sample mean using bootstrapping.\nWe can take the same approach for regression coefficients:\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\n# Fit model\nm1 &lt;- lm(support_war01 ~ age + education_n + sex, df_drww)\n# set seed\nset.seed(123)\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n# Estimate Boostrapped Models\nm1_bs &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + education_n + sex, data =.))\n# Tidy coefficients\nm1_bs_df &lt;- map_df(m1_bs, tidy, .id = \"id\")"
  },
  {
    "objectID": "slides/11-slides-old.html#bootstrapped-standard-errors",
    "href": "slides/11-slides-old.html#bootstrapped-standard-errors",
    "title": "Week 11:",
    "section": "Bootstrapped Standard Errors",
    "text": "Bootstrapped Standard Errors\nThen we simply calculate the standard error of for the sampling distribution of each coefficient\n\nm1_bs_df %&gt;%\n  group_by(term)%&gt;%\n  summarize(\n    bs_se = sd(estimate)\n  ) -&gt; m1_bs_se\nm1_bs_se\n\n# A tibble: 4 × 2\n  term           bs_se\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept) 0.0564  \n2 age         0.000717\n3 education_n 0.0108  \n4 sexMale     0.0223"
  },
  {
    "objectID": "slides/11-slides-old.html#analytic-standard-errors.",
    "href": "slides/11-slides-old.html#analytic-standard-errors.",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nAlternatively, we can derive the standard error of the sampling distribution analytically using asymptotic theory (e.g. the Central Limit Theorem).\nThe process starts by defining the quantity we want to know: the variance of our estimated coefficients \\(\\hat{\\beta}\\) around their true values in the population \\(\\beta\\):\n\\[V(\\hat{\\beta}|X) =(E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)' |X]\\]\nYour textbook walks through the math to estimate this quantity on pages 375-380 for simple bivariate regression (i.e. a regression with 1 predictor)\nIn the notes, you’ll find some further discussion of the math for the more general case of with multiple predictors.\nThis is also an excellent walk through of the linear algebra"
  },
  {
    "objectID": "slides/11-slides-old.html#analytic-standard-errors.-1",
    "href": "slides/11-slides-old.html#analytic-standard-errors.-1",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nEssentially, if you expand terms from:\n\\[V(\\hat{\\beta}|X) =(E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)' |X]\\]\nmake substitutions and some assumptions about the distribution of \\(\\epsilon\\), you arrive at a formula for the “Variance Covariance Matrix” of the model:\n\\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] Which is a function of\n\n\\(\\sigma^2\\) the “Sum of Square Errors”\n\\((X'X)^{-1}\\) roughly captures the underlying variance and covariance of the predictors in your model, where:\n\n\\(X\\) is a matrix of predictors (each row is an observation, each column a variable)\n\\(X'X\\) is a symmetric maxtrix (like squaring a variable, but in linear algerba)\n\\(X'X^{-1}\\) is the inverse of this matrix\n\\(sigma^2(X'X)^{-1}\\) is like dividing \\(\\sigma^2\\) by \\(X'X\\)"
  },
  {
    "objectID": "slides/11-slides-old.html#analytic-standard-errors.-2",
    "href": "slides/11-slides-old.html#analytic-standard-errors.-2",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nThe square root of the elements on the diagonal of \\(V(\\hat{\\beta}|X)\\) provides the standard error for each coefficient, which is larger if:\n\nif \\(\\sigma^2\\) is large (When there’s a lot of unexplained variance, our uncertainty is high)\nthe variance of \\(X\\) is small (When predictors don’t vary much our uncertainty increases)"
  },
  {
    "objectID": "slides/11-slides-old.html#analytic-standard-errors.-3",
    "href": "slides/11-slides-old.html#analytic-standard-errors.-3",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nIn practice, you will let your computer calculate these standard errors.\n\nsummary(m1)$coef\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.277540588 0.0529197543  5.244555 1.797036e-07\nage          0.009216952 0.0007129459 12.927982 2.827291e-36\neducation_n -0.015814493 0.0108357258 -1.459477 1.446491e-01\nsexMale      0.094320479 0.0225358310  4.185356 3.017253e-05"
  },
  {
    "objectID": "slides/11-slides-old.html#analytic-standard-errors-by-hand",
    "href": "slides/11-slides-old.html#analytic-standard-errors-by-hand",
    "title": "Week 11:",
    "section": "Analytic Standard Errors by Hand",
    "text": "Analytic Standard Errors by Hand\nJust for fun:\n\n# Sum of Squared Residuals\nsigma_2 &lt;- sum(resid(m1)^2)/m1$df.residual\n# X transpose X\nXtXinv &lt;- solve(t(model.matrix(m1))%*%model.matrix(m1))\n# SE is square root of diagonal of Variance Covariance Matrix\nsqrt(diag(sigma_2*XtXinv))\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310 \n\nsummary(m1)$coef[,2]\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310"
  },
  {
    "objectID": "slides/11-slides-old.html#constructing-confidence-intervals-for-regression-coefficients",
    "href": "slides/11-slides-old.html#constructing-confidence-intervals-for-regression-coefficients",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nEstimate the model to obtain coefficients \\(\\hat{\\beta}\\)\nCalculate Standard Errors using simulation or asymptotic theory\nChoose desired confidence level \\(\\alpha\\) with a corresponding critical value \\(z_{\\alpha/2}\\) derived from an approximation of the hypotehtical sampling distribution\nConstruct a \\((1-\\alpha)\\times 100 \\%\\) percent confidence interval:\n\n\\[CI(\\alpha) = [\\hat{\\beta} - z_{\\alpha/2} \\times \\text{standard error}, \\hat{\\beta} + z_{\\alpha/2} \\times \\text{standard error}]\\]\n\nEstimate the model to obtain coefficients \\(\\hat{\\beta}\\)\n\n\nbeta &lt;- coef(m1)\nbeta\n\n (Intercept)          age  education_n      sexMale \n 0.277540588  0.009216952 -0.015814493  0.094320479 \n\n\n\nCalculate Standard Errors using simulation or asymptotic theory\n\n\nse &lt;- summary(m1)$coef[,2]\nse\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310 \n\n# Similar to bs\nm1_bs_se\n\n# A tibble: 4 × 2\n  term           bs_se\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept) 0.0564  \n2 age         0.000717\n3 education_n 0.0108  \n4 sexMale     0.0223"
  },
  {
    "objectID": "slides/11-slides-old.html#constructing-confidence-intervals-for-regression-coefficients-1",
    "href": "slides/11-slides-old.html#constructing-confidence-intervals-for-regression-coefficients-1",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nCalculate critical value\n\n\nz_fs &lt;- abs(qt(.05/2, m1$df.residual))\nz_fs\n\n[1] 1.961591\n\n\n\nNote: For finite samples we use a \\(t\\) distribution…"
  },
  {
    "objectID": "slides/11-slides-old.html#constructing-confidence-intervals-for-regression-coefficients-2",
    "href": "slides/11-slides-old.html#constructing-confidence-intervals-for-regression-coefficients-2",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nll &lt;- beta - z_fs *se\nul &lt;- beta + z_fs *se\ncbind(ll,ul)\n\n                      ll          ul\n(Intercept)  0.173733660 0.381347516\nage          0.007818443 0.010615460\neducation_n -0.037069758 0.005440772\nsexMale      0.050114390 0.138526568\n\n# Compare to R:\nconfint(m1)\n\n                   2.5 %      97.5 %\n(Intercept)  0.173733660 0.381347516\nage          0.007818443 0.010615460\neducation_n -0.037069758 0.005440772\nsexMale      0.050114390 0.138526568"
  },
  {
    "objectID": "slides/11-slides-old.html#presenting-confidence-intervals-in-a-regression-table",
    "href": "slides/11-slides-old.html#presenting-confidence-intervals-in-a-regression-table",
    "title": "Week 11:",
    "section": "Presenting Confidence Intervals in a Regression Table:",
    "text": "Presenting Confidence Intervals in a Regression Table:\n\n```{r, results=\"asis\"}`r''`\ntexreg::htmlreg(m1, \n  digits = 3, \n  ci.force = T)\n```\n\ntexreg::htmlreg(m1,\n        digits = 3, \n        ci.force = T)\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.278*\n\n\n\n\n \n\n\n[ 0.174; 0.381]\n\n\n\n\nage\n\n\n0.009*\n\n\n\n\n \n\n\n[ 0.008; 0.011]\n\n\n\n\neducation_n\n\n\n-0.016\n\n\n\n\n \n\n\n[-0.037; 0.005]\n\n\n\n\nsexMale\n\n\n0.094*\n\n\n\n\n \n\n\n[ 0.050; 0.138]\n\n\n\n\nR2\n\n\n0.107\n\n\n\n\nAdj. R2\n\n\n0.106\n\n\n\n\nNum. obs.\n\n\n1463\n\n\n\n\n\n\n* 0 outside the confidence interval."
  },
  {
    "objectID": "slides/11-slides-old.html#tidying-regression-models",
    "href": "slides/11-slides-old.html#tidying-regression-models",
    "title": "Week 11:",
    "section": "Tidying Regression Models",
    "text": "Tidying Regression Models\n\nm1 %&gt;%\n  tidy(., conf.int =  T) -&gt; m1_df\nm1_df\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.278    0.0529        5.24 1.80e- 7  0.174     0.381  \n2 age          0.00922  0.000713     12.9  2.83e-36  0.00782   0.0106 \n3 education_n -0.0158   0.0108       -1.46 1.45e- 1 -0.0371    0.00544\n4 sexMale      0.0943   0.0225        4.19 3.02e- 5  0.0501    0.139"
  },
  {
    "objectID": "slides/11-slides-old.html#coefficient-plots-as-an-alternative-to-regression-tables",
    "href": "slides/11-slides-old.html#coefficient-plots-as-an-alternative-to-regression-tables",
    "title": "Week 11:",
    "section": "Coefficient Plots as an Alternative to Regression Tables",
    "text": "Coefficient Plots as an Alternative to Regression Tables\n\nm1_df %&gt;%\n  filter(term !=  \"(Intercept)\")%&gt;%\n  ggplot(aes(x = estimate, \n             y= term, \n             xmin = conf.low,\n             xmax = conf.high,\n             label = round(estimate,3)\n             ))+\n  geom_pointrange()+\n  geom_text(vjust=-1.5)+\n  geom_vline(xintercept = 0, linetype =2)+\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides-old.html#confidence-intervals-summary",
    "href": "slides/11-slides-old.html#confidence-intervals-summary",
    "title": "Week 11:",
    "section": "Confidence Intervals: Summary",
    "text": "Confidence Intervals: Summary\nWhat is a sampling distribution?\n\nA distribution of values we would have observed upon repeated sampling\nBootstrapping (sampling from a sample) approximates the width of the sampling distribution\n\nWhat is a standard error?\n\nStandard deviation of the sampling distribution\n\nDescribes the width or range of plausible observations we would see.\nDecreases as the sample size increases"
  },
  {
    "objectID": "slides/11-slides-old.html#confidence-intervals-summary-1",
    "href": "slides/11-slides-old.html#confidence-intervals-summary-1",
    "title": "Week 11:",
    "section": "Confidence Intervals: Summary",
    "text": "Confidence Intervals: Summary\nWhat is a confidence interval\n\nCoverage interval for a sampling distribution\n\n“A confidence interval is a way of expressing the precision or repeatability of a statistic, how much variation would likely be present across the possible different random samples from the population”\n\nThree components:\n\nPoint Estimate (i.e. a mean, or coefficient)\nConfidence Level (Often 95 percent by convention)\nMargin of Error (+/- some range (typically 2*SD for 95 percent CI))\n\nConfidence is about the interval\n\n95 percent of the intervals construct in this manner would contain the truth.\n\n\nclass: inverse, center, middle # 💡 # Hypothesis Testing ## How likely is it that we would see what did if our hypothesis were true"
  },
  {
    "objectID": "slides/11-slides-old.html#what-is-a-hypothesis-test",
    "href": "slides/11-slides-old.html#what-is-a-hypothesis-test",
    "title": "Week 11:",
    "section": "What is a hypothesis test",
    "text": "What is a hypothesis test\n\nA formal way of assessing statistical evidence. Combines\n\nDeductive reasoning (distribution of a test statistic, if the a null hypothesis were true )\nInductive reasoning (based on the test statistic we observed, how likely is it that we would observe it if the null were true?)"
  },
  {
    "objectID": "slides/11-slides-old.html#what-is-a-test-statistic",
    "href": "slides/11-slides-old.html#what-is-a-test-statistic",
    "title": "Week 11:",
    "section": "What is a test statistic?",
    "text": "What is a test statistic?\n\nA way of summarizing data\n\ndifference of means\ncoefficients from a linear model\ncoefficients from a linear model divded by their standard errors\nR^2"
  },
  {
    "objectID": "slides/11-slides-old.html#what-is-a-null-hypothesis",
    "href": "slides/11-slides-old.html#what-is-a-null-hypothesis",
    "title": "Week 11:",
    "section": "What is a null hypothesis?",
    "text": "What is a null hypothesis?\n\nA statement about the world\n\nOnly interesting if we reject it\nWould yield a distribution of test statistics “under the null”\nTypically something like “X has no effect on Y” (Null = no effect)\nNever accept the null can only reject"
  },
  {
    "objectID": "slides/11-slides-old.html#what-is-a-p-value",
    "href": "slides/11-slides-old.html#what-is-a-p-value",
    "title": "Week 11:",
    "section": "What is a p-value?",
    "text": "What is a p-value?\n\nA p-value is a conditional probability summarizing the likelihood of observing a test statistic as far from our hypothesis or farther, if our hypothesis were true.\nIt’s the area in the “tails of the curve” of the distribution of the test statistic under the null."
  },
  {
    "objectID": "slides/11-slides-old.html#how-do-we-do-hypothesis-testing",
    "href": "slides/11-slides-old.html#how-do-we-do-hypothesis-testing",
    "title": "Week 11:",
    "section": "How do we do hypothesis testing?",
    "text": "How do we do hypothesis testing?\n\nPosit a hypothesis (e.g. \\(\\beta = 0\\))\n\n–\n\nCalculate the test statistic (e.g. \\((\\hat{\\beta}-\\beta)/se_\\beta\\))\n\n–\n\nDerive the distribution of the test statistic under the null via\n\n\nSimulation\nAsymptotic theory\n\n–\n\nCompare the test statistic to the distribution under the null\n\nIf it’s in the tails \\(\\to\\) very unlikely that we would observe what we did if our hypothesis were true\n\n\n–\n\nCalculate p-value\n\nQuantify how often we would see test statistics as big or bigger\nTwo-side tests: how often do we see test statics as big or bigger in absolute value as our observed test statistic\nOne-side test: how often do we see test statistics as extreme as our observed statistic in a particular direction (positive/negative)\n\n\n–\n\nReject or fail to reject/retain our hypothesis based on some threshold of statistical significance (e.g. p &lt; 0.05)"
  },
  {
    "objectID": "slides/11-slides-old.html#outcomes-of-hypothesis-tests",
    "href": "slides/11-slides-old.html#outcomes-of-hypothesis-tests",
    "title": "Week 11:",
    "section": "Outcomes of Hypothesis Tests",
    "text": "Outcomes of Hypothesis Tests\n\nTwo conclusions from of a hypothesis test: we can reject or fail to reject a hypothesis test.\nWe never “accept” a hypothesis, since there are, in theory, an infinite number of other hypotheses we could have tested.\n\nOur decision can produce four outcomes and two types of error:\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative"
  },
  {
    "objectID": "slides/11-slides-old.html#outcomes-of-hypothesis-tests-1",
    "href": "slides/11-slides-old.html#outcomes-of-hypothesis-tests-1",
    "title": "Week 11:",
    "section": "Outcomes of Hypothesis Tests",
    "text": "Outcomes of Hypothesis Tests\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative\n\n\n\nSuppose we chose to reject a hypothesis if our p-value was less than 0.05.\nWhat we’re saying is that we’re willing to falsely reject our hypothesis 5 times out of 100.\nTypically we want to minimize this false positive rate (Type 1 error), but there’s a trade off:\n\nReducing Type 1 error means, we’re more likely to make a Type 2 error – failing to reject when our null is false.\n\nclass:inverse, middle, center # 💪 ## Application: Hypothesis Testing for Linear Models"
  },
  {
    "objectID": "slides/11-slides-old.html#posit-a-null-hypothesis",
    "href": "slides/11-slides-old.html#posit-a-null-hypothesis",
    "title": "Week 11:",
    "section": "0. Posit a Null Hypothesis",
    "text": "0. Posit a Null Hypothesis\n\nTypically, we will test a null hypothesis that the coefficient for variable \\(X\\) equals 0 (e.g. \\(H_0: \\beta_x = 0\\) )\nOur alternative hypothesis in a two sided test then is that \\(\\beta_0\\) doesn’t equal 0 (\\(H_1: \\beta_x \\neq 0\\))\nIn a one-sided test, our alternative is directional (\\(H_1: \\beta_x &gt; 0\\) or \\(H_1: \\beta_x &lt; 0\\) )\n\nOne-sided tests are rare in practice – need a strong substantive reason for directional expectation"
  },
  {
    "objectID": "slides/11-slides-old.html#calculate-the-test-statistic",
    "href": "slides/11-slides-old.html#calculate-the-test-statistic",
    "title": "Week 11:",
    "section": "1. Calculate the test statistic",
    "text": "1. Calculate the test statistic\nFor a linear regression, we could use the \\(\\hat{\\beta}\\) as our test statistic.\nIn practice, we we use a “t-stat” which is our observed coefficiet, \\(\\hat{\\beta}\\) minus our hypothesized value \\(\\beta\\) (e.g. 0), divided by the standard error of \\(\\hat{\\beta}\\).\n\\[t= \\frac{\\hat\\beta-\\beta}{\\widehat{SE}_{\\hat{\\beta}}} \\sim \\text{Students's } t \\text{ with } n-k \\text{ degrees of freedom}\\] Fisher showed that this statistic from a regression follows a \\(t\\) distribution – which looks like a normal distribution but with “fatter tails” (e.g. more probability assigned to extreme values)"
  },
  {
    "objectID": "slides/11-slides-old.html#calculate-the-test-statistic-in-r",
    "href": "slides/11-slides-old.html#calculate-the-test-statistic-in-r",
    "title": "Week 11:",
    "section": "1. Calculate the test statistic in R",
    "text": "1. Calculate the test statistic in R\nWe can caclulate test statistics from our model by dividing the coefficients by their standard errors\n\nt_stat &lt;- coef(m1) / summary(m1)$coef[,2]\nt_stat\n\n(Intercept)         age education_n     sexMale \n   5.244555   12.927982   -1.459477    4.185356 \n\n\nWhich is exactly what the third column of summary() shows\n\nsummary(m1)$coef\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.277540588 0.0529197543  5.244555 1.797036e-07\nage          0.009216952 0.0007129459 12.927982 2.827291e-36\neducation_n -0.015814493 0.0108357258 -1.459477 1.446491e-01\nsexMale      0.094320479 0.0225358310  4.185356 3.017253e-05"
  },
  {
    "objectID": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-via",
    "href": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-via",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null via",
    "text": "2. Derive the distribution of the test statistic under the null via\nTwo approaches:\n\nSimulation: Calculate test statistics in a world where \\(H_0\\) is true\nAsymptotic theory: Use statistics to approximate this distribution"
  },
  {
    "objectID": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null",
    "href": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nWe can make the null true, by randomly permuting (sampling without replacement) the outcome of our model:\n\n# One perumtation\nset.seed(123)\ndf_drww$support_war01_null &lt;- sample(df_drww$support_war01)\n\nOur permuted outcome is now uncorrelated with any predictors, it’s just a random sample of 0s, and 1s.\n\ncor(df_drww$support_war01, df_drww$age, use = \"complete.obs\")\n\n[1] 0.3093656\n\ncor(df_drww$support_war01_null, df_drww$age, use = \"complete.obs\")\n\n[1] -0.0005132924"
  },
  {
    "objectID": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-1",
    "href": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-1",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nAs such when we estimate a model with data where we have made our null hypothesis true (e.g \\(\\beta = 0\\) for all predictors), we get coefficients that are close to 0. Of course, by chance some might be a little negative, or a little positive.\n\nm1_null &lt;- lm(support_war01_null ~ age + education_n + sex, df_drww)\n\ntidy(m1_null)%&gt;%mutate_if(is.numeric, round,2)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     0.74      0.05     13.6     0   \n2 age             0         0        -0.07    0.94\n3 education_n    -0.01      0.01     -0.8     0.43\n4 sexMale         0.02      0.02      0.65    0.52"
  },
  {
    "objectID": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-2",
    "href": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-2",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nThe logic of hypothesis testing is to compare what we observed, to what we could have observed when our null is true.\nBy repeatedly permuting our outcome, estimating models, caclulating test statistics to generate the distributions under the null\nTo do this, we’ll write a function to make our life easier:\n\nmy_null_fn &lt;- function(\n  df=df_drww, \n  y = \"support_war01\", \n  f = formula(m1)\n  ){\n  df[, y] &lt;- sample(df[,y])\n  m &lt;- lm(f, df)\n  stat &lt;- summary(m)$coef[,3]\n  return(stat)\n}\nmy_null_fn()\n\n(Intercept)         age education_n     sexMale \n 11.0493734   1.8113230   0.9787517   0.6637328 \n\nmy_null_fn()\n\n(Intercept)         age education_n     sexMale \n 13.8126681   0.3840811  -1.4405531  -0.9781048"
  },
  {
    "objectID": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-3",
    "href": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-3",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nThen we’ll use the replicate() function to repeat this process 1000, saving the distribution of test statistics \\(((\\hat{\\beta}-\\beta)/se_\\beta)\\) to an object called m1_null\n\nset.seed(123)\nm1_null &lt;- data.frame(t(\n  replicate(1000, my_null_fn(df_drww, \"support_war01\",formula(m1)))))\nhead(m1_null)\n\n  X.Intercept.         age education_n    sexMale\n1     13.58601 -0.06917489  -0.7952217  0.6460818\n2     11.04937  1.81132298   0.9787517  0.6637328\n3     13.81267  0.38408113  -1.4405531 -0.9781048\n4     13.15015  0.04525689  -0.9986349  1.0681923\n5     13.41492 -0.59487147   0.4026906 -1.5637179\n6     13.43857 -1.11574477   0.3126860 -0.7188458"
  },
  {
    "objectID": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-4",
    "href": "slides/11-slides-old.html#derive-the-distribution-of-the-test-statistic-under-the-null-4",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nWe can visualize this distribution of test statistics for the education_n:\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides-old.html#compare-permutation-distribution-to-asymptotic-t-distribution",
    "href": "slides/11-slides-old.html#compare-permutation-distribution-to-asymptotic-t-distribution",
    "title": "Week 11:",
    "section": "2. Compare Permutation Distribution to Asymptotic t-Distribution",
    "text": "2. Compare Permutation Distribution to Asymptotic t-Distribution\nThis simulated distribution (grey) can be approximated by a \\(t\\) distribution (black), with \\(n\\) (the number of observations) minus \\(k\\) (the number of coefficients in our model) degrees of freedom.\n\nRoughly, you can think of “degrees of freedom” as a way of reflecting the fact that we have to estimate parameters of the distribution, and so we use up pieces of information (degrees of freedom),\nBecause we’re estimating parameters our uncertainty (the spread of the distribution) increases as the degrees of freedom decreases\nFor a small \\(N\\) reducing degrees of freedom, can increase uncertainty a lot\nFor a large \\(N\\) the differences are minute\n\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col = \"grey\")+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  stat_function(fun =dt,args =list(df = m1$df.residual), #&lt;&lt;\n                xlim =c(-3.5,3.5))+ #&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides-old.html#compare-the-test-statistic-to-the-distribution-assuming-the-null-were-true",
    "href": "slides/11-slides-old.html#compare-the-test-statistic-to-the-distribution-assuming-the-null-were-true",
    "title": "Week 11:",
    "section": "3. Compare the test statistic to the distribution assuming the null were true",
    "text": "3. Compare the test statistic to the distribution assuming the null were true\nNext we’ll compare our observed test statistic -1.45 to its hypothesized distribution under the null\n\nFor a two sided test, we interested in absolute distance from the null, so we’ll put lines at both -1.45 and 1.45\n\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col = \"grey\")+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  stat_function(fun =dt,args =list(df = m1$df.residual),\n                xlim =c(-3.5,3.5))+\n  geom_vline(xintercept =t_stat[\"education_n\"], linetype = 3)+ #&lt;&lt;\n  geom_vline(xintercept =t_stat[\"education_n\"]*-1, linetype = 3)+ #&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides-old.html#calculate-p-value",
    "href": "slides/11-slides-old.html#calculate-p-value",
    "title": "Week 11:",
    "section": "4. Calculate p-value",
    "text": "4. Calculate p-value\nTo quantify how often we would see test statistics as big or bigger, we can simply take the mean of a logical comparison of the absolute value of the test statistics under the null greater than the absolute value of our observed test statistic\n\n# Simulation\nmean(abs(m1_null$education_n) &gt; abs(t_stat[\"education_n\"]))\n\n[1] 0.145\n\n\nOr, we can calculate the area under the curve for a \\(t\\)-distribution, as with 1459 degrees of freedom, that is less than -1.45 and multiply this by 2 (because the distribution is symetric), to get the p-value\n\n# Asymptotic\n2*pt(t_stat[\"education_n\"], m1$df.residual)\n\neducation_n \n  0.1446491"
  },
  {
    "objectID": "slides/11-slides-old.html#calculate-p-value-1",
    "href": "slides/11-slides-old.html#calculate-p-value-1",
    "title": "Week 11:",
    "section": "4. Calculate p-value",
    "text": "4. Calculate p-value\n\n# Asymptotic\n2*pt(t_stat[\"education_n\"], m1$df.residual)\n\neducation_n \n  0.1446491 \n\n\nOs exactly what R’s summary() function returns:\n\n# R's summary\nsummary(m1)$coef[4,]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n9.432048e-02 2.253583e-02 4.185356e+00 3.017253e-05 \n\n\nVisually, these calculations look like this:\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col=\"grey\")+\n  geom_rug(col = ifelse(abs(m1_null$education_n) &gt;= abs(t_stat[\"education_n\"]), \"red\",\"black\"))+ #&lt;&lt;\n  geom_vline(xintercept =0, linetype = 2)+\n  geom_vline(xintercept =t_stat[\"education_n\"], linetype = 3)+\n  geom_vline(xintercept =t_stat[\"education_n\"]*-1, linetype = 3)+\n  stat_function(fun =dt,args =list(df = m1$df.residual),\n                xlim =c(-3.5,3.5))+\n  stat_function(fun =dt,args =list(df = m1$df.residual), #&lt;&lt;\n                geom =\"area\", xlim = c(-3.5,t_stat[\"education_n\"]),#&lt;&lt;\n                alpha = .5,fill =\"red\")+#&lt;&lt;\n  stat_function(fun =dt, args =list(df = m1$df.residual), #&lt;&lt;\n                geom =\"area\", xlim = c(abs(t_stat[\"education_n\"]),3.5),#&lt;&lt;\n                alpha = .5, fill =\"red\")+#&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides-old.html#reject-or-fail-to-rejectretain-our-hypothesis",
    "href": "slides/11-slides-old.html#reject-or-fail-to-rejectretain-our-hypothesis",
    "title": "Week 11:",
    "section": "Reject or fail to reject/retain our hypothesis",
    "text": "Reject or fail to reject/retain our hypothesis\nFor an estimated p-value of 0.145 for coefficient on education_n and a significance threshold of p &lt; 0.05, we would…\n–\nFail to reject the null hypothesis that \\(H_0: \\beta_{education} = 0\\)\nBecause in a world where the truth was \\(\\beta_{education} = 0\\) test statistics reflecting coefficients as far as \\(t-stat = |-1.459|\\) about 14.5 percent of the time.\nIf we were to reject the null, 14.5 percent of the time, we would be making a Type-1 error | False Positive | concluding there was a relationship when in fact their wasn’t.\nclass:inverse, middle, center # Break\nclass:inverse, middle, center # Final Projects"
  },
  {
    "objectID": "slides/11-slides-old.html#timelines-this-week",
    "href": "slides/11-slides-old.html#timelines-this-week",
    "title": "Week 11:",
    "section": "Timelines: This Week",
    "text": "Timelines: This Week\nNOTE: I’m leaving these slides in for nw\n\nFeedback on Data today\nQuestions/office hours today tomorrow/by appointment\nThursday: Work on analyzing your data\nDrafts are still due April 24th.\nYour draft need not, and probably will not, be a completed project.\nAt a minimum what you want is:\n\nA clean data set\nExploratory descriptives\nInitial results\n\nThe rest can come later. Our goal on Thursday is to move you from the data portions of your projects, into the analysis."
  },
  {
    "objectID": "slides/11-slides-old.html#timelines-next-week",
    "href": "slides/11-slides-old.html#timelines-next-week",
    "title": "Week 11:",
    "section": "Timelines: Next Week",
    "text": "Timelines: Next Week\n\nTuesday, April 26: Workshop: Review and Presentations\nThursday April 28: Workshop: Producing Presentations\n\n7-10 Slides\nDrafts/Analysis, just need to fill those slides\n\nSunday, May 1: Presentations due (Monday at the latests)"
  },
  {
    "objectID": "slides/11-slides-old.html#timelines-first-week-of-may",
    "href": "slides/11-slides-old.html#timelines-first-week-of-may",
    "title": "Week 11:",
    "section": "Timelines: First Week of May:",
    "text": "Timelines: First Week of May:\n\nTuesday, May 3: Presentations\nThursday, May 5: Last class and Food?\n\nClass at Dolores at 5pm?\nBagels/Treats in class\n\nSunday May 8: Final Papers Due"
  },
  {
    "objectID": "slides/11-slides-old.html#strucutre-of-final-paper-and-drafts",
    "href": "slides/11-slides-old.html#strucutre-of-final-paper-and-drafts",
    "title": "Week 11:",
    "section": "Strucutre of Final Paper and Drafts:",
    "text": "Strucutre of Final Paper and Drafts:\nSeven sections:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides-old.html#focus-on-for-sunday",
    "href": "slides/11-slides-old.html#focus-on-for-sunday",
    "title": "Week 11:",
    "section": "Focus on for Sunday",
    "text": "Focus on for Sunday\nSeven sections:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides-old.html#motivating-questions",
    "href": "slides/11-slides-old.html#motivating-questions",
    "title": "Week 11:",
    "section": "Motivating Questions:",
    "text": "Motivating Questions:\nIn the reset of today’s class, we’ll get some practice putting together the various skills you need for your drafts by exploring the following:\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\nHave these perceptions about vaccines changed over time?"
  },
  {
    "objectID": "slides/11-slides-old.html#tasks",
    "href": "slides/11-slides-old.html#tasks",
    "title": "Week 11:",
    "section": "Tasks:",
    "text": "Tasks:\nTo explore these questions, we need to\n\nGet setup to work\nLoad our data\nRecode our data\nSummarize our data\nSpecify our expectations\nEstimate models to test these expectations\nPresent and interpret results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/11-slides-old.html#new-packages-1",
    "href": "slides/11-slides-old.html#new-packages-1",
    "title": "Week 11:",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anesr)"
  },
  {
    "objectID": "slides/11-slides-old.html#packages-for-today-1",
    "href": "slides/11-slides-old.html#packages-for-today-1",
    "title": "Week 11:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"modelr\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/11-slides-old.html#define-a-function-to-load-and-if-needed-install-packages-1",
    "href": "slides/11-slides-old.html#define-a-function-to-load-and-if-needed-install-packages-1",
    "title": "Week 11:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/11-slides-old.html#load-packages-for-today-1",
    "href": "slides/11-slides-old.html#load-packages-for-today-1",
    "title": "Week 11:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         purrr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign        modelr           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Loading Data"
  },
  {
    "objectID": "slides/11-slides-old.html#data",
    "href": "slides/11-slides-old.html#data",
    "title": "Week 11:",
    "section": "Data",
    "text": "Data\nNow that we have anesr installed, let’s load data from the 2016 and 2020 National Election Studies:\n\n# Load data\ndata(timeseries_2016, package = \"anesr\")\ndata(timeseries_2020, package = \"anesr\")\n\nAnd copy those data frames into new dataframes with shorter names\n\n# Rename datasets\nnes16 &lt;- timeseries_2016\nnes20 &lt;- timeseries_2020\n\nclass:inverse, center, middle # 💪 ## Recoding Data"
  },
  {
    "objectID": "slides/11-slides-old.html#finding-variables-of-interest",
    "href": "slides/11-slides-old.html#finding-variables-of-interest",
    "title": "Week 11:",
    "section": "Finding variables of interest",
    "text": "Finding variables of interest\nOur primary outcome of interest are beliefs about vaccines.\nVariables V162162x in the 2016 NES and V202383x in the 2020 NES will serve as our primary outcome of interest, summarizing respondents answer to the following question:\n\nDo the health benefits of vaccinations generally outweigh the risks, do the risks outweigh the benefits, or is there no difference?\n\nSimilarly, V161158x in the 2016 NES and V201231x in the 2020 NES will serve our key predictor (respondent’s partisanship).\nFinally, we’ll control respondents’ age, using V161267 in the 2016 NES and V201507x in the 2020 NES\nLet’s take a look at the values and distributions of these raw variables and think about what we need to do to recode these data so that they are suitable for analysis"
  },
  {
    "objectID": "slides/11-slides-old.html#look-at-the-distribution-and-coding-of-our-outcome-vaccine-beliefs",
    "href": "slides/11-slides-old.html#look-at-the-distribution-and-coding-of-our-outcome-vaccine-beliefs",
    "title": "Week 11:",
    "section": "Look at the distribution and coding of our outcome: Vaccine Beliefs",
    "text": "Look at the distribution and coding of our outcome: Vaccine Beliefs\nThe variables in the NES datasets are of a class labelled which allows numeric values to have substantive labels\n\nclass(nes16$V162162x)\n\n[1] \"haven_labelled\"\n\n\nOur outcome variable has the following labels:\n\nlabelled::val_labels(nes16$V162162x)\n\n                                   -9. Refused \n                                            -9 \n                                -8. Don't know \n                                            -8 \n-7. No post data, deleted due to incomplete IW \n                                            -7 \n                -6. No post-election interview \n                                            -6 \n                      1. Benefits much greater \n                                             1 \n                2. Benefits moderately greater \n                                             2 \n                  3. Benefits slightly greater \n                                             3 \n                              4. No difference \n                                             4 \n                     5. Risks slightly greater \n                                             5 \n                   6. Risks moderately greater \n                                             6 \n                         7. Risks much greater \n                                             7 \n\n\nAnd distribution of responses:\n\ntable(nes16$V162162x)\n\n\n  -9   -8   -7   -6    1    2    3    4    5    6    7 \n  21   28   86  536 1687  726  258  539   96  211   82"
  },
  {
    "objectID": "slides/11-slides-old.html#recoding-our-outcome-variable",
    "href": "slides/11-slides-old.html#recoding-our-outcome-variable",
    "title": "Week 11:",
    "section": "Recoding our outcome variable",
    "text": "Recoding our outcome variable\nWhat transformations do we need to make to V162162x in nes16 and V202383x in nes20 so that these variables are suitable for analysis?\n–\n\nRecode negative values to be NA\n\n–\n\nReverse code so that higher values indicate greater belief in the benefits of vaccines\n\n–\n\nCreate an indicator of people who are skeptical of the benefits of vaccines"
  },
  {
    "objectID": "slides/11-slides-old.html#recoding-v162162x-in-2016-nes",
    "href": "slides/11-slides-old.html#recoding-v162162x-in-2016-nes",
    "title": "Week 11:",
    "section": "Recoding V162162x in 2016 NES",
    "text": "Recoding V162162x in 2016 NES\n\nnes16 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V162162x &lt; 0, NA, (V162162x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes16 # Save recodes to nes16"
  },
  {
    "objectID": "slides/11-slides-old.html#recoding-v202383x-in-2020-nes",
    "href": "slides/11-slides-old.html#recoding-v202383x-in-2020-nes",
    "title": "Week 11:",
    "section": "Recoding V202383x in 2020 NES",
    "text": "Recoding V202383x in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V202383x &lt; 0, NA, (V202383x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes20 # Save recodes to nes20"
  },
  {
    "objectID": "slides/11-slides-old.html#recoding-predictors",
    "href": "slides/11-slides-old.html#recoding-predictors",
    "title": "Week 11:",
    "section": "Recoding Predictors",
    "text": "Recoding Predictors\nNow we repeat this process for our key predictor, partisanship.\n\nRecode the the summary partisanship variables V161158x in nes16 and V201231x in nes20\nCreate indicators from this recoded variable that classify partisanship as categorical variable (with Democrats as the reference category)\n\nAnd our covariate, age variables V161267 in nes16 and V201507x in nes20\n\nRecode negative values to be NA"
  },
  {
    "objectID": "slides/11-slides-old.html#recoding-partisanship-v161158x-in-2016-nes",
    "href": "slides/11-slides-old.html#recoding-partisanship-v161158x-in-2016-nes",
    "title": "Week 11:",
    "section": "Recoding Partisanship (V161158x) in 2016 NES",
    "text": "Recoding Partisanship (V161158x) in 2016 NES\n\nnes16 %&gt;%\n  mutate(\n    pid = ifelse(V161158x &lt; 0, NA, V161158x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V161267 &lt; 0, NA, V161267)\n  ) -&gt; nes16"
  },
  {
    "objectID": "slides/11-slides-old.html#recoding-partisanship-v201231x-in-2020-nes",
    "href": "slides/11-slides-old.html#recoding-partisanship-v201231x-in-2020-nes",
    "title": "Week 11:",
    "section": "Recoding Partisanship (V201231x) in 2020 NES",
    "text": "Recoding Partisanship (V201231x) in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    pid = ifelse(V201231x &lt; 0, NA, V201231x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V201507x &lt; 0, NA, V201507x)\n  ) -&gt; nes20"
  },
  {
    "objectID": "slides/11-slides-old.html#progress-report",
    "href": "slides/11-slides-old.html#progress-report",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data📥\nSpecify our expectations\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Summarizing data"
  },
  {
    "objectID": "slides/11-slides-old.html#tables-of-descriptive-statistics",
    "href": "slides/11-slides-old.html#tables-of-descriptive-statistics",
    "title": "Week 11:",
    "section": "Tables of descriptive statistics",
    "text": "Tables of descriptive statistics\n\nCreate a object with the names of the variables you want to summarize\nSelect these variables\nPivot the data\nCalculate summary statistics\nFormat as an html table"
  },
  {
    "objectID": "slides/11-slides-old.html#tables-of-descriptive-statistics-1",
    "href": "slides/11-slides-old.html#tables-of-descriptive-statistics-1",
    "title": "Week 11:",
    "section": "Tables of descriptive statistics",
    "text": "Tables of descriptive statistics\n\n# 1. Create a object with the names of the variables you want to summarize\nthe_vars &lt;- c(\"vaccine_skeptic01\",\"pid\",\"age\")\n# 2. Select these variables\nnes16 %&gt;%\n  select(all_of(the_vars)) %&gt;%\n# 3. Pivot the data\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  # 3. Calculate summary statistics\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) -&gt; sum_tab"
  },
  {
    "objectID": "slides/11-slides-old.html#format-table",
    "href": "slides/11-slides-old.html#format-table",
    "title": "Week 11:",
    "section": "Format Table",
    "text": "Format Table\n\nknitr::kable(sum_tab,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =2) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 3, end_row =3)\n\n\n\n\nDescriptive Statistics\n\n\nVariable\nmin\np25\nMedian\nmean\np75\nmax\nmissing\n\n\n\n\nOutcome\n\n\nvaccine_skeptic01\n0\n0\n0\n0.26\n0\n1\n671\n\n\nKey Predictors\n\n\npid\n1\n2\n4\n3.86\n2\n7\n23\n\n\nCovariates\n\n\nage\n18\n34\n50\n49.58\n34\n90\n121"
  },
  {
    "objectID": "slides/11-slides-old.html#progress-report-1",
    "href": "slides/11-slides-old.html#progress-report-1",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data ✅\nSpecify our expectations 📥\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Specificying Expecations"
  },
  {
    "objectID": "slides/11-slides-old.html#specificying-expecations",
    "href": "slides/11-slides-old.html#specificying-expecations",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nConsider our first two motivating questions\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\n\nAnd some illustrative stereotypes:\n\n“Republicans are anti-science”\n“Liberal always for Goopy pseudo-science”\n“Independents love to do their own research”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides-old.html#specificying-expecations-1",
    "href": "slides/11-slides-old.html#specificying-expecations-1",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nSimilarly, consider our third question:\n\nHave these perceptions about vaccines changed over time?\n\nAnd some similar simplified claims:\n\n“The Covid-19 vaccine is a miracle of modern science”\n“Social media is rife with misinformation about the Covid-19 vaccine”\n“Politicians are politicizing vaccine politics for political benefits”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides-old.html#specificying-expecations-2",
    "href": "slides/11-slides-old.html#specificying-expecations-2",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nOur goal is to take claims/conventional wisdom/theories, and derive their empirical implications:\n\nH1: Partisan Differences in Vaccine Skepticism\n\nH1a: Republicans will be the most skeptical of vaccines\nH1b: Democrats will be the most skeptical of vaccines\nH1a: Independents will be the most skeptical of vaccines\n\nH2: Temporal Differences in Vaccine Skepticism\n\nH2a: Vaccine skepticism will decrease from 2016 to 2020 with the widespread roll out of the Covid-19 vaccine\nH2b: Vaccine skepticism will increase from 2016 to 2020 with increased amounts of misinformation about the Covid-19 vaccine\n\nH3: Partisan Difference in Vaccine Skepticism Over Time Partisan differences in Vaccine Skepticism will increase from 2016 to 2020 with the politicization of Covid-19 policies"
  },
  {
    "objectID": "slides/11-slides-old.html#motivating-your-expectations.",
    "href": "slides/11-slides-old.html#motivating-your-expectations.",
    "title": "Week 11:",
    "section": "Motivating your expectations.",
    "text": "Motivating your expectations.\nIn your papers, unlike in these slides, your expectations should be grounded in existing theory, research, and evidence. For the present question, we might cite sources such as:\n\nEnders, Adam M., and Steven M. Smallpage. “Informational cues, partisan-motivated reasoning, and the manipulation of conspiracy beliefs.” Political Communication 36.1 (2019): 83-102.\nStecula, Dominik A., and Mark Pickup. “How populism and conservative media fuel conspiracy beliefs about COVID-19 and what it means for COVID-19 behaviors.” Research & Politics 8.1 (2021): 2053168021993979.\nJennings, Will, et al. “Lack of trust, conspiracy beliefs, and social media use predict COVID-19 vaccine hesitancy.” Vaccines 9.6 (2021): 593.\nHollander, Barry A. “Partisanship, individual differences, and news media exposure as predictors of conspiracy beliefs.” Journalism & Mass Communication Quarterly 95.3 (2018): 691-713."
  },
  {
    "objectID": "slides/11-slides-old.html#model-specification",
    "href": "slides/11-slides-old.html#model-specification",
    "title": "Week 11:",
    "section": "Model Specification",
    "text": "Model Specification\nTranslate these expectations into empirical models requires choices about how to specify our models\n–\n\nHow should we measure/operationalize our outcome\n\nShould we measure beliefs about vaccines with 7-point ordinal scale or as a binary indicator of vaccine skepticism\n\nHow should we measure/operationalize our key predictor(s)\n\nShould we measure partisanship using a 7 point scale or as categorical variable?\n\nWhat should we control for in our model?\n\nFactors likely to predict both our outcome and our key predictor of interest\n\n\n–\n\nThere are rarely definitive answers to these questions.\nIn practice, we will often estimate multiple models to try and show that our findings are robust to alternative modeling strategies/specifications"
  },
  {
    "objectID": "slides/11-slides-old.html#model-specification-1",
    "href": "slides/11-slides-old.html#model-specification-1",
    "title": "Week 11:",
    "section": "Model Specification",
    "text": "Model Specification\nFor your projects, every group will almost surely estimate some form of the following:\n\nBaseline bivariate model: The simplest test of the relationship between your outcome and key predictor\nMultiple regression model: A test of the robustness of this relationship, controlling for alternative explanations\n\nIn practice, I suspect you may estimate multiple regression models such as:\n\nAlternative specifications/operationalizations of outcomes and predictors\nInteraction models to test conditional relationships\nPolynomial models to test non-linear relationships"
  },
  {
    "objectID": "slides/11-slides-old.html#translating-theoretical-expectations-into-empirical-models",
    "href": "slides/11-slides-old.html#translating-theoretical-expectations-into-empirical-models",
    "title": "Week 11:",
    "section": "Translating Theoretical Expectations into Empirical Models",
    "text": "Translating Theoretical Expectations into Empirical Models\nBefore we estimate our models in R, we will write down our models formally and empirical implications of our theoretical expectations in terms of the coefficients of our model.\nFor example, test for partisan differences in vaccine skepticism, we might fit the following baseline model:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} + X\\beta + \\epsilon\\] - If \\(\\beta_1\\) is positive this is consistent with H1a (greater skepticism among Republicans), - If \\(\\beta_2\\) is negative this is consistent with H1b (greater skepticism among Democrats),\nBut how could we test H1c – greater skepticism among Independents, who are “4s” on \\(\\text{PID}_{7pt}\\)?"
  },
  {
    "objectID": "slides/11-slides-old.html#translating-theoretical-expectations-into-empirical-models-1",
    "href": "slides/11-slides-old.html#translating-theoretical-expectations-into-empirical-models-1",
    "title": "Week 11:",
    "section": "Translating Theoretical Expectations into Empirical Models",
    "text": "Translating Theoretical Expectations into Empirical Models\nWe could fit a polynomial regression, including both partisanship and partinaship squared to allow the relationship between partisanship and vaccine skepticism to vary non-linearly\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} +  \\beta_2 \\text{PID}_{7pt}^2+ X\\beta+ \\epsilon\\] Or we could estimate a model treating Partisanship as a categorical variable rather than an ordinal interval variable. In our recoding, we set \"Democrat\" to be the first level of the variable pid3cat, so the model R will estimate by default is:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{Ind} +  \\beta_2 \\text{PID}_{Rep}+ X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides-old.html#testing-differences-over-time.",
    "href": "slides/11-slides-old.html#testing-differences-over-time.",
    "title": "Week 11:",
    "section": "Testing differences over time.",
    "text": "Testing differences over time.\nTesting Hypotheses 2 and 3 involve making comparisons across models estimated on data from different surveys.\nFormally, testing these expectations is a little more complicated\n\nwe could pool our two surveys together include an interaction term for survey year\n\nFor our purposes, we’ll treat these as more qualitative/exploratory hypotheses:\n\nH2a/b implies overall rates of vaccine skepticism will be lower/higher in 2020 compared to 2016\nH3 implies that whatever partisan differences we find in 2016 should be larger in 2020."
  },
  {
    "objectID": "slides/11-slides-old.html#progress-report-2",
    "href": "slides/11-slides-old.html#progress-report-2",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations 📥\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Estimating Empirical Models"
  },
  {
    "objectID": "slides/11-slides-old.html#estimating-empirical-models",
    "href": "slides/11-slides-old.html#estimating-empirical-models",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nHaving derived empirical implications of our theoretical expectations expressed in terms of linear regressions, now we simply have to estimate our models in R.\nWhen estimating the same model on different datasets we can write the formulas once\n\nf1 &lt;- formula(vaccine_skeptic01 ~ pid + age)\nf2 &lt;- formula(vaccine_skeptic01 ~ pid + I(pid^2) + age)\nf3 &lt;- formula(vaccine_skeptic01 ~ pid3cat + age)"
  },
  {
    "objectID": "slides/11-slides-old.html#estimating-empirical-models-1",
    "href": "slides/11-slides-old.html#estimating-empirical-models-1",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nAnd then pass it to lm() with different data arguments:\n\nm1_2016 &lt;- lm(formula = f1, data = nes16)\nm1_2020 &lt;- lm(formula = f1, data = nes20)\nm2_2016 &lt;- lm(formula = f2, data = nes16)\nm2_2020 &lt;- lm(formula = f2, data = nes20)\nm3_2016 &lt;- lm(formula = f3, data = nes16)\nm3_2020 &lt;- lm(formula = f3, data = nes20)"
  },
  {
    "objectID": "slides/11-slides-old.html#estimating-empirical-models-2",
    "href": "slides/11-slides-old.html#estimating-empirical-models-2",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nIf you’ve\n\ncoded your data correctly\ndeveloped clear testable implications from your theoretical expectations\n\nSpecifying and estimating empirical models is straightforward. Literally a few lines of code."
  },
  {
    "objectID": "slides/11-slides-old.html#progress-report-3",
    "href": "slides/11-slides-old.html#progress-report-3",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations ✅\nPresent our results 📥\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis testing (new!)\n\n\nclass:inverse, center, middle # 💪 ## Presenting and Interpreting Your Results"
  },
  {
    "objectID": "slides/11-slides-old.html#presenting-and-interpreting-your-results",
    "href": "slides/11-slides-old.html#presenting-and-interpreting-your-results",
    "title": "Week 11:",
    "section": "Presenting and Interpreting Your Results",
    "text": "Presenting and Interpreting Your Results\nPresenting and interpreting your results is requires both art and science.\nYour goal is to tell a story with your results, walking your reader through the substantive and statsitical interpretation of tables and figures.\nLet’s start by producing a regression table, which provides a concise summary of multiple regression models.\nLike figures, producing a good regression table is an interactive process."
  },
  {
    "objectID": "slides/11-slides-old.html#regression-tables-with-htmlreg",
    "href": "slides/11-slides-old.html#regression-tables-with-htmlreg",
    "title": "Week 11:",
    "section": "Regression Tables with htmlreg",
    "text": "Regression Tables with htmlreg\nThe following code produces a basic regression table.\n\ntexreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020)\n)"
  },
  {
    "objectID": "slides/11-slides-old.html#regression-tables-with-htmlreg-1",
    "href": "slides/11-slides-old.html#regression-tables-with-htmlreg-1",
    "title": "Week 11:",
    "section": "Regression Tables with htmlreg",
    "text": "Regression Tables with htmlreg\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.46***\n\n\n0.35***\n\n\n0.42***\n\n\n0.34***\n\n\n0.32***\n\n\n0.35***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\npid\n\n\n-0.00\n\n\n0.06***\n\n\n \n\n\n0.02***\n\n\n0.04***\n\n\n \n\n\n\n\n \n\n\n(0.00)\n\n\n(0.02)\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n \n\n\n\n\nage\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\npid^2\n\n\n \n\n\n-0.01***\n\n\n \n\n\n \n\n\n-0.00\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n\n\npid3catIndependent\n\n\n \n\n\n \n\n\n0.17***\n\n\n \n\n\n \n\n\n0.20***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\npid3catRepublican\n\n\n \n\n\n \n\n\n-0.02\n\n\n \n\n\n \n\n\n0.10***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides-old.html#customizing-our-regression-table",
    "href": "slides/11-slides-old.html#customizing-our-regression-table",
    "title": "Week 11:",
    "section": "Customizing our Regression Table",
    "text": "Customizing our Regression Table\nLet’s make this regression table more reader friendly and informative by:\n\nGiving the variables in substantive names\nReporting coefficients to 3 decimal places\nUsing a single significance threshold of \\(p &lt; 0.05\\)\nGiving the models custom names\nAdding a header to group models by year\nChanging the caption of the table\n\n\ntexreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020),\n  # Reporting coefficients to 3 decimal places\n  digits = 3,\n  # Using a single significance threshold \n  stars = 0.05,\n  # Giving the variables in substantive names\n  custom.coef.names = c(\n    \"(Intercept)\",\n    \"PID (7pt)\",\n    \"Age\",\n    \"PID&lt;sup&gt;2&lt;/sup&gt; (7pt)\",\n    \"Independent\",\n    \"Republican\"\n  ),\n  # Giving the models custom names\n  custom.model.names = paste(\"Model\",c(1:3,1:3)),\n  # Adding a header to group models by year\n  custom.header = list(\"NES 2016\" = 1:3, \"NES 2020\" = 4:6),\n  # Changing the caption of the table\n  caption = \"Partisanship and Vaccine Skepticism\"\n)\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides-old.html#telling-a-story-with-regression",
    "href": "slides/11-slides-old.html#telling-a-story-with-regression",
    "title": "Week 11:",
    "section": "Telling a Story with Regression",
    "text": "Telling a Story with Regression\nFirst, provide an overview the models presented in the table\n\nExplain what each model is doing conceptually\n\nThen, start with your simplest model (Typically the first column in your table).\n\nUse this as a chance to explain core concepts from the course\n\nWhat is regression\nHow should I interpret a coefficient substantively\nHow should I interepret the statistical signficance of a give coefficient\n\nAs you move from left to right (simple to more complex)\n\nyou need not interpret every single coefficient in the model\ninstead highlight the factors that are important for the reader to note (e.g. a comparison between one coefficient in model or another.)\n\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides-old.html#overview",
    "href": "slides/11-slides-old.html#overview",
    "title": "Week 11:",
    "section": "Overview",
    "text": "Overview\n\nTable 1 presents the results of three specifications exploring the relationship between partisanship and vaccine skepticism using data from the 2016 (Models 1-3) and 2020 (Models 4-5) National Election Studies.\nModels 1 and 4 operationalize partisanship as a 7-point scale, where 1 corresponds to Strong Democrats, 4 to Indepndents, and 7 to Strong Republicans in the 2016 (Model 1) and 2020 (Model 2) surveys.\nModels 2 and 5 allow the relationship between partisanship and vaccine skepticism to vary non-linear again for the 2016 (Model 2) and 2020 (Model 5) elections.\nModels 3 and 6 treat partisanship as categorical variable, describing how Independents and Republicans differ from Democrats, the reference category in these models.\nAll models control age, since (put in substantive justification for controlling for age here)"
  },
  {
    "objectID": "slides/11-slides-old.html#story-testing-for-partisan-differences",
    "href": "slides/11-slides-old.html#story-testing-for-partisan-differences",
    "title": "Week 11:",
    "section": "Story: Testing for Partisan Differences",
    "text": "Story: Testing for Partisan Differences\n\nThe results from Model 1 provide little initial evidence for partisan differences in vaccine skepticism in the 2016 Election.\n\nThe coefficient on the partisanship variable is -0.005, suggesting that a unit increase in partisanship (going from being a Strong Democrat to just a Democrat, or an Independent to an independent who leans Republican), is associated with just a 0.5 percentage point increase in the probability of being a vaccine skeptic (believing that the risks of vaccination outweigh the benefits or that their is no difference in the risks versus benefits).\n\nFurthermore the 95-percent confidence interval for this estimate (-0.011, 0.002) brackets 0, suggesting the true population estimate from this model could be either positive or negative. Similarly, we fail to reject the null hypothesis that the true coefficient on partisanship in this model is 0 as the test statistic for this estimate ( -1.38) corresponds to a p-value of 0.168 suggesting that we would see test statistics this large or larger fairly often when the true relationship was 0.\nIn some the results from Model 1 provide little support for any of the expectations described by H1"
  },
  {
    "objectID": "slides/11-slides-old.html#testing-for-partisan-differences-model-2",
    "href": "slides/11-slides-old.html#testing-for-partisan-differences-model-2",
    "title": "Week 11:",
    "section": "Testing for Partisan Differences: Model 2",
    "text": "Testing for Partisan Differences: Model 2\n\nWhile coefficients from Model 1 suggest little evidence of partisan differences in vaccine skepticism, the coefficients on both partisanship, and partisanship squared are statistically significant (p &lt; 0.05)."
  },
  {
    "objectID": "slides/11-slides-old.html#interpreting-model-2",
    "href": "slides/11-slides-old.html#interpreting-model-2",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nThe coefficients from polynomial regressions can be difficult to interpret jointly and so Figure 1 presents the predicted values from Model 2, holding age constant at its sample mean.\n\n\npred_df_m2 &lt;- expand_grid(\n  pid = 1:7,\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m2 &lt;- cbind(pred_df_m2, predict(m2_2016,pred_df_m2, interval =\"confidence\"))\npred_df_m2\n\n  pid      age       fit       lwr       upr\n1   1 49.58231 0.2366157 0.2082979 0.2649335\n2   2 49.58231 0.2743408 0.2551659 0.2935157\n3   3 49.58231 0.2945841 0.2729532 0.3162151\n4   4 49.58231 0.2973457 0.2738012 0.3208902\n5   5 49.58231 0.2826255 0.2611041 0.3041469\n6   6 49.58231 0.2504236 0.2300546 0.2707925\n7   7 49.58231 0.2007398 0.1688900 0.2325897"
  },
  {
    "objectID": "slides/11-slides-old.html#interpreting-model-2-1",
    "href": "slides/11-slides-old.html#interpreting-model-2-1",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\npred_df_m2 %&gt;%\n  ggplot(aes(pid, fit, ymin =lwr, ymax =upr))+\n  geom_line()+\n  geom_ribbon(alpha=.2, fill=\"grey\")+\n  theme_bw()+\n  labs(x = \"Partisanship\",\n       y = \"Predicted Vaccine Skepticism\",\n       title = \"Independents are the most skeptical of vaccines\",\n       subtitle = \"Data: 2016 NES\"\n       )"
  },
  {
    "objectID": "slides/11-slides-old.html#interpreting-model-2-2",
    "href": "slides/11-slides-old.html#interpreting-model-2-2",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nWe see from Model 2 that 29.7 percent [27.3%, 32.1%] of Independents in the 2016 NES were predicted to be vaccine skeptics compared to 23.7 percent [20.8%, 26.5%] of Strong Democrats and only 20.1 percent [16.9%, 23.3%] of Strong Republicans."
  },
  {
    "objectID": "slides/11-slides-old.html#testing-for-partisan-differences-model-3",
    "href": "slides/11-slides-old.html#testing-for-partisan-differences-model-3",
    "title": "Week 11:",
    "section": "Testing for Partisan Differences: Model 3",
    "text": "Testing for Partisan Differences: Model 3\nModel 3 tells a similar story to model 2. Again, adjusting for differences in vaccine skepticism explained by age, Model 3 predicts that 41.7 percent [37.7%, 45.6%] of Independents in the 2016 NES are vaccine skeptics compared to 24.2 percent [22.1%, 26.2%] of Democrats, and 22.6 percent [20.4%, 24.8%] of Republicans.\nNote the coefficients from Model 3 imply that the differences between Independents and Democrats are statistically significant (\\(\\beta_{Ind} = 0.175, p &lt; 0.05\\)), the differences between Republicans and Democrats are not (\\(\\beta_{Rep} = -0.004, p = 0.31\\))\n\npred_df_m3 &lt;- expand_grid(\n  pid3cat = c(\"Democrat\", \"Independent\",\"Republican\"),\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m3 &lt;- cbind(pred_df_m3, predict(m3_2016,pred_df_m3, interval =\"confidence\"))\npred_df_m3\n\n      pid3cat      age       fit       lwr       upr\n1    Democrat 49.58231 0.2419547 0.2211228 0.2627867\n2 Independent 49.58231 0.4169043 0.3773539 0.4564547\n3  Republican 49.58231 0.2261496 0.2038046 0.2484947"
  },
  {
    "objectID": "slides/11-slides-old.html#testing-for-differences-over-time",
    "href": "slides/11-slides-old.html#testing-for-differences-over-time",
    "title": "Week 11:",
    "section": "Testing for Differences Over Time",
    "text": "Testing for Differences Over Time\nThe results for the 2016 NES suggest political independents are most skeptical of vaccines.\nThe results for 2020 suggest the relationship between partisanship and vaccine skepticism has changed overtime.\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05\n\n\n\n\n\n\nThe coefficient on partisanship in model 4 is now positive and statistically significant (p &lt; 0.05), suggesting that as respondents become more Republican, they are more likely to be skeptical of vaccines\nThe coefficients from Model 5 suggest the relationship between partisanship skepticism is non linear, which is confirmed by model 6.\nIn Model 6, we see that independents remain the most skeptical of vaccines in 2020 \\((\\beta = 0.20,\\, p &lt;0.05)\\), but that Republicans now tend to be more skeptical of vaccines than Democrats \\((\\beta = 0.10,\\, p &lt;0.05)\\)\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/00-slides.html#overview",
    "href": "slides/00-slides.html#overview",
    "title": "Welcome to POLS 1600",
    "section": "Overview",
    "text": "Overview\n\n\nGoals and Expectations\n\n\n\n\nCourse Structure\n\n\n\n\nCourse Policies\n\n\n\n\nA Few Fundamental Truths"
  },
  {
    "objectID": "slides/00-slides.html#what-you-will-learn",
    "href": "slides/00-slides.html#what-you-will-learn",
    "title": "Welcome to POLS 1600",
    "section": "What you will learn",
    "text": "What you will learn\nYou will learn\n\n\nhow to think like a social scientist\n\n\n\n\nhow to use data to make descriptive, predictive, and causal claims\n\n\n\n\nhow to quantify uncertainty about these claims\n\n\n\n\nhow to present, interpret, and critique these claims"
  },
  {
    "objectID": "slides/00-slides.html#reasons-to-take-this-class",
    "href": "slides/00-slides.html#reasons-to-take-this-class",
    "title": "Welcome to POLS 1600",
    "section": "Reasons to take this class",
    "text": "Reasons to take this class\n\nYou want to change the world"
  },
  {
    "objectID": "slides/00-slides.html#why-is-this-study-important",
    "href": "slides/00-slides.html#why-is-this-study-important",
    "title": "Welcome to POLS 1600",
    "section": "Why is this study important?",
    "text": "Why is this study important?\n\n\nFindings provide evidence of benefits of social spending/universal basic income"
  },
  {
    "objectID": "slides/00-slides.html#why-should-we-believe-these-results",
    "href": "slides/00-slides.html#why-should-we-believe-these-results",
    "title": "Welcome to POLS 1600",
    "section": "Why should we believe these results",
    "text": "Why should we believe these results\n\n\nBecause it’s in the Times?\nBecause the authors are professors at good schools?\nBecause of how the study was done!\n\nRandom assignment provides a reasoned basis for inference\nCreates informative counter-factual comparisons\nPre-registered hypotheses ensure that we’re not cherry-picking results"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\n\nHow strong are the effects?\n\nIs a fifth of a standard deviation a lot?\n\nWhy do we care about brain waves?\nWhat’s the mechanism?\nHow confident are we that these results couldn’t have happened just by chance"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-1",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-1",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-2",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-2",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-3",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-3",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/00-slides.html#reasons-to-take-this-class-1",
    "href": "slides/00-slides.html#reasons-to-take-this-class-1",
    "title": "Welcome to POLS 1600",
    "section": "Reasons to take this class",
    "text": "Reasons to take this class\n\n\nYou want to change the world\n\nData, design, and analysis are incredlibly powerful tools\nYou want to understand their strengths and limits\n\nYou want to be a better consumer of data and knowledge\nYou want to be a better consumer of data and knowledge\nYou want to get a job / go to grad school\nYou have to\nYou’re just in it for the memes"
  },
  {
    "objectID": "slides/00-slides.html#great-expectations",
    "href": "slides/00-slides.html#great-expectations",
    "title": "Welcome to POLS 1600",
    "section": "Great expectations",
    "text": "Great expectations\n\nI expect that you will come to class ready to engage with:\n\nsocial science\ndata\nprogramming\nmath"
  },
  {
    "objectID": "slides/00-slides.html#requirements",
    "href": "slides/00-slides.html#requirements",
    "title": "Welcome to POLS 1600",
    "section": "Requirements",
    "text": "Requirements\nI assume that you will\n\n\nDo the readings\n\n\n\n\nBring your computers 1\n\n\n\n\nWork through classwork\n\n\n\n\nAsk questions\n\n\nIf you only have a desktop/or tablet let me know and we’ll figure out a solution."
  },
  {
    "objectID": "slides/00-slides.html#course-structure",
    "href": "slides/00-slides.html#course-structure",
    "title": "Welcome to POLS 1600",
    "section": "Course structure",
    "text": "Course structure"
  },
  {
    "objectID": "slides/00-slides.html#class",
    "href": "slides/00-slides.html#class",
    "title": "Welcome to POLS 1600",
    "section": "Class",
    "text": "Class\n\nTuesday: Lecture/Demonstration\nThursday: Lab/Exploration"
  },
  {
    "objectID": "slides/00-slides.html#class-websites",
    "href": "slides/00-slides.html#class-websites",
    "title": "Welcome to POLS 1600",
    "section": "Class websites",
    "text": "Class websites\n\nSlides, labsm and additional resources available here: pols1600.paultesta.org\nAssignments uploaded here: Canvas"
  },
  {
    "objectID": "slides/00-slides.html#software-and-computing",
    "href": "slides/00-slides.html#software-and-computing",
    "title": "Welcome to POLS 1600",
    "section": "Software and computing",
    "text": "Software and computing\n\nStatistics done using R\n\nOpen source (free) statistical language\n\nThrough R Studio\n\nAn integrated development environment for R\n\nResults written up using R Markdown\n\nLanguage for combing R code with html Markdown"
  },
  {
    "objectID": "slides/00-slides.html#r",
    "href": "slides/00-slides.html#r",
    "title": "Welcome to POLS 1600",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "slides/00-slides.html#r-studio",
    "href": "slides/00-slides.html#r-studio",
    "title": "Welcome to POLS 1600",
    "section": "R Studio",
    "text": "R Studio"
  },
  {
    "objectID": "slides/00-slides.html#quarto",
    "href": "slides/00-slides.html#quarto",
    "title": "Welcome to POLS 1600",
    "section": "Quarto",
    "text": "Quarto\n\n\n\nProject options in YAML\nCode in triple backtick chunks:\n\nChunk options set with “#|” (hashpipe)\n\n\n```{r}\n#| label = \"simulate_data\"\nx &lt;- rnorm(100)\ny &lt;- 2*x + rnorm(100)\n```\n\nWrite up in Markdown\nOutput rendered as an html file"
  },
  {
    "objectID": "slides/00-slides.html#getting-set-up-for-the-course",
    "href": "slides/00-slides.html#getting-set-up-for-the-course",
    "title": "Welcome to POLS 1600",
    "section": "Getting set up for the course:",
    "text": "Getting set up for the course:\nHere’s a link to a guide to get you setup for the course.\nTake a crack at it after class, over the weekend.\nEmail me with any issues (there are always issues), and drop by my office hours on Tuesday so we can trouble shoot."
  },
  {
    "objectID": "slides/00-slides.html#textbook",
    "href": "slides/00-slides.html#textbook",
    "title": "Welcome to POLS 1600",
    "section": "Textbook",
    "text": "Textbook\n\nhttps://press.princeton.edu/books/paperback/9780691222288/quantitative-social-science"
  },
  {
    "objectID": "slides/00-slides.html#how-to-read-imai",
    "href": "slides/00-slides.html#how-to-read-imai",
    "title": "Welcome to POLS 1600",
    "section": "How to Read Imai",
    "text": "How to Read Imai\n\nActive reading\nCopy and run the code in the text. To do so, do the following:\n\n\nif (!require(\"devtools\")){\n  install.packages(\"devtools\")\n  }\nlibrary(\"devtools\")\ninstall_github(\"kosukeimai/qss-package\",  \n               build_vignettes  =  TRUE)"
  },
  {
    "objectID": "slides/00-slides.html#how-to-read-imai-1",
    "href": "slides/00-slides.html#how-to-read-imai-1",
    "title": "Welcome to POLS 1600",
    "section": "How to Read Imai",
    "text": "How to Read Imai\nOnce you’ve rune the following\n\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\nAnywhere the text loads data:\n\nafghan &lt;- read_csv(\"afgahn.csv\")\n\nYou can do\n\nlibrary(\"qss\")\ndata(\"afghan\")\nsummary(afghan$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.00   22.00   30.00   32.39   40.00   80.00"
  },
  {
    "objectID": "slides/00-slides.html#additional-readings",
    "href": "slides/00-slides.html#additional-readings",
    "title": "Welcome to POLS 1600",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nOccasionally, I will provide additional readings, available on both Canvas and pols1600.paultesta.org"
  },
  {
    "objectID": "slides/00-slides.html#assignments-1",
    "href": "slides/00-slides.html#assignments-1",
    "title": "Welcome to POLS 1600",
    "section": "Assignments",
    "text": "Assignments\nYou have three types of assignments in this course\n\nLabs\nTutorials\nFinal Project"
  },
  {
    "objectID": "slides/00-slides.html#labs",
    "href": "slides/00-slides.html#labs",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs\n\nEach Thursday we will work in groups to complete an in-class lab\nThe labs are designed to reinforce and extend concepts from lecture using real world data."
  },
  {
    "objectID": "slides/00-slides.html#labs-1",
    "href": "slides/00-slides.html#labs-1",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "slides/00-slides.html#labs-2",
    "href": "slides/00-slides.html#labs-2",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "slides/00-slides.html#labs-3",
    "href": "slides/00-slides.html#labs-3",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs\n\nWeeks 1 and 2 we’ll work collectively\nWeeks 3 on, you’ll be assigned to small groups\nEach week:\n\nLog on to the Canvas, download the lab .qmd file\nOpen R Studio\nRender the qmd file to get ready to work\nComplete the lab\nUpload the rendered html file to Canvas by the end of class\n\nOne question randomly graded\n\n100% if correct\n85% if incorrect, but you tried\n0% if you did not try/absent for the lab\n\nComments/Answers posted immediately after class"
  },
  {
    "objectID": "slides/00-slides.html#problem-setstutorials",
    "href": "slides/00-slides.html#problem-setstutorials",
    "title": "Welcome to POLS 1600",
    "section": "Problem Sets/Tutorials",
    "text": "Problem Sets/Tutorials\n\nCoding tutorials to reinforce concepts from lecture and textbook.\nAccessed by running\n\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\n\nComplete the tutorial. Save output as “LASTNAME_TutorialNumber.pdf”\nUpload output to Canvas by Friday by 11:59 pm\nGrades:\n\n100% any upload\n0% no upload"
  },
  {
    "objectID": "slides/00-slides.html#final-project",
    "href": "slides/00-slides.html#final-project",
    "title": "Welcome to POLS 1600",
    "section": "Final Project",
    "text": "Final Project"
  },
  {
    "objectID": "slides/00-slides.html#your-first-assignment",
    "href": "slides/00-slides.html#your-first-assignment",
    "title": "Welcome to POLS 1600",
    "section": "Your First Assignment:",
    "text": "Your First Assignment:\n\nDownload and install R and R Studio\n\nEmail me if you have troubles\nTroubleshoot by Zoom or in-person (111 Thayer Room 339)\n\nWork through 00-software_setup before next class."
  },
  {
    "objectID": "slides/00-slides.html#portals-of-discovery",
    "href": "slides/00-slides.html#portals-of-discovery",
    "title": "Welcome to POLS 1600",
    "section": "Portals of Discovery",
    "text": "Portals of Discovery"
  },
  {
    "objectID": "slides/00-slides.html#errors",
    "href": "slides/00-slides.html#errors",
    "title": "Welcome to POLS 1600",
    "section": "Errors",
    "text": "Errors\n\nish happens\nSeeing red is a good thing\nWe learn by making errors"
  },
  {
    "objectID": "slides/00-slides.html#final-reports",
    "href": "slides/00-slides.html#final-reports",
    "title": "Welcome to POLS 1600",
    "section": "Final Reports",
    "text": "Final Reports\n\nCan be on any topic you like\nMore info to come\nDue dates:\n\nWeek 2 Groups assigned\nWeek 3 Research Topics\nWeek 6 Data Proposal\nWeek 8 Data Explorations\nWeek 11 Drafts\nWeek 12 Presentations\nWeek 13 Final Paper"
  },
  {
    "objectID": "slides/00-slides.html#grading",
    "href": "slides/00-slides.html#grading",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-1",
    "href": "slides/00-slides.html#grading-1",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-2",
    "href": "slides/00-slides.html#grading-2",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-3",
    "href": "slides/00-slides.html#grading-3",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-4",
    "href": "slides/00-slides.html#grading-4",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading\n\n5% Attendance\n10% Class involvement and participation\n10% Tutorials\n30% Labs\n20% Assignments for final paper\n20% Final paper"
  },
  {
    "objectID": "slides/00-slides.html#course-policies",
    "href": "slides/00-slides.html#course-policies",
    "title": "Welcome to POLS 1600",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nCommunity standards\nIncomplete/late work"
  },
  {
    "objectID": "slides/00-slides.html#two-fundamental-truths",
    "href": "slides/00-slides.html#two-fundamental-truths",
    "title": "Welcome to POLS 1600",
    "section": "Two Fundamental Truths",
    "text": "Two Fundamental Truths"
  },
  {
    "objectID": "slides/00-slides.html#testas-first-fundamental-truth",
    "href": "slides/00-slides.html#testas-first-fundamental-truth",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s first fundamental truth",
    "text": "Testa’s first fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#testas-first-fundamental-truth-1",
    "href": "slides/00-slides.html#testas-first-fundamental-truth-1",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s first fundamental truth",
    "text": "Testa’s first fundamental truth\nWhy would I profess my utter ignorance on the first day of class?\nFour possible reasons…"
  },
  {
    "objectID": "slides/00-slides.html#expectation-management",
    "href": "slides/00-slides.html#expectation-management",
    "title": "Welcome to POLS 1600",
    "section": "1. Expectation Management",
    "text": "1. Expectation Management"
  },
  {
    "objectID": "slides/00-slides.html#pedagogical-tomfoolery",
    "href": "slides/00-slides.html#pedagogical-tomfoolery",
    "title": "Welcome to POLS 1600",
    "section": "2. Pedagogical Tomfoolery",
    "text": "2. Pedagogical Tomfoolery"
  },
  {
    "objectID": "slides/00-slides.html#positionality",
    "href": "slides/00-slides.html#positionality",
    "title": "Welcome to POLS 1600",
    "section": "3. Positionality",
    "text": "3. Positionality"
  },
  {
    "objectID": "slides/00-slides.html#epistemology",
    "href": "slides/00-slides.html#epistemology",
    "title": "Welcome to POLS 1600",
    "section": "4. Epistemology",
    "text": "4. Epistemology"
  },
  {
    "objectID": "slides/00-slides.html#testas-second-fundamental-truth",
    "href": "slides/00-slides.html#testas-second-fundamental-truth",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s second fundamental truth",
    "text": "Testa’s second fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#testas-second-fundamental-truth-1",
    "href": "slides/00-slides.html#testas-second-fundamental-truth-1",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s second fundamental truth",
    "text": "Testa’s second fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#two-kinds-of-people-in-this-world",
    "href": "slides/00-slides.html#two-kinds-of-people-in-this-world",
    "title": "Welcome to POLS 1600",
    "section": "Two kinds of people in this world",
    "text": "Two kinds of people in this world"
  },
  {
    "objectID": "slides/00-slides.html#what-is-it-that-we-say-we-do-here",
    "href": "slides/00-slides.html#what-is-it-that-we-say-we-do-here",
    "title": "Welcome to POLS 1600",
    "section": "What is it that we say we do here",
    "text": "What is it that we say we do here"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do",
    "href": "slides/00-slides.html#what-does-quantitative-research-do",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions"
  },
  {
    "objectID": "slides/00-slides.html#descriptions",
    "href": "slides/00-slides.html#descriptions",
    "title": "Welcome to POLS 1600",
    "section": "Descriptions",
    "text": "Descriptions"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-1",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-1",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations"
  },
  {
    "objectID": "slides/00-slides.html#explanations",
    "href": "slides/00-slides.html#explanations",
    "title": "Welcome to POLS 1600",
    "section": "Explanations",
    "text": "Explanations"
  },
  {
    "objectID": "slides/00-slides.html#explanations-1",
    "href": "slides/00-slides.html#explanations-1",
    "title": "Welcome to POLS 1600",
    "section": "Explanations",
    "text": "Explanations"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-2",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-2",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations\nPredictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty",
    "href": "slides/00-slides.html#predictions-and-uncertainty",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty-1",
    "href": "slides/00-slides.html#predictions-and-uncertainty-1",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty-2",
    "href": "slides/00-slides.html#predictions-and-uncertainty-2",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-3",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-3",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations\nPredictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#two-kinds-of-people-in-this-world-1",
    "href": "slides/00-slides.html#two-kinds-of-people-in-this-world-1",
    "title": "Welcome to POLS 1600",
    "section": "Two kinds of people in this world",
    "text": "Two kinds of people in this world"
  },
  {
    "objectID": "slides/00-slides.html#introductions-1",
    "href": "slides/00-slides.html#introductions-1",
    "title": "Welcome to POLS 1600",
    "section": "Introductions",
    "text": "Introductions"
  },
  {
    "objectID": "slides/00-slides.html#my-research",
    "href": "slides/00-slides.html#my-research",
    "title": "Welcome to POLS 1600",
    "section": "My research",
    "text": "My research\n\nI study American Poltical Behavior with focus on poltics of race and criminal justice\nHow do we break cycles of inequality when those most affected by injustice are the least likely to participate and those unaffected are the least likely to care?\nHow can we use methodological tools to better answer these questions?"
  },
  {
    "objectID": "slides/00-slides.html#but-enough-about-me",
    "href": "slides/00-slides.html#but-enough-about-me",
    "title": "Welcome to POLS 1600",
    "section": "But enough about me",
    "text": "But enough about me"
  },
  {
    "objectID": "slides/00-slides.html#class-survey",
    "href": "slides/00-slides.html#class-survey",
    "title": "Welcome to POLS 1600",
    "section": "Class survey",
    "text": "Class survey\nPlease click here to take a brief survey that will help me structure the class going forward."
  },
  {
    "objectID": "slides/00-slides.html#next-week",
    "href": "slides/00-slides.html#next-week",
    "title": "Welcome to POLS 1600",
    "section": "Next Week:",
    "text": "Next Week:\n\nComplete the class survey\nDownload and Install R and R studio\nRead Chapters 1 (Friday) and start Chapter 3 in QSS\nTuesday: Lecture: Describing Data in R\nThursday: Lab: Exploring COVID-19 data in the US\nFriday: Submit Tutorials: “00-intro” & “01-measurement”\n\nOnly time you’ll have two tutorials due (Ok to submit late)\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/08-slides-old.html#class-plan",
    "href": "slides/08-slides-old.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/08-slides-old.html#annoucements",
    "href": "slides/08-slides-old.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/08-slides-old.html#packages-for-today",
    "href": "slides/08-slides-old.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/08-slides-old.html#feedback",
    "href": "slides/08-slides-old.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback\n\nload(url(\"https://pols1600.paultesta.org/files/data/cps_clean.rda\"))\n#load(\"../files/data/cps_clean.rda\")"
  },
  {
    "objectID": "slides/08-slides-old.html#goals",
    "href": "slides/08-slides-old.html#goals",
    "title": "POLS 1600",
    "section": "Goals",
    "text": "Goals"
  },
  {
    "objectID": "slides/08-slides-old.html#variation-in-turnout-overtime",
    "href": "slides/08-slides-old.html#variation-in-turnout-overtime",
    "title": "POLS 1600",
    "section": "Variation in Turnout overtime",
    "text": "Variation in Turnout overtime"
  },
  {
    "objectID": "slides/08-slides-old.html#review-1",
    "href": "slides/08-slides-old.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "slides/08-slides-old.html#concept-1",
    "href": "slides/08-slides-old.html#concept-1",
    "title": "POLS 1600",
    "section": "Concept",
    "text": "Concept"
  },
  {
    "objectID": "slides/08-slides-old.html#code-1",
    "href": "slides/08-slides-old.html#code-1",
    "title": "POLS 1600",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "slides/08-slides-old.html#summary-1",
    "href": "slides/08-slides-old.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/08-slides-old.html#references",
    "href": "slides/08-slides-old.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/09-slides-old.html#general-plan",
    "href": "slides/09-slides-old.html#general-plan",
    "title": "Week 09:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nFeedback\nReview\n\nProbability Distributions\n\nLecture\n\nThe Law of Large Numbers\nThe Central Limit Theorem\nGeneralized Linear Models (Maybe…)"
  },
  {
    "objectID": "slides/09-slides-old.html#goals",
    "href": "slides/09-slides-old.html#goals",
    "title": "Week 09:",
    "section": "Goals",
    "text": "Goals\n\nThe Law of Large Number’s says that as our sample size increases, our sample mean will converge to the population value\n\n–\n\nThe Central Limit Theorem says that the distribution of those sample means will follow a normal distribution\n\n–\n\nGeneralized Linear Models allow us to more accurately model different types of data-generating processes using Maximum Likelihood Estimation."
  },
  {
    "objectID": "slides/09-slides-old.html#emoji-slide-notation",
    "href": "slides/09-slides-old.html#emoji-slide-notation",
    "title": "Week 09:",
    "section": "Emoji Slide notation",
    "text": "Emoji Slide notation\n\n💪: Exercises\n📢: Feedback\n🔍: Review\n💡: Core concept\n🦉: In case you’re interested\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/09-slides-old.html#new-packages",
    "href": "slides/09-slides-old.html#new-packages",
    "title": "Week 09:",
    "section": "New packages",
    "text": "New packages\nNone!"
  },
  {
    "objectID": "slides/09-slides-old.html#packages-for-today",
    "href": "slides/09-slides-old.html#packages-for-today",
    "title": "Week 09:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/09-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/09-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 09:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/09-slides-old.html#load-packages-for-today",
    "href": "slides/09-slides-old.html#load-packages-for-today",
    "title": "Week 09:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n          zoo \n         TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today\nclass:inverse, middle, center # 🔍 # Review ## Random Variables and Probability Distributions"
  },
  {
    "objectID": "slides/09-slides-old.html#probability",
    "href": "slides/09-slides-old.html#probability",
    "title": "Week 09:",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event happening.\nStatistics uses probability to quantify uncertainty about estimates and hypotheses.\nThree rules of probability (Kolmogorov axioms)\n\nPositivity: \\[Pr(A) \\geq 0 \\]\nCertainty: \\[Pr(\\Omega) = 1 \\]\nAdditivity: \\[Pr(A \\text{ or } B) = Pr(A) + Pr(B)\\] iff A and B are mutually exclusive"
  },
  {
    "objectID": "slides/09-slides-old.html#probability-1",
    "href": "slides/09-slides-old.html#probability-1",
    "title": "Week 09:",
    "section": "Probability",
    "text": "Probability\n\nTwo interpretations interpreting probabilities (Frequentist and Bayesian)\nConditional Probability and Bayes Rule:\n\n\\[Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#random-variables",
    "href": "slides/09-slides-old.html#random-variables",
    "title": "Week 09:",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables assign numeric values to each event in an experiment.\n\nMutually exclusive and exhaustive, together cover the entire sample space.\n\nDiscrete random variables take on finite, or countably infinite distinct values.\nContinuous variables can take on an uncountably infinite number of values."
  },
  {
    "objectID": "slides/09-slides-old.html#example-toss-two-coins",
    "href": "slides/09-slides-old.html#example-toss-two-coins",
    "title": "Week 09:",
    "section": "Example: Toss Two Coins",
    "text": "Example: Toss Two Coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#probability-distributions",
    "href": "slides/09-slides-old.html#probability-distributions",
    "title": "Week 09:",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\n\n\\[\\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities}\\]\nThe can be represented in terms of:\n\nProbability Mass/Density Functions\n\nDiscrete variables have probability mass functions (PMF)\nContinuous variables have probability density functions (PDF)\n\nCumulative Density Functions\n\nDiscrete: Summation of discrete probabilities\nContinuous: Integration over a range of values"
  },
  {
    "objectID": "slides/09-slides-old.html#discrete-distributions",
    "href": "slides/09-slides-old.html#discrete-distributions",
    "title": "Week 09:",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nProbability Mass Function (pmf): \\(f(x)=p(X=x)\\)\n\nAssigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\n\nCumulative Distribution Function (cdf) \\(F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)\\)\n\nSum of the probability mass for events less than or equal to \\(x_j\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#example-toss-two-coins-1",
    "href": "slides/09-slides-old.html#example-toss-two-coins-1",
    "title": "Week 09:",
    "section": "Example: Toss Two coins",
    "text": "Example: Toss Two coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)\n\n\\(f(X=0)=p(X=0)=1/4\\)\n\\(f(X=1)=p(X=1)=1/2\\)\n\\(F(X\\leq 1) = p(X \\leq 1)= 3/4\\)\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X&lt;=2) = 1/6 + 1/6 = 1/3"
  },
  {
    "objectID": "slides/09-slides-old.html#continuous-distributions",
    "href": "slides/09-slides-old.html#continuous-distributions",
    "title": "Week 09:",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nProbability Density Functions (PDF): \\(f(x)\\)\n\nAssigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\nBut… since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n\nCumulative Distribution Function (CDF) \\(F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx\\)\n\nInstead of summing up to a specific value (discrete) we integrate over all possible values up to \\(x\\)\nProbability of having a value less than x"
  },
  {
    "objectID": "slides/09-slides-old.html#integrals",
    "href": "slides/09-slides-old.html#integrals",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nFirst, a brief aside on integral calculus:\nWhat’s the area of the rectangle? \\(base\\times height\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#integrals-1",
    "href": "slides/09-slides-old.html#integrals-1",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nHow would we find the area under a curve?"
  },
  {
    "objectID": "slides/09-slides-old.html#integrals-2",
    "href": "slides/09-slides-old.html#integrals-2",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nWell suppose we added up the areas of a bunch of rectangles roughly whose height’s approximated the height of the curve?\n\nCan we do any better?"
  },
  {
    "objectID": "slides/09-slides-old.html#integrals-3",
    "href": "slides/09-slides-old.html#integrals-3",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nLet’s make the rectangles smaller\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get’s even better:"
  },
  {
    "objectID": "slides/09-slides-old.html#link-between-pdf-and-cdf",
    "href": "slides/09-slides-old.html#link-between-pdf-and-cdf",
    "title": "Week 09:",
    "section": "🦉 Link between PDF and CDF",
    "text": "🦉 Link between PDF and CDF\nIf \\[F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx \\]\nThen by the fundamental theorem of calculus\n\\[\\frac{d}{dx}F(x)=f(x)\\]\nIn words\n\nthe PDF (\\(f(x)\\)) is the derivative (rate of change) of the CDF (\\(F(X)\\))\nthe CDF describes the area under the curve defined by f(x) up to x"
  },
  {
    "objectID": "slides/09-slides-old.html#properties-of-the-cdf",
    "href": "slides/09-slides-old.html#properties-of-the-cdf",
    "title": "Week 09:",
    "section": "Properties of the CDF",
    "text": "Properties of the CDF\n\n\\(0\\leq F(x) \\leq 1\\)\n\\(F\\) is non-decreasing and right continuous\n\\(\\lim_{x\\to-\\infty}F(x)=0\\)\n\\(\\lim_{x\\to\\infty}F(x)=1\\)\nFor all \\(a,b \\in \\mathbb{R}\\) s.t. \\(a&lt;b\\)\n\n\\[p(a &lt; X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx \\]"
  },
  {
    "objectID": "slides/09-slides-old.html#recall-the-pmf-and-cdf-of-a-die",
    "href": "slides/09-slides-old.html#recall-the-pmf-and-cdf-of-a-die",
    "title": "Week 09:",
    "section": "Recall the PMF and CDF of a die",
    "text": "Recall the PMF and CDF of a die"
  },
  {
    "objectID": "slides/09-slides-old.html#whats-the-probability",
    "href": "slides/09-slides-old.html#whats-the-probability",
    "title": "Week 09:",
    "section": "What’s the probability",
    "text": "What’s the probability\n\n\\(p(X=1)...p(X=6) = 1/6\\)\n\\(p( 2 &lt; X \\leq 5) = F(5)-F(2)=5/6-2/6=3/6=1/2\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#common-probablity-distirbutions",
    "href": "slides/09-slides-old.html#common-probablity-distirbutions",
    "title": "Week 09:",
    "section": "Common Probablity Distirbutions",
    "text": "Common Probablity Distirbutions\nIn this course, we’ll use probability distributions to\n\nmodel the data generating process as a function of parameters we can estimate (using Generalized Linear Models)\nperform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)\n\nThere are a lot of probability distributions:\n\n\n\n\n\n\n\n\n\nFortunately, the distributions you need to know to really master data science, are probably more something like\n\n\n\n\n\n\n\n\n\nAnd the distributions we’ll work with the most in this class are an even smaller subset.\n\nBernoulli: Coinflips with probability of heads, \\(p\\)\nUniform: Coinflip with more than two outcomes\nBinomial: Adding up coinflips\nPoisson: Counting the total number of events\nGeometric: Counting till a specific event occurs\nExponential: Counting till a specific event occurs in continous time\nNormal:\n\nThe limit of a Binomial distribution as \\(n\\to \\infty\\)\nThe maximum entropy when we only know the mean and variance\n\nt: A finite sample approximation of the normal\n\\(\\chi^2\\): Distribution of sums of squared variables from Normal distribution"
  },
  {
    "objectID": "slides/09-slides-old.html#bernoulli-random-variables",
    "href": "slides/09-slides-old.html#bernoulli-random-variables",
    "title": "Week 09:",
    "section": "Bernoulli Random Variables",
    "text": "Bernoulli Random Variables\nLet’s start with our old friend the coin flip\nA coin flip is an example of a Bernoulli random variable defined by 1 parameter \\(p\\), the probability of success. It has a pmf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF of\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x&lt;1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x&lt;1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array}\n    \\right.\\]\nNote that in our coin flip example \\(p=0.5\\) but it need not. Just imagine a weighted coin like the Patriots use at Foxborough"
  },
  {
    "objectID": "slides/09-slides-old.html#uniform-distribution",
    "href": "slides/09-slides-old.html#uniform-distribution",
    "title": "Week 09:",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nOur fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range \\([a,b]\\), the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                \\frac{1}{b-a}& \\mathrm{if\\ } a \\leq x\\leq b \\\\\n                0 & \\text{otherwise} \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF:\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                        0 & x &lt;a \\\\\n                \\frac{x-a}{b-a}& \\mathrm{if\\ } a \\leq x &lt; b \\\\\n                1 & x \\geq b \\\\\n        \\end{array}\n    \\right.\\]\nWe won’t run into uniform distributions all that often except in examples like rolling a fair sided die, but often they’re used in Bayesian analysis as a form of uninformative prior."
  },
  {
    "objectID": "slides/09-slides-old.html#binomial-distributions",
    "href": "slides/09-slides-old.html#binomial-distributions",
    "title": "Week 09:",
    "section": "Binomial Distributions",
    "text": "Binomial Distributions\nThe binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.\nThe key parameters are the number of trials \\(n\\) and the probability of success for each trial \\(p\\) and the pdf of a binomial distribution is:\n\\[f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n\\] So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads\n\nThe binomial distribution will come in handy when trying to model binary outcomes."
  },
  {
    "objectID": "slides/09-slides-old.html#poisson-distributions",
    "href": "slides/09-slides-old.html#poisson-distributions",
    "title": "Week 09:",
    "section": "Poisson Distributions",
    "text": "Poisson Distributions\nWhat would happen if you let the \\(n\\) in a binomial distribution go to infinity and \\(p\\) go to 0 so that \\(np\\) stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter \\(\\lambda\\) which represents rate at which events occur.\n\\[f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\nWe use these distributions to try and predict to predict the probability of a given number of events occurring in a fixed interval of time. Things like how many acts of political participation would a voter engage in over a year."
  },
  {
    "objectID": "slides/09-slides-old.html#geometric-distributions",
    "href": "slides/09-slides-old.html#geometric-distributions",
    "title": "Week 09:",
    "section": "Geometric Distributions",
    "text": "Geometric Distributions\nWhat if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:\n\\[f(x)=p(1-p) ^{x}\\]\nGeometric and related distributions are useful for describing the time until an event occurs"
  },
  {
    "objectID": "slides/09-slides-old.html#exponential-distributions",
    "href": "slides/09-slides-old.html#exponential-distributions",
    "title": "Week 09:",
    "section": "Exponential Distributions",
    "text": "Exponential Distributions\nTaking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a \\(\\lambda = \\frac{1}{\\beta}\\) rate parameter\n\\[f(x)=\\frac{1}{\\beta}\\exp\\left[-x/\\beta\\right]\\]\nCioffa-Revilla (1984) uses an exponential distribution to model the stability of Italian governments."
  },
  {
    "objectID": "slides/09-slides-old.html#normal-distribution",
    "href": "slides/09-slides-old.html#normal-distribution",
    "title": "Week 09:",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nFinally, there’s the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter \\(\\mu\\) that determines the center of a distribution and a scale parameter \\(\\sigma^2\\) that determines the spread of a distribution\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]\\]\nStandard normal: \\(X \\sim N(\\mu =0,\\sigma^2=1)\\)\n\n\n\n\n\n\n\n\n\n\nAs we’ll see normal distributions tend to arise when ever you’re summing variables.\nThat is sum together a bunch of values from almost any distribution and the distribution of their sums tends to follow a normal distribution.\nSince lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)\n\nConsider a binomial distribution with N=100 and p=.5.\nThe pmf of this variable (black lollipops) follows a distribution that’s closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.\nA relationship explained more generally by the Central Limit Theorem, which we’ll cover next week.\n\n\n\n\n\n\n\n\n\nWhat’s the \\(p(X \\leq 0)\\) for a normal distirbution with mean 0 and sd 1\nSince the normal distribution is so common, it’s useful to get practice working with it’s pdf and cdf.\nConsider the following question: If X is normally distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\), what’s the probability that X is less than 0 \\(p(X\\leq0)=?\\) We could solve:\n\\[\\int_{-\\infty}^{0}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.5\\]\nBut R’s pnorm() function will quickly tell us\n\n\\(p(X\\leq0)=\\) 0.5\n\nAnd we can visualize this as follows:\n\n\n\n\n\n\n\n\n\nConsider some other questions?\n\n\\(p(X=0)=0\\)\n\nThe probability that a continuous variable is exactly some value is always 0.\n\n\\(p(X&lt;0)=0.5\\)\n\\(p(-1&lt; X&lt; 1)\\)\n\\(p(-2&lt; X&lt; 2)\\)\n\np(-1 &lt; X &lt; 1)\n\n\\(p(-1&lt; X&lt; 1)=pr(X&lt;1)-pr(X&lt;-1)\\)\n\n\\[\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.841-0.158=0.682\\]\n\n\n\n\n\n\n\n\n\np(-2 &lt; X &lt; 2)\n\n\\(p(-2&lt; X\\leq 2)=\\) 0.9544997\n\n\n\n\n\n\n\n\n\n\nWe’ll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0."
  },
  {
    "objectID": "slides/09-slides-old.html#expected-value",
    "href": "slides/09-slides-old.html#expected-value",
    "title": "Week 09:",
    "section": "Expected Value",
    "text": "Expected Value\nA (probability) weighted average of the possible outcomes of a random variable, often labeled \\(\\mu\\)\nDiscrete:\n\\[\\mu_X=E(X)=\\sum xp(x)\\]\nContinuous\n\\[\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "href": "slides/09-slides-old.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "title": "Week 09:",
    "section": "What’s the expected value of a 1 roll of fair die?",
    "text": "What’s the expected value of a 1 roll of fair die?\n\\[\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#properties-of-expected-values",
    "href": "slides/09-slides-old.html#properties-of-expected-values",
    "title": "Week 09:",
    "section": "Properties of Expected Values",
    "text": "Properties of Expected Values\n\n\\(E(c)=c\\)\n\\(E(a+bX)=a+bE[X]\\)\n\\(E[E[X]]=X\\)\n\\(E[E[Y|X]]=E[Y]\\)\n\\(E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx\\)\n\\(E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)\\)\n\\(E[XY]=E[X]E[Y]\\) if \\(X\\) and \\(Y\\) are independent"
  },
  {
    "objectID": "slides/09-slides-old.html#variance",
    "href": "slides/09-slides-old.html#variance",
    "title": "Week 09:",
    "section": "Variance",
    "text": "Variance\nIf \\(X\\) has a finite mean \\(E[X]=\\mu\\), the \\(E[(X-\\mu)^2]\\) is finite and called the variance of \\(X\\) which we write as \\(\\sigma^2\\) or \\(Var[X]\\).\nNote:\n\\[\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}\\]\n\n“The variance of X is equal to the expected value of X-squared, minus the square of X’s expected value.”\n\\(\\sigma^2=E[X^2]-E[X]^2\\) is a useful identity in proofs and derivations"
  },
  {
    "objectID": "slides/09-slides-old.html#variance-and-standard-deviations",
    "href": "slides/09-slides-old.html#variance-and-standard-deviations",
    "title": "Week 09:",
    "section": "Variance and Standard Deviations",
    "text": "Variance and Standard Deviations\nWe often think of variances \\(Var[X]\\) as describing the spread of a distribution\n\\[\\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2\\]\nA standard deviation is just the square root of the variance\n\\[\\sigma=\\sqrt{Var[X]}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#covariance",
    "href": "slides/09-slides-old.html#covariance",
    "title": "Week 09:",
    "section": "Covariance",
    "text": "Covariance\nCovariance measures the degree to which two random variables vary together.\n\n\\(Cov[X,Y] \\to +\\) An increase in \\(X\\) tends to be larger than its mean when \\(Y\\) is larger than its mean\n\n\\[Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#properties-of-variance-and-covariance",
    "href": "slides/09-slides-old.html#properties-of-variance-and-covariance",
    "title": "Week 09:",
    "section": "Properties of Variance and Covariance",
    "text": "Properties of Variance and Covariance\n\n\\(Cov[X,Y]=E[XY]-E[X]E[Y]\\)\n\\(Var[X]=E[X^2]-(E[X])^2\\)\n\\(Var[X|Y]=E[X^2|Y]-(E[X|Y])^2\\)\n\\(Cov[X,Y]=Cov[X,E[Y|X]]\\)\n\\(Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]\\)\n\\(Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#correlation",
    "href": "slides/09-slides-old.html#correlation",
    "title": "Week 09:",
    "section": "Correlation",
    "text": "Correlation\n\nThe correlation between \\(X\\) and \\(Y\\) is simply the covariance of \\(X\\) and \\(Y\\) divided by the standard deviation of each.\n\n\\[\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}\\]\n\nNormalize covariance to a scale that runs between [-1,1]\n\nclass:inverse, center, middle # 💡 # The Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides-old.html#the-law-of-large-numbers-intuitive",
    "href": "slides/09-slides-old.html#the-law-of-large-numbers-intuitive",
    "title": "Week 09:",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we wanted to know the average height of our class.\nWe could pick someone at random, measure their height and get an estimate. It would be a pretty bad estimate (it would vary a lot from person to person), but it would be an unbiased estimate\nHow would we improve our estimate?"
  },
  {
    "objectID": "slides/09-slides-old.html#the-law-of-large-numbers-intuitive-1",
    "href": "slides/09-slides-old.html#the-law-of-large-numbers-intuitive-1",
    "title": "Week 09:",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we increased our sample size from N=1 to N = 5.\nNow our estimate reflects the average of 5 people’s heights as opposed to just 1. Both are are unbiased estimates of the truth, but the N=5 sample has a lower variance.\n–\nNow suppose we took a sample of size N = N-1. That is we measured everyone except one person. Our estimate will be quite close to the truth, varying slightly based on the height of the person left out.\n–\nFinally we took a sample of size N = 24 (e.g. the class size). Since our sample is the population, our estimate will be exactly equal to to the population. Each sample will give us the same “true” value. That is, it wil not vary at all.\n–\nThe idea that as the sample size increases, the distance of a sample mean from the population mean \\(\\mu\\) goes to 0 is called the Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides-old.html#the-weak-law-of-large-numbers-formally",
    "href": "slides/09-slides-old.html#the-weak-law-of-large-numbers-formally",
    "title": "Week 09:",
    "section": "The (Weak) Law of Large Numbers (Formally)",
    "text": "The (Weak) Law of Large Numbers (Formally)\nLet \\(X_1, X_2, \\dots\\) be independent and identically distributed (i.i.d.) random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nThen for every \\(\\epsilon&gt;0\\), as the sample size increases (1), the distance of a sample mean from the population mean \\(\\mu\\) (2) goes to 0 (3).\n\\[\\overbrace{Pr(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon)}^{\\text{2. The distance of the sample mean from the truth}} \\overbrace{\\to 0}^{\\text{3. Goes to 0}} \\underbrace{\\text{ as }n \\to \\infty}_{\\text{1. As the sample size increases}}\\]\nEquivalently:\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#simulating-the-lln",
    "href": "slides/09-slides-old.html#simulating-the-lln",
    "title": "Week 09:",
    "section": "💪 Simulating the LLN",
    "text": "💪 Simulating the LLN\nRhe expected value of rolling a die 3.5.\n\\[ E[X] = \\Sigma x_ip(X=x_i) = 1/6 * (1+2+3+4+5+6)\\]\nIn terms of the LLN, think of our sample size as the number of times we roll a die.\nIf we rolled the die just once and took the average of our role, we could get a 1, 2, 3, 4, 5, or 6. which would be pretty far from our expected value of 3.5\nIf we rolled the die two times and took an average, we could still get an value of 1 or 6 for average, but values closer to our expected value of 3.5, happen more often\n\n# Calculate the average from 2 rows\ntable(rowMeans(expand.grid(1:6, 1:6)))\n\n\n  1 1.5   2 2.5   3 3.5   4 4.5   5 5.5   6 \n  1   2   3   4   5   6   5   4   3   2   1 \n\n\nAs we increase our sample size (roll the die more times), the LLN says the chance that our sample average is far from the truth \\((p(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon))\\), gets vanishingly small.\n\ndie &lt;- 1:6\nroll_fn &lt;- function(n) {\n  rolls &lt;- data.frame(rolls = sample(die, size = n, replace = TRUE))\n  # summarize rolls \n  df &lt;- rolls %&gt;%\n    summarise(\n    # number of rolls\n      n_rolls = n(),\n    # number of times 1 was rolled\n      ones = sum(rolls == 1),\n    # number of times 2 was rolled, etc..\n      twos = sum(rolls == 2),\n      threes = sum(rolls == 3),\n      fours = sum(rolls == 4),\n      fives = sum(rolls == 5),\n      sixes = sum(rolls == 6),\n      # Average of all our rolls\n      average =  mean(rolls),\n      # Absolute difference between averages and rolls\n      abs_error = abs(3.5-average)\n    )\n  # Return summary df\n  df\n}\n\nThen we could use a for-loop to simulate rolling our die once and calculating the average all the way up to rolling our die a 1000 times.\n\n# Holder\nsim_df &lt;- NULL\n\n# Set seed\nset.seed(123)\n\nfor(i in 1:1000){\n  sim_df &lt;- rbind(sim_df,\n                  roll_fn(i)\n  )\n}\n\nWith only a few rolls, our average bounces around a lot\n\nhead(sim_df)\n\n  n_rolls ones twos threes fours fives sixes  average abs_error\n1       1    0    0      1     0     0     0 3.000000 0.5000000\n2       2    0    0      1     0     0     1 4.500000 1.0000000\n3       3    0    2      0     0     0     1 3.333333 0.1666667\n4       4    0    0      1     1     1     1 4.500000 1.0000000\n5       5    1    1      1     0     1     1 3.400000 0.1000000\n6       6    3    0      2     1     0     0 2.166667 1.3333333\n\n\nWith a lot of rolls, our average is very close to 3.5\n\ntail(sim_df)\n\n     n_rolls ones twos threes fours fives sixes  average  abs_error\n995      995  197  160    151   154   171   162 3.430151 0.06984925\n996      996  184  164    176   149   175   148 3.412651 0.08734940\n997      997  163  159    170   163   171   171 3.534604 0.03460381\n998      998  162  163    142   173   185   173 3.576152 0.07615230\n999      999  209  154    151   154   163   168 3.412412 0.08758759\n1000    1000  181  189    147   179   146   158 3.394000 0.10600000\n\n\nLet’s visualize see how our average changes with the number of rolls, using ggplot()\n\np_die_lln &lt;- ggplot(sim_df, aes(n_rolls, average))+\n  geom_line()\n\n\nYour turn! Plot how the absolute value of the error changes as the number of rolls increases. Does it increase or decrease? How does the rate at which it goes up or down seem to change?\n\n# Write your code here:\n\nclass: inverse, center, middle #🦉 ## ICYI: Proving the Weak LLN"
  },
  {
    "objectID": "slides/09-slides-old.html#proving-the-weak-lln",
    "href": "slides/09-slides-old.html#proving-the-weak-lln",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nA proof of the LLN is as follows:\nFirst define \\(U\\) such that its a sample mean for sample of size \\(n\\)\n\\[U=\\frac{X_1+\\dots +X_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#proving-the-weak-lln-1",
    "href": "slides/09-slides-old.html#proving-the-weak-lln-1",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen show that the sample mean, \\(U\\) is an unbiased estimator of the population mean \\(\\mu\\)\n\\[\\begin{align*}\nE[U]&=E[\\frac{X_1+\\dots +X_n}{n}]=\\frac{1}{n}E[X_1+\\dots +X_n]\\\\\n&=\\frac{n\\mu}{n}=\\mu\n\\end{align*}\\]\nWith a variance\n\\[\\begin{align*}\nVar[U]&=Var[\\frac{X_1+\\dots +X_n}{n}]=\\\\\n    &=Var[\\frac{X_1}{n}]\\dots Var[\\frac{+X_n}{n}]\\\\\n    &\\frac{\\sigma^2}{n^2}\\dots \\frac{\\sigma^2}{n^2}\\\\\n    &\\frac{n \\sigma^2}{n^2}\\\\\n    &\\frac{\\sigma^2}{n}\\\\\n\\end{align*}\\]\nThat decreases with N."
  },
  {
    "objectID": "slides/09-slides-old.html#proving-the-weak-lln-2",
    "href": "slides/09-slides-old.html#proving-the-weak-lln-2",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen, by Chebyshev’s inequality, a theorem specifying, for a given distribution, the maximum fraction of values that can be some distance from that distribution’s mean:\n\\[Pr(\\left|U-\\mu\\right| &gt; \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2}\\]\nWhich \\(\\to 0\\) as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#the-strong-law-of-large-numbers",
    "href": "slides/09-slides-old.html#the-strong-law-of-large-numbers",
    "title": "Week 09:",
    "section": "The Strong Law of Large Numbers",
    "text": "The Strong Law of Large Numbers\nAs you may have inferred, there is a weak law of large numbers and a strong law of large numbers.\nThe weak law of large numbers states that as the sample size increases, the sample mean converges in probability to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]\nThe strong law of large numbers states that as the sample size increases, the sample mean converges almost surely to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n = \\mu|) = 1\\] The differences in types of convergence won’t matter much for us in this course\nclass:inverse, center, middle # Break\nclass:inverse, center, middle # 💡 ## The Central Limit Theorem\nSo the LLN tells us that as our sample size grows, an unbiased estimator like the sample average, will get increasingly close to the to the “true” value of the population of mean.\nIif we took a bunch of samples of the same size and calculated the mean of each sample:\n\nthe distribution of those sample means (the sampling distribution) would be centered around the truth (because the estimator is unbiased).\nthe width of the distribution (its variance) would decrease as we increased the size of each sample (by the LLN)\n\nThe Central Limit Theorem tells us about the shape of that distribution."
  },
  {
    "objectID": "slides/09-slides-old.html#review-z-scores-and-standardization",
    "href": "slides/09-slides-old.html#review-z-scores-and-standardization",
    "title": "Week 09:",
    "section": "Review: Z-scores and Standardization",
    "text": "Review: Z-scores and Standardization\nGiven a R.V. \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we can define a new R.V. \\(Z\\) as the standardization of \\(X\\):\n\\[Z=\\frac{X-\\mu}{\\sigma}\\]\nWhere Z has \\(\\mu=0\\) and \\(\\sigma=1\\)."
  },
  {
    "objectID": "slides/09-slides-old.html#notation-for-the-clt",
    "href": "slides/09-slides-old.html#notation-for-the-clt",
    "title": "Week 09:",
    "section": "Notation for the CLT",
    "text": "Notation for the CLT\nNext let’s define some variables \\(S\\) and \\(\\bar{X}\\) that are the sum \\((S)\\) and sample mean \\((\\bar{X})\\) of \\(n\\) iid draws of \\(X\\)\nLet \\(X_1,X_2,\\dots,X_n\\) be independent and identically distributed RVs with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nDefine \\(S_n\\) and \\(\\bar{X}_n\\) as follows:\n\\[S_n= X_1,X_2,\\dots,X_n= \\sum_{i=1}^n X_i\\]\n\\[\\bar{X}=\\frac{X_1,X_2,\\dots,X_n}{n}= \\frac{S_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#additional-facts-for-the-clt",
    "href": "slides/09-slides-old.html#additional-facts-for-the-clt",
    "title": "Week 09:",
    "section": "Additional facts for the CLT",
    "text": "Additional facts for the CLT\nWe can show that:\n\\[\\begin{alignat*}{3}\nE[S_n]&=n\\mu \\hspace{2em}Var[S_n]&=n\\sigma^2 \\hspace{2em} \\sigma_S&=\\sqrt{n}\\sigma\\\\\nE[\\bar{X}_n]&=\\mu \\hspace{2em}Var[\\bar{X}_n]&=\\frac{\\sigma^2}{n} \\hspace{2em}\\sigma_{\\bar{X}}&=\\frac{\\sigma}{\\sqrt{n}}\\\\\n\\end{alignat*}\\]\nBasically: the expected value and variance of the sum is just \\(n\\) times the population parameters (the true values for the distribution).\nSince the mean is just the sum divided by the sample size, the expected value of the mean is equal to the population value and the variance and standard deviations of the mean are decreasing in \\(n\\).\nFinally, we can define \\(Z\\) to be a function of either \\(S\\) or \\(\\bar{X}\\)\n\\[Z_n=\\frac{S_n-n\\mu}{\\sqrt{n}\\sigma}=\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#central-limit-theorem",
    "href": "slides/09-slides-old.html#central-limit-theorem",
    "title": "Week 09:",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nFor a sufficiently large \\(n\\)\n\\[\\begin{align*}\n\\bar{X_n}&\\approx N(\\mu,\\sigma^2/n) \\\\\n\\bar{S_n} &\\approx N(n\\mu,n\\sigma^2) \\\\\n\\bar{Z_n}&\\approx N(0,1)\n\\end{align*}\\]\n\nThe distribution of means \\((\\bar{X_n})\\) from almost any distribution \\(X\\) is approximately normal (converges in distribution), but with a smaller variance than (\\(\\sigma^2/n\\))\nProof: Several ways, but requires a little more math than is required for this course"
  },
  {
    "objectID": "slides/09-slides-old.html#clt-why-it-matters",
    "href": "slides/09-slides-old.html#clt-why-it-matters",
    "title": "Week 09:",
    "section": "CLT: Why it matters",
    "text": "CLT: Why it matters\nWhy is this result so important?\nWell lots of our questions come of the form, how does a typical value of Y vary with X.\nWe may not know the true underlying distribution of Y, but we can often approximate the distribution of a typical value of Y \\((E[Y])\\) using a normal distribution."
  },
  {
    "objectID": "slides/09-slides-old.html#simulating-the-clt",
    "href": "slides/09-slides-old.html#simulating-the-clt",
    "title": "Week 09:",
    "section": "Simulating the CLT",
    "text": "Simulating the CLT\nFor almost any distribution, the distribution of means from a sample of that distribution will converge to some Normal distribution.\nLet’s consider a decidedly non-Normal Binomial distribution: with p = 0.2.\nThe expected value of Binomial Distribution \\(X \\sim B(n,p)\\) is \\(E[X] = n*p\\).\nIf we were to flip a coin 20 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 4.\nIf we were to flip a coin 100 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 20.\n\n\n\n\n\n\n\n\n\nSimulating 10,000 draws from Binomial Distributions of Different Sizes\n\n# Probability of success\np &lt;- .2\n# Sample sizes\nsamp_sizes &lt;- c(20, 50, 100,1000)\n# Number of simulations\nnsims &lt;- 10000\n# Holder for simulations\ndf_sim &lt;- tibble(\n  expand_grid(\n    samp_size = samp_sizes,\n    sim = 1:nsims,\n    sample_mean = NA\n  )\n)\n\nSimulating 1,000 draws from Binomial Distributions of Different Sizes\nBelow we loop through each sample size in samp_sizes\n\nfor(i in samp_sizes){\n  df_sim$sample_mean[df_sim$samp_size == i] &lt;- replicate(nsims, i*mean(rbinom(i, 1, p)))\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s consider a decided non normal distribution:\n\ndist &lt;- sample(18:80,size=10000, replace = T, prob = runif(length(18:80)))\n\nsamp_mean25 &lt;- replicate(10000,mean(sample(dist,25, replace=F)))\nsamp_mean100 &lt;- replicate(10000,mean(sample(dist,100, replace=F)))\nsamp_mean500 &lt;- replicate(10000,mean(sample(dist,500, replace=F)))\n\nex_df &lt;- tibble(\n  distribution = dist,\n  samp_mean25 = samp_mean25,\n  samp_mean100 = samp_mean100,\n  samp_mean500 = samp_mean500\n)"
  },
  {
    "objectID": "slides/09-slides-old.html#summary",
    "href": "slides/09-slides-old.html#summary",
    "title": "Week 09:",
    "section": "Summary",
    "text": "Summary\n\nSo we see that our sampling distributions are centered on the truth, and as the sample size increases, the width of the distribution decreases (Law of Large Numbers)\nThe shapes of distributions of sample means can be approximated by a Normal Distribution \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\)\n\nclass: inverse, center, middle #🦉 ## ICYI: Maximum Likelihood Estimation"
  },
  {
    "objectID": "slides/09-slides-old.html#maximum-likelihood-estimation-1",
    "href": "slides/09-slides-old.html#maximum-likelihood-estimation-1",
    "title": "Week 09:",
    "section": "🦉 Maximum Likelihood Estimation",
    "text": "🦉 Maximum Likelihood Estimation\nFormally, consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as\n\\[\\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta)\\]\nwhere \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\).\nMLE tries to find \\(\\hat{\\theta}_{MLE}\\) that maximizes \\(\\mathcal{L}(\\theta \\mid X)\\)"
  },
  {
    "objectID": "slides/09-slides-old.html#properties-of-maximum-likelihood-estimators",
    "href": "slides/09-slides-old.html#properties-of-maximum-likelihood-estimators",
    "title": "Week 09:",
    "section": "🦉 Properties of Maximum Likelihood Estimators",
    "text": "🦉 Properties of Maximum Likelihood Estimators\nMLE Estimators are\n\nFunctionally Invariant (The “Plug in Principle”)\n\nIf \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\) than then the MLE of some function of \\(\\theta\\), \\(f(\\theta)\\) is \\(f(\\hat\\theta_{MLE})\\)\nIf we have the MLE of the variance, the square root of this will give us the MLE of the standard deviation\n\nConsistent (by the LLN)\n\n\\(\\hat\\theta_{MLE}\\) collapses to a spike over \\(\\theta\\) as \\(n \\to \\infty\\)\n\nAsympotically Normal (by the CLT)\n\nA \\(n \\to \\infty\\) the sampling distribution of \\(\\hat\\theta_{MLE}\\) becomes Normally distributed\nMakes calculating quantities for inference easy\n\nAsympotically Efficient\n\nAs \\(n \\to \\infty\\), \\(\\hat\\theta_{MLE}\\) tends to be the estimator with the lowest error\n\n\nclass: inverse, center, middle # 💡 # Generalized Linear Models"
  },
  {
    "objectID": "slides/09-slides-old.html#generalized-linear-models",
    "href": "slides/09-slides-old.html#generalized-linear-models",
    "title": "Week 09:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nOLS provides a linear estimate to the conditional mean function\n\n–\n\nIf the conditional mean function is linear and the errors are normally distributed, OLS is the MLE.\n\n–\n\nWhat if the conditional mean function is non-linear?\n\n–\n\nSometimes we can transform the mean function so that it is linear, and estimate a generalized linear model (GLM) using MLE\n\n–\n\nUsing a GLM often produces more “reasonable” estimates, and can make more efficient use of the data, although there are many cases where a linear estimate to conditional mean function works just fine (or better)"
  },
  {
    "objectID": "slides/09-slides-old.html#mle-and-generalized-linear-models",
    "href": "slides/09-slides-old.html#mle-and-generalized-linear-models",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nWe can think some variable \\(y\\) as having a distribution \\(f\\) that contains both a stochastic (random) and systematic components\n\\[\\begin{aligned}\n\\text{Stochastic:    }&& y \\sim f(\\mu,\\alpha)\\\\\n\\text{Systematic:    }&&\\mu = g(X\\beta)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#mle-and-generalized-linear-models-1",
    "href": "slides/09-slides-old.html#mle-and-generalized-linear-models-1",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nIn the past we’ve described the process of modeling \\(y\\) using a linear regression:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nand with multiple predictors:\n\\[y = X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/09-slides-old.html#mle-and-generalized-linear-models-2",
    "href": "slides/09-slides-old.html#mle-and-generalized-linear-models-2",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nWe haven’t really talked about the distribution of \\(\\epsilon\\), in part because OLS doesn’t require any distributional assumptions to be unbiased.\nBut if we assumed \\(\\epsilon\\) are normally distributed, with mean 0 and variance \\(\\sigma^2\\)\n\\[\\epsilon \\sim f_\\mathcal{N}(0,\\sigma^2)\\]\nThen we could write our model for \\(y\\) as follows:\n\\[\\begin{aligned} y &\\sim f_{\\mathcal{N}}(\\mu,\\sigma^2)\\\\\n\\mu &= X\\beta\\end{aligned}\\]\nWhere the systematic component of why is modeled by \\(X\\beta\\) (i.e. g() is the identity function), with errors that are Normally distributed.\nThe \\(\\beta\\)s that OLS estimates turn out to be the same values that would get by maximizing the likelihood of this function, given our data, \\(X\\), assuming normally distributed errors."
  },
  {
    "objectID": "slides/09-slides-old.html#generalized-linear-models-1",
    "href": "slides/09-slides-old.html#generalized-linear-models-1",
    "title": "Week 09:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nBut what if our outcome doesn’t follow a normal distribution?\nSay for example, we have a binary outcome,that we think follows a Bernoulli distribution with \\(\\pi\\) probability of success.\nWe could model the systematic portion of this using the logistic function, \\(g()\\)\n\\[\\begin{aligned}y &\\sim f_{Bern}(\\pi)\\\\\n\\pi &= \\frac{1}{1+\\exp(-{X\\beta})}\\end{aligned}\\]\nAgain, we could estimate \\(\\beta\\) using the MLE to fit a logistic regression."
  },
  {
    "objectID": "slides/09-slides-old.html#mle-and-generalized-linear-models-3",
    "href": "slides/09-slides-old.html#mle-and-generalized-linear-models-3",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nOr if we had a count variable, we might use a Poisson distribution:\n\\[\\begin{aligned}y &\\sim f_{Pois}(\\lambda)\\\\\n\\lambda &= \\exp(X\\beta)\\end{aligned}\\]\nAgain estimating \\(\\beta\\) using MLE.\nIn this class, we’ll let R handle mechanics of actually fitting these models, and instead focus on interpreting their substantive differences"
  },
  {
    "objectID": "slides/09-slides-old.html#ols-vs-logistic-regression",
    "href": "slides/09-slides-old.html#ols-vs-logistic-regression",
    "title": "Week 09:",
    "section": "OLS vs Logistic Regression",
    "text": "OLS vs Logistic Regression\nOne situation where we’d use MLE is the case of binary responses variable coded using \\(0\\) and \\(1\\).\nIn practice, these \\(0\\) and \\(1\\)s will code for two classes such as yes/no, non-voter/voter,, etc.\nHow should we model this relationship?\nWe could use OLS to produce a linear estimate of the conditional mean function \\((\\text{E}[Y \\mid {\\bf X} = {\\bf x}])\\), by finding \\(\\beta\\)s that minimize the sum of squared errors\nOr\nWe could use a logistic regression, to produce a linear estimate of the “log-odds” of the conditional mean function of our binary variable by finding \\(\\beta\\)s that maximize the likelihood of this function.\nLet’s simulate data from the following model:\n\\[\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -2 + 3 x\\]\nWe’ll codify this into a function:\n\nsim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {\n  x = rnorm(n = sample_size)\n  eta = beta_0 + beta_1 * x\n  p = 1 / (1 + exp(-eta))\n  y = rbinom(n = sample_size, size = 1, prob = p)\n  data.frame(y, x)\n}\n\nAnd use it to generate some data\n\nset.seed(1)\nexample_data = sim_logistic_data()\nhead(example_data)\n\n  y          x\n1 0 -0.6264538\n2 1  0.1836433\n3 0 -0.8356286\n4 1  1.5952808\n5 0  0.3295078\n6 0 -0.8204684\n\n\nAfter simulating a dataset, we’ll then fit both ordinary linear regression and logistic regression.\n\n# ordinary linear regression\nfit_lm  = lm(y ~ x, data = example_data)\n# logistic regression\nfit_glm = glm(y ~ x, data = example_data, family = binomial)\n\nNotice that the syntax is extremely similar. What’s changed?\n\nlm() has become glm()\nWe’ve added family = binomial argument\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.31**\n\n\n-2.31*\n\n\n\n\n \n\n\n(0.08)\n\n\n(1.13)\n\n\n\n\nx\n\n\n0.30**\n\n\n3.66*\n\n\n\n\n \n\n\n(0.09)\n\n\n(1.65)\n\n\n\n\nR2\n\n\n0.34\n\n\n \n\n\n\n\nAdj. R2\n\n\n0.31\n\n\n \n\n\n\n\nNum. obs.\n\n\n25\n\n\n25\n\n\n\n\nAIC\n\n\n \n\n\n22.74\n\n\n\n\nBIC\n\n\n \n\n\n25.18\n\n\n\n\nLog Likelihood\n\n\n \n\n\n-9.37\n\n\n\n\nDeviance\n\n\n \n\n\n18.74\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nMaking predictions with an object of type glm is slightly different than making predictions after fitting with lm().\nIn the case of logistic regression, with family = binomial, we have:\n\n\n\n\n\n\n\ntype\nReturned\n\n\n\n\n\"link\" [default]\n\\(\\hat{\\eta}({\\bf x}) = \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right)\\)\n\n\n\"response\"\n\\(\\hat{p}({\\bf x})\\)\n\n\n\nThat is, type = \"link\" will get you the log odds, while type = \"response\" will return \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) for each observation.\n\nplot(y ~ x, data = example_data, \n     pch = 20, ylab = \"Estimated Probability\", \n     main = \"Ordinary vs Logistic Regression\")\nabline(fit_lm, col = \"darkorange\")\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \n      add = TRUE, col = \"dodgerblue\", lty = 2)\nlegend(\"topleft\", c(\"Ordinary\", \"Logistic\", \"Data\"), lty = c(1, 2, 0), \n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))"
  },
  {
    "objectID": "slides/09-slides-old.html#ols-vs-logistic-regression-1",
    "href": "slides/09-slides-old.html#ols-vs-logistic-regression-1",
    "title": "Week 09:",
    "section": "OLS vs Logistic Regression",
    "text": "OLS vs Logistic Regression\n\nOLS produces impossible predictions\nThe coefficients from logistic regression aren’t directly interpertable \\(\\to\\) need predicted values.\n\nCan also calculate things like odds-ratios but I find this convoluted.\n\nThe marginal effect of \\(X\\) varies in a logistic regression"
  },
  {
    "objectID": "slides/09-slides-old.html#interpreting-logistic-regression-coefficients",
    "href": "slides/09-slides-old.html#interpreting-logistic-regression-coefficients",
    "title": "Week 09:",
    "section": "Interpreting Logistic Regression Coefficients",
    "text": "Interpreting Logistic Regression Coefficients\nOur estimated model is then:\n\\[\\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right) = -2.3 + 3.7 x\\]\nBecause we’re not directly estimating the mean, but instead a function of the mean, we need to be careful with our interpretation of \\(\\hat{\\beta}_1 = 3.7\\).\nThis means that, for a one unit increase in \\(x\\), the log odds change (in this case increase) by \\(3.7\\). Also, since \\(\\hat{\\beta}_1\\) is positive, as we increase \\(x\\) we also increase \\(p({\\bf x})\\).\nFor example, we have:\n\\[\\hat{P}[Y = 1 \\mid X = -0.5] = \\frac{e^{-2.3 + 3.7 \\cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \\cdot (-0.5)}} \\approx 0.016\\]\n\npredict(fit_glm, newdata = data.frame(x=-0.5), type = \"response\")\n\n         1 \n0.01567416 \n\n\n\\[\\hat{P}[Y = 1 \\mid X = 0] = \\frac{e^{-2.3 + 3.7 \\cdot (0)}}{1 + e^{-2.3 + 3.7 \\cdot (0)}} \\approx 0.09\\]\n\npredict(fit_glm, newdata = data.frame(x=0), type = \"response\")\n\n         1 \n0.09016056 \n\n\n\\[\\hat{P}[Y = 1 \\mid X = 1] = \\frac{e^{-2.3 + 3.7 \\cdot (1)}}{1 + e^{-2.3 + 3.7 \\cdot (1)}} \\approx 0.38\\]\n\npredict(fit_glm, newdata = data.frame(x=.5), type = \"response\")\n\n        1 \n0.3814476 \n\n\nbackground-image:url(“https://resourcemoon.com/wp-content/uploads/2018/09/summery.png”) background-size:cover"
  },
  {
    "objectID": "slides/09-slides-old.html#summary-1",
    "href": "slides/09-slides-old.html#summary-1",
    "title": "Week 09:",
    "section": "Summary",
    "text": "Summary\n\nThe Law of Large Number’s says that as our sample size increases, our sample mean will converge to the population value\nThe Central Limit Theorem says that the distribution of those sample means will follow a normal distribution\nGeneralized Linear Models allow us to more accurately model different types of data-generating processes using Maxium Likelihood Estimation.\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/13-slides.html#presentation-order",
    "href": "slides/13-slides.html#presentation-order",
    "title": "POLS 1600",
    "section": "Presentation Order",
    "text": "Presentation Order\n\n\nGroup 7 - Mariana, Kahrie, Shannon, Jarret\nGroup 4 - Tiffany, Dan, Zoe, Lorena\nGroup 5 - Chris, Dan, Neve, Olivia\nGroup 1 - Maia, Guadalupe, Stephen, Jeremiah\nGroup 3 - Serafym, Rachel, Kai, Emma\nGroup 8 - Logan, Keiley, Lydell, Mahir\nGroup 6 - Mia, Emily, Davis, Talia\nGroup 2 - Andrew, Spencer, Lucinda, Serenity\n\n~ Roughly 8 minutes each"
  },
  {
    "objectID": "slides/13-slides.html#class-survey-and-course-evals",
    "href": "slides/13-slides.html#class-survey-and-course-evals",
    "title": "POLS 1600",
    "section": "Class Survey and Course Evals",
    "text": "Class Survey and Course Evals\n\nClick here to take the last survey ever\nAlso, at some point please complete the course evaluations at:\nhttps://brown.evaluationkit.com"
  },
  {
    "objectID": "slides/13-slides.html#parting-thoughts",
    "href": "slides/13-slides.html#parting-thoughts",
    "title": "POLS 1600",
    "section": "Parting thoughts",
    "text": "Parting thoughts\n\n\n\n\nSeriously, thank you. It’s been a pleasure teaching you all!\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/01-slides.html#class-plan",
    "href": "slides/01-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nLogistics (15 minutes)\n\nAnnouncements\nFeedback\n\nClass plan (60 minutes)\n\nIntroduction to R, R Studio and Quarto\nLoading and Looking at Data in R\nTransforming, Recoding, and Cleaning Data in R\nDescribing Data in R\nExploring Covid-19 Data for Lab"
  },
  {
    "objectID": "slides/01-slides.html#annoucements",
    "href": "slides/01-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nIf it’s your first time here you’ll need to work through Software Setup to follow along today\n\nTalk to me after class if you’re having installation issues\n\nIf you’re still on the waitlist on CAB, speak to me after class"
  },
  {
    "objectID": "slides/01-slides.html#announcements",
    "href": "slides/01-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\n\n“Uh ohhh, the Cavs are playing playoff basketball” pic.twitter.com/WrOrzeuEtW\n\n— Bottlegate ((Bottlegate?)) April 21, 2017"
  },
  {
    "objectID": "slides/01-slides.html#tutorials",
    "href": "slides/01-slides.html#tutorials",
    "title": "POLS 1600",
    "section": "Tutorials",
    "text": "Tutorials\nOnce you’ve done the following\n\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nYou can see the available problem sets by running the following code in your console:\n\nlearnr::run_tutorial(package = \"qsslearnr\")"
  },
  {
    "objectID": "slides/01-slides.html#tutorials-1",
    "href": "slides/01-slides.html#tutorials-1",
    "title": "POLS 1600",
    "section": "Tutorials",
    "text": "Tutorials\nAnd start a specific tutorial by running:\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nPlease upload tutorials 00-intro and 01-measurement1 to Canvas by Friday"
  },
  {
    "objectID": "slides/01-slides.html#youve-committed-a-murder",
    "href": "slides/01-slides.html#youve-committed-a-murder",
    "title": "POLS 1600",
    "section": "You’ve committed a murder",
    "text": "You’ve committed a murder"
  },
  {
    "objectID": "slides/01-slides.html#why",
    "href": "slides/01-slides.html#why",
    "title": "POLS 1600",
    "section": "Why",
    "text": "Why"
  },
  {
    "objectID": "slides/01-slides.html#why-do-you-ask",
    "href": "slides/01-slides.html#why-do-you-ask",
    "title": "POLS 1600",
    "section": "Why do you ask?",
    "text": "Why do you ask?\nIt’s cousin Nick’s fault…\n\nFunny icebreaker, but lots of assumptions…\n\nYou’re not a murderer\nYou don’t know someone who’s committed a murder or been murdered\nYou’ve got a mom and dad"
  },
  {
    "objectID": "slides/01-slides.html#why-do-you-ask-1",
    "href": "slides/01-slides.html#why-do-you-ask-1",
    "title": "POLS 1600",
    "section": "Why do you ask?",
    "text": "Why do you ask?\n\nHow might we make this question better?\n\nUse a screener question\n\n“Would you feel comfortable…”\n\n“Pipe” in responses from a prior question\n\n“Who are two people who raised you…”\n\n\n\n\nWhat questions we ask and how we ask them matters"
  },
  {
    "objectID": "slides/01-slides.html#hopes-and-dreams-fears-and-worries",
    "href": "slides/01-slides.html#hopes-and-dreams-fears-and-worries",
    "title": "POLS 1600",
    "section": "Hopes and Dreams, Fears and Worries",
    "text": "Hopes and Dreams, Fears and Worries\n\n\nWhat are we excited about?\n\nEngaging with social science\nLearning statistics and math\nLearning to code\n\n\nWhat are weworried about?\n\nEngaging with social science\nLearning statistics and math\nLearning to code"
  },
  {
    "objectID": "slides/01-slides.html#overview-1",
    "href": "slides/01-slides.html#overview-1",
    "title": "POLS 1600",
    "section": "Overview",
    "text": "Overview\n\nR, R Studio and Quarto\nGetting set up to work in R\nBasic Programming in R"
  },
  {
    "objectID": "slides/01-slides.html#r-r-studio-and-quarto",
    "href": "slides/01-slides.html#r-r-studio-and-quarto",
    "title": "POLS 1600",
    "section": "R, R Studio and Quarto",
    "text": "R, R Studio and Quarto\n\nR is an open source statistical programming language (cheatsheet)\nR Studio is an integrated development environment (IDE) that makes working in R much easier (cheatsheet)\nQuarto is a publishing system that allows us to write and present code in different formats (cheatsheet)"
  },
  {
    "objectID": "slides/01-slides.html#general-tuesday-workflow",
    "href": "slides/01-slides.html#general-tuesday-workflow",
    "title": "POLS 1600",
    "section": "General Tuesday Workflow",
    "text": "General Tuesday Workflow\n\nGo to https://pols1600.paultesta.org\nGo to class content for current week\nOpen slides in browser\nOpen R Studio\nCreate .qmd file titled wk01-notes.qmd and save in course folder\nGet set up to work\nTake notes and follow along"
  },
  {
    "objectID": "slides/01-slides.html#lets-create-a-.qmd-file",
    "href": "slides/01-slides.html#lets-create-a-.qmd-file",
    "title": "POLS 1600",
    "section": " Let’s create a .qmd file",
    "text": "Let’s create a .qmd file"
  },
  {
    "objectID": "slides/01-slides.html#three-components-of-a-.qmd",
    "href": "slides/01-slides.html#three-components-of-a-.qmd",
    "title": "POLS 1600",
    "section": "Three components of a .qmd",
    "text": "Three components of a .qmd\n\n\nControl output with YAML header\n\n  ---\n  title: \"Title here\"\n  author: \"Your name\"\n  format:\n    html:\n      toc: true\n  ---\n\nWrite code Blocks/Chunks\n\n```{r}\n#| echo: true\n2+2\n```\n\nDescribe code using Markdown\n\nSee Help &gt; Markdown quick reference"
  },
  {
    "objectID": "slides/01-slides.html#the-basics-of-r",
    "href": "slides/01-slides.html#the-basics-of-r",
    "title": "POLS 1600",
    "section": "The Basics of R",
    "text": "The Basics of R\n\nR is an interpreter (&gt;)\n“Everything that exists in R is an object”\n“Everything that happens in R is the result of a function”\nData come in different types, shapes, and sizes\nPackages make R great"
  },
  {
    "objectID": "slides/01-slides.html#r-is-an-interpreter",
    "href": "slides/01-slides.html#r-is-an-interpreter",
    "title": "POLS 1600",
    "section": "R is an interpreter (>)",
    "text": "R is an interpreter (&gt;)\nEnter commands line-by-line in the console\n\nThe &gt; means R is a ready for a command\nThe + means your last command isn’t complete\n\nIf you get stuck with a + use your escape key!\n\nSend code from .qmd file to the console:\n\ncntrl + Enter (PC) | cmd + Return (Mac) -&gt; run current line\ncntrl + shift + Enter (PC) | cmd + shift  + Return (Mac) -&gt; run all code in current chunk"
  },
  {
    "objectID": "slides/01-slides.html#r-is-a-calculator",
    "href": "slides/01-slides.html#r-is-a-calculator",
    "title": "POLS 1600",
    "section": "R is a Calculator",
    "text": "R is a Calculator\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n+\naddition\nx + y\n\n\n-\nsubtraction\nx - y\n\n\n*\nmultiplication\nx * y\n\n\n/\ndivision\nx / y\n\n\n^\nraised to the power of\nx ^ y\n\n\nabs\nabsolute value\nabs(x)\n\n\n%/%\ninteger division\nx %/% y\n\n\n%%\nremainder after division\nx %% y"
  },
  {
    "objectID": "slides/01-slides.html#r-is-logical",
    "href": "slides/01-slides.html#r-is-logical",
    "title": "POLS 1600",
    "section": "R is logical",
    "text": "R is logical\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n&\nand\nx & y\n\n\n|\nor\nx | y\n\n\nxor\nexactly x or y\nxor(x, y)\n\n\n!\nnot\n!x"
  },
  {
    "objectID": "slides/01-slides.html#r-is-logical-1",
    "href": "slides/01-slides.html#r-is-logical-1",
    "title": "POLS 1600",
    "section": "R is logical",
    "text": "R is logical\n\nx &lt;- T; y &lt;- F\n\nx == T\n\n[1] TRUE\n\nx == T & y == T\n\n[1] FALSE\n\nx == T | y == T\n\n[1] TRUE\n\n!x\n\n[1] FALSE"
  },
  {
    "objectID": "slides/01-slides.html#r-can-make-comparisons",
    "href": "slides/01-slides.html#r-can-make-comparisons",
    "title": "POLS 1600",
    "section": "R can make comparisons",
    "text": "R can make comparisons\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n&lt;\nless than\nx &lt; y\n\n\n&lt;=\nless than or equal to\nx &lt;= y\n\n\n&gt;\ngreater than\nx &gt; y\n\n\n&gt;=\ngreater than or equal to\nx &gt;= y\n\n\n==\nexactly equal to\nx == y\n\n\n!=\nnot equal to\nx != y\n\n\n%in%\ngroup membership*\nx %in% y\n\n\nis.na\nis missing\nis.na(x)\n\n\n!is.na\nis not missing\n!is.na(x)"
  },
  {
    "objectID": "slides/01-slides.html#everything-that-exists-in-r-is-an-object",
    "href": "slides/01-slides.html#everything-that-exists-in-r-is-an-object",
    "title": "POLS 1600",
    "section": "Everything that exists in R is an object",
    "text": "Everything that exists in R is an object\n\n\nThe number 5 is an object in R\n\n\n5\n\n[1] 5\n\n\n\nWe can assign the object 5, the name x, using the assignment operator &lt;-+\n\n\nx &lt;- 5 # Read this as \"x gets 5\"\n\n\nNow if we tell R to show us x, we’ll get\n\n\nx\n\n[1] 5\n\nprint(x)\n\n[1] 5"
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-types",
    "href": "slides/01-slides.html#data-come-in-different-types",
    "title": "POLS 1600",
    "section": "Data come in different types",
    "text": "Data come in different types"
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-types-1",
    "href": "slides/01-slides.html#data-come-in-different-types-1",
    "title": "POLS 1600",
    "section": "Data come in different types",
    "text": "Data come in different types\n\n\n\n# Create some data\n\n# Numeric\nx &lt;- 2 # Double\ny &lt;- 6L # Integer\n\n# Logical\nonly_two_types_of_people &lt;- TRUE \n\n# Character\nme &lt;- \"Paul\"\n\n# Factor\ngrades = factor(c(\"A\",\"B\",\"C\"))\n\n\n\n# What type are they?\nclass(x)\n\n[1] \"numeric\"\n\nclass(y)\n\n[1] \"integer\"\n\nclass(only_two_types_of_people)\n\n[1] \"logical\"\n\nclass(me)\n\n[1] \"character\"\n\nclass(grades)\n\n[1] \"factor\""
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-shapes-and-sizes",
    "href": "slides/01-slides.html#data-come-in-different-shapes-and-sizes",
    "title": "POLS 1600",
    "section": "Data come in different “shapes” and “sizes”",
    "text": "Data come in different “shapes” and “sizes”\n\nSource: Gaurav Tiwari"
  },
  {
    "objectID": "slides/01-slides.html#section-1",
    "href": "slides/01-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Name\n“Size”\nType of Data\nR code\n\n\n\n\nscalar\n1\nnumeric, character, factor, logical\nx &lt;- 5\n\n\nvector\nN elements: length(x)\nall the same\nv &lt;- c(1, 2, T, \"false\")\n\n\nmatrix\nN rows by columns K: dim(x)\nall the same\nm &lt;- matrix(y,2,2)\n\n\narray\nN row by K column by J dimensions: dim(x)\nall the same\na &lt;- array(m,c(2,2,3))\n\n\ndata frames\nN row by K column matrix\ncan be different\nd &lt;-data.frame(x=x, y=y)\n\n\ntibbles\nN row by K column matrix\ncan be different\nd &lt;-tibble(x=x, y=y)\n\n\nlists\ncan vary\ncan be different\nl &lt;-list(x,y,m,a,d)"
  },
  {
    "objectID": "slides/01-slides.html#everything-that-happens-in-r-is-the-result-of-a-function",
    "href": "slides/01-slides.html#everything-that-happens-in-r-is-the-result-of-a-function",
    "title": "POLS 1600",
    "section": "Everything that happens in R is the result of a function",
    "text": "Everything that happens in R is the result of a function\n\nYou’ve already seen and used some R functions\n\nthe &lt;- is the assignement operator that assigns a value to a name\nc() is the combine function that combines elements together\ninstall.packages() installs packages\nlibrary() loads packages you’ve installed so you can use functions and data that are part of that package"
  },
  {
    "objectID": "slides/01-slides.html#three-sources-of-functions",
    "href": "slides/01-slides.html#three-sources-of-functions",
    "title": "POLS 1600",
    "section": "Three sources of functions",
    "text": "Three sources of functions\nThree sources of functions:\n\nbase R\n\n&lt;-; mean(x); library(\"package_name\")\n\npackages\n\ninstall.packages(\"packageName)\"\nremotes::intall_github(\"user/repository\")\n\nYou\n\nmy_function &lt;- function(x){x^2}"
  },
  {
    "objectID": "slides/01-slides.html#functions-are-like-recipes",
    "href": "slides/01-slides.html#functions-are-like-recipes",
    "title": "POLS 1600",
    "section": "Functions are like recipes",
    "text": "Functions are like recipes\nThey have:\n\n\n\nnames\ningredients (inputs)\nsteps that tell you what to do with the ingredients (statements/code)\ntasty results from applying those steps to given ingredients (outputs)\n\n\n (Source)"
  },
  {
    "objectID": "slides/01-slides.html#can-i-kick-it",
    "href": "slides/01-slides.html#can-i-kick-it",
    "title": "POLS 1600",
    "section": "Can I kick it?",
    "text": "Can I kick it?\n\ncan_x_kick_it &lt;- function(x){\n  # Determine if x can kick it\n  # If x in A Tribe Called Quest\n  if(x %in% c(\"Q-Tip\",\"Phife Dawg\",\n              \"Ali Shaheed Muhammad\", \n              \"Jarobi White\")){\n    return(\"Yes you can\")\n  }else{\n    return(\"Before this, did you really know what live was?\")\n  }\n\n}\ncan_x_kick_it(\"Q-Tip\")\n\n[1] \"Yes you can\"\n\ncan_x_kick_it(\"Paul\")\n\n[1] \"Before this, did you really know what live was?\""
  },
  {
    "objectID": "slides/01-slides.html#getting-setup-to-work-in-r",
    "href": "slides/01-slides.html#getting-setup-to-work-in-r",
    "title": "POLS 1600",
    "section": "Getting setup to work in R",
    "text": "Getting setup to work in R\nEach time you start a project in R, you will want to:\n\nSet your working directory\nLoad (and if needed, install) the R packages you will use\nSet any “global” options you want\nLoad the data you’ll be using"
  },
  {
    "objectID": "slides/01-slides.html#set-your-working-directory",
    "href": "slides/01-slides.html#set-your-working-directory",
    "title": "POLS 1600",
    "section": "Set your working directory",
    "text": "Set your working directory"
  },
  {
    "objectID": "slides/01-slides.html#load-and-if-needed-install-the-r-packages-you-will-use",
    "href": "slides/01-slides.html#load-and-if-needed-install-the-r-packages-you-will-use",
    "title": "POLS 1600",
    "section": "Load (and if needed, install) the R packages you will use",
    "text": "Load (and if needed, install) the R packages you will use\n\n\nInstall packages once1 with install.packages(\"package_name\")\nLoad packages every session with library(\"package_name\")\n\n\nOccasionally, you’ll have to update packages to newer versions and will likely need to reinstall when you upgrade R"
  },
  {
    "objectID": "slides/01-slides.html#install-packages-for-the-lab",
    "href": "slides/01-slides.html#install-packages-for-the-lab",
    "title": "POLS 1600",
    "section": " Install packages for the lab",
    "text": "Install packages for the lab"
  },
  {
    "objectID": "slides/01-slides.html#install-packages-for-the-lab-1",
    "href": "slides/01-slides.html#install-packages-for-the-lab-1",
    "title": "POLS 1600",
    "section": "Install packages for the lab",
    "text": "Install packages for the lab\nLet’s install the tidyverse and COVID19.\n\n\nCreate a new code chunk\nLabel it libraries\nCopy and paste the following into your console\n\n\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"COVID19\")\n\n\n\nOnce you’ve installed these packages comment out the code\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"COVID19\")\n\n\n\n\n\n\n\nKeyboard Shortcuts to toggle # comments\n\n\nmacOS: CMD + SHIFT + C\nPC: CTRL + SHIFT + C"
  },
  {
    "objectID": "slides/01-slides.html#loading-the-tidyverse-and-covid19-packages",
    "href": "slides/01-slides.html#loading-the-tidyverse-and-covid19-packages",
    "title": "POLS 1600",
    "section": "Loading the tidyverse and COVID19 packages",
    "text": "Loading the tidyverse and COVID19 packages\n\nType the following into your code chunk:\n\n\nlibrary(\"tidyverse\")\nlibrary(COVID19)"
  },
  {
    "objectID": "slides/01-slides.html#set-any-global-options-you-want",
    "href": "slides/01-slides.html#set-any-global-options-you-want",
    "title": "POLS 1600",
    "section": "Set any “global” options you want",
    "text": "Set any “global” options you want\nHere are the global options for these slides:1\n\n# Options for these slides\nknitr::opts_chunk$set(\n  warning = FALSE,       # Don't display warnings\n  message = FALSE,       # Don't display messages\n  comment = NA,          # No prefix before line of text\n  dpi = 300,             # Figure resolution\n  fig.align = \"center\",  # Figure alignment\n  out.width = \"80%\",     # Figure width\n  cache = FALSE          # Don't cache code chunks\n  )\n\nI’ll generally do this for you, but it’s useful to know that you can change options globally (for every chunk) and locallys (for specific chunks)"
  },
  {
    "objectID": "slides/01-slides.html#load-the-data-youll-be-using",
    "href": "slides/01-slides.html#load-the-data-youll-be-using",
    "title": "POLS 1600",
    "section": "Load the data you’ll be using",
    "text": "Load the data you’ll be using\nThere are three ways to load data.\n\nLoad a pre-existing dataset\n\ndata(\"dataset\") will load the dataset named “dataset”\ndata() will list all datasets\n\nLoad a .Rdata/.rda file using load(\"dataset.rda\")\nRead data of a different format (.csv, .dta, .spss) into R using specific functions from packages like haven and readr"
  },
  {
    "objectID": "slides/01-slides.html#overview-working-with-data-in-r",
    "href": "slides/01-slides.html#overview-working-with-data-in-r",
    "title": "POLS 1600",
    "section": "Overview: Working with Data in R",
    "text": "Overview: Working with Data in R\n\nLoading data into R\nLooking at your data\nCleaning and transforming your data"
  },
  {
    "objectID": "slides/01-slides.html#loading-data-into-r",
    "href": "slides/01-slides.html#loading-data-into-r",
    "title": "POLS 1600",
    "section": "Loading data into R",
    "text": "Loading data into R\nThere are three ways to load data.\n\nLoad a pre-existing dataset\n\ndata(\"dataset\") will load the dataset named “dataset”\ndata() will list all datasets\nUseful for tutorials, working through examples/help\n\nLoad a .Rdata/.rda file using load(\"dataset.rda\")\nRead data of a different format (.csv, .dta, .spss) into R using specific functions\n\nWe will use functions from the haven and readr packages to read data from the web and stored locally on your computer"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data",
    "href": "slides/01-slides.html#loading-state-level-covid-data",
    "title": "POLS 1600",
    "section": " Loading state-level Covid data",
    "text": "Loading state-level Covid data"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data-1",
    "href": "slides/01-slides.html#loading-state-level-covid-data-1",
    "title": "POLS 1600",
    "section": "Loading state-level Covid data",
    "text": "Loading state-level Covid data\nThe code below downloads two years of daily state-level Covid data:\n\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  start = \"2020-01-01\",\n  end = \"2022-12-31\",\n  level = 2,\n  verbose = F\n    \n)\n\nPlease run the following1\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n\n30 people trying to download this data live sometimes causes errors with the server"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data-2",
    "href": "slides/01-slides.html#loading-state-level-covid-data-2",
    "title": "POLS 1600",
    "section": "Loading state-level Covid data",
    "text": "Loading state-level Covid data\n\n\ncountry = US tells the function we want data for the US\nstart = \"2020-01-01\" sets the start date for the data\nstart = \"2020-01-01\" sets the end date for the data\nlevel = 2 tells the function we want state-level data\nverbose = F tells the function not to print other stuff\n\n\n\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  start = \"2020-01-01\",\n  end = \"2022-12-31\",\n  level = 2,\n  verbose = F\n    \n)"
  },
  {
    "objectID": "slides/01-slides.html#looking-at-your-data",
    "href": "slides/01-slides.html#looking-at-your-data",
    "title": "POLS 1600",
    "section": "Looking at your data",
    "text": "Looking at your data\nAnytime you load data into R, try some combination of the following to get a high-level overview (HLO) of the data\n\ndim(data) gives you the dimensions (# of rows and columns)\nView(data) opens data in a separate pane\nprint(data); data will display a truncated view of data in your console\nglimpse(data) will show a transposed (switch columns and rows) version of data with information on variable type\nhead(data) shows you the first 5 rows\ntail(data) shows you the last 5 rows\ndata$variable extracts variable from data\ntable(data$variable) creates a frequency table\n\nGood for categorical data\n\nsummary(data$variable) summary statistics\n\nGood for numeric data"
  },
  {
    "objectID": "slides/01-slides.html#hlos-allow-you-to",
    "href": "slides/01-slides.html#hlos-allow-you-to",
    "title": "POLS 1600",
    "section": "HLOs allow you to",
    "text": "HLOs allow you to\n\nDescribe the structure of your data:\n\nHow many observations (rows)\nHow many variables (columns)\n\nDescribe the unit of analysis\n\nWhat does a row in your data correspond to\n\nIdentify the class and type of variables (columns)\n\nNumeric, character, logical\nIs there missing data (NAs)\n\nFigure out what transformations, cleaning, and recoding you need to do"
  },
  {
    "objectID": "slides/01-slides.html#the-tidyverse",
    "href": "slides/01-slides.html#the-tidyverse",
    "title": "POLS 1600",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nFor more check out R for Data Science"
  },
  {
    "objectID": "slides/01-slides.html#tidy-data",
    "href": "slides/01-slides.html#tidy-data",
    "title": "POLS 1600",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data is a standard way of mapping the meaning of a dataset to its structure.\nA dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nMore"
  },
  {
    "objectID": "slides/01-slides.html#dplyr-functions-for-data-wrangling",
    "href": "slides/01-slides.html#dplyr-functions-for-data-wrangling",
    "title": "POLS 1600",
    "section": "dplyr functions for data wrangling",
    "text": "dplyr functions for data wrangling\nToday and this week will begin learning some tools for selecting and transforming data:\n\nselect() to select columns from a dataframe\nfilter() to select rows from a dataframe when some statement is TRUE\nmutate() to create new colums\n\ncase_when() to recode values when some statement is TRUE\n\nsummarise() to transform many values into one value\ngroup_by() to create a grouped table so that other functions are applied separately to each group and then combined"
  },
  {
    "objectID": "slides/01-slides.html#the-pipe-operator",
    "href": "slides/01-slides.html#the-pipe-operator",
    "title": "POLS 1600",
    "section": "The %>% (“pipe” operator)",
    "text": "The %&gt;% (“pipe” operator)\nThe %&gt;% lets us chain functions together so we can read left to right\n\nslice(filter(covid, administrative_area_level_2 == \"Rhode Island\"), 1)\n\nBecomes\n\ncovid %&gt;% \n  filter(administrative_area_level_2 == \"Rhode Island\") %&gt;% \n  slice(1)\n\n\n\n\n\n\n\nKeyboard Shortcuts for %&gt;%\n\n\nmacOS: CMD + SHIFT + M\nPC: CTRL + SHIFT + M"
  },
  {
    "objectID": "slides/01-slides.html#descriptive-statistics",
    "href": "slides/01-slides.html#descriptive-statistics",
    "title": "POLS 1600",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWhen social scientists talk about descriptive inference, we’re trying to summarize our data and make claims about what’s typical of our data\n\nWhat’s a typical value\n\nMeasures of central tendency\nmean, median, mode\n\nHow do our data vary around typical values\n\nMeasures of dispersion\nvariance, standard deviation, range, percentile ranges\n\nHow does variation in one variable relate to variation in another\n\nMeasures of association\ncovariance, correlation"
  },
  {
    "objectID": "slides/01-slides.html#using-r-to-summarize-data",
    "href": "slides/01-slides.html#using-r-to-summarize-data",
    "title": "POLS 1600",
    "section": "Using R to Summarize Data",
    "text": "Using R to Summarize Data\nHere are some common ways of summarizing data and how to calculate them with R\n\n\n\nDescription\nUsage\n\n\n\n\nsum\nsum(x)\n\n\nminimum\nmin(x)\n\n\nmaximum\nmax(x)\n\n\nrange\nrange(x)\n\n\nmean\nmean(x)\n\n\nmedian\nmedian(x)\n\n\npercentile\nquantile(x)\n\n\nvariance\nvar(x)\n\n\nstandard deviation\nsd(x)\n\n\nrank\nrank(x)\n\n\n\n\n\nAll of these functions have an argument called na.rm=F. If your data have missing values, you’ll need to set na.rm=F (e.g. mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/01-slides.html#exploring-covid-19-data-for-lab-1",
    "href": "slides/01-slides.html#exploring-covid-19-data-for-lab-1",
    "title": "POLS 1600",
    "section": "Exploring Covid-19 Data for Lab",
    "text": "Exploring Covid-19 Data for Lab\nLet’s spend the rest of class, exploring what seems like a simple question\n\nOn average, did states that adopted mask mandates have lower rates of new cases?"
  },
  {
    "objectID": "slides/01-slides.html#tasks",
    "href": "slides/01-slides.html#tasks",
    "title": "POLS 1600",
    "section": "Tasks",
    "text": "Tasks\n\nGet a high level overview of our data\nSubset the data to just U.S. States\nRecode our data to get a measure of new Covid cases and what face mask policy policy was in place\nSummarize the average number of new cases by face mask policy."
  },
  {
    "objectID": "slides/01-slides.html#get-a-high-level-overview-of-our-data",
    "href": "slides/01-slides.html#get-a-high-level-overview-of-our-data",
    "title": "POLS 1600",
    "section": "1. Get a high level overview of our data",
    "text": "1. Get a high level overview of our data\n\n\n\n\nCreate a new chunk\nLabel it #| label:\"HLO\"\nRun the following code\nComment code with #\n\n\n\n\ndim(covid)\nhead(covid)\nglimpse(covid)\ntable(covid$administrative_area_level_2)\nlength(unique(covid$administrative_area_level_2))\ncovid$confirmed[1:10]\ncovid %&gt;%\n  select(administrative_area_level_2, date, confirmed) %&gt;%\n  slice(10:20)\nsummary(covid$confirmed)\ntable(covid$facial_coverings)"
  },
  {
    "objectID": "slides/01-slides.html#answer-the-following",
    "href": "slides/01-slides.html#answer-the-following",
    "title": "POLS 1600",
    "section": "Answer the following",
    "text": "Answer the following\n\n\nHow many observations are there (rows)\nHow many variables (columns)\nWhat’s the unit of analysis?\n\nIn words, how would you describe what a row in your data set corresponds to?\n\nAre there any missing values for confirmed\nWhat range of values can facial_coverings take?1\n\n\nSee: https://covid19datahub.io/articles/docs.html"
  },
  {
    "objectID": "slides/01-slides.html#subsetting-our-data-to-only-u.s.-states",
    "href": "slides/01-slides.html#subsetting-our-data-to-only-u.s.-states",
    "title": "POLS 1600",
    "section": "Subsetting our data to only U.S. States",
    "text": "Subsetting our data to only U.S. States\nGoal: Subset our Covid data to include only the 50 states + DC\nSteps:\n\nCreate a vector of the territories we don’t want\nUse the filter() command to “filter” out these territories"
  },
  {
    "objectID": "slides/01-slides.html#create-a-vector-of-the-territories-we-dont-want",
    "href": "slides/01-slides.html#create-a-vector-of-the-territories-we-dont-want",
    "title": "POLS 1600",
    "section": "1. Create a vector of the territories we don’t want",
    "text": "1. Create a vector of the territories we don’t want\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )"
  },
  {
    "objectID": "slides/01-slides.html#use-the-filter-command-to-filter-out-these-territories",
    "href": "slides/01-slides.html#use-the-filter-command-to-filter-out-these-territories",
    "title": "POLS 1600",
    "section": "2. Use the filter() command to “filter” out these territories",
    "text": "2. Use the filter() command to “filter” out these territories\n\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories )\n\ndim(covid)\n\n[1] 58809    47\n\ndim(covid_us)\n\n[1] 53678    47"
  },
  {
    "objectID": "slides/01-slides.html#creating-new-variables-for-analysis",
    "href": "slides/01-slides.html#creating-new-variables-for-analysis",
    "title": "POLS 1600",
    "section": "Creating new variables for analysis",
    "text": "Creating new variables for analysis\n\nGoal: We need new variables that describe:\n\nthe number of new Covid-19 cases on a given date\nthe face mask policy in place\n\nSteps:\n\nUse mutate(), group_by() and lag() to calculate new_cases from total confirmed cases\nUse mutate(), case_when() and abs() to turn numeric facial_coverings into categorical factor variable"
  },
  {
    "objectID": "slides/01-slides.html#calculate-new-covid-19-cases",
    "href": "slides/01-slides.html#calculate-new-covid-19-cases",
    "title": "POLS 1600",
    "section": "Calculate new Covid-19 cases",
    "text": "Calculate new Covid-19 cases\nPlease run and comment the following code:\n\ndim(covid_us)\n\n[1] 53678    47\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases/population *100000\n    ) -&gt; covid_us\ndim(covid_us)\n\n[1] 53678    50"
  },
  {
    "objectID": "slides/01-slides.html#create-face-mask-policy-variable",
    "href": "slides/01-slides.html#create-face-mask-policy-variable",
    "title": "POLS 1600",
    "section": "Create Face Mask Policy variable",
    "text": "Create Face Mask Policy variable\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n      \n    ) \n  ) -&gt; covid_us\n\nlevels(factor(covid_us$face_masks))\n\n[1] \"No policy\"              \"Recommended\"            \"Required all times\"    \n[4] \"Required shared places\" \"Some requirements\""
  },
  {
    "objectID": "slides/01-slides.html#make-face_masks-a-factor-to-reflect-order-of-policies",
    "href": "slides/01-slides.html#make-face_masks-a-factor-to-reflect-order-of-policies",
    "title": "POLS 1600",
    "section": "Make face_masks a factor to reflect order of policies",
    "text": "Make face_masks a factor to reflect order of policies\n\nlevels(factor(covid_us$face_masks))\n\n[1] \"No policy\"              \"Recommended\"            \"Required all times\"    \n[4] \"Required shared places\" \"Some requirements\"     \n\ncovid_us %&gt;%\n  mutate(\n    face_masks = factor(\n      face_masks,\n      levels = c(\n        \"No policy\", \n        \"Recommended\", \n        \"Some requirements\",\n        \"Required shared places\",\n        \"Required all times\"\n        )\n    )\n  ) -&gt; covid_us\n\nlevels(covid_us$face_masks)\n\n[1] \"No policy\"              \"Recommended\"            \"Some requirements\"     \n[4] \"Required shared places\" \"Required all times\""
  },
  {
    "objectID": "slides/01-slides.html#calculate-the-average-number-of-covid-19-cases-by-face-mask-policy",
    "href": "slides/01-slides.html#calculate-the-average-number-of-covid-19-cases-by-face-mask-policy",
    "title": "POLS 1600",
    "section": "Calculate the Average Number of Covid-19 cases by Face Mask Policy",
    "text": "Calculate the Average Number of Covid-19 cases by Face Mask Policy\n\nGoal: On average, did states that adopted mask mandates have lower rates of new cases?\nSteps: use filter(), group_by() and summarise() and mean() to calculate the average number of cases for each level of the face_masks policy variable"
  },
  {
    "objectID": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k",
    "href": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k",
    "title": "POLS 1600",
    "section": "Face Masks and New Covid-19 Cases (per 100k)",
    "text": "Face Masks and New Covid-19 Cases (per 100k)\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(face_masks)%&gt;%\n  summarize(\n    `Average No. of New Cases` = round(mean(new_cases_pc, na.rm=T),2)\n  )%&gt;%\n  rename(\n    \"Face Mask Policy\" = face_masks\n  ) -&gt; face_mask_summary"
  },
  {
    "objectID": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k-1",
    "href": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k-1",
    "title": "POLS 1600",
    "section": "Face Masks and New Covid-19 Cases (per 100k)",
    "text": "Face Masks and New Covid-19 Cases (per 100k)\n\n\n\n\nWhat should we conclude?\nWhat’s wrong with this simple comparison?\nWhat’s a better comparison? (Thursday)\n\n\n\n\n\n\nFace Mask Policy\nAverage No. of New Cases\n\n\n\n\nNo policy\n10.26\n\n\nRecommended\n16.61\n\n\nSome requirements\n36.18\n\n\nRequired shared places\n29.38\n\n\nRequired all times\n32.18"
  },
  {
    "objectID": "slides/01-slides.html#commented-code",
    "href": "slides/01-slides.html#commented-code",
    "title": "POLS 1600",
    "section": "Commented Code",
    "text": "Commented Code\n\n# ---- Libraries ----\n## Uncomment to install\n# install.packages(\"tidyverse\")\n# install.packages(\"COVID19\")\n\nlibrary(\"tidyverse\")\nlibrary(\"COVID19\")\n\n# ---- Load data ----\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n\n# ---- Subset to US states and DC ----\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories )\n\n## Check subsetting\ndim(covid)[1] &gt; dim(covid_us)[1]\n\n# ---- Recode covid_us ----\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases/population *100000\n    ) %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\"\n      ) \n    ) %&gt;%\n  mutate(\n    face_masks = factor(\n      face_masks,\n      levels = c(\n        \"No policy\", \n        \"Recommended\", \n        \"Some requirements\",\n        \"Required shared places\",\n        \"Required all times\"\n        )\n      )\n    )-&gt; covid_us\n\n\n# ---- Calculate new cases per capita by facemask policy\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(face_masks)%&gt;%\n  summarize(\n    `Average No. of New Cases` = round(mean(new_cases_pc, na.rm=T),2)\n  )%&gt;%\n  rename(\n    \"Face Mask Policy\" = face_masks\n  ) -&gt; face_mask_summary\n\nface_mask_summary"
  },
  {
    "objectID": "slides/01-slides.html#summary-1",
    "href": "slides/01-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\nAfter today, you should have a better sense of\n\nHow to write R code using Quarto and R Markdown\nHow to install packages and load libraries\nSome of different types and shapes of data\nHow to get a high level overview of your data\nHow to transform, recode, and summarise data using dplyr and the tidyverse\nHow describe typical values and variation in data\nHow to explore substantive questions using these these typical values"
  },
  {
    "objectID": "slides/01-slides.html#congrats",
    "href": "slides/01-slides.html#congrats",
    "title": "POLS 1600",
    "section": "Congrats!",
    "text": "Congrats!\n\nWe covered A LOT\nIt’s OK to feel overwhelmed\n\nBut please don’t suffer in silence\n\nDon’t worry if everything didn’t make sense.\n\nEventually it will, but this takes time and practice\nTesta’s 50-50 rule\nFAAFO\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/09-slides.html#class-plan",
    "href": "slides/09-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements (5 min)\nFeedback (5 min)\nClass plan\n\nProbability Distributions (20 min)\nLaw of Large Numbers (20 min)\nCentral Limit Theorem (20 min)\nStandard Errors (10 min)"
  },
  {
    "objectID": "slides/09-slides.html#goals",
    "href": "slides/09-slides.html#goals",
    "title": "POLS 1600",
    "section": "Goals",
    "text": "Goals\n\nProbability distributions allow us to describe different data generating processes and quantify uncertainty about estimates\nThe Law of Large Numbers tells us that the mean of a sample converges to the mean of population as the size of the sample grows larger.\nThe Central Limit Theorem tells us that distribution of sample means of a given sample size converges in distribution to a Normal probability distribution\nStandard Errors describe the width of a sampling distribution and allow us to assess the statistical significance of regression estimates"
  },
  {
    "objectID": "slides/09-slides.html#annoucements-assignment-2",
    "href": "slides/09-slides.html#annoucements-assignment-2",
    "title": "POLS 1600",
    "section": "Annoucements: Assignment 2",
    "text": "Annoucements: Assignment 2\n\nFeedback on Assignment 2 before your labs on Thursday\nProposal:\n\nSubstitute Labs 11 with in class workshops on Final Project\nThis will count as both your grade on Assignment 3 and the Lab for that week"
  },
  {
    "objectID": "slides/09-slides.html#setup-packages-for-today",
    "href": "slides/09-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/09-slides.html#feedback",
    "href": "slides/09-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/09-slides.html#what-did-we-like",
    "href": "slides/09-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/09-slides.html#what-did-we-dislike",
    "href": "slides/09-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/09-slides.html#how-do-we-want-to-be-remembered",
    "href": "slides/09-slides.html#how-do-we-want-to-be-remembered",
    "title": "POLS 1600",
    "section": "How do we want to be remembered",
    "text": "How do we want to be remembered"
  },
  {
    "objectID": "slides/09-slides.html#probability",
    "href": "slides/09-slides.html#probability",
    "title": "POLS 1600",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event happening.\nStatistics uses probability to quantify uncertainty about estimates and hypotheses.\nThree rules of probability (Kolmogorov axioms)\n\nPositivity: \\[Pr(A) \\geq 0 \\]\nCertainty: \\[Pr(\\Omega) = 1 \\]\nAdditivity: \\[Pr(A \\text{ or } B) = Pr(A) + Pr(B)\\] iff A and B are mutually exclusive"
  },
  {
    "objectID": "slides/09-slides.html#probability-1",
    "href": "slides/09-slides.html#probability-1",
    "title": "POLS 1600",
    "section": "Probability",
    "text": "Probability\n\n\nTwo interpretations interpreting probabilities (Frequentist and Bayesian)\nConditional Probability and Bayes Rule:\n\n\\[Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}\\]"
  },
  {
    "objectID": "slides/09-slides.html#random-variables",
    "href": "slides/09-slides.html#random-variables",
    "title": "POLS 1600",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables assign numeric values to each event in an experiment.\n\nMutually exclusive and exhaustive, together cover the entire sample space.\n\nDiscrete random variables take on finite, or countably infinite distinct values.\nContinuous variables can take on an uncountably infinite number of values."
  },
  {
    "objectID": "slides/09-slides.html#probability-distributions-1",
    "href": "slides/09-slides.html#probability-distributions-1",
    "title": "POLS 1600",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\n\\[\\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities}\\]\nUseful for:\n\nDescribing the data generating process\nQuantifying uncertainty about our estimates"
  },
  {
    "objectID": "slides/09-slides.html#probability-distributions-2",
    "href": "slides/09-slides.html#probability-distributions-2",
    "title": "POLS 1600",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nThe can be represented in terms of:\n\nProbability Mass/Density Functions\n\nDiscrete variables have probability mass functions (PMF)\nContinuous variables have probability density functions (PDF)\n\nCumulative Density Functions\n\nDiscrete: Summation of discrete probabilities\nContinuous: Integration over a range of values"
  },
  {
    "objectID": "slides/09-slides.html#common-probability-distributions",
    "href": "slides/09-slides.html#common-probability-distributions",
    "title": "POLS 1600",
    "section": "Common Probability Distributions",
    "text": "Common Probability Distributions\n\nSource"
  },
  {
    "objectID": "slides/09-slides.html#common-discrete-distributions",
    "href": "slides/09-slides.html#common-discrete-distributions",
    "title": "POLS 1600",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\n\nBernoulli: Coin flips with probability of heads, \\(p\\)\nUniform: Coin flip with more than two outcomes\nBinomial: Adding up coin flips\nPoisson: Counting the number of events that occur at some average rate\nGeometric: Counting until a specific event occurs"
  },
  {
    "objectID": "slides/09-slides.html#common-continuous-distributions",
    "href": "slides/09-slides.html#common-continuous-distributions",
    "title": "POLS 1600",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\n\nExponential: Counting till a specific event occurs in continuous time\nNormal: Describe the outcomes that are sums of random variables (with finite means)\n\nThe limit of a Binomial distribution as \\(n\\to \\infty\\)\nThe maximum entropy when we only know the mean and variance\n\nt: A finite sample approximation of the normal\n\\(\\chi^2\\): Distribution of sums of squared variables from a Normal distribution"
  },
  {
    "objectID": "slides/09-slides.html#section",
    "href": "slides/09-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Bernoulli Distribution\n\nBernoulli Distribution Visualize Distribution\n\n\nA Bernoulli random variable describes of “coin flip”, where parameter \\(p\\), the probability of success (e.g “Heads”)\n\\[Pr(X=x)=f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array}\n    \\right.\\]\n\\[\nF(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x&lt;1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x&lt;1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array}\n    \\right.\n\\]\n\\[\nE[X] = p\n\\]\n\\[\nVar[X] = p(1-p)\n\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "slides/09-slides.html#section-1",
    "href": "slides/09-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Binomial Distribution\n\nBinomial Distribution Visualize Distribution\n\n\nA Binomial random variable is sum of successes from a series of \\(n\\) trials from a Bernoulli Distribution with probability of success \\(p\\)\n\\[\nPr(X=x) = f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n\n\\]\n\\[\nE[X] = np\n\\]\n\\[\nVar[X] = np(1-p)\n\\]\n\n\n\n\n\n\nTip\n\n\nBinomial distributions are useful for modeling the a binary (yes/no) outcome like Voting\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "slides/09-slides.html#section-2",
    "href": "slides/09-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Poisson Distribution\n\nPoisson Distribution Visualize Distribution\n\n\nA Poisson random variable describes the probability of observing a discrete number of events in a fixed period of time given that occur with a fixed average rate of \\(\\lambda\\)\n\\[\nPr(X=x) = f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\n\\]\n\\[\nE[X] = \\lambda\n\\]\n\\[\nVar[X] =  \\lambda\n\\]\n\n\n\n\n\n\nTip\n\n\nPoisson distributions are useful for modeling counts (yes/no) like total acts of political participation\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "slides/09-slides.html#section-3",
    "href": "slides/09-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Normal Distribution\n\nNormal Distribution Visualize DistributionPr(X&lt;0)Pr(-1 &lt; X &lt; 1)\n\n\nA Normal distribution is a continuous random variable defined by two parameters: a location parameter \\(\\mu\\) that determines the center of a distribution and a scale parameter \\(\\sigma\\) that determines the spread of a distribution\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]\n\\]\n\\[\nE[X] = \\mu\n\\]\n\\[\nVar[X] =  \\sigma^2\n\\]\n\n\n\n\n\n\nTip\n\n\nAs we will see shortly distributions that involve summing random variables (say, like the distribution of E[Y|X]) will tend towards normal distributions\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive",
    "title": "POLS 1600",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we wanted to know the average height of our class.\nPick 1 person at random and use this as our estimate of the average\nIt would be a pretty bad estimate (it would vary a lot from person to person), but it would be an unbiased estimate\n\nHow would we improve our estimate?"
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive-1",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive-1",
    "title": "POLS 1600",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we increased our sample size from N=1 to N = 5.\nNow our estimate reflects the average of 5 people’s heights as opposed to just 1. Both are are unbiased estimates of the truth, but the N=5 sample has a lower variance.\nNow suppose we took a sample of size N = N-1. That is we measured everyone except one person. Our estimate will be quite close to the truth, varying slightly based on the height of the person left out."
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive-2",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive-2",
    "title": "POLS 1600",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nFinally, suppose we took a sample of size N = 32 (e.g. the class size). Since our sample is the population, our estimate will be exactly equal to to the population.\nEach sample will give us the same “true” value. That is, it will not vary at all.\nThe idea that as the sample size increases, the distance of a sample mean from the population mean \\(\\mu\\) goes to 0 is called the Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides.html#the-weak-law-of-large-numbers-formally",
    "href": "slides/09-slides.html#the-weak-law-of-large-numbers-formally",
    "title": "POLS 1600",
    "section": "The (Weak) Law of Large Numbers (Formally)",
    "text": "The (Weak) Law of Large Numbers (Formally)\nLet \\(X_1, X_2, \\dots\\) be independent and identically distributed (i.i.d.) random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nThen for every \\(\\epsilon&gt;0\\), as the sample size increases (1), the distance of a sample mean from the population mean \\(\\mu\\) (2) goes to 0 (3).\n\\[\\overbrace{Pr(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon)}^{\\text{2. The distance of the sample mean from the truth}} \\overbrace{\\to 0}^{\\text{3. Goes to 0}} \\underbrace{\\text{ as }n \\to \\infty}_{\\text{1. As the sample size increases}}\\]\nEquivalently:\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]"
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-lln",
    "href": "slides/09-slides.html#simulating-the-lln",
    "title": "POLS 1600",
    "section": "Simulating the LLN",
    "text": "Simulating the LLN\nThe expected value of rolling a die 3.5.\n\\[ E[X] = \\Sigma x_ip(X=x_i) = 1/6 * (1+2+3+4+5+6)\\]\nLet our sample size, \\(N\\) be the number of times we roll a die as our\nIf \\(N=1\\), we could get a 1, 2, 3, 4, 5, or 6. which would be pretty far from our expected value of 3.5"
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-lln-1",
    "href": "slides/09-slides.html#simulating-the-lln-1",
    "title": "POLS 1600",
    "section": "Simulating the LLN",
    "text": "Simulating the LLN\nIf we rolled the die two times and took an average, we could still get an value of 1 or 6 for average, but values closer to our expected value of 3.5, happen more often\n\n# Calculate the average from 2 rows\ntable(rowMeans(expand.grid(1:6, 1:6)))\n\n\n  1 1.5   2 2.5   3 3.5   4 4.5   5 5.5   6 \n  1   2   3   4   5   6   5   4   3   2   1"
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-lln-2",
    "href": "slides/09-slides.html#simulating-the-lln-2",
    "title": "POLS 1600",
    "section": "Simulating the LLN",
    "text": "Simulating the LLN\n\nLLN Code Simulations LLN\n\n\nAs we increase our sample size (roll the die more times), the LLN says the chance that our sample average is far from the truth \\((p(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon))\\), gets vanishingly small.\nLet’s write some code to simulate this process\n\n\n\n# Create a 6-sided die\ndie &lt;- 1:6\n\n# Create function to simulate rolling a die N times\n\nroll_fn &lt;- function(n) {\n  rolls &lt;- data.frame(rolls = sample(die, size = n, replace = TRUE))\n  # summarize rolls \n  df &lt;- rolls %&gt;%\n    summarise(\n    # number of rolls\n      n_rolls = n(),\n    # number of times 1 was rolled\n      ones = sum(rolls == 1),\n    # number of times 2 was rolled, etc..\n      twos = sum(rolls == 2),\n      threes = sum(rolls == 3),\n      fours = sum(rolls == 4),\n      fives = sum(rolls == 5),\n      sixes = sum(rolls == 6),\n      # Average of all our rolls\n      average =  mean(rolls),\n      # Absolute difference between averages and rolls\n      abs_error = abs(3.5-average)\n    )\n  # Return summary df\n  df\n}\n\n\n# Holder for simulatoin\n\nsim_df &lt;- NULL\n\n# Set seed\nset.seed(123)\n\nfor(i in 1:1000){\n  sim_df &lt;- rbind(sim_df,\n                  roll_fn(i)\n  )\n}\n\nfig_lln &lt;- sim_df %&gt;% \n  pivot_longer(\n    cols = c(\"average\", \"abs_error\"),\n    names_to = \"Measure\",\n    values_to = \"Estimate\"\n  ) %&gt;% \n  mutate(\n    Measure = ifelse(Measure == \"average\",\"Average\",\"Absolute Error\") %&gt;% \n      factor(., levels = c(\"Average\",\"Absolute Error\"))\n  ) %&gt;% \nggplot(aes(n_rolls, Estimate))+\n  geom_line()+\n  geom_smooth()+\n  facet_wrap(~Measure,scales = \"free_y\")+\n  theme_minimal()"
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln",
    "href": "slides/09-slides.html#proving-the-weak-lln",
    "title": "POLS 1600",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nA proof of the LLN is as follows:\nFirst define \\(U\\) such that its a sample mean for sample of size \\(n\\)\n\\[U=\\frac{X_1+\\dots +X_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln-1",
    "href": "slides/09-slides.html#proving-the-weak-lln-1",
    "title": "POLS 1600",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen show that the sample mean, \\(U\\) is an unbiased estimator of the population mean \\(\\mu\\)\n\\[\\begin{align*}\nE[U]&=E[\\frac{X_1+\\dots +X_n}{n}]=\\frac{1}{n}E[X_1+\\dots +X_n]\\\\\n&=\\frac{n\\mu}{n}=\\mu\n\\end{align*}\\]\nWith a variance\n\\[\\begin{align*}\nVar[U]&=Var[\\frac{X_1+\\dots +X_n}{n}]=\\\\\n    &=Var[\\frac{X_1}{n}]\\dots Var[\\frac{+X_n}{n}]\\\\\n    &\\frac{\\sigma^2}{n^2}\\dots \\frac{\\sigma^2}{n^2}\\\\\n    &\\frac{n \\sigma^2}{n^2}\\\\\n    &\\frac{\\sigma^2}{n}\\\\\n\\end{align*}\\]\nThat decreases with N."
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln-2",
    "href": "slides/09-slides.html#proving-the-weak-lln-2",
    "title": "POLS 1600",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nBy Chebyshev’s inequality the maximum fraction of values that can be some distance from that distribution’s mean:\n\\[Pr(\\left|U-\\mu\\right| &gt; \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2}\\]\nWhich \\(\\to 0\\) as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "slides/09-slides.html#the-strong-law-of-large-numbers",
    "href": "slides/09-slides.html#the-strong-law-of-large-numbers",
    "title": "POLS 1600",
    "section": "The Strong Law of Large Numbers",
    "text": "The Strong Law of Large Numbers\nAs you may have inferred, there is a weak law of large numbers and a strong law of large numbers.\nThe weak law of large numbers states that as the sample size increases, the sample mean converges in probability to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]\nThe strong law of large numbers states that as the sample size increases, the sample mean converges almost surely to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n = \\mu|) = 1\\] The differences in types of convergence won’t matter much for us in this course"
  },
  {
    "objectID": "slides/09-slides.html#the-central-limit-theorem-1",
    "href": "slides/09-slides.html#the-central-limit-theorem-1",
    "title": "POLS 1600",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nSo the LLN tells us that as our sample size grows, an unbiased estimator like the sample average, will get increasingly close to the to the “true” value of the population of mean.\nIf we took a bunch of samples of the same size and calculated the mean of each sample:\n\nthe distribution of those sample means (the sampling distribution) would be centered around the truth (because the estimator is unbiased).\nthe width of the distribution (its variance) would decrease as the sample size increased\nThe Central Limit Theorem tells us about the shape of that sampling distribution."
  },
  {
    "objectID": "slides/09-slides.html#z-scores-and-standardization",
    "href": "slides/09-slides.html#z-scores-and-standardization",
    "title": "POLS 1600",
    "section": "Z-scores and Standardization",
    "text": "Z-scores and Standardization\nLet \\(X\\) be a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nDefine a new R.V. \\(Z\\) as the standardization of \\(X\\):\n\\[Z=\\frac{X-\\mu}{\\sigma}\\]\nWhere Z has \\(\\mu=0\\) and \\(\\sigma=1\\)."
  },
  {
    "objectID": "slides/09-slides.html#notation-for-the-clt",
    "href": "slides/09-slides.html#notation-for-the-clt",
    "title": "POLS 1600",
    "section": "Notation for the CLT",
    "text": "Notation for the CLT\nLet \\(X_1,X_2,\\dots,X_n\\) be independent and identically distributed RVs with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nDefine \\(S_n\\) and \\(\\bar{X}_n\\) as follows:\n\\[S_n= X_1,X_2,\\dots,X_n= \\sum_{i=1}^n X_i\\]\n\\[\\bar{X}=\\frac{X_1,X_2,\\dots,X_n}{n}= \\frac{S_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides.html#additional-facts-for-the-clt",
    "href": "slides/09-slides.html#additional-facts-for-the-clt",
    "title": "POLS 1600",
    "section": "Additional facts for the CLT",
    "text": "Additional facts for the CLT\nWe can show that:\n\\[\\begin{alignat*}{3}\nE[S_n]&=n\\mu \\hspace{2em}Var[S_n]&=n\\sigma^2 \\hspace{2em} \\sigma_S&=\\sqrt{n}\\sigma\\\\\nE[\\bar{X}_n]&=\\mu \\hspace{2em}Var[\\bar{X}_n]&=\\frac{\\sigma^2}{n} \\hspace{2em}\\sigma_{\\bar{X}}&=\\frac{\\sigma}{\\sqrt{n}}\\\\\n\\end{alignat*}\\]\nBasically: the expected value and variance of the sum is just \\(n\\) times the population parameters (the true values for the distribution).\nSince the mean is just the sum divided by the sample size, the expected value of the mean is equal to the population value and the variance and standard deviations of the mean are decreasing in \\(n\\).\nFinally, we can define \\(Z\\) to in terms of either \\(S\\) or \\(\\bar{X}\\)\n\\[Z_n=\\frac{S_n-n\\mu}{\\sqrt{n}\\sigma}=\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/09-slides.html#central-limit-theorem",
    "href": "slides/09-slides.html#central-limit-theorem",
    "title": "POLS 1600",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nFor a sufficiently large \\(n\\)\n\\[\\begin{align*}\n\\bar{X_n}&\\approx N(\\mu,\\sigma^2/n) \\\\\n\\bar{S_n} &\\approx N(n\\mu,n\\sigma^2) \\\\\n\\bar{Z_n}&\\approx N(0,1)\n\\end{align*}\\]\n\nThe distribution of means \\((\\bar{X_n})\\) from almost any distribution \\(X\\) is approximately Normal (converges in distribution), but with a smaller variance than (\\(\\sigma^2/n\\))\nProof: Several ways, but requires a little more math than is required for this course"
  },
  {
    "objectID": "slides/09-slides.html#clt-why-it-matters",
    "href": "slides/09-slides.html#clt-why-it-matters",
    "title": "POLS 1600",
    "section": "CLT: Why it matters",
    "text": "CLT: Why it matters\n\nWhy is this result so important?\nLots of our questions come of the form, how does a typical value of Y vary with X.\nWe may not know the true underlying distribution of Y\nBut the CLT says we can often approximate the distribution of a typical value of Y conditional on X \\((E[Y|X])\\) using a normal (or related) distributions.\nKnowing these distribution, in turn allows us to conduct statistical inference"
  },
  {
    "objectID": "slides/09-slides.html#section-4",
    "href": "slides/09-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Simulating the CLT\n\nConcept Messy Code Samp Distrib Normal Approx\n\n\n\nThe following code simulates the process of:\n\ntaking repeated (\\(N_{sim} = 2000\\))samples of varying sizes (\\(N_{samp}= 10, 100, 100\\))\nfrom two very not Normal populations (Poison(\\(\\lambda = 5\\)), Weird mixture of distributions)\nCalculating the means from each sample\nPlotting the sampling distributions of sample means\nApproximating the distribution of sample means with Normal distributions\n\n\n\n\n\n\n\nTip\n\n\nEven if random variable’s distribution is not at all Normal, the distribution of sample means often can be reasonably approximated by Normal Distributions\n\n\n\n\n\n\n\n# Define Population\n\nN &lt;- 10000\nset.seed(123)\npop_df &lt;- tibble(\n  Poisson = rpois(N, 5),\n  # Binomial = rbinom(size=20, n=N, prob = .25),\n  type = sample(0:2,N,replace =T,prob=c(.4,.2,.4)),\n  Weird = case_when(\n    type == 0 ~ rbeta(N,5,2)*2,\n    type == 1 ~ (rexp(N,4)-6.5)*-1,\n    type == 2 ~ rnorm(N,8,2)\n    \n  )\n  ) %&gt;% select(Poisson, Weird)\n\nfig_pop_dist &lt;- pop_df %&gt;% \n  pivot_longer(\n    col = everything(),\n    names_to = \"Distribution\"\n  ) %&gt;% \n  ggplot(aes(value,fill=Distribution,group=Distribution))+\n  geom_histogram()+\n  xlim(0,16)+\n  facet_grid(~Distribution,scales = \"free_x\")+\n  stat_summary(aes(x=0, y=value),fun.data =\\(x) data.frame(xintercept = mean(x)), geom=\"vline\")\n\nsample_sizes &lt;- c(10,100,1000)\n\ncalculate_sample_mean &lt;- function(n,pop){\n  df &lt;- tibble(\n    size = n,\n    `Sample Mean` = mean(sample(pop,n,replace = F))\n  )\n  return(df)\n}\n\nsimulate_clt_fn &lt;- function(nsims = 100, the_pop,the_n, ...){\n  sim &lt;- 1:nsims %&gt;% purrr::map_df(\\(x)calculate_sample_mean(pop=the_pop, n=the_n))\n  return(sim)\n}\n\n\n\n# binomial_clt &lt;- sample_sizes %&gt;% \n#   purrr::map_df( \\(x)  simulate_clt_fn(nsims= 2000,the_pop = pop_df$Binomial, the_n = x)) %&gt;% \n#   mutate(\n#     id = 1:n(),\n#     Distribution = \"Binomial\"\n#   )\n\npoisson_clt &lt;- sample_sizes %&gt;% \n  purrr::map_df( \\(x)  simulate_clt_fn(nsims= 2000,the_pop = pop_df$Poisson, the_n = x)) %&gt;% \n  mutate(\n    id = 1:n(),\n    Distribution = \"Poisson\"\n  )\n\nweird_clt &lt;- sample_sizes %&gt;% \n  purrr::map_df( \\(x)  simulate_clt_fn(nsims= 2000,the_pop = pop_df$Weird, the_n = x)) %&gt;% \n  mutate(\n    id = 1:n(),\n    Distribution = \"Weird\"\n  )\n\nsample_df &lt;- poisson_clt %&gt;% bind_rows(weird_clt) %&gt;% \n  mutate(\n    `Sample Size` = factor(size)\n  )\n\n\nfig_samp_dist &lt;- sample_df %&gt;% \n  ggplot(aes(`Sample Mean`,col=`Sample Size`))+\n  geom_density()+\n  geom_rug()+\n  # theme( strip.background.y = element_blank(),\n  #     strip.text.y = element_blank())+\n  xlim(0,16)+\n  facet_grid(`Sample Size`~Distribution,scales = \"free_y\")\n\n  \nfig_clt &lt;- ggarrange(fig_pop_dist,fig_samp_dist,ncol=1)\n\n\np10_weird &lt;- sample_df %&gt;% \n  filter(Distribution == \"Weird\") %&gt;% \n  filter(size == 10) %&gt;% \n  ggplot(aes(`Sample Mean`))+\n  geom_density(aes(col=\"Sample Size =10\"))+\n  geom_rug(aes(col=\"Sample Size =10\"))+\n  stat_function(\n    fun=dnorm, args = list(mean=mean(pop_df$Weird),  sd=sd(pop_df$Weird)/sqrt(10)),\n    col=\"black\",linetype = \"dashed\"\n    )+\n  xlim(0,10)+\n  theme_minimal()+\n  guides(col=\"none\")+\n  labs(\n    title = \"Normal Approximation to Sampling Distribution\",\n    subtitle = \"Weird Distribution, N = 10\"\n  )\n\np1000_weird &lt;- sample_df %&gt;% \n  filter(Distribution == \"Weird\") %&gt;% \n  filter(size == 1000) %&gt;% \n  ggplot(aes(`Sample Mean`))+\n  geom_density(aes(col=\"Sample Size =1000\"))+\n  geom_rug(aes(col=\"Sample Size =1000\"))+\n  stat_function(\n    fun=dnorm, args = list(mean=mean(pop_df$Weird),  sd=sd(pop_df$Weird)/sqrt(1000)),\n    col=\"black\",linetype = \"dashed\"\n    )+\n  xlim(4,6)+\n  theme_minimal()+\n  guides(col=\"none\")+\n  labs(\n    title = \"Normal Approximation to Sampling Distribution\",\n    subtitle = \"Weird Distribution, N = 1000\"\n  )\n\np10_poisson &lt;- sample_df %&gt;% \n  filter(Distribution == \"Poisson\") %&gt;% \n  filter(size == 10) %&gt;% \n  ggplot(aes(`Sample Mean`))+\n  geom_density(aes(col=\"Sample Size =10\"))+\n  geom_rug(aes(col=\"Sample Size =10\"))+\n  stat_function(\n    fun=dnorm, args = list(mean=5,  sd=sd(pop_df$Poisson)/sqrt(10)),\n    col=\"black\",linetype = \"dashed\"\n    )+\n  xlim(0,10)+\n  theme_minimal()+\n  guides(col=\"none\")+\n  labs(\n    title = \"Normal Approximation to Sampling Distribution\",\n    subtitle = \"Poisson(Lambda = 5), N = 10\"\n  )\n\np1000_poisson &lt;- sample_df %&gt;% \n  filter(Distribution == \"Poisson\") %&gt;% \n  filter(size == 1000) %&gt;% \n  ggplot(aes(`Sample Mean`))+\n  geom_density(aes(col=\"Sample Size =1000\"))+\n  geom_rug(aes(col=\"Sample Size =1000\"))+\n  stat_function(\n    fun=dnorm, args = list(mean=5,  sd=sd(pop_df$Poisson)/sqrt(1000)),\n    col=\"black\",linetype = \"dashed\"\n    )+\n  xlim(4,6)+\n  theme_minimal()+\n  guides(col=\"none\")+\n  labs(\n    title = \"Normal Approximation to Sampling Distribution\",\n    subtitle = \"Poisson(Lambda = 5), N = 1000\"\n  )\n\nfig_clt_approx &lt;- ggarrange(p10_weird, p1000_weird, p10_poisson,p1000_poisson)"
  },
  {
    "objectID": "slides/09-slides.html#summary",
    "href": "slides/09-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nSo we see that our sampling distributions are centered on the truth, and as the sample size increases, the width of the distribution decreases (Law of Large Numbers)\nThe shapes of distributions of sample means can be approximated by a Normal Distribution \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\)"
  },
  {
    "objectID": "slides/09-slides.html#lab-8",
    "href": "slides/09-slides.html#lab-8",
    "title": "POLS 1600",
    "section": "Lab 8",
    "text": "Lab 8\n\nLab 8 got into the weeds on standard errors, asking you to use lm_robust() to calculate robust clustered standard errors\nA standard error is simply the standard deviation of a theoretical sampling distribution\nA sampling distribution describes the range of estimates we could have seen\nStandard errors are key to quantifying uncertainty and making claims about statistical significance"
  },
  {
    "objectID": "slides/09-slides.html#errors-and-residuals",
    "href": "slides/09-slides.html#errors-and-residuals",
    "title": "POLS 1600",
    "section": "Errors and Residuals",
    "text": "Errors and Residuals\nErrors (\\(\\epsilon\\)) represent the difference between the outcome and the true mean:\n\\[\n\\begin{aligned}\ny = X\\beta + \\epsilon\\\\\n\\epsilon = y -X\\beta\n\\end{aligned}\n\\] Residuals (\\(\\hat{\\epsilon}\\)) represent the difference between the outcome and our estimate\n\\[\n\\begin{aligned}\ny = X\\beta + \\hat{\\epsilon}\\\\\n\\hat{\\epsilon} = y -X\\hat{\\beta}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-slides.html#variance-of-regression-coefficients-depends-on-the-errors",
    "href": "slides/09-slides.html#variance-of-regression-coefficients-depends-on-the-errors",
    "title": "POLS 1600",
    "section": "Variance of Regression Coefficients depends on the errors",
    "text": "Variance of Regression Coefficients depends on the errors\n\\[\n\\begin{aligned}\n\\hat{\\beta} &= (X'X)^{-1}X'y \\\\\n&= (X'X)^{-1}X'(X\\beta + \\epsilon) \\\\\n&= \\beta + (X'X)^{-1}X'\\epsilon \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-slides.html#variance-of-regression-coefficients-depends-on-the-errors-1",
    "href": "slides/09-slides.html#variance-of-regression-coefficients-depends-on-the-errors-1",
    "title": "POLS 1600",
    "section": "Variance of Regression Coefficients depends on the errors",
    "text": "Variance of Regression Coefficients depends on the errors\nRecall that\n\\[\n\\begin{aligned}\nE[\\text{c}] &= \\text{c} \\\\\nVar[\\text{c}] &= 0\\\\\nVar[X] &= Var[X^2] - Var[X]^2\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nVar[\\hat{\\beta}] &= Var[\\beta] +Vav[(X'X)^{-1}X'\\epsilon] \\\\\n&= 0 +E[(X'X)^{-1}X'\\epsilon \\epsilon'X(X'X)^{-1}] - E[(X'X)^{-1}X'\\epsilon]E](X'X)^{-1}X'\\epsilon] \\\\\n&= E[(X'X)^{-1}X'\\epsilon \\epsilon'X(X'X)^{-1}] - 0 \\\\\n& = (X'X)^{-1}X'E[\\epsilon \\epsilon']X(X'X)^{-1} \\\\\n& = (X'X)^{-1}X'\\Sigma X(X'X)^{-1} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-slides.html#constant-error-variance",
    "href": "slides/09-slides.html#constant-error-variance",
    "title": "POLS 1600",
    "section": "Constant Error Variance",
    "text": "Constant Error Variance\nSome motivations for OLS regression assume that the errors are independent and identically distributed\n\\[\n\\begin{aligned}\nVar(\\epsilon|X) = E[\\epsilon\\epsilon'] = \\Sigma &=\n\\begin{bmatrix}\n\\sigma^2 & 0 & 0 & \\cdots & 0 \\\\\n0 &\\sigma^2  & 0 & \\cdots &0 \\\\\n0 & 0 &\\sigma^2 & \\cdots &0 \\\\\n\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & \\sigma^2 \\\\\n\\end{bmatrix} = \\sigma^2\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 &1  & 0 & \\cdots &0 \\\\\n0 & 0 &1 & \\cdots &0 \\\\\n\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1 \\\\\n\\end{bmatrix} = \\sigma^2\\text{I}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-slides.html#constant-error-variance-1",
    "href": "slides/09-slides.html#constant-error-variance-1",
    "title": "POLS 1600",
    "section": "Constant Error Variance",
    "text": "Constant Error Variance\nIn which case, \\(Var[\\hat{\\beta}]\\) reduces to:\n\\[\nVar[\\hat{\\beta}]= (X'X)^{-1}X'\\Sigma X(X'X)^{-1} = \\sigma^2(X'X)^{-1}\n\\] And we can estimate, \\(\\sigma^2\\) with the residuals from the model\n\\[\n\\hat{\\sigma}^2 = \\frac{\\hat{\\epsilon}'\\hat{\\epsilon}}{n-k}\n\\]"
  },
  {
    "objectID": "slides/09-slides.html#non-constant-error-variance",
    "href": "slides/09-slides.html#non-constant-error-variance",
    "title": "POLS 1600",
    "section": "Non-Constant Error Variance",
    "text": "Non-Constant Error Variance\nConstant error variance or (homoskedasticity) is often an unrealistic assumption\nIf there is non-constant error variance (heteroskedasticity) then:\n\\[\n\\begin{aligned}\nVar(\\epsilon|X) = E[\\epsilon\\epsilon'] = \\Sigma &=\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & 0 & \\cdots & 0 \\\\\n0 &\\sigma_2^2  & 0 & \\cdots &0 \\\\\n0 & 0 &\\sigma_3^2 & \\cdots &0 \\\\\n\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & \\sigma_n^2 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-slides.html#conseqeunces-of-non-constant-error-variance",
    "href": "slides/09-slides.html#conseqeunces-of-non-constant-error-variance",
    "title": "POLS 1600",
    "section": "Conseqeunces of Non-Constant Error Variance",
    "text": "Conseqeunces of Non-Constant Error Variance\n\n\\(\\sigma^2(X'X)^{-1}\\) is no longer an unbiased estimator for \\(Var[\\hat{\\beta}]\\)\nOur statistical tests using \\(\\sigma^2(X'X)^{-1}\\) to calculate standard errors will not live up to their promised error rates and coverage probabilities (more to come)\n\\(\\hat{\\beta}\\), however are still unbiased estimates of \\(\\beta\\)"
  },
  {
    "objectID": "slides/09-slides.html#constant-error-variance-2",
    "href": "slides/09-slides.html#constant-error-variance-2",
    "title": "POLS 1600",
    "section": "Constant Error Variance",
    "text": "Constant Error Variance"
  },
  {
    "objectID": "slides/09-slides.html#robust-standard-errors",
    "href": "slides/09-slides.html#robust-standard-errors",
    "title": "POLS 1600",
    "section": "Robust Standard Errors",
    "text": "Robust Standard Errors\nRobust standard errors attempt to estimat \\(\\sigma_i^2\\) using the residuals from the model \\(\\hat{\\epsilon_i}\\) and additional adjustments to yield robust standard errors that are consistent, even when there is heteroskedasiticity.\n\\[\nVar[\\hat{\\beta}]= (X'X)^{-1}X'\n\\begin{bmatrix}\n\\hat{\\epsilon}_1^2 & 0 & 0 & \\cdots & 0 \\\\\n0 &\\hat{\\epsilon}_2^2  & 0 & \\cdots &0 \\\\\n0 & 0 &\\hat{\\epsilon}_3^2 & \\cdots &0 \\\\\n\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & \\hat{\\epsilon}_n^2 \\\\\n\\end{bmatrix}\nX(X'X)^{-1} =\n\\]\nClustered standard errors go a step further, summing up the residuals within clusters (groups) in the data."
  },
  {
    "objectID": "slides/09-slides.html#references",
    "href": "slides/09-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/12-slides.html#general-plan",
    "href": "slides/12-slides.html#general-plan",
    "title": "Week 12:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nFeedback\nReview\n\nStatistical Inference\nCausal Inference\nLinear Models\nConfidence intervals and Hypothesis tests\n\nPresentations\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/12-slides.html#new-packages",
    "href": "slides/12-slides.html#new-packages",
    "title": "Week 12:",
    "section": "New packages",
    "text": "New packages\nFirst we’ll install some packages that you will need for your presentations\n\n# Uncomment and run:\n# install.packages(remotes)\n# remotes::install_github('yihui/xaringan')\n# devtools::install_github(\"gadenbuie/xaringanExtra\")\n# install.packages(\"xaringanthemer\")"
  },
  {
    "objectID": "slides/12-slides.html#packages-for-today",
    "href": "slides/12-slides.html#packages-for-today",
    "title": "Week 12:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/12-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/12-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 12:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/12-slides.html#load-packages-for-today",
    "href": "slides/12-slides.html#load-packages-for-today",
    "title": "Week 12:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce scatterplot3d       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE \n\n\nclass:inverse, middle, center # 📢 ## Feedback"
  },
  {
    "objectID": "slides/12-slides.html#feedback-on-drafts",
    "href": "slides/12-slides.html#feedback-on-drafts",
    "title": "Week 12:",
    "section": "Feedback on Drafts",
    "text": "Feedback on Drafts\n\nPosted before class on Thursday.\nIf you haven’t submitted a file on Canvas, do it by COB today.\nFor Thursday’s class:\n\nToday and tomorrow, work on translating your draft into a slide presentation\nCome to class with a set of tasks to work on for your presentation.\n\nSunday, May 1st, upload final presentations to Canvas\nSunday, May 8, upload final papers to Canvas\n\nYou can have a 1-week extension, just email, no questions asked, but must submit by May 15.\n\n\nclass:inverse, middle, center # 🔍 ## Review"
  },
  {
    "objectID": "slides/12-slides.html#what-i-hope-youve-learned",
    "href": "slides/12-slides.html#what-i-hope-youve-learned",
    "title": "Week 12:",
    "section": "What I hope you’ve learned:",
    "text": "What I hope you’ve learned:\nCore Concepts:\n\nStatistical Inference\nCausal Inference\nLinear Models\nConfidence intervals and Hypothesis tests\n\nKey Skills:\n\nHow to load, transform, summarize, and visualize data\nHow to estimate, evaluate, present, and interpret linear models\n\nclass:inverse, middle, center # 🔍 # Core Concepts"
  },
  {
    "objectID": "slides/12-slides.html#statistical-inference",
    "href": "slides/12-slides.html#statistical-inference",
    "title": "Week 12:",
    "section": "Statistical Inference:",
    "text": "Statistical Inference:\n\nStatistical inference involves quantifying uncertainty about what could have happened\nWe describe uncertainty about what could have happened with distributions\nWe can generate these distributions via\n\nsimulation (Bootstrapping and permutations)\nanalytic theory (Limit Theorems)\n\nWe quantify uncertainty using confidence intervals and hypothesis tests"
  },
  {
    "objectID": "slides/12-slides.html#causal-inference",
    "href": "slides/12-slides.html#causal-inference",
    "title": "Week 12:",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal inference involves making counterfactual claims about what would have happened had some causal factor \\((Z)\\) been present or absent.\nThe fundamental problem of causal inference is that for an individual observation, we only observe one of many potential outcomes \\((Y(Z))\\).\nThe statistical solution to this problem moves from individual causal effects (\\(\\tau_i = Y_i(1) - Y_i(0)\\)) to average causal effects \\((\\tau = ATE = E[Y(1)]-E[Y(0)])\\)\nExperimental designs identify the ATE by randomly assigning treatment \\(\\to\\) \\(Y(1), Y(0), X, U, \\perp Z\\)\nObservational designs approximate the experimental ideal based on identifying assumptions that claim conditional independence \\(\\to\\) \\(Y(1), Y(0), X, U, \\perp Z |X\\).\n\nDifference in Difference \\(\\to\\) Parallel Trends\nRegression Discontinuity \\(\\to\\) Continuity at the cut off\nInstrumental Variables \\(\\to\\) The exclusion restriction\nRegression \\(\\to\\) Selection on observable"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression",
    "href": "slides/12-slides.html#linear-regression",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear regression provides a linear estimate of the Conditional Expectation Function\n\nBivariate: \\(E[Y|x] = \\beta_0 + \\beta_1 x +\\epsilon\\)\nMultiple regression: \\(E[Y|X] = X\\beta +\\epsilon = \\beta_0 + \\beta_1 x_1 \\dots \\beta_k x_k +\\epsilon\\)\n\nOrdinary Least Linear regression finds coefficients \\(\\beta\\) by minimizing the sum of squared errors \\((\\sum \\epsilon^2)\\)\nLinear regressions partition variance in the outcome into variance explained by the model \\((X\\beta)\\) and variance not explained by the model (\\(\\epsilon\\)).\n\nA model’s \\(R^2\\) describes the proportion of the overall variance in outcome explained by the predictors"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression-1",
    "href": "slides/12-slides.html#linear-regression-1",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nThe coefficient on a predictor describes how the outcome is expected to change with a 1-unit change in that predictor\nControlling for multiple variables isolates the variation in the outcome explained by one predictor by removing (controlling for) the variation in the outcome and that predictor explained by the other predictors.\n\nWe control for covariates that are common causes of both our key predictor and our outcome to address omitted variable bias (spurious correlation)\nWe avoid controlling for covariates that our common consequences of our outcome and predictor (collider bias)"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression-2",
    "href": "slides/12-slides.html#linear-regression-2",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nWe present linear regression using regression tables where:\n\nEach column corresponds to a model\nEach row corresponds to a coefficient in the model (with standard errors in parentheses and asterisks denoting p&lt;0.05)\n\nWe use generalized linear models to help us incorporate information about our outcomes to improve our models’ predictions\n\nLogistic regression is commonly used to model binary outcomes\nPoisson regression is commonly used to model counts\n\nPlots of predicted values can help us interpret more complicated regression models such as:\n\npolynomial regressions where the marginal effect of one predictor varies non-linearly\ninteraction models, where the marginal effect of one predictor varies with the value of another predictor\ngeneralized linear models"
  },
  {
    "objectID": "slides/12-slides.html#confidence-intervals",
    "href": "slides/12-slides.html#confidence-intervals",
    "title": "Week 12:",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nA confidence interval describes a range of plausible values for the true (population) value of our estimate\nOur confidence is about the interval:\n\n\\((1-\\alpha)\\times 100%\\) of the intervals we could construct in repeated sampling are expected to contain the true (population) value of the thing we’re estimating\n\nTo construct a confidence interval we need:\n\nAn estimate \\((\\hat{\\theta})\\)\nA standard error \\((\\hat{\\sigma_{\\hat{\\theta}}})\\) (the standard deviation of sampling distribution)\nA critical value derived from the hypothetical sampling \\((z_{\\alpha/2})\\)\n\nWith these three components the \\((1-\\alpha)\\times 100%\\) is \\(\\hat{\\theta}\\pm z_{\\alpha/2} \\times \\hat{\\sigma_{\\hat{\\theta}}}\\)\nWe report confidence intervals in text: \\(\\beta = 0.9\\) \\([0.7, 0.11]\\)\nWe interpret estimates as being statistically significant, if 0 is outside the confidence interval"
  },
  {
    "objectID": "slides/12-slides.html#hypothesis-tests",
    "href": "slides/12-slides.html#hypothesis-tests",
    "title": "Week 12:",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\nA hypothesis test quantifies how likely it is that we would observe what we did (our test statistic), if some claim about the world were true (our hypothesis).\nTypically, we test a null hypothesis that expresses our belief that their is no relationship between variables.\n\n\\(\\tau = E[Y|Z=1] - E[Y|Z=0] = 0 \\to\\) No average treatment effect\n\\(\\beta = 0 \\to\\) No relationship between predictor and outcome\n\nIf our claim were true, then under the null, our test statistic would have a distribution centered around the truth.\nWe can describe this distribution via:\n\nsimulation (e.g. permuting the outcome)\nanalytic theory (CLT)\n\nWe quantify our uncertainty using a p-value which describes the probability of observing a test statistic as extreme or more extreme in a world where our null hypothesis was true\n\nIf our p-value is small (p &lt; 0.05), we reject the null hypothesis\nIf our p-value is large (p &gt; 0.05), we fail to reject the null, or retain the null hypothesis\n\n\nclass:inverse, middle, center # 🔍 # Key Skills"
  },
  {
    "objectID": "slides/12-slides.html#how-to-load-explore-and-transform-data",
    "href": "slides/12-slides.html#how-to-load-explore-and-transform-data",
    "title": "Week 12:",
    "section": "How to load, explore and transform data",
    "text": "How to load, explore and transform data\n\n# Load data\nload(\"df.rda\")\ndf &lt;- readr::read_csv(\"df.rda\")\nlibrary(tidyverse)\n## Explore data\nhead(df)\ntable(df$y)\n\n# Transfrom data:\ndf %&gt;%\n  mutate(\n    dv = ifelse(var &lt; 0, NA, y),\n    iv = case_when(\n      x == 1 ~ \"Low\",\n      x == 2 ~ \"Medium\"\n      x == 3 ~ \"High\",\n      T ~ NA_character_\n    ),\n    covar_std = scale(z)\n  ) -&gt; df"
  },
  {
    "objectID": "slides/12-slides.html#how-to-summarize-data",
    "href": "slides/12-slides.html#how-to-summarize-data",
    "title": "Week 12:",
    "section": "How to summarize data",
    "text": "How to summarize data\n\nsummary(df$dv)\n\ndf %&gt;%\n  group_by(iv) %&gt;%\n  summarize(\n    min = min(dv,na.rm = T),\n    median = median(dv,na.rm = T),\n    mean = median(dv,na.rm = T),\n  )"
  },
  {
    "objectID": "slides/12-slides.html#how-to-visualize-data",
    "href": "slides/12-slides.html#how-to-visualize-data",
    "title": "Week 12:",
    "section": "How to visualize data",
    "text": "How to visualize data\n\n# data\ndf %&gt;%\n  # aesthetics\n  ggplot(aes(x = iv, y = dv))+\n  # geometries\n  geom_point() -&gt; figure1"
  },
  {
    "objectID": "slides/12-slides.html#how-to-estimate-and-evaluate",
    "href": "slides/12-slides.html#how-to-estimate-and-evaluate",
    "title": "Week 12:",
    "section": "How to estimate and evaluate",
    "text": "How to estimate and evaluate\n\n# Estimate models\nm1 &lt;- lm(dv ~ iv, df)\nm2 &lt;- lm(dv ~ iv + covar_std, df)\nm3 &lt;- glm(dv ~ iv + covar_std, df, family = binomial)\n\n# evaluate models\nsummary(m1)\nconfint(m2)"
  },
  {
    "objectID": "slides/12-slides.html#how-to-present-and-interpret-linear-models",
    "href": "slides/12-slides.html#how-to-present-and-interpret-linear-models",
    "title": "Week 12:",
    "section": "How to present, and interpret linear models",
    "text": "How to present, and interpret linear models\n\n# Regression Table\ntexreg::htmlreg(list(m1, m2, m3))\n\n\n# Produce Predicted values\n\npred_df &lt;- expand_grid(\n  iv = c(\"Low\",\"Medium\",\"High\"),\n  covar_std = 0\n)\n\npred_df_m2 &lt;- cbind(pred_df, predict(m2, newdata = pred_df), interval = \"confidence\")\n\n# Plot Predicted values\npred_df_m2 %&gt;%\n  ggplot(aes(iv, fit))+\n  geom_pointrange(aes(ymin = lwr, ymax = upr))\n\nclass: inverse, center, middle # 💡 # Final Presentations"
  },
  {
    "objectID": "slides/12-slides.html#final-presentations",
    "href": "slides/12-slides.html#final-presentations",
    "title": "Week 12:",
    "section": "Final Presentations",
    "text": "Final Presentations\n\nNext Tuesday your groups will present some of the findings from your projects\n\n10 Minutes per group\n8-12 slides (15 max)\n2 Minute Q&A\n\nOn Thursday, we will work through the templates you’ve been provided\nDon’t have to present the finished product"
  },
  {
    "objectID": "slides/12-slides.html#final-presentation-structure",
    "href": "slides/12-slides.html#final-presentation-structure",
    "title": "Week 12:",
    "section": "Final Presentation Structure",
    "text": "Final Presentation Structure\n\nMotivation\nResearch Question\nTheory\nExpectations\nData\n\n\nSummary\nDescriptive Table and/or Figure (Optional)\n\n\nDesign\nResults\n\n\nSummary\nTable (Optional)\nFigure (Optional)\n\n\nConclusion\n\n\nAppendices (Extra Slides Optional)"
  },
  {
    "objectID": "slides/12-slides.html#template",
    "href": "slides/12-slides.html#template",
    "title": "Week 12:",
    "section": "Template",
    "text": "Template\nLet’s open up the template and explore.\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "More about this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "class/07-class.html",
    "href": "class/07-class.html",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4\n Grumbach and Hill (2022) download",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#readings",
    "href": "class/07-class.html#readings",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4\n Grumbach and Hill (2022) download",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#lecture",
    "href": "class/07-class.html#lecture",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#lab",
    "href": "class/07-class.html#lab",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s lab here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#assignments",
    "href": "class/07-class.html#assignments",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "Assignments",
    "text": "Assignments\n\n Work on Assignment 2: Data\n No tutorial",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/00-class.html",
    "href": "class/00-class.html",
    "title": "Introductions",
    "section": "",
    "text": "Welcome! I’ll see you in class on Thursday.\nTune in via Zoom: https://brown.zoom.us/j/97039852954",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#readings",
    "href": "class/00-class.html#readings",
    "title": "Introductions",
    "section": "Readings",
    "text": "Readings\n\nNone this week",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#lecture",
    "href": "class/00-class.html#lecture",
    "title": "Introductions",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#lab",
    "href": "class/00-class.html#lab",
    "title": "Introductions",
    "section": "Lab",
    "text": "Lab\n\nNone",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#assignments",
    "href": "class/00-class.html#assignments",
    "title": "Introductions",
    "section": "Assignments",
    "text": "Assignments\n\nWork through Software Setup before class next week",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/06-class.html",
    "href": "class/06-class.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#readings",
    "href": "class/06-class.html#readings",
    "title": "Multiple Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#lecture",
    "href": "class/06-class.html#lecture",
    "title": "Multiple Regression",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#lab",
    "href": "class/06-class.html#lab",
    "title": "Multiple Regression",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#assignments",
    "href": "class/06-class.html#assignments",
    "title": "Multiple Regression",
    "section": "Assignments",
    "text": "Assignments\n\n Complete Assignment 1: Research Questions\n Complete QSS Tutorial 6: Prediction 2\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial( package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"06-prediction2 \", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/03-class.html",
    "href": "class/03-class.html",
    "title": "Causation I",
    "section": "",
    "text": "Imai (2022) Chapter 2\n Broockman and Kalla (2016) pdf",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#readings",
    "href": "class/03-class.html#readings",
    "title": "Causation I",
    "section": "",
    "text": "Imai (2022) Chapter 2\n Broockman and Kalla (2016) pdf",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#slides",
    "href": "class/03-class.html#slides",
    "title": "Causation I",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#lecture",
    "href": "class/03-class.html#lecture",
    "title": "Causation I",
    "section": "Lecture",
    "text": "Lecture",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#lab",
    "href": "class/03-class.html#lab",
    "title": "Causation I",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#assignments",
    "href": "class/03-class.html#assignments",
    "title": "Causation I",
    "section": "Assignments",
    "text": "Assignments\n\n Complete QSS Tutorial 3: Causality 1\n\n\nlearnr::run_tutorial(\"03-causality1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/11-class.html",
    "href": "class/11-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#readings",
    "href": "class/11-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#lecture",
    "href": "class/11-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#lab",
    "href": "class/11-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#assignments",
    "href": "class/11-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/02-class.html",
    "href": "class/02-class.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Imai (2022) Chapter 3\n R 4 Data Science Chapter 3 (Optional but very helpful)",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#readings",
    "href": "class/02-class.html#readings",
    "title": "Data Visualization",
    "section": "",
    "text": "Imai (2022) Chapter 3\n R 4 Data Science Chapter 3 (Optional but very helpful)",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#slides",
    "href": "class/02-class.html#slides",
    "title": "Data Visualization",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#lecture",
    "href": "class/02-class.html#lecture",
    "title": "Data Visualization",
    "section": "Lecture",
    "text": "Lecture",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#lab",
    "href": "class/02-class.html#lab",
    "title": "Data Visualization",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#assignments",
    "href": "class/02-class.html#assignments",
    "title": "Data Visualization",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 2: Measurement II\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"02-measurement2\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/04-class.html",
    "href": "class/04-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#readings",
    "href": "class/04-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#lecture",
    "href": "class/04-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#lab",
    "href": "class/04-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#assignments",
    "href": "class/04-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "files/img/hexlogo.html",
    "href": "files/img/hexlogo.html",
    "title": "Hex Logo for POLS 1600",
    "section": "",
    "text": "Linking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  }
]