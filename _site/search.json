[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This class is an introduction to applied statistics as practiced in political science. It is computing intensive, and, as such, will enable students to execute basic quantitative analyses of social science data using the linear model with statistical inference arising from re-sampling and permutation based techniques as applied in the R statistical computing language with RStudio. By the end of the course, a successful student will be able to find social science data online, download it, analyze it, and write about how the analyses bear on focused social science or policy questions.\n\n\n\nMore than anything I assume a willingness to engage with mathematics, data analysis, computer programming, and the practice of social science thinking and writing. I also assume you’ve taken at least one class in algebra at the level taught in most high schools in the United States and have used a personal computer to read and type email and other documents and have some experience with the Internet.\nI also assume that you will read the syllabus and that you keep up to date on changes in the syllabus which will be announced in class. You should not expect a response to emails that ask a question already answered in the syllabus.\nThis is an experimental class so you should expect that the syllabus will change throughout the term. Make sure you have the syllabus with the latest date stamp. I will announce syllabus changes via the emails sent from Canvas.\n\n\n\nNeither the University nor I tolerate cheating or plagiarism. The Brown Writing Center defines plagiarism as ``appropriating another person’s ideas or words (spoken or written) without attributing those word or ideas to their true source.’’ The consequences for plagiarism are often severe, and can include suspension or expulsion. This course will follow the guidelines in the Academic Code for determining what is and isn’t plagiarism:\n\nIn preparing assignments a student often needs or is required to employ outside sources of information or opinion. All such sources should be listed in the bibliography. Citations and footnote references are required for all specific facts that are not common knowledge and about which there is not general agreement. New discoveries or debatable opinions must be credited to the source, with specific references to edition and page even when the student restates the matter in his or her own words. Word-for-word inclusion of any part of someone else’s written or oral sentence, even if only a phrase or sentence, requires citation in quotation marks and use of the appropriate conventions for attribution. Citations should normally include author, title, edition, and page. (Quotations longer than one sentence are generally indented from the text of the essay, without quotation marks, and identified by author, title, edition, and page.) Paraphrasing or summarizing the contents of another’s work is not dishonest if the source or sources are clearly identified (author, title, edition, and page), but such paraphrasing does not constitute independent work and may be rejected by the instructor. Students who have questions about accurate and proper citation methods are expected to consult reference guides as well as course instructors.\n\nWe will discuss specific information about your written work in class in more detail, but if you are unsure of how to properly cite material, please ask for clarification. If you are having difficulty with writing or would like more information or assistance, consult the Writing Center, the Brown library and/or the Academic Code for more information.\n\n\n\nAll students and the instructor must be respectful of others in the classroom. If you ever feel that the classroom environment is discouraging your participation or problematic in any way, please contact me.\n\n\n\nBrown University is committed to full inclusion of all students. Please inform me if you have a disability or other condition that might require accommodations or modification of any of these course procedures. You may speak with me after class or during office hours. For more information contact Student and Employee Accessibility Services at 401-863-9588 or SEAS@brown.edu.\n\n\n\nAny student with a documented disability is welcome to contact me as early in the semester as possible so that we may arrange reasonable accommodations. As part of this process, please be in touch with Student Accessibility Services by calling 401-863-9588 or online\n\n\n\nThis course is designed to support an inclusive learning environment where diverse perspectives are recognized, respected and seen as a source of strength. It is my intent to provide materials and activities that are respectful of various levels of diversity: mathematical background, previous computing skills, gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Toward that goal:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official Brown records, please let me know!\nIf there are things going on inside or outside of class that are affecting your performance in class, please don’t hesitate to talk to me, provide anonymous feedback through our course survey, or contact one of Brown’s Academic Deans."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "This class is an introduction to applied statistics as practiced in political science. It is computing intensive, and, as such, will enable students to execute basic quantitative analyses of social science data using the linear model with statistical inference arising from re-sampling and permutation based techniques as applied in the R statistical computing language with RStudio. By the end of the course, a successful student will be able to find social science data online, download it, analyze it, and write about how the analyses bear on focused social science or policy questions."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "",
    "text": "More than anything I assume a willingness to engage with mathematics, data analysis, computer programming, and the practice of social science thinking and writing. I also assume you’ve taken at least one class in algebra at the level taught in most high schools in the United States and have used a personal computer to read and type email and other documents and have some experience with the Internet.\nI also assume that you will read the syllabus and that you keep up to date on changes in the syllabus which will be announced in class. You should not expect a response to emails that ask a question already answered in the syllabus.\nThis is an experimental class so you should expect that the syllabus will change throughout the term. Make sure you have the syllabus with the latest date stamp. I will announce syllabus changes via the emails sent from Canvas."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "",
    "text": "Neither the University nor I tolerate cheating or plagiarism. The Brown Writing Center defines plagiarism as ``appropriating another person’s ideas or words (spoken or written) without attributing those word or ideas to their true source.’’ The consequences for plagiarism are often severe, and can include suspension or expulsion. This course will follow the guidelines in the Academic Code for determining what is and isn’t plagiarism:\n\nIn preparing assignments a student often needs or is required to employ outside sources of information or opinion. All such sources should be listed in the bibliography. Citations and footnote references are required for all specific facts that are not common knowledge and about which there is not general agreement. New discoveries or debatable opinions must be credited to the source, with specific references to edition and page even when the student restates the matter in his or her own words. Word-for-word inclusion of any part of someone else’s written or oral sentence, even if only a phrase or sentence, requires citation in quotation marks and use of the appropriate conventions for attribution. Citations should normally include author, title, edition, and page. (Quotations longer than one sentence are generally indented from the text of the essay, without quotation marks, and identified by author, title, edition, and page.) Paraphrasing or summarizing the contents of another’s work is not dishonest if the source or sources are clearly identified (author, title, edition, and page), but such paraphrasing does not constitute independent work and may be rejected by the instructor. Students who have questions about accurate and proper citation methods are expected to consult reference guides as well as course instructors.\n\nWe will discuss specific information about your written work in class in more detail, but if you are unsure of how to properly cite material, please ask for clarification. If you are having difficulty with writing or would like more information or assistance, consult the Writing Center, the Brown library and/or the Academic Code for more information."
  },
  {
    "objectID": "syllabus.html#community-standards",
    "href": "syllabus.html#community-standards",
    "title": "Syllabus",
    "section": "",
    "text": "All students and the instructor must be respectful of others in the classroom. If you ever feel that the classroom environment is discouraging your participation or problematic in any way, please contact me."
  },
  {
    "objectID": "syllabus.html#accessibility",
    "href": "syllabus.html#accessibility",
    "title": "Syllabus",
    "section": "",
    "text": "Brown University is committed to full inclusion of all students. Please inform me if you have a disability or other condition that might require accommodations or modification of any of these course procedures. You may speak with me after class or during office hours. For more information contact Student and Employee Accessibility Services at 401-863-9588 or SEAS@brown.edu."
  },
  {
    "objectID": "syllabus.html#academic-accommodations",
    "href": "syllabus.html#academic-accommodations",
    "title": "Syllabus",
    "section": "",
    "text": "Any student with a documented disability is welcome to contact me as early in the semester as possible so that we may arrange reasonable accommodations. As part of this process, please be in touch with Student Accessibility Services by calling 401-863-9588 or online"
  },
  {
    "objectID": "syllabus.html#diversity-and-inclusion",
    "href": "syllabus.html#diversity-and-inclusion",
    "title": "Syllabus",
    "section": "",
    "text": "This course is designed to support an inclusive learning environment where diverse perspectives are recognized, respected and seen as a source of strength. It is my intent to provide materials and activities that are respectful of various levels of diversity: mathematical background, previous computing skills, gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Toward that goal:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official Brown records, please let me know!\nIf there are things going on inside or outside of class that are affecting your performance in class, please don’t hesitate to talk to me, provide anonymous feedback through our course survey, or contact one of Brown’s Academic Deans."
  },
  {
    "objectID": "syllabus.html#requirements",
    "href": "syllabus.html#requirements",
    "title": "Syllabus",
    "section": "Requirements",
    "text": "Requirements\nTo accomplish this metamorphosis, we’ll need the following:\n\nSome math\nSome programming and computing skills\nSome general life skills"
  },
  {
    "objectID": "syllabus.html#math",
    "href": "syllabus.html#math",
    "title": "Syllabus",
    "section": "Math",
    "text": "Math\nYou either already know, or will learn, all the math you need to take this course.1 We’ll go over some key theorems of probability and statistics in class, emphasizing conceptual understanding (often illustrated via simulation) over formal proofs.2. Along the way, we’ll need some calculus and linear algebra to make our lives easier, and so we’ll briefly review this material together in class."
  },
  {
    "objectID": "syllabus.html#computing",
    "href": "syllabus.html#computing",
    "title": "Syllabus",
    "section": "Computing",
    "text": "Computing\nDoing quantitative, empirical social science research requires working with data. Today, working with data requires a computer and statistical software. I assume that you have, or will acquire, a laptop that you will bring to class. In terms of software, there are many possible options. In this class,R.3.\nAll the slides, notes, and assignments in this class are produced using R Markdown, a free, open-source tool for creating reproducible research. It’s a short but steep learning curve, the benefits of which (pretty documents, nicely formatted tables and figures, easy integration with citation managers) far outweigh the costs (finicky syntax)"
  },
  {
    "objectID": "syllabus.html#general",
    "href": "syllabus.html#general",
    "title": "Syllabus",
    "section": "General",
    "text": "General\nLike any course, success in this class requires preparation, participation and perseverance. In terms of preparation, I expect that you will have done the readings and submitted your assignments on time (more on that below). In short, you’ll get out of this class what you put in. In terms of participation, I expect that you will come to class eager to learn and engage with that week’s topics. If you have a question, ask it. If you’re getting an error, share it. In some ways, your job is to make errors. To paraphrase Joyce: people of genius make no mistakes. Our errors are volitional and portals of discovery. While this experience can be challenging and frustrating, it is also incredibly rewarding. I fully expect you persevere through the problems and difficulties that inevitably arise in this course, and will do everything I can to help in this process."
  },
  {
    "objectID": "syllabus.html#class",
    "href": "syllabus.html#class",
    "title": "Syllabus",
    "section": "Class",
    "text": "Class\nThis course meets two times a week for 80 minutes on Tuesdays and Thursdays. Tuesday’s class will be devoted to lecture, demonstration and review. Recorded versions of these lectures will be provided on Canvas after class. Thursday’s class will focus on applications of these concepts through brief labs where you’ll work with real data from a variety of sources. I assume that you will come to class having done each week’s assigned readings and reviewed material from the previous week’s lectures and labs. Slides and labs are available on Canvas and https://pols1600.paultesta.org"
  },
  {
    "objectID": "syllabus.html#attendance",
    "href": "syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nYou may miss two classes without it having any effect on the attendance portion of your grade. After two absences, each additional absence (without a written note from the University) will reduce your final grade by 1 percent."
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Syllabus",
    "section": "Readings",
    "text": "Readings\nImai (2022) required textbooks for the course (Estimated cost: \\(\\sim\\)$38.50 for the ebook, $55.00 for the paperback):\n\nImai, Kosuke. 2022. Quantitative Social Science An Introduction in Tidyverse. Princeton University Press.\n\nMost chapters are spread over multiple weeks. You should read this text with your laptop and R Studio open. Execute the code in the main text and ideally try to complete the assignments and exercises at the end of the chapter.4\nAdditional readings will be listed below and available to download on the course website and Canvas."
  },
  {
    "objectID": "syllabus.html#labs",
    "href": "syllabus.html#labs",
    "title": "Syllabus",
    "section": "Labs",
    "text": "Labs\nThe bulk of the work and learning you’ll do in the course comes in the form of weekly labs in which you’ll explore a given data set or paper using R. You’ll be given an R Markdown document that will guide you through a set of exercises to teach concepts covered in the lectures and reading. You’ll code in R and summaries of your findings in R Markdown. You will compile your document to produce an html document which you will submit on Canvas by the end of each class.\nAll work in this class MUST BE SUBMITTED ONLINE VIA CANVAS.\nYou will work in groups on these labs. One member of your group will submit a lab. One question from the lab will be randomly selected for grading."
  },
  {
    "objectID": "syllabus.html#tutorials",
    "href": "syllabus.html#tutorials",
    "title": "Syllabus",
    "section": "Tutorials",
    "text": "Tutorials\nIn addition to weekly labs, you will also work through weekly tutorials made available to you through the `qsslearnr’ package. These tutorials provide you with an opportunity to practice your programming and review concepts from the text and lecture. After completing each tutorial, you will download your progress report and upload this file to Canvas by midnight on Friday each week a tutorial is assigned. If you upload a report by Friday, you receive a grade of 100 % on that Tutorial. If you upload a report after Friday, you receive a grade of 50 %. If you do not upload a report, you receive a grade of 0 %. There are 11 total tutorials for the course. Your lowest grade on the Tutorials will be dropped.\nThese Tutorials are for your personal benefit. You may collaborate with peers, but you must submit your own file."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nIn addition to weekly labs, you will complete periodic group assignments developing an original research presentation applying skills you have learned in this class to a topic of your choosing. All assignments are due the Friday after the class with which they are associated.\nThe timeline of assignments for your final paper is as follows:\n\nWeek 4: Research Topics\nWeek 6: Identifying Datasets\nWeek 8: Data Explorations\nWeek 11: Draft of Research Presentation\nWeek 12: Research Presentations\nWeek 13: Final Paper\n\nAssignments must be submitted on time to Canvas. No late work will be accepted without prior approval of the instructor or a note from the University."
  },
  {
    "objectID": "syllabus.html#grades",
    "href": "syllabus.html#grades",
    "title": "Syllabus",
    "section": "Grades",
    "text": "Grades\nYour final grade for this course will be calculated as follows:\n\n5 % Class attendance\n10 % Class involvement and participation\n10 % Tutorials\n30 % Labs\n20 % Assignments not including the final Paper\n25 % Final Project\n\nLabs, assignments excluding the final presentation, will be graded graded out of 100 roughly on a ✓ + (100, completed on time, acceptable), ✓ (85, completed on time, passable), ✓ - (0 not submitted on time, unacceptable). The lowest two lab grades will be dropped from your final lab grade. Tutorials are graded on pass (submitted on time = 100 % ) - fail (not submitted =0) based submitting your progress report from the tutorial by Friday each week. If you submit a Tutorial after the week it’s do, you will receive partial credit (50 %). Your final projects will be graded on 100-point scales with rubrics provided beforehand.\nIncomplete Work Assignments not turned in will be counted as zero in the calculation of the final grade.\nComputers in class Please bring your laptops if you have them. We will install R and RStudio together. If you do not own a laptop, you can still work in a group of other people who have laptops and will be able to complete the in-class worksheets without a problem. In fact, it is ideal if each group of 2-4 people works with one laptop and then shares the work among themselves. Of course, feel free to work on your own outside of class."
  },
  {
    "objectID": "syllabus.html#time",
    "href": "syllabus.html#time",
    "title": "Syllabus",
    "section": "Time",
    "text": "Time\nThis course meets 27 times over 13 weeks in the semester. Each class is 80 minutes long so you should expect to spend approximately 36 hours total in class; approximately 4 hours per week reading the textbook and reviewing material (42 hours total); approximately 22 hours on tutorials each week, approximately 30 hours on assignments for the final paper; approximately 50 hours researching, writing, and revising your final presentation; and at least .5 hours meeting with me in person to discuss your work (Estimated Total Time: 180.5 hours)"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not the same as all the math you need to know be a successful, methodologically sophisticated political scientist. But it’s a start, and one that will hopefully help you figure out what additional training you’ll need.↩︎\nWe’ll do the proofs as well, but your focus should be on making sure you understand concepts and implications rather than specific derivations↩︎\nAvailable for free at https://cran.r-project.org/. Python is also increasingly common among social scientists.↩︎\nSeriously. Working carefully through these examples will be incredibly helpful and rewarding. If you’re taking the time to read this footnote, send me picture of a cute animal and I’ll add 1 point of extra credit to your final paper grade. See, your hard work is already paying off.↩︎"
  },
  {
    "objectID": "class/09-class.html",
    "href": "class/09-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#readings",
    "href": "class/09-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#lecture",
    "href": "class/09-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#lab",
    "href": "class/09-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#assignments",
    "href": "class/09-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/10-class.html",
    "href": "class/10-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#readings",
    "href": "class/10-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#lecture",
    "href": "class/10-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#lab",
    "href": "class/10-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#assignments",
    "href": "class/10-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/08-class.html",
    "href": "class/08-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#readings",
    "href": "class/08-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#lecture",
    "href": "class/08-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#lab",
    "href": "class/08-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#assignments",
    "href": "class/08-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/05-class.html",
    "href": "class/05-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#readings",
    "href": "class/05-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#lecture",
    "href": "class/05-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#lab",
    "href": "class/05-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#assignments",
    "href": "class/05-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/index.html",
    "href": "class/index.html",
    "title": "General Workflow for POLS 1600",
    "section": "",
    "text": "Before class on Tuesday\n\nDo the assigned readings\nReview last week’s lab and slides\n\nTuesday: Come to class ready to engage with lecture and slides\nWednesday:\n\nStart the tutorials for that week\nDownload, render, and skim Thursday’s lab\n\nThursday: Come to class ready to work through that week’s lab\nFriday: Upload that week’s tutorials to Canvas",
    "crumbs": [
      "Class",
      "Overview",
      "General Workflow for POLS 1600"
    ]
  },
  {
    "objectID": "class/12-class.html",
    "href": "class/12-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#readings",
    "href": "class/12-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#lecture",
    "href": "class/12-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#lab",
    "href": "class/12-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#assignments",
    "href": "class/12-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/13-class.html",
    "href": "class/13-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#readings",
    "href": "class/13-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#lecture",
    "href": "class/13-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#lab",
    "href": "class/13-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#assignments",
    "href": "class/13-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/01-class.html",
    "href": "class/01-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#readings",
    "href": "class/01-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#lecture",
    "href": "class/01-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#lab",
    "href": "class/01-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#assignments",
    "href": "class/01-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "slides/06-slides.html#general-plan",
    "href": "slides/06-slides.html#general-plan",
    "title": "Week 06:",
    "section": "General Plan",
    "text": "General Plan\n\nGroup Assignment 2: Data\nSetup\n\nPackages\nData\n\nFeedback\nReview\n\nCausal Inference in Observational Designs\nSimple Linear Regression\n\nMultiple Regression\n\nOverview\nEstimating and Interpreting Multiple Regression\nReading Regression Tables\n\n\nclass: inverse, center, middle # Research Questions"
  },
  {
    "objectID": "slides/06-slides.html#assignment-1-research-questions",
    "href": "slides/06-slides.html#assignment-1-research-questions",
    "title": "Week 06:",
    "section": "Assignment 1 Research Questions:",
    "text": "Assignment 1 Research Questions:\n\nFeedback posted to Canvas.\nGeneral Comments:\n\nReally great questions!\nFeedback tries to balance interest/feasibility/data availability\n“Not an experiment” \\(\\to\\) What would you have to randomize to answer your question\nNot every question need to be causal\n“Make your theories elaborate” - R.A. Fisher to William Cochran\n\nBut keep your models simple - Testa to POLS 1600"
  },
  {
    "objectID": "slides/06-slides.html#assignment-2-data-explorations",
    "href": "slides/06-slides.html#assignment-2-data-explorations",
    "title": "Week 06:",
    "section": "Assignment 2: Data Explorations",
    "text": "Assignment 2: Data Explorations\n\nPrompt posted here\nUpload to Canvas next Sunday March 19\n\n\nA revised description of your group’s research project\nA description of a linear model implied by your question\nR code that loads some potentially relevant data to your question and at least one descriptive summary of that data.\nSome information about your group such as:\n\nA group name1\nA group color or color scheme\nA group motto, mascot, crest, etc.\nYour group’s theme song\nYour group’s astrological sign\nAnything else that you think well help you form strong ingroup bounds that facilitate collaboration\n\n\nclass:inverse, middle, center # 💪 ## Get set up to work\nIf you’re Group 01 don’t change your name to Group 4"
  },
  {
    "objectID": "slides/06-slides.html#packages",
    "href": "slides/06-slides.html#packages",
    "title": "Week 06:",
    "section": "Packages",
    "text": "Packages\nHopefully, you were all able to install the following packages\n\ndataverse (necessary for this week)\ntidycensus (necessary for this week)\neasystats (useful later)\nDeclareDesign (useful later)"
  },
  {
    "objectID": "slides/06-slides.html#census-api",
    "href": "slides/06-slides.html#census-api",
    "title": "Week 06:",
    "section": "Census API:",
    "text": "Census API:\nAdditionally, I hope you have all followed the steps here:\n\nInstall the tidycensus package\nLoad the installed package\nRequest an API key from the Census\nCheck your email\nActivate your key\nInstall your API key in R\nCheck that everything worked\n\nTo install the an API key so we can download data directly from the US Census"
  },
  {
    "objectID": "slides/06-slides.html#packages-for-today",
    "href": "slides/06-slides.html#packages-for-today",
    "title": "Week 06:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/06-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/06-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 06:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/06-slides.html#load-packages-for-today",
    "href": "slides/06-slides.html#load-packages-for-today",
    "title": "Week 06:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce scatterplot3d       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for Thursday’s Lab"
  },
  {
    "objectID": "slides/06-slides.html#load-the-covid-19-data",
    "href": "slides/06-slides.html#load-the-covid-19-data",
    "title": "Week 06:",
    "section": "Load the Covid-19 Data",
    "text": "Load the Covid-19 Data\n\n# covid &lt;- COVID19::covid19(\n#   country = \"US\",\n#   level = 2,\n#   verbose = F\n# )\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#filter-covid-19-data-to-us-states-now-excluding-d.c.-as-well",
    "href": "slides/06-slides.html#filter-covid-19-data-to-us-states-now-excluding-d.c.-as-well",
    "title": "Week 06:",
    "section": "Filter Covid-19 Data to US States (Now Excluding D.C. as well)",
    "text": "Filter Covid-19 Data to US States (Now Excluding D.C. as well)\n\n# Vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\",\n  \"District of Columbia\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )"
  },
  {
    "objectID": "slides/06-slides.html#mutate-calculate-new-deaths-per-capita-and-percent-vaccinated",
    "href": "slides/06-slides.html#mutate-calculate-new-deaths-per-capita-and-percent-vaccinated",
    "title": "Week 06:",
    "section": "Mutate: Calculate New Deaths Per Capita and Percent Vaccinated",
    "text": "Mutate: Calculate New Deaths Per Capita and Percent Vaccinated\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_14da = zoo::rollmean(new_deaths_pc, k = 14, align = \"right\", fill=NA ),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/06-slides.html#load-data-on-presidential-elections",
    "href": "slides/06-slides.html#load-data-on-presidential-elections",
    "title": "Week 06:",
    "section": "Load Data on Presidential Elections",
    "text": "Load Data on Presidential Elections\n\n# Try this code first\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# If the code above fails, comment out and uncomment the code below:\n\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#hlo-of-presidential-elections-data",
    "href": "slides/06-slides.html#hlo-of-presidential-elections-data",
    "title": "Week 06:",
    "section": "HLO of Presidential Elections Data",
    "text": "HLO of Presidential Elections Data\n\nhead(pres_df)\n\n# A tibble: 6 × 15\n   year state   state_po state_fips state_cen state_ic office       candidate   \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, JI…\n2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GERA…\n3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, LE…\n4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BEN…\n5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\" \n6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE, …\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;"
  },
  {
    "objectID": "slides/06-slides.html#transform-data-to-get-just-2020-election",
    "href": "slides/06-slides.html#transform-data-to-get-just-2020-election",
    "title": "Week 06:",
    "section": "Transform Data to get just 2020 Election",
    "text": "Transform Data to get just 2020 Election\n\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes *100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df"
  },
  {
    "objectID": "slides/06-slides.html#transform-data-to-get-just-2020-election-1",
    "href": "slides/06-slides.html#transform-data-to-get-just-2020-election-1",
    "title": "Week 06:",
    "section": "Transform Data to get just 2020 Election",
    "text": "Transform Data to get just 2020 Election\n\nhead(pres2020_df)\n\n# A tibble: 6 × 9\n  state      state_po year_election totalvotes DEMOCRAT REPUBLICAN dem_voteshare\n  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 Alabama    AL                2020    2323282   849624    1441170          36.6\n2 Alaska     AK                2020     359530   153778     189951          42.8\n3 Arizona    AZ                2020    3387326  1672143    1661686          49.4\n4 Arkansas   AR                2020    1219069   423932     760647          34.8\n5 California CA                2020   17500881 11110250    6006429          63.5\n6 Colorado   CO                2020    3279980  1804352    1364607          55.0\n# ℹ 2 more variables: rep_voteshare &lt;dbl&gt;, winner &lt;fct&gt;"
  },
  {
    "objectID": "slides/06-slides.html#load-data-on-median-state-income-from-the-census",
    "href": "slides/06-slides.html#load-data-on-median-state-income-from-the-census",
    "title": "Week 06:",
    "section": "Load Data on Median State Income from the Census",
    "text": "Load Data on Median State Income from the Census\n\nacs_df &lt;- get_acs(geography = \"state\", \n              variables = c(med_income = \"B19013_001\",\n                            med_age = \"B01002_001\"), \n              year = 2019)\n\n# Uncomment if get_acs() doesn't work:\n# load(url(\"https://pols1600.paultesta.org/files/data/acs_df.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#hlo-census-data",
    "href": "slides/06-slides.html#hlo-census-data",
    "title": "Week 06:",
    "section": "HLO: Census Data",
    "text": "HLO: Census Data\n\nhead(acs_df)\n\n# A tibble: 6 × 5\n  GEOID NAME    variable   estimate    moe\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 01    Alabama med_age        39      0.2\n2 01    Alabama med_income  50536    304  \n3 02    Alaska  med_age        34.3    0.1\n4 02    Alaska  med_income  77640   1015  \n5 04    Arizona med_age        37.7    0.2\n6 04    Arizona med_income  58945    266"
  },
  {
    "objectID": "slides/06-slides.html#tidy-census-data",
    "href": "slides/06-slides.html#tidy-census-data",
    "title": "Week 06:",
    "section": "Tidy Census Data",
    "text": "Tidy Census Data\n\nacs_df %&gt;%\n  mutate(\n    state = NAME,\n  ) %&gt;%\n  select(state, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable,\n              values_from = estimate) -&gt; acs_df"
  },
  {
    "objectID": "slides/06-slides.html#tidy-census-data-1",
    "href": "slides/06-slides.html#tidy-census-data-1",
    "title": "Week 06:",
    "section": "Tidy Census Data",
    "text": "Tidy Census Data\n\nhead(acs_df)\n\n# A tibble: 6 × 3\n  state      med_age med_income\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Alabama       39        50536\n2 Alaska        34.3      77640\n3 Arizona       37.7      58945\n4 Arkansas      38.1      47597\n5 California    36.5      75235\n6 Colorado      36.7      72331"
  },
  {
    "objectID": "slides/06-slides.html#merge-election-data-into-covid-data",
    "href": "slides/06-slides.html#merge-election-data-into-covid-data",
    "title": "Week 06:",
    "section": "Merge election data into Covid data",
    "text": "Merge election data into Covid data\n\n# Always check dimensions before and after merging\ndim(covid_us)\n\n[1] 52646    52\n\ndim(pres2020_df)\n\n[1] 51  9\n\n# Merge covid_us with pres2020_df and save as tmp file\ntmp &lt;- covid_us %&gt;% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(tmp) # Same number of rows as covid_us w/ 8 additional columns\n\n[1] 52646    60"
  },
  {
    "objectID": "slides/06-slides.html#merge-census-data-into-covid-data",
    "href": "slides/06-slides.html#merge-census-data-into-covid-data",
    "title": "Week 06:",
    "section": "Merge Census data into Covid data",
    "text": "Merge Census data into Covid data\n\ndim(tmp)\n\n[1] 52646    60\n\ndim(acs_df)\n\n[1] 52  3\n\n# Merge tmp with acs_df and save as final covid_df file\ncovid_df &lt;- tmp %&gt;% left_join(\n  acs_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_df)  # Same number of rows as tmp w/ 2 additional columns\n\n[1] 52646    62"
  },
  {
    "objectID": "slides/06-slides.html#subset-merged-data-to-include-only-the-variables-and-observations-we-want",
    "href": "slides/06-slides.html#subset-merged-data-to-include-only-the-variables-and-observations-we-want",
    "title": "Week 06:",
    "section": "Subset Merged data to include only the variables and observations we want",
    "text": "Subset Merged data to include only the variables and observations we want\n\nthe_vars &lt;- c(\n  # Covid variables\n  \"state\",\"state_po\",\"date\",\"new_deaths_pc_14da\", \"percent_vaccinated\",\n  # Election variables\n  \"winner\",\"rep_voteshare\",\n  # Demographic variables\n  \"med_age\",\"med_income\",\"population\")\n\n\ncovid_lab &lt;- covid_df %&gt;%\n  filter( date == \"2021-09-23\")%&gt;%\n  select(all_of(the_vars))%&gt;%\n  ungroup()\n\nlength(the_vars)\n\n[1] 10\n\ndim(covid_lab)\n\n[1] 50 10"
  },
  {
    "objectID": "slides/06-slides.html#a-preview-of-where-were-headed",
    "href": "slides/06-slides.html#a-preview-of-where-were-headed",
    "title": "Week 06:",
    "section": "A Preview of Where We’re Headed:",
    "text": "A Preview of Where We’re Headed:\nConsider the following multiple regression (which we will on Thursday):\n\\[\\text{New Covid Deaths} = \\beta_0 +\\beta_1 \\text{Rep. Vote Share} +\\beta_2 \\text{Median Age} +\\beta_3 \\text{Median Income} + \\epsilon\\] - The tell us how the outcome, New Covid Deaths, is expected to change with a unit change in a given predictor, controlling for/holding constant the other predictors in the model (more on “controlling for” shortly). - In the model above, \\(\\beta_2\\) tells us how New Covid Deaths is predicted to change, if the Median Age of a state increased by one year (i.e. a unit change). - Similarly, \\(\\beta_3\\) tells us how New Covid Deaths is predicted to change, if the Median Income of a state increased by one dollar (i.e. a unit change).\n- Since vote share, median income and age are measured on very different scales, interpreting these coefficients in the same model can be cumbersome."
  },
  {
    "objectID": "slides/06-slides.html#standardizing-variables.",
    "href": "slides/06-slides.html#standardizing-variables.",
    "title": "Week 06:",
    "section": "Standardizing variables.",
    "text": "Standardizing variables.\n\nWhen variables are measured in different units multiple regression coefficients can be hard to interpret\n\nCoefficients, \\(\\beta\\), tell us about the predicted change in \\(y\\) for a unit change in \\(x\\) or \\(z\\)\nFor example \\(\\beta_{age}\\)\n\nz-scores standardize variables so that their unit of measurement no longer matters (QSS p. 103).\nTo calculate the z-score of a variable \\(x\\), we simply, substract off the mean and divide by the standard deviaton\n\n\\[z\\text{-score of x} = \\frac{x_i - \\mu_{x}}{\\sigma_x}\\]\n\nThe z-score of Age is\n\n\\[z\\text{-score of Age} = \\frac{\\text{Age}_i - \\mu_{Age}}{\\sigma_{Age}}\\]"
  },
  {
    "objectID": "slides/06-slides.html#standardizing-predictors-in-r",
    "href": "slides/06-slides.html#standardizing-predictors-in-r",
    "title": "Week 06:",
    "section": "Standardizing Predictors in R",
    "text": "Standardizing Predictors in R\n\ncovid_lab %&gt;%\n  mutate(\n    rep_voteshare_std = (rep_voteshare - mean(rep_voteshare))/sd(rep_voteshare),\n    med_age_std = ( med_age - mean( med_age))/sd( med_age),\n    med_income_std = (med_income - mean(med_income))/sd(med_income),\n    percent_vaccinated_std = (percent_vaccinated - mean(percent_vaccinated))/sd(percent_vaccinated)\n  ) -&gt; covid_lab"
  },
  {
    "objectID": "slides/06-slides.html#multiple-regression-with-standardized-predictors",
    "href": "slides/06-slides.html#multiple-regression-with-standardized-predictors",
    "title": "Week 06:",
    "section": "Multiple Regression with Standardized Predictors",
    "text": "Multiple Regression with Standardized Predictors\nIf we were to estimate a model with standardized predictors:\n\\[\\text{New Covid Deaths} = \\beta_0 +\\beta_1 \\text{Rep. Vote Share}_{std} +\\beta_2 \\text{Median Age}_{std} +\\beta_3 \\text{Median Income}_{std} + \\epsilon\\] The coefficients still tell us predicted change in New Covid Deaths for a unit change in a predictor, but now a unit change corresponds to a 1-standard deviation increase of each predictor:\n\ncovid_lab %&gt;%\n  summarise(\n    sd_rep_vote = sd(rep_voteshare),\n    sd_med_age = sd(med_age),\n    sd_med_income = sd(med_income),\n    sd_rep_vote_std = sd(rep_voteshare_std),\n    sd_med_age_std = sd(med_age_std),\n    sd_med_income_std = sd(med_income_std)\n  )\n\n# A tibble: 1 × 6\n  sd_rep_vote sd_med_age sd_med_income sd_rep_vote_std sd_med_age_std\n        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1        10.4       2.36        10288.               1              1\n# ℹ 1 more variable: sd_med_income_std &lt;dbl&gt;"
  },
  {
    "objectID": "slides/06-slides.html#saving-data",
    "href": "slides/06-slides.html#saving-data",
    "title": "Week 06:",
    "section": "Saving Data",
    "text": "Saving Data\nFinally, I’ll save the data for Thursday’s lab\n\n# Don't run this code\nsave(covid_lab, file = \"../files/data/06_lab.rda\")\n\nAnd on Thursday, we’ll be able to load the covid_lab just by running:\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nclass:inverse, middle, center # 🔍 ## Review"
  },
  {
    "objectID": "slides/06-slides.html#review",
    "href": "slides/06-slides.html#review",
    "title": "Week 06:",
    "section": "Review",
    "text": "Review\n\nCasual Inference in Observational Designs\n\nDifference-in-Differences\nRegression Discontinuity Design\nInstrumental Variables\n\nSimple Linear Regression"
  },
  {
    "objectID": "slides/06-slides.html#review-1",
    "href": "slides/06-slides.html#review-1",
    "title": "Week 06:",
    "section": "Review",
    "text": "Review\n\nCasual Inference in Observational Designs\n\nDifference-in-Differences\nRegression Discontinuity Design\nInstrumental Variables\n\nSimple Linear Regression\n\nclass:inverse, middle, center # 🔍 ## Casual Inference in Observational Designs"
  },
  {
    "objectID": "slides/06-slides.html#review-casual-inference-in-observational-designs",
    "href": "slides/06-slides.html#review-casual-inference-in-observational-designs",
    "title": "Week 06:",
    "section": "Review: Casual Inference in Observational Designs",
    "text": "Review: Casual Inference in Observational Designs\n\nObservational designs that try to estimate causal effects need to justify assumptions about conditional independence:\n\n\\[\nY_i(1),Y_i(0), Y_i, U_i \\perp D_i |X_i\n\\]\n–\n\nThis assumption goes by many, jargony names: Selection on Observables, Conditional Independence, No unmeasured confounders.\n\n–\n\nCredibility of this assumption depends less on having a lot of data, and more on how your data were generated.\n\n–\n\n\\(Y_i(1),Y_i(0) \\perp D_i \\mid X_i\\) doesn’t mean that D has no effect on \\(Y\\)\nInstead it means, that what we can estimate \\(E[Y_i|D_i=1,X_i]\\) and treat this as a good (unbiased) estimate of \\(E[Y_i(1)]\\) and similarly \\(E[Y_i|D_i=0,X_i]\\) is a good estimate of \\(E[Y_i(0)]\\)"
  },
  {
    "objectID": "slides/06-slides.html#observational-designs-for-causal-inference",
    "href": "slides/06-slides.html#observational-designs-for-causal-inference",
    "title": "Week 06:",
    "section": "Observational Designs for Causal Inference",
    "text": "Observational Designs for Causal Inference\n\nThree common research designs:\n\nDifference in Difference Design\nRegression Discontinuity Designs\nInstrumental Variable Designs\n\n\nclass: inverse, center, middle # 💡 Difference in Differences\nclass: inverse, center, middle background-image:url(https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc) background-size: cover ## London in the Time of Cholera"
  },
  {
    "objectID": "slides/06-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/06-slides.html#motivating-example-what-causes-cholera",
    "title": "Week 06:",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/06-slides.html#notation",
    "href": "slides/06-slides.html#notation",
    "title": "Week 06:",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/06-slides.html#causal-effects",
    "href": "slides/06-slides.html#causal-effects",
    "title": "Week 06:",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/06-slides.html#average-treatment-on-treated",
    "href": "slides/06-slides.html#average-treatment-on-treated",
    "title": "Week 06:",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/06-slides.html#average-treatment-on-treated-1",
    "href": "slides/06-slides.html#average-treatment-on-treated-1",
    "title": "Week 06:",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n              | Post-Period (T=1) | Pre-Period (T=0) |\n||–|-| | Treated \\(D_{i}=1\\) | \\(E[Y_{1i}(1)\\vert D_i = 1]\\) | \\(E[Y_{0i}(0)\\vert D_i = 1]\\) | | Control \\(D_i=0\\) | \\(E[Y_{0i}(1)\\vert D_i = 0]\\) | \\(E[Y_{0i}(0)\\vert D_i = 0]\\) |"
  },
  {
    "objectID": "slides/06-slides.html#data",
    "href": "slides/06-slides.html#data",
    "title": "Week 06:",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1854 (T=1)\n1849 (T=0)\n\n\n\n\nLambeth (D=1)\n19\n85\n\n\nSouthwark and Vauxhall (D=0\n147\n135"
  },
  {
    "objectID": "slides/06-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/06-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "Week 06:",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/06-slides.html#before-vs-after-comparisons",
    "href": "slides/06-slides.html#before-vs-after-comparisons",
    "title": "Week 06:",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nThis comparison assumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854"
  },
  {
    "objectID": "slides/06-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/06-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "Week 06:",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences",
    "href": "slides/06-slides.html#difference-in-differences",
    "title": "Week 06:",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]\\}}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period\n\nWhich is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]\\}}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-1",
    "href": "slides/06-slides.html#difference-in-differences-1",
    "title": "Week 06:",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/06-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/06-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "Week 06:",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V\n\n\nWhere:\n\n\\(E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\)\n\\(E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}\\} = 0]\\)\n\\(E[Y_{1i}(1) − Y_{0i}(1)|D_{i} = 1]\\)"
  },
  {
    "objectID": "slides/06-slides.html#summary",
    "href": "slides/06-slides.html#summary",
    "title": "Week 06:",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated-control comparison\n\nTaking the pre-post difference removes any fixed differences between the units\nThen taking the difference between treated and control differences removes any common differences over time\n\nThe key identifying assumption of a DiD design is the “assumption of parallel trends”\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to test"
  },
  {
    "objectID": "slides/06-slides.html#extensions-and-limitations",
    "href": "slides/06-slides.html#extensions-and-limitations",
    "title": "Week 06:",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nDiD easy to estimate with linear regression\nGeneralizes to multiple periods and treatment interventions\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\n\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/06-slides.html#applications",
    "href": "slides/06-slides.html#applications",
    "title": "Week 06:",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?\n\nclass:inverse, middle, center # 🔍 # Simple Linear Regression ### Linear regression provides a linear estimate of the CEF"
  },
  {
    "objectID": "slides/06-slides.html#review-simple-linear-regression",
    "href": "slides/06-slides.html#review-simple-linear-regression",
    "title": "Week 06:",
    "section": "Review: Simple Linear Regression",
    "text": "Review: Simple Linear Regression\n\nConceptual\n\nSimple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n\\]\n\nPractical\n\nWe estimate linear models in R using the lm() function\n\n\n\nlm(y ~ x, data = df)\n\n\nTechnical/Definitional\n\nLinear regression chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]\n\nTheoretical\n\nLinear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\)"
  },
  {
    "objectID": "slides/06-slides.html#linear-regression-and-the-cef",
    "href": "slides/06-slides.html#linear-regression-and-the-cef",
    "title": "Week 06:",
    "section": "Linear Regression and the CEF",
    "text": "Linear Regression and the CEF\nRecall the first model we fit in last week’s lab\n\\[\\text{new_deaths} = \\beta_0 + \\beta_1 \\text{winner} + \\epsilon\\]\n\nm1 &lt;- lm(new_deaths ~ winner, covid_df)\nm1\n\n\nCall:\nlm(formula = new_deaths ~ winner, data = covid_df)\n\nCoefficients:\n(Intercept)  winnerBiden  \n     19.349        3.519  \n\n\nm1 is the equation for a very simple line defined by two points:\n\n\\(\\beta_0\\) = E[new_deaths|winner = Trump]\n\\(\\beta_0 + \\beta_1\\) = E[new_deaths|winner = Biden]"
  },
  {
    "objectID": "slides/06-slides.html#linear-regression-and-the-cef-1",
    "href": "slides/06-slides.html#linear-regression-and-the-cef-1",
    "title": "Week 06:",
    "section": "Linear Regression and the CEF",
    "text": "Linear Regression and the CEF\nIt’s helpful to take a look at what’s going on under the hood in lm()\nWe gave it the following data:\n\nhead(m1$model)\n\n   new_deaths winner\n67          0  Biden\n68          0  Biden\n69          0  Biden\n70          0  Biden\n71          0  Biden\n72          0  Biden"
  },
  {
    "objectID": "slides/06-slides.html#linear-regression-and-the-cef-2",
    "href": "slides/06-slides.html#linear-regression-and-the-cef-2",
    "title": "Week 06:",
    "section": "Linear Regression and the CEF",
    "text": "Linear Regression and the CEF\n\nR doesn’t know how to calculate the mean of Biden (or any character or factor data).\nInstead it creates an indicator variable winnerBiden which equals 1 when winner equals Biden and 0 otherwise.\n\n\nhead(model.matrix(m1))\n\n   (Intercept) winnerBiden\n67           1           1\n68           1           1\n69           1           1\n70           1           1\n71           1           1\n72           1           1"
  },
  {
    "objectID": "slides/06-slides.html#linear-regression-and-the-cef-3",
    "href": "slides/06-slides.html#linear-regression-and-the-cef-3",
    "title": "Week 06:",
    "section": "Linear Regression and the CEF",
    "text": "Linear Regression and the CEF\n\ntable(m1$model$winner)\n\n\nTrump Biden \n25738 25988 \n\ntable(model.matrix(m1)[,2])\n\n\n    0     1 \n25738 25988 \n\n\n\ncovid_df%&gt;%\n  mutate(\n    winner01 = ifelse(winner==\"Biden\",1,0)\n  )%&gt;%\n  ggplot(aes(winner01, new_deaths))+\n  geom_point()+\n  stat_summary(geom=\"point\",fun=mean,col= \"red\",size=2)+\n  geom_segment(aes(x=0,xend =1, \n                   y=coef(m1)[1],\n                   yend = coef(m1)[1]+coef(m1)[2]*1\n                   ),\n               col= \"red\")"
  },
  {
    "objectID": "slides/06-slides.html#linear-regression-and-the-cef-4",
    "href": "slides/06-slides.html#linear-regression-and-the-cef-4",
    "title": "Week 06:",
    "section": "Linear Regression and the CEF",
    "text": "Linear Regression and the CEF\n\nWhen the CEF function is linear, lm() provides you coefficients that give you conditional means like in m1\n\nThe CEF is linear for saturated models in like m1 where the coefficients define every possible category of comparison.\n\nFor continuous variables, like m3 from the lab below, lm() provides a linear approximation to the CEF\n\n\nm3 &lt;- lm(new_deaths_pc_14da ~ percent_vaccinated, covid_df,\n         subset = date == \"2021-09-23\")\nm3\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ percent_vaccinated, data = covid_df, \n    subset = date == \"2021-09-23\")\n\nCoefficients:\n       (Intercept)  percent_vaccinated  \n           2.41522            -0.03292  \n\n\n\ncovid_df %&gt;%\n  filter(date == \"2021-09-23\") %&gt;%\n  ggplot(aes(percent_vaccinated, new_deaths_pc_14da))+\n  geom_point(size=.5,alpha=.5)+\n  stat_summary_bin(fun = \"mean\", \n                   geom = \"point\", \n                   col=\"red\",\n                   bins =10\n                   )+\n  geom_smooth(method =\"lm\")\n\n\nclass: inverse, middle, center # 💡 ## Multiple Regression"
  },
  {
    "objectID": "slides/06-slides.html#overiew-multiple-regression",
    "href": "slides/06-slides.html#overiew-multiple-regression",
    "title": "Week 06:",
    "section": "Overiew: Multiple Regression",
    "text": "Overiew: Multiple Regression\n\nConceptual\n\nMultiple linear regression generalizes simple regression to models with multiple predictors\n\n\n\\[y_i = \\beta_0 + \\beta_1x_{1,i} +\\beta_2x_{2,i} + \\epsilon_i\\]\n\\[y_i = X\\beta + \\epsilon_i\\]\n\nPractical\n\nWe estimate linear models in R using the lm() function using the + to add predictors\nWe use the * to include the main effects \\((\\beta_1 x, \\beta_2z)\\) and interactions \\((\\beta_3 (x\\cdot z))\\)of two predictors\n\n\n\nlm(y ~ x + z, data = df)\nlm(y ~ x*z, data = df) # Is a shortcut for:\nlm(y ~ x + z + x:z, data = df)"
  },
  {
    "objectID": "slides/06-slides.html#overiew-multiple-regression-1",
    "href": "slides/06-slides.html#overiew-multiple-regression-1",
    "title": "Week 06:",
    "section": "Overiew: Multiple Regression",
    "text": "Overiew: Multiple Regression\n\nTechnical/Definitional\n\nSimple linear regression chooses a \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\hat{\\beta_0}+\\hat{\\beta_1}x_i))^2\\] - Multiple linear regression chooses a vector of coefficients \\(\\hat{\\beta}\\) to minimize the Sum of Squared Residuals (SSR):\n\\[\\textrm{Find }\\widehat{\\beta} \\text{ argmin }_{\\widehat{\\beta}} \\sum \\epsilon^2=\\epsilon^\\prime\\epsilon=(Y-X\\widehat{\\beta})^\\prime(Y-X\\widehat{\\beta})\\]\n\nTheoretical\n\nMultiple Linear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\) where \\(Y\\) is now a function of multiple predictors, \\(X\\)\n\n\nclass: inverse, middle, center # 💡 ## Estimating and Interpreting Multiple Regression Regression Models"
  },
  {
    "objectID": "slides/06-slides.html#political-interest-and-partisan-evaluations",
    "href": "slides/06-slides.html#political-interest-and-partisan-evaluations",
    "title": "Week 06:",
    "section": "Political Interest and Partisan Evaluations",
    "text": "Political Interest and Partisan Evaluations\nLet’s load some data from the 2016 NES and explore the relationship between political interest and evaluations of presidential candidates:\n\nPolitical Interest: “How interested are you in in politics?\n\nVery Interested\nSomewhat interested\nNot very Interested\nNot at all Interested\n\nFeeling Thermometer: “… On the feeling thermometer scale of 0 to 100, how would you rate”\n\nDonald Trump\nHillary Clinton"
  },
  {
    "objectID": "slides/06-slides.html#data-1",
    "href": "slides/06-slides.html#data-1",
    "title": "Week 06:",
    "section": "Data",
    "text": "Data\n\nload(url(\"https://pols1600.paultesta.org/files/data/nes.rda\"))\n# Look at data\ndim(nes)\n\n[1] 1200   14\n\nhead(nes)\n\n  caseid    state age gender educ faminc pid7 ideo5 pol_interest church_atd\n1    745  Alabama  19      2    3     97    5     3            1          2\n2   1115  Alabama  46      1    3      3    1     1            3          6\n3    258  Alabama  59      2    2      6    2     3            2          1\n4    126  Alabama  55      2    4      6    1     2            3          5\n5    414  Alabama  66      1    3      8    7     4            3          3\n6    523  Alabama  61      1    2     97    1     2            3          6\n  bornagain01 ft_trump ft_hrc vote_choice\n1           0        3     19       Other\n2           0        0     36         HRC\n3           1       22      2        &lt;NA&gt;\n4           0        1     80         HRC\n5           1      100      3        &lt;NA&gt;\n6           1        0    100        &lt;NA&gt;"
  },
  {
    "objectID": "slides/06-slides.html#data-hlo",
    "href": "slides/06-slides.html#data-hlo",
    "title": "Week 06:",
    "section": "Data: HLO",
    "text": "Data: HLO\n\ntable(nes$pol_interest)\n\n\n  0   1   2   3 \n 78 178 348 568 \n\nsummary(nes$ft_trump)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    2.00   30.00   38.38   72.00  100.00       3 \n\nsummary(nes$ft_hrc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    3.00   44.00   42.99   76.00  100.00       1"
  },
  {
    "objectID": "slides/06-slides.html#data-wranglingrecoding",
    "href": "slides/06-slides.html#data-wranglingrecoding",
    "title": "Week 06:",
    "section": "Data Wrangling/Recoding",
    "text": "Data Wrangling/Recoding\n\nnes %&gt;%\n  mutate(\n    income = ifelse(faminc &gt; 16, NA, faminc),\n    interested = ifelse(pol_interest==3,T,F),\n    pol_interest_f = factor(case_when(\n      pol_interest == 0 ~ \"Not at all Interested\",\n      pol_interest == 1 ~ \"Not very Interested\",\n      pol_interest == 2 ~ \"Somewhat Interested\",\n      pol_interest == 3 ~ \"Very Interested\"\n    )),\n    tc_diff = abs(ft_trump - ft_hrc)\n  ) -&gt; nes"
  },
  {
    "objectID": "slides/06-slides.html#roadmap",
    "href": "slides/06-slides.html#roadmap",
    "title": "Week 06:",
    "section": "Roadmap",
    "text": "Roadmap\nLet’s estimate the following series of models, predicting the absolute value of the difference in feeling thermometer ratings between Trump and Clinton (tc_diff)\n\\[\\text{tc_diff} = \\beta_0\\] \\[\\text{tc_diff} = \\beta_0 + \\beta_1\\text{interested}\\] \\[\\text{tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest_f}\\] \\[\\text{tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest}\\] \\[\\text{tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age}\\] \\[\\text{tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} + \\beta_3\\text{interested} \\times \\text{age}\\] \\[\\text{tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income}\\]"
  },
  {
    "objectID": "slides/06-slides.html#m0-the-empty-model",
    "href": "slides/06-slides.html#m0-the-empty-model",
    "title": "Week 06:",
    "section": "m0: “The Empty Model”",
    "text": "m0: “The Empty Model”\nA linear model with just an intercept returns the mean of tc_diff\n\nm0 &lt;- lm(tc_diff ~ 1, nes)\nm0\n\n\nCall:\nlm(formula = tc_diff ~ 1, data = nes)\n\nCoefficients:\n(Intercept)  \n      53.58  \n\nmean(nes$tc_diff, na.rm=T)\n\n[1] 53.57776\n\n# Save Sum of Squared Residuals\nssr0 &lt;- sum(m0$residuals^2)\n\nm1 A model with a single dichotomous predictor\nNow let’s model tc_diff as function of whether respondents are very interested in politics (interested = T) or not (interested = F)\n\nm1 &lt;- lm(tc_diff ~ interested, nes)\ncoef(m1)\n\n   (Intercept) interestedTRUE \n      47.84500       12.74655 \n\nmean(nes$tc_diff[nes$interested == F],na.rm = T)\n\n[1] 47.845\n\ncoef(m1)[1] + coef(m1)[2]\n\n(Intercept) \n   60.59155 \n\nmean(nes$tc_diff[nes$interested == T],na.rm = T)\n\n[1] 60.59155\n\n\nm1 A model with a single dichotomous predictor\n\nIf we tell R not to include an intercept, it returns those conditional means exactly.\n\n\nm1_alt &lt;- lm(tc_diff ~ -1+ interested, nes)\ncoef(m1_alt)\n\ninterestedFALSE  interestedTRUE \n       47.84500        60.59155 \n\ntapply(nes$tc_diff, nes$interested, mean,na.rm=T)\n\n   FALSE     TRUE \n47.84500 60.59155 \n\n\nm1 A model with a single dichotomous predictor\n\nWhen we compare the SSR from m1 we see that it has decreased from m0 reflecting the fact some of the total variation in tc_diff is being explained by the variation in interested\n\n\n# Save Sum of Squared Residuals\nssr1 &lt;- sum(m1$residuals^2)\nssr1 &lt; ssr0\n\n[1] TRUE\n\n\nm2 A model with a single categoical predictor\nNow let’s model tc_diff as function of the categorical predictor pol_interest_f, which is a factor variable of the four unique levels of political interest\n\nm2 &lt;- lm(tc_diff ~ pol_interest_f, nes)\nm2\n\n\nCall:\nlm(formula = tc_diff ~ pol_interest_f, data = nes)\n\nCoefficients:\n                      (Intercept)  pol_interest_fNot very Interested  \n                           46.104                              1.727  \npol_interest_fSomewhat Interested      pol_interest_fVery Interested  \n                            2.136                             14.488  \n\n\n\nWoah, why did R turn a single variable into model with four coefficients?\n\nWhen we give lm() factor or character data,\n\nhead(m2$model)[1:4,]\n\n  tc_diff      pol_interest_f\n1      16 Not very Interested\n2      36     Very Interested\n3      20 Somewhat Interested\n4      79     Very Interested\n\n\nIt converts this data in separate binary indicators for each level excluding the first (“Not at all interested”), that take a value of 1 when pol_interest_f equals that level and 0 otherwise\n\nhead(model.matrix(m2))[1:4,]\n\n  (Intercept) pol_interest_fNot very Interested\n1           1                                 1\n2           1                                 0\n3           1                                 0\n4           1                                 0\n  pol_interest_fSomewhat Interested pol_interest_fVery Interested\n1                                 0                             0\n2                                 0                             1\n3                                 1                             0\n4                                 0                             1"
  },
  {
    "objectID": "slides/06-slides.html#m2-a-model-with-a-single-categoical-predictor-1",
    "href": "slides/06-slides.html#m2-a-model-with-a-single-categoical-predictor-1",
    "title": "Week 06:",
    "section": "m2 A model with a single categoical predictor",
    "text": "m2 A model with a single categoical predictor\nThe coefficients in m2 tell us how the conditional mean of tc_diff for someone with a given level of interest is likely to very from the conditional mean of tc_diff for someone that is not at all interested in politics.\n\ntapply(nes$tc_diff, nes$pol_interest_f, mean,na.rm=T)\n\nNot at all Interested   Not very Interested   Somewhat Interested \n             46.10390              47.83051              48.23988 \n      Very Interested \n             60.59155 \n\n\n\nNot at all interested is the reference or excluded category, described by the intercept coefficient in m2"
  },
  {
    "objectID": "slides/06-slides.html#m2-a-model-with-a-single-categoical-predictor-2",
    "href": "slides/06-slides.html#m2-a-model-with-a-single-categoical-predictor-2",
    "title": "Week 06:",
    "section": "m2 A model with a single categoical predictor",
    "text": "m2 A model with a single categoical predictor\n\n# Compare to\n# Not at all\ncoef(m2)[1]\n\n(Intercept) \n    46.1039 \n\n# Not very\ncoef(m2)[1] + coef(m2)[2]\n\n(Intercept) \n   47.83051 \n\n# Somewhat\ncoef(m2)[1] + coef(m2)[3]\n\n(Intercept) \n   48.23988 \n\n# Very\ncoef(m2)[1] + coef(m2)[4]\n\n(Intercept) \n   60.59155"
  },
  {
    "objectID": "slides/06-slides.html#m2-a-model-with-a-single-categorical-predictor",
    "href": "slides/06-slides.html#m2-a-model-with-a-single-categorical-predictor",
    "title": "Week 06:",
    "section": "m2 A model with a single categorical predictor",
    "text": "m2 A model with a single categorical predictor\n\nAgain if we excluded the intercept lm() returns the conditional means\n\n\nm2_alt &lt;- lm(tc_diff ~ -1+ pol_interest_f, nes)\ncoef(m2_alt)\n\npol_interest_fNot at all Interested   pol_interest_fNot very Interested \n                           46.10390                            47.83051 \n  pol_interest_fSomewhat Interested       pol_interest_fVery Interested \n                           48.23988                            60.59155 \n\ntapply(nes$tc_diff, nes$pol_interest_f, mean,na.rm=T)\n\nNot at all Interested   Not very Interested   Somewhat Interested \n             46.10390              47.83051              48.23988 \n      Very Interested \n             60.59155"
  },
  {
    "objectID": "slides/06-slides.html#m2-a-model-with-a-single-categorical-predictor-1",
    "href": "slides/06-slides.html#m2-a-model-with-a-single-categorical-predictor-1",
    "title": "Week 06:",
    "section": "m2 A model with a single categorical predictor",
    "text": "m2 A model with a single categorical predictor\n\nFinally, note that the SSR for m2 is lower than that of m1. Including more information about varying levels of political interest helped us explain more variation in our outcome\n\n\n# Save Sum of Squared Residuals\nssr2 &lt;- sum(m2$residuals^2)\nssr2 &lt; ssr1\n\n[1] TRUE"
  },
  {
    "objectID": "slides/06-slides.html#m3-a-model-with-a-single-numeric-predictor",
    "href": "slides/06-slides.html#m3-a-model-with-a-single-numeric-predictor",
    "title": "Week 06:",
    "section": "m3 A model with a single numeric predictor",
    "text": "m3 A model with a single numeric predictor\nWhat if we had instead modeled tc_diff as a function of pol_interest\n\nm3 &lt;- lm(tc_diff ~ pol_interest, nes)\nm3\n\n\nCall:\nlm(formula = tc_diff ~ pol_interest, data = nes)\n\nCoefficients:\n (Intercept)  pol_interest  \n      40.799         6.012  \n\n\nlm() returns a single coefficient, because pol_interest is a numeric variable."
  },
  {
    "objectID": "slides/06-slides.html#comparing-m2-to-m3",
    "href": "slides/06-slides.html#comparing-m2-to-m3",
    "title": "Week 06:",
    "section": "Comparing m2 to m3",
    "text": "Comparing m2 to m3\n\nnes %&gt;%\n  ggplot(aes(pol_interest,tc_diff))+\n  geom_jitter(size=.5, alpha=.5)+\n  geom_smooth(method =\"lm\", aes(col = \"m3\"),se=F)+\n  stat_summary(geom =\"point\",\n               fun = mean,\n               aes(col = \"m2\")\n               )+\n  labs(col = \"Model\")\n\nComparing m2 to m3\n\n\n\n\n\n\n\n\n\nm4: A model with both a dichtomous and continuous predictor\nNow let’s estimate a multiple regression model “controlling” for age\n\nm4 &lt;- lm(tc_diff ~ interested + age, nes)\ncoef(m4)\n\n   (Intercept) interestedTRUE            age \n    32.1757944      9.7739375      0.3533876 \n\n\nNote the size of the coefficient on interested has decreased, compared to m1. Part of the variation tc_diff explained by interested reflected variation explained by age\n\ncoef(m1)[1]\n\n(Intercept) \n     47.845 \n\ncoef(m4)[2]\n\ninterestedTRUE \n      9.773938 \n\n\n\nMechanically, we sometimes say the coefficient on interested shifts the intercept of the line describing the relationship between tc_diff and age.\n\n\n\n\n\n\n\n\n\n\nm5: A model with an interaction between a dichtomous and continuous predictor\nWhat if we think the relationship between age and tc_diff varies depending on a person’s level of political interest?\nWe could fit an interacton model:\n\nm5 &lt;- lm(tc_diff ~ interested*age, nes)\ncoef(m5)\n\n       (Intercept)     interestedTRUE                age interestedTRUE:age \n        39.7626421         -6.7040163          0.1822814          0.3396523 \n\n\nMechanically, R takes the data\n\nhead(m5$model)\n\n  tc_diff interested age\n1      16      FALSE  19\n2      36       TRUE  46\n3      20      FALSE  59\n4      79       TRUE  55\n5      97       TRUE  66\n6     100       TRUE  61\n\n\nAnd creates a model matrix:\n\nhead(model.matrix(m5))\n\n  (Intercept) interestedTRUE age interestedTRUE:age\n1           1              0  19                  0\n2           1              1  46                 46\n3           1              0  59                  0\n4           1              1  55                 55\n5           1              1  66                 66\n6           1              1  61                 61\n\n\nm5: A model with an interaction between a dichtomous and continuous predictor\nThis interaction model essentially estimates separate models describing the relationship between tc_diff and age for the uninterested…\n\nm5_uninterested &lt;- lm(tc_diff ~ age, nes,\n                    subset = interested == F)\ncoef(m5_uninterested)\n\n(Intercept)         age \n 39.7626421   0.1822814 \n\ncoef(m5)[1]\n\n(Intercept) \n   39.76264 \n\ncoef(m5)[3]\n\n      age \n0.1822814 \n\n\nm5: A model with an interaction between a dichtomous and continuous predictor\nAnd interested\n\nm5_interested &lt;- lm(tc_diff ~ age, nes,\n                    subset = interested == T)\ncoef(m5_interested)\n\n(Intercept)         age \n 33.0586258   0.5219337 \n\ncoef(m5)[1] +coef(m5)[2]\n\n(Intercept) \n   33.05863 \n\ncoef(m5)[3] +coef(m5)[4]\n\n      age \n0.5219337 \n\n\n\nnes %&gt;%\n  filter(!is.na(interested))%&gt;%\n  ggplot(aes(age, tc_diff, col = interested))+\n  geom_point(size = .5, alpha = .5)+\n  geom_smooth(method =\"lm\")\n\n\n\n\n\n\n\n\nm6: A model with an two continuous predictors\nNow lets model tc_diff as a function of two predictors: age and income\n\nm6 &lt;- lm(tc_diff ~ age + income, nes)\nm6\n\n\nCall:\nlm(formula = tc_diff ~ age + income, data = nes)\n\nCoefficients:\n(Intercept)          age       income  \n    33.1994       0.4003       0.1573  \n\n\nWe interpret this model as saying:\n\nControlling for income, as age increases by 1 we can expect the predicted difference in tc_diff to increase by 0.40\nControlling for age, as income increases by 1 we can expect the predicted difference in tc_diff to increase by 0.16"
  },
  {
    "objectID": "slides/06-slides.html#multiplicative-interactions-with-a-continuous-moderator",
    "href": "slides/06-slides.html#multiplicative-interactions-with-a-continuous-moderator",
    "title": "Week 06:",
    "section": "Multiplicative interactions with a continuous moderator",
    "text": "Multiplicative interactions with a continuous moderator\nSuppose we wanted to fit an interaction between two continuous variables like age and income.\nThe general form of this model is\n\\[\nY= \\beta_0 + \\beta_1X + \\beta_2 +\\beta_3X\\times Z + \\epsilon\n\\]\n\nThe marginal effect of X on Y, now depends on the value of Z at which you evaluate the relationship\n\n\\[\n\\frac{\\partial Y}{\\partial X}=\\beta_1+\\beta_3Z\n\\]\nEasier to show this with some simulated data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass: inverse, middle, center # 💡 ## Reading Regression Tables"
  },
  {
    "objectID": "slides/06-slides.html#regression-tables",
    "href": "slides/06-slides.html#regression-tables",
    "title": "Week 06:",
    "section": "Regression Tables",
    "text": "Regression Tables\n\nAcademic articles are littered with regression tables.\nBelow we’ll see how to produce regression tables in R\nLearn some heuristics for how to interpret regression tables\n\nWe’ll cover the theory behind these heuristics in the coming weeks"
  },
  {
    "objectID": "slides/06-slides.html#making-regression-tables-in-r",
    "href": "slides/06-slides.html#making-regression-tables-in-r",
    "title": "Week 06:",
    "section": "Making Regression Tables in R",
    "text": "Making Regression Tables in R\nWe can make a very simple regression table using the htmlreg function from the texreg package\n\ntexreg::htmlreg(\n  list(m0,m1,m3,m4,m5,m6)\n)\n\n\n\n&lt;table class=\"texreg\" style=\"margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;\"&gt;\n&lt;caption&gt;Statistical models&lt;/caption&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/th&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;Model 1&lt;/th&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;Model 2&lt;/th&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;Model 3&lt;/th&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;Model 4&lt;/th&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;Model 5&lt;/th&gt;\n&lt;th style=\"padding-left: 5px;padding-right: 5px;\"&gt;Model 6&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(Intercept)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;53.58&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;47.84&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;40.80&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;32.18&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;39.76&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;33.20&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.92)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(1.26)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(2.35)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(2.71)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(3.62)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(3.35)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;interestedTRUE&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;12.75&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;9.77&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;-6.70&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(1.81)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(1.84)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(5.55)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;pol_interest&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;6.01&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.98)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;age&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.35&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.18&lt;sup&gt;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.40&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.05)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.08)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.06)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;interestedTRUE:age&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.34&lt;sup&gt;&#42;&#42;&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.11)&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;income&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.16&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;&nbsp;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;(0.29)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr style=\"border-top: 1px solid #000000;\"&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.00&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.04&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.03&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.07&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.08&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;Adj. R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.00&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.04&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.03&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.07&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.08&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr style=\"border-bottom: 2px solid #000000;\"&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;Num. obs.&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;1196&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;1168&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;1168&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;1168&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;1168&lt;/td&gt;\n&lt;td style=\"padding-left: 5px;padding-right: 5px;\"&gt;1049&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;tfoot&gt;\n&lt;tr&gt;\n&lt;td style=\"font-size: 0.8em;\" colspan=\"7\"&gt;&lt;sup&gt;&#42;&#42;&#42;&lt;/sup&gt;p &lt; 0.001; &lt;sup&gt;&#42;&#42;&lt;/sup&gt;p &lt; 0.01; &lt;sup&gt;&#42;&lt;/sup&gt;p &lt; 0.05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tfoot&gt;\n&lt;/table&gt;\n\n\nYuck, we need to set a argument in the chunk header to results=“asis”\n\n```{r echo=F, results = \"asis\"}`r`''`\ntexreg::htmlreg(\n  list(m0,m1,m3,m4,m5,m6)\n)\n\n```\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n53.58***\n\n\n47.84***\n\n\n40.80***\n\n\n32.18***\n\n\n39.76***\n\n\n33.20***\n\n\n\n\n \n\n\n(0.92)\n\n\n(1.26)\n\n\n(2.35)\n\n\n(2.71)\n\n\n(3.62)\n\n\n(3.35)\n\n\n\n\ninterestedTRUE\n\n\n \n\n\n12.75***\n\n\n \n\n\n9.77***\n\n\n-6.70\n\n\n \n\n\n\n\n \n\n\n \n\n\n(1.81)\n\n\n \n\n\n(1.84)\n\n\n(5.55)\n\n\n \n\n\n\n\npol_interest\n\n\n \n\n\n \n\n\n6.01***\n\n\n \n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.98)\n\n\n \n\n\n \n\n\n \n\n\n\n\nage\n\n\n \n\n\n \n\n\n \n\n\n0.35***\n\n\n0.18*\n\n\n0.40***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.08)\n\n\n(0.06)\n\n\n\n\ninterestedTRUE:age\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.34**\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.11)\n\n\n \n\n\n\n\nincome\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n0.16\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.29)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.04\n\n\n0.03\n\n\n0.07\n\n\n0.08\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.04\n\n\n0.03\n\n\n0.07\n\n\n0.08\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1196\n\n\n1168\n\n\n1168\n\n\n1168\n\n\n1168\n\n\n1049\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/06-slides.html#interpreting-regression-tables-stargazing",
    "href": "slides/06-slides.html#interpreting-regression-tables-stargazing",
    "title": "Week 06:",
    "section": "Interpreting Regression Tables (Stargazing)",
    "text": "Interpreting Regression Tables (Stargazing)\n\nEach column is a model\nEach row is a coefficient from that model with its standard error (more to come) in parantheses below\nThe bottom of the table shows summary stastitics of the model ( \\(R^2\\) = proportion of variance explained)\nCoefficients with asterisks * are statistically significant\n\nIt is unlikely that we would see a coefficient this big or bigger if the true coefficient were 0\n\nRule of thumb:\n\n\\[\\text{If }\\frac{\\beta}{se} &gt; 2 \\to \\text{Statistically Significant}\\]\nclass: inverse, middle, center # 💡 ## Summary"
  },
  {
    "objectID": "slides/06-slides.html#summary-1",
    "href": "slides/06-slides.html#summary-1",
    "title": "Week 06:",
    "section": "Summary",
    "text": "Summary\nToday we estimated and interpreted a series of multiple regression models\n\nThe coefficients in these models still describe slopes (partial derivatives), now for planes (and hyper planes)\nControlling for variables is easy in lm()\n\n\nm1 &lt;- lm(y ~ x, data=df)\nm2 &lt;- lm(y ~ x + z, data=df)\n\nIn the lab and next week, we will start to explore\n\nWhat it means to “control for x”\nHow can we compare different models\nHow can we interpret predictions from multiple regression\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/09-slides.html#general-plan",
    "href": "slides/09-slides.html#general-plan",
    "title": "Week 09:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nFeedback\nReview\n\nProbability Distributions\n\nLecture\n\nThe Law of Large Numbers\nThe Central Limit Theorem\nGeneralized Linear Models (Maybe…)"
  },
  {
    "objectID": "slides/09-slides.html#goals",
    "href": "slides/09-slides.html#goals",
    "title": "Week 09:",
    "section": "Goals",
    "text": "Goals\n\nThe Law of Large Number’s says that as our sample size increases, our sample mean will converge to the population value\n\n–\n\nThe Central Limit Theorem says that the distribution of those sample means will follow a normal distribution\n\n–\n\nGeneralized Linear Models allow us to more accurately model different types of data-generating processes using Maximum Likelihood Estimation."
  },
  {
    "objectID": "slides/09-slides.html#emoji-slide-notation",
    "href": "slides/09-slides.html#emoji-slide-notation",
    "title": "Week 09:",
    "section": "Emoji Slide notation",
    "text": "Emoji Slide notation\n\n💪: Exercises\n📢: Feedback\n🔍: Review\n💡: Core concept\n🦉: In case you’re interested\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/09-slides.html#new-packages",
    "href": "slides/09-slides.html#new-packages",
    "title": "Week 09:",
    "section": "New packages",
    "text": "New packages\nNone!"
  },
  {
    "objectID": "slides/09-slides.html#packages-for-today",
    "href": "slides/09-slides.html#packages-for-today",
    "title": "Week 09:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/09-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/09-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 09:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/09-slides.html#load-packages-for-today",
    "href": "slides/09-slides.html#load-packages-for-today",
    "title": "Week 09:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n          zoo \n         TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today\nclass:inverse, middle, center # 🔍 # Review ## Random Variables and Probability Distributions"
  },
  {
    "objectID": "slides/09-slides.html#probability",
    "href": "slides/09-slides.html#probability",
    "title": "Week 09:",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event happening.\nStatistics uses probability to quantify uncertainty about estimates and hypotheses.\nThree rules of probability (Kolmogorov axioms)\n\nPositivity: \\[Pr(A) \\geq 0 \\]\nCertainty: \\[Pr(\\Omega) = 1 \\]\nAdditivity: \\[Pr(A \\text{ or } B) = Pr(A) + Pr(B)\\] iff A and B are mutually exclusive"
  },
  {
    "objectID": "slides/09-slides.html#probability-1",
    "href": "slides/09-slides.html#probability-1",
    "title": "Week 09:",
    "section": "Probability",
    "text": "Probability\n\nTwo interpretations interpreting probabilities (Frequentist and Bayesian)\nConditional Probability and Bayes Rule:\n\n\\[Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}\\]"
  },
  {
    "objectID": "slides/09-slides.html#random-variables",
    "href": "slides/09-slides.html#random-variables",
    "title": "Week 09:",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables assign numeric values to each event in an experiment.\n\nMutually exclusive and exhaustive, together cover the entire sample space.\n\nDiscrete random variables take on finite, or countably infinite distinct values.\nContinuous variables can take on an uncountably infinite number of values."
  },
  {
    "objectID": "slides/09-slides.html#example-toss-two-coins",
    "href": "slides/09-slides.html#example-toss-two-coins",
    "title": "Week 09:",
    "section": "Example: Toss Two Coins",
    "text": "Example: Toss Two Coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)"
  },
  {
    "objectID": "slides/09-slides.html#probability-distributions",
    "href": "slides/09-slides.html#probability-distributions",
    "title": "Week 09:",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\n\n\\[\\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities}\\]\nThe can be represented in terms of:\n\nProbability Mass/Density Functions\n\nDiscrete variables have probability mass functions (PMF)\nContinuous variables have probability density functions (PDF)\n\nCumulative Density Functions\n\nDiscrete: Summation of discrete probabilities\nContinuous: Integration over a range of values"
  },
  {
    "objectID": "slides/09-slides.html#discrete-distributions",
    "href": "slides/09-slides.html#discrete-distributions",
    "title": "Week 09:",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nProbability Mass Function (pmf): \\(f(x)=p(X=x)\\)\n\nAssigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\n\nCumulative Distribution Function (cdf) \\(F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)\\)\n\nSum of the probability mass for events less than or equal to \\(x_j\\)"
  },
  {
    "objectID": "slides/09-slides.html#example-toss-two-coins-1",
    "href": "slides/09-slides.html#example-toss-two-coins-1",
    "title": "Week 09:",
    "section": "Example: Toss Two coins",
    "text": "Example: Toss Two coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)\n\n\\(f(X=0)=p(X=0)=1/4\\)\n\\(f(X=1)=p(X=1)=1/2\\)\n\\(F(X\\leq 1) = p(X \\leq 1)= 3/4\\)\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X&lt;=2) = 1/6 + 1/6 = 1/3"
  },
  {
    "objectID": "slides/09-slides.html#continuous-distributions",
    "href": "slides/09-slides.html#continuous-distributions",
    "title": "Week 09:",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nProbability Density Functions (PDF): \\(f(x)\\)\n\nAssigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\nBut… since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n\nCumulative Distribution Function (CDF) \\(F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx\\)\n\nInstead of summing up to a specific value (discrete) we integrate over all possible values up to \\(x\\)\nProbability of having a value less than x"
  },
  {
    "objectID": "slides/09-slides.html#integrals",
    "href": "slides/09-slides.html#integrals",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nFirst, a brief aside on integral calculus:\nWhat’s the area of the rectangle? \\(base\\times height\\)"
  },
  {
    "objectID": "slides/09-slides.html#integrals-1",
    "href": "slides/09-slides.html#integrals-1",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nHow would we find the area under a curve?"
  },
  {
    "objectID": "slides/09-slides.html#integrals-2",
    "href": "slides/09-slides.html#integrals-2",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nWell suppose we added up the areas of a bunch of rectangles roughly whose height’s approximated the height of the curve?\n\nCan we do any better?"
  },
  {
    "objectID": "slides/09-slides.html#integrals-3",
    "href": "slides/09-slides.html#integrals-3",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nLet’s make the rectangles smaller\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get’s even better:"
  },
  {
    "objectID": "slides/09-slides.html#link-between-pdf-and-cdf",
    "href": "slides/09-slides.html#link-between-pdf-and-cdf",
    "title": "Week 09:",
    "section": "🦉 Link between PDF and CDF",
    "text": "🦉 Link between PDF and CDF\nIf \\[F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx \\]\nThen by the fundamental theorem of calculus\n\\[\\frac{d}{dx}F(x)=f(x)\\]\nIn words\n\nthe PDF (\\(f(x)\\)) is the derivative (rate of change) of the CDF (\\(F(X)\\))\nthe CDF describes the area under the curve defined by f(x) up to x"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-the-cdf",
    "href": "slides/09-slides.html#properties-of-the-cdf",
    "title": "Week 09:",
    "section": "Properties of the CDF",
    "text": "Properties of the CDF\n\n\\(0\\leq F(x) \\leq 1\\)\n\\(F\\) is non-decreasing and right continuous\n\\(\\lim_{x\\to-\\infty}F(x)=0\\)\n\\(\\lim_{x\\to\\infty}F(x)=1\\)\nFor all \\(a,b \\in \\mathbb{R}\\) s.t. \\(a&lt;b\\)\n\n\\[p(a &lt; X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx \\]"
  },
  {
    "objectID": "slides/09-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "href": "slides/09-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "title": "Week 09:",
    "section": "Recall the PMF and CDF of a die",
    "text": "Recall the PMF and CDF of a die"
  },
  {
    "objectID": "slides/09-slides.html#whats-the-probability",
    "href": "slides/09-slides.html#whats-the-probability",
    "title": "Week 09:",
    "section": "What’s the probability",
    "text": "What’s the probability\n\n\\(p(X=1)...p(X=6) = 1/6\\)\n\\(p( 2 &lt; X \\leq 5) = F(5)-F(2)=5/6-2/6=3/6=1/2\\)"
  },
  {
    "objectID": "slides/09-slides.html#common-probablity-distirbutions",
    "href": "slides/09-slides.html#common-probablity-distirbutions",
    "title": "Week 09:",
    "section": "Common Probablity Distirbutions",
    "text": "Common Probablity Distirbutions\nIn this course, we’ll use probability distributions to\n\nmodel the data generating process as a function of parameters we can estimate (using Generalized Linear Models)\nperform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)\n\nThere are a lot of probability distributions:\n\n\n\n\n\n\n\n\n\nFortunately, the distributions you need to know to really master data science, are probably more something like\n\n\n\n\n\n\n\n\n\nAnd the distributions we’ll work with the most in this class are an even smaller subset.\n\nBernoulli: Coinflips with probability of heads, \\(p\\)\nUniform: Coinflip with more than two outcomes\nBinomial: Adding up coinflips\nPoisson: Counting the total number of events\nGeometric: Counting till a specific event occurs\nExponential: Counting till a specific event occurs in continous time\nNormal:\n\nThe limit of a Binomial distribution as \\(n\\to \\infty\\)\nThe maximum entropy when we only know the mean and variance\n\nt: A finite sample approximation of the normal\n\\(\\chi^2\\): Distribution of sums of squared variables from Normal distribution"
  },
  {
    "objectID": "slides/09-slides.html#bernoulli-random-variables",
    "href": "slides/09-slides.html#bernoulli-random-variables",
    "title": "Week 09:",
    "section": "Bernoulli Random Variables",
    "text": "Bernoulli Random Variables\nLet’s start with our old friend the coin flip\nA coin flip is an example of a Bernoulli random variable defined by 1 parameter \\(p\\), the probability of success. It has a pmf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF of\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x&lt;1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x&lt;1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array}\n    \\right.\\]\nNote that in our coin flip example \\(p=0.5\\) but it need not. Just imagine a weighted coin like the Patriots use at Foxborough"
  },
  {
    "objectID": "slides/09-slides.html#uniform-distribution",
    "href": "slides/09-slides.html#uniform-distribution",
    "title": "Week 09:",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nOur fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range \\([a,b]\\), the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                \\frac{1}{b-a}& \\mathrm{if\\ } a \\leq x\\leq b \\\\\n                0 & \\text{otherwise} \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF:\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                        0 & x &lt;a \\\\\n                \\frac{x-a}{b-a}& \\mathrm{if\\ } a \\leq x &lt; b \\\\\n                1 & x \\geq b \\\\\n        \\end{array}\n    \\right.\\]\nWe won’t run into uniform distributions all that often except in examples like rolling a fair sided die, but often they’re used in Bayesian analysis as a form of uninformative prior."
  },
  {
    "objectID": "slides/09-slides.html#binomial-distributions",
    "href": "slides/09-slides.html#binomial-distributions",
    "title": "Week 09:",
    "section": "Binomial Distributions",
    "text": "Binomial Distributions\nThe binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.\nThe key parameters are the number of trials \\(n\\) and the probability of success for each trial \\(p\\) and the pdf of a binomial distribution is:\n\\[f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n\\] So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads\n\nThe binomial distribution will come in handy when trying to model binary outcomes."
  },
  {
    "objectID": "slides/09-slides.html#poisson-distributions",
    "href": "slides/09-slides.html#poisson-distributions",
    "title": "Week 09:",
    "section": "Poisson Distributions",
    "text": "Poisson Distributions\nWhat would happen if you let the \\(n\\) in a binomial distribution go to infinity and \\(p\\) go to 0 so that \\(np\\) stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter \\(\\lambda\\) which represents rate at which events occur.\n\\[f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\nWe use these distributions to try and predict to predict the probability of a given number of events occurring in a fixed interval of time. Things like how many acts of political participation would a voter engage in over a year."
  },
  {
    "objectID": "slides/09-slides.html#geometric-distributions",
    "href": "slides/09-slides.html#geometric-distributions",
    "title": "Week 09:",
    "section": "Geometric Distributions",
    "text": "Geometric Distributions\nWhat if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:\n\\[f(x)=p(1-p) ^{x}\\]\nGeometric and related distributions are useful for describing the time until an event occurs"
  },
  {
    "objectID": "slides/09-slides.html#exponential-distributions",
    "href": "slides/09-slides.html#exponential-distributions",
    "title": "Week 09:",
    "section": "Exponential Distributions",
    "text": "Exponential Distributions\nTaking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a \\(\\lambda = \\frac{1}{\\beta}\\) rate parameter\n\\[f(x)=\\frac{1}{\\beta}\\exp\\left[-x/\\beta\\right]\\]\nCioffa-Revilla (1984) uses an exponential distribution to model the stability of Italian governments."
  },
  {
    "objectID": "slides/09-slides.html#normal-distribution",
    "href": "slides/09-slides.html#normal-distribution",
    "title": "Week 09:",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nFinally, there’s the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter \\(\\mu\\) that determines the center of a distribution and a scale parameter \\(\\sigma^2\\) that determines the spread of a distribution\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]\\]\nStandard normal: \\(X \\sim N(\\mu =0,\\sigma^2=1)\\)\n\n\n\n\n\n\n\n\n\n\nAs we’ll see normal distributions tend to arise when ever you’re summing variables.\nThat is sum together a bunch of values from almost any distribution and the distribution of their sums tends to follow a normal distribution.\nSince lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)\n\nConsider a binomial distribution with N=100 and p=.5.\nThe pmf of this variable (black lollipops) follows a distribution that’s closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.\nA relationship explained more generally by the Central Limit Theorem, which we’ll cover next week.\n\n\n\n\n\n\n\n\n\nWhat’s the \\(p(X \\leq 0)\\) for a normal distirbution with mean 0 and sd 1\nSince the normal distribution is so common, it’s useful to get practice working with it’s pdf and cdf.\nConsider the following question: If X is normally distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\), what’s the probability that X is less than 0 \\(p(X\\leq0)=?\\) We could solve:\n\\[\\int_{-\\infty}^{0}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.5\\]\nBut R’s pnorm() function will quickly tell us\n\n\\(p(X\\leq0)=\\) 0.5\n\nAnd we can visualize this as follows:\n\n\n\n\n\n\n\n\n\nConsider some other questions?\n\n\\(p(X=0)=0\\)\n\nThe probability that a continuous variable is exactly some value is always 0.\n\n\\(p(X&lt;0)=0.5\\)\n\\(p(-1&lt; X&lt; 1)\\)\n\\(p(-2&lt; X&lt; 2)\\)\n\np(-1 &lt; X &lt; 1)\n\n\\(p(-1&lt; X&lt; 1)=pr(X&lt;1)-pr(X&lt;-1)\\)\n\n\\[\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.841-0.158=0.682\\]\n\n\n\n\n\n\n\n\n\np(-2 &lt; X &lt; 2)\n\n\\(p(-2&lt; X\\leq 2)=\\) 0.9544997\n\n\n\n\n\n\n\n\n\n\nWe’ll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0."
  },
  {
    "objectID": "slides/09-slides.html#expected-value",
    "href": "slides/09-slides.html#expected-value",
    "title": "Week 09:",
    "section": "Expected Value",
    "text": "Expected Value\nA (probability) weighted average of the possible outcomes of a random variable, often labeled \\(\\mu\\)\nDiscrete:\n\\[\\mu_X=E(X)=\\sum xp(x)\\]\nContinuous\n\\[\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx\\]"
  },
  {
    "objectID": "slides/09-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "href": "slides/09-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "title": "Week 09:",
    "section": "What’s the expected value of a 1 roll of fair die?",
    "text": "What’s the expected value of a 1 roll of fair die?\n\\[\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-expected-values",
    "href": "slides/09-slides.html#properties-of-expected-values",
    "title": "Week 09:",
    "section": "Properties of Expected Values",
    "text": "Properties of Expected Values\n\n\\(E(c)=c\\)\n\\(E(a+bX)=a+bE[X]\\)\n\\(E[E[X]]=X\\)\n\\(E[E[Y|X]]=E[Y]\\)\n\\(E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx\\)\n\\(E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)\\)\n\\(E[XY]=E[X]E[Y]\\) if \\(X\\) and \\(Y\\) are independent"
  },
  {
    "objectID": "slides/09-slides.html#variance",
    "href": "slides/09-slides.html#variance",
    "title": "Week 09:",
    "section": "Variance",
    "text": "Variance\nIf \\(X\\) has a finite mean \\(E[X]=\\mu\\), the \\(E[(X-\\mu)^2]\\) is finite and called the variance of \\(X\\) which we write as \\(\\sigma^2\\) or \\(Var[X]\\).\nNote:\n\\[\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}\\]\n\n“The variance of X is equal to the expected value of X-squared, minus the square of X’s expected value.”\n\\(\\sigma^2=E[X^2]-E[X]^2\\) is a useful identity in proofs and derivations"
  },
  {
    "objectID": "slides/09-slides.html#variance-and-standard-deviations",
    "href": "slides/09-slides.html#variance-and-standard-deviations",
    "title": "Week 09:",
    "section": "Variance and Standard Deviations",
    "text": "Variance and Standard Deviations\nWe often think of variances \\(Var[X]\\) as describing the spread of a distribution\n\\[\\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2\\]\nA standard deviation is just the square root of the variance\n\\[\\sigma=\\sqrt{Var[X]}\\]"
  },
  {
    "objectID": "slides/09-slides.html#covariance",
    "href": "slides/09-slides.html#covariance",
    "title": "Week 09:",
    "section": "Covariance",
    "text": "Covariance\nCovariance measures the degree to which two random variables vary together.\n\n\\(Cov[X,Y] \\to +\\) An increase in \\(X\\) tends to be larger than its mean when \\(Y\\) is larger than its mean\n\n\\[Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\\]"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-variance-and-covariance",
    "href": "slides/09-slides.html#properties-of-variance-and-covariance",
    "title": "Week 09:",
    "section": "Properties of Variance and Covariance",
    "text": "Properties of Variance and Covariance\n\n\\(Cov[X,Y]=E[XY]-E[X]E[Y]\\)\n\\(Var[X]=E[X^2]-(E[X])^2\\)\n\\(Var[X|Y]=E[X^2|Y]-(E[X|Y])^2\\)\n\\(Cov[X,Y]=Cov[X,E[Y|X]]\\)\n\\(Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]\\)\n\\(Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]\\)"
  },
  {
    "objectID": "slides/09-slides.html#correlation",
    "href": "slides/09-slides.html#correlation",
    "title": "Week 09:",
    "section": "Correlation",
    "text": "Correlation\n\nThe correlation between \\(X\\) and \\(Y\\) is simply the covariance of \\(X\\) and \\(Y\\) divided by the standard deviation of each.\n\n\\[\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}\\]\n\nNormalize covariance to a scale that runs between [-1,1]\n\nclass:inverse, center, middle # 💡 # The Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive",
    "title": "Week 09:",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we wanted to know the average height of our class.\nWe could pick someone at random, measure their height and get an estimate. It would be a pretty bad estimate (it would vary a lot from person to person), but it would be an unbiased estimate\nHow would we improve our estimate?"
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive-1",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive-1",
    "title": "Week 09:",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we increased our sample size from N=1 to N = 5.\nNow our estimate reflects the average of 5 people’s heights as opposed to just 1. Both are are unbiased estimates of the truth, but the N=5 sample has a lower variance.\n–\nNow suppose we took a sample of size N = N-1. That is we measured everyone except one person. Our estimate will be quite close to the truth, varying slightly based on the height of the person left out.\n–\nFinally we took a sample of size N = 24 (e.g. the class size). Since our sample is the population, our estimate will be exactly equal to to the population. Each sample will give us the same “true” value. That is, it wil not vary at all.\n–\nThe idea that as the sample size increases, the distance of a sample mean from the population mean \\(\\mu\\) goes to 0 is called the Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides.html#the-weak-law-of-large-numbers-formally",
    "href": "slides/09-slides.html#the-weak-law-of-large-numbers-formally",
    "title": "Week 09:",
    "section": "The (Weak) Law of Large Numbers (Formally)",
    "text": "The (Weak) Law of Large Numbers (Formally)\nLet \\(X_1, X_2, \\dots\\) be independent and identically distributed (i.i.d.) random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nThen for every \\(\\epsilon&gt;0\\), as the sample size increases (1), the distance of a sample mean from the population mean \\(\\mu\\) (2) goes to 0 (3).\n\\[\\overbrace{Pr(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon)}^{\\text{2. The distance of the sample mean from the truth}} \\overbrace{\\to 0}^{\\text{3. Goes to 0}} \\underbrace{\\text{ as }n \\to \\infty}_{\\text{1. As the sample size increases}}\\]\nEquivalently:\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]"
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-lln",
    "href": "slides/09-slides.html#simulating-the-lln",
    "title": "Week 09:",
    "section": "💪 Simulating the LLN",
    "text": "💪 Simulating the LLN\nRhe expected value of rolling a die 3.5.\n\\[ E[X] = \\Sigma x_ip(X=x_i) = 1/6 * (1+2+3+4+5+6)\\]\nIn terms of the LLN, think of our sample size as the number of times we roll a die.\nIf we rolled the die just once and took the average of our role, we could get a 1, 2, 3, 4, 5, or 6. which would be pretty far from our expected value of 3.5\nIf we rolled the die two times and took an average, we could still get an value of 1 or 6 for average, but values closer to our expected value of 3.5, happen more often\n\n# Calculate the average from 2 rows\ntable(rowMeans(expand.grid(1:6, 1:6)))\n\n\n  1 1.5   2 2.5   3 3.5   4 4.5   5 5.5   6 \n  1   2   3   4   5   6   5   4   3   2   1 \n\n\nAs we increase our sample size (roll the die more times), the LLN says the chance that our sample average is far from the truth \\((p(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon))\\), gets vanishingly small.\n\ndie &lt;- 1:6\nroll_fn &lt;- function(n) {\n  rolls &lt;- data.frame(rolls = sample(die, size = n, replace = TRUE))\n  # summarize rolls \n  df &lt;- rolls %&gt;%\n    summarise(\n    # number of rolls\n      n_rolls = n(),\n    # number of times 1 was rolled\n      ones = sum(rolls == 1),\n    # number of times 2 was rolled, etc..\n      twos = sum(rolls == 2),\n      threes = sum(rolls == 3),\n      fours = sum(rolls == 4),\n      fives = sum(rolls == 5),\n      sixes = sum(rolls == 6),\n      # Average of all our rolls\n      average =  mean(rolls),\n      # Absolute difference between averages and rolls\n      abs_error = abs(3.5-average)\n    )\n  # Return summary df\n  df\n}\n\nThen we could use a for-loop to simulate rolling our die once and calculating the average all the way up to rolling our die a 1000 times.\n\n# Holder\nsim_df &lt;- NULL\n\n# Set seed\nset.seed(123)\n\nfor(i in 1:1000){\n  sim_df &lt;- rbind(sim_df,\n                  roll_fn(i)\n  )\n}\n\nWith only a few rolls, our average bounces around a lot\n\nhead(sim_df)\n\n  n_rolls ones twos threes fours fives sixes  average abs_error\n1       1    0    0      1     0     0     0 3.000000 0.5000000\n2       2    0    0      1     0     0     1 4.500000 1.0000000\n3       3    0    2      0     0     0     1 3.333333 0.1666667\n4       4    0    0      1     1     1     1 4.500000 1.0000000\n5       5    1    1      1     0     1     1 3.400000 0.1000000\n6       6    3    0      2     1     0     0 2.166667 1.3333333\n\n\nWith a lot of rolls, our average is very close to 3.5\n\ntail(sim_df)\n\n     n_rolls ones twos threes fours fives sixes  average  abs_error\n995      995  197  160    151   154   171   162 3.430151 0.06984925\n996      996  184  164    176   149   175   148 3.412651 0.08734940\n997      997  163  159    170   163   171   171 3.534604 0.03460381\n998      998  162  163    142   173   185   173 3.576152 0.07615230\n999      999  209  154    151   154   163   168 3.412412 0.08758759\n1000    1000  181  189    147   179   146   158 3.394000 0.10600000\n\n\nLet’s visualize see how our average changes with the number of rolls, using ggplot()\n\np_die_lln &lt;- ggplot(sim_df, aes(n_rolls, average))+\n  geom_line()\n\n\nYour turn! Plot how the absolute value of the error changes as the number of rolls increases. Does it increase or decrease? How does the rate at which it goes up or down seem to change?\n\n# Write your code here:\n\nclass: inverse, center, middle #🦉 ## ICYI: Proving the Weak LLN"
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln",
    "href": "slides/09-slides.html#proving-the-weak-lln",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nA proof of the LLN is as follows:\nFirst define \\(U\\) such that its a sample mean for sample of size \\(n\\)\n\\[U=\\frac{X_1+\\dots +X_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln-1",
    "href": "slides/09-slides.html#proving-the-weak-lln-1",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen show that the sample mean, \\(U\\) is an unbiased estimator of the population mean \\(\\mu\\)\n\\[\\begin{align*}\nE[U]&=E[\\frac{X_1+\\dots +X_n}{n}]=\\frac{1}{n}E[X_1+\\dots +X_n]\\\\\n&=\\frac{n\\mu}{n}=\\mu\n\\end{align*}\\]\nWith a variance\n\\[\\begin{align*}\nVar[U]&=Var[\\frac{X_1+\\dots +X_n}{n}]=\\\\\n    &=Var[\\frac{X_1}{n}]\\dots Var[\\frac{+X_n}{n}]\\\\\n    &\\frac{\\sigma^2}{n^2}\\dots \\frac{\\sigma^2}{n^2}\\\\\n    &\\frac{n \\sigma^2}{n^2}\\\\\n    &\\frac{\\sigma^2}{n}\\\\\n\\end{align*}\\]\nThat decreases with N."
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln-2",
    "href": "slides/09-slides.html#proving-the-weak-lln-2",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen, by Chebyshev’s inequality, a theorem specifying, for a given distribution, the maximum fraction of values that can be some distance from that distribution’s mean:\n\\[Pr(\\left|U-\\mu\\right| &gt; \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2}\\]\nWhich \\(\\to 0\\) as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "slides/09-slides.html#the-strong-law-of-large-numbers",
    "href": "slides/09-slides.html#the-strong-law-of-large-numbers",
    "title": "Week 09:",
    "section": "The Strong Law of Large Numbers",
    "text": "The Strong Law of Large Numbers\nAs you may have inferred, there is a weak law of large numbers and a strong law of large numbers.\nThe weak law of large numbers states that as the sample size increases, the sample mean converges in probability to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]\nThe strong law of large numbers states that as the sample size increases, the sample mean converges almost surely to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n = \\mu|) = 1\\] The differences in types of convergence won’t matter much for us in this course\nclass:inverse, center, middle # Break\nclass:inverse, center, middle # 💡 ## The Central Limit Theorem\nSo the LLN tells us that as our sample size grows, an unbiased estimator like the sample average, will get increasingly close to the to the “true” value of the population of mean.\nIif we took a bunch of samples of the same size and calculated the mean of each sample:\n\nthe distribution of those sample means (the sampling distribution) would be centered around the truth (because the estimator is unbiased).\nthe width of the distribution (its variance) would decrease as we increased the size of each sample (by the LLN)\n\nThe Central Limit Theorem tells us about the shape of that distribution."
  },
  {
    "objectID": "slides/09-slides.html#review-z-scores-and-standardization",
    "href": "slides/09-slides.html#review-z-scores-and-standardization",
    "title": "Week 09:",
    "section": "Review: Z-scores and Standardization",
    "text": "Review: Z-scores and Standardization\nGiven a R.V. \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we can define a new R.V. \\(Z\\) as the standardization of \\(X\\):\n\\[Z=\\frac{X-\\mu}{\\sigma}\\]\nWhere Z has \\(\\mu=0\\) and \\(\\sigma=1\\)."
  },
  {
    "objectID": "slides/09-slides.html#notation-for-the-clt",
    "href": "slides/09-slides.html#notation-for-the-clt",
    "title": "Week 09:",
    "section": "Notation for the CLT",
    "text": "Notation for the CLT\nNext let’s define some variables \\(S\\) and \\(\\bar{X}\\) that are the sum \\((S)\\) and sample mean \\((\\bar{X})\\) of \\(n\\) iid draws of \\(X\\)\nLet \\(X_1,X_2,\\dots,X_n\\) be independent and identically distributed RVs with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nDefine \\(S_n\\) and \\(\\bar{X}_n\\) as follows:\n\\[S_n= X_1,X_2,\\dots,X_n= \\sum_{i=1}^n X_i\\]\n\\[\\bar{X}=\\frac{X_1,X_2,\\dots,X_n}{n}= \\frac{S_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides.html#additional-facts-for-the-clt",
    "href": "slides/09-slides.html#additional-facts-for-the-clt",
    "title": "Week 09:",
    "section": "Additional facts for the CLT",
    "text": "Additional facts for the CLT\nWe can show that:\n\\[\\begin{alignat*}{3}\nE[S_n]&=n\\mu \\hspace{2em}Var[S_n]&=n\\sigma^2 \\hspace{2em} \\sigma_S&=\\sqrt{n}\\sigma\\\\\nE[\\bar{X}_n]&=\\mu \\hspace{2em}Var[\\bar{X}_n]&=\\frac{\\sigma^2}{n} \\hspace{2em}\\sigma_{\\bar{X}}&=\\frac{\\sigma}{\\sqrt{n}}\\\\\n\\end{alignat*}\\]\nBasically: the expected value and variance of the sum is just \\(n\\) times the population parameters (the true values for the distribution).\nSince the mean is just the sum divided by the sample size, the expected value of the mean is equal to the population value and the variance and standard deviations of the mean are decreasing in \\(n\\).\nFinally, we can define \\(Z\\) to be a function of either \\(S\\) or \\(\\bar{X}\\)\n\\[Z_n=\\frac{S_n-n\\mu}{\\sqrt{n}\\sigma}=\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/09-slides.html#central-limit-theorem",
    "href": "slides/09-slides.html#central-limit-theorem",
    "title": "Week 09:",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nFor a sufficiently large \\(n\\)\n\\[\\begin{align*}\n\\bar{X_n}&\\approx N(\\mu,\\sigma^2/n) \\\\\n\\bar{S_n} &\\approx N(n\\mu,n\\sigma^2) \\\\\n\\bar{Z_n}&\\approx N(0,1)\n\\end{align*}\\]\n\nThe distribution of means \\((\\bar{X_n})\\) from almost any distribution \\(X\\) is approximately normal (converges in distribution), but with a smaller variance than (\\(\\sigma^2/n\\))\nProof: Several ways, but requires a little more math than is required for this course"
  },
  {
    "objectID": "slides/09-slides.html#clt-why-it-matters",
    "href": "slides/09-slides.html#clt-why-it-matters",
    "title": "Week 09:",
    "section": "CLT: Why it matters",
    "text": "CLT: Why it matters\nWhy is this result so important?\nWell lots of our questions come of the form, how does a typical value of Y vary with X.\nWe may not know the true underlying distribution of Y, but we can often approximate the distribution of a typical value of Y \\((E[Y])\\) using a normal distribution."
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-clt",
    "href": "slides/09-slides.html#simulating-the-clt",
    "title": "Week 09:",
    "section": "Simulating the CLT",
    "text": "Simulating the CLT\nFor almost any distribution, the distribution of means from a sample of that distribution will converge to some Normal distribution.\nLet’s consider a decidedly non-Normal Binomial distribution: with p = 0.2.\nThe expected value of Binomial Distribution \\(X \\sim B(n,p)\\) is \\(E[X] = n*p\\).\nIf we were to flip a coin 20 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 4.\nIf we were to flip a coin 100 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 20.\n\n\n\n\n\n\n\n\n\nSimulating 10,000 draws from Binomial Distributions of Different Sizes\n\n# Probability of success\np &lt;- .2\n# Sample sizes\nsamp_sizes &lt;- c(20, 50, 100,1000)\n# Number of simulations\nnsims &lt;- 10000\n# Holder for simulations\ndf_sim &lt;- tibble(\n  expand_grid(\n    samp_size = samp_sizes,\n    sim = 1:nsims,\n    sample_mean = NA\n  )\n)\n\nSimulating 1,000 draws from Binomial Distributions of Different Sizes\nBelow we loop through each sample size in samp_sizes\n\nfor(i in samp_sizes){\n  df_sim$sample_mean[df_sim$samp_size == i] &lt;- replicate(nsims, i*mean(rbinom(i, 1, p)))\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s consider a decided non normal distribution:\n\ndist &lt;- sample(18:80,size=10000, replace = T, prob = runif(length(18:80)))\n\nsamp_mean25 &lt;- replicate(10000,mean(sample(dist,25, replace=F)))\nsamp_mean100 &lt;- replicate(10000,mean(sample(dist,100, replace=F)))\nsamp_mean500 &lt;- replicate(10000,mean(sample(dist,500, replace=F)))\n\nex_df &lt;- tibble(\n  distribution = dist,\n  samp_mean25 = samp_mean25,\n  samp_mean100 = samp_mean100,\n  samp_mean500 = samp_mean500\n)"
  },
  {
    "objectID": "slides/09-slides.html#summary",
    "href": "slides/09-slides.html#summary",
    "title": "Week 09:",
    "section": "Summary",
    "text": "Summary\n\nSo we see that our sampling distributions are centered on the truth, and as the sample size increases, the width of the distribution decreases (Law of Large Numbers)\nThe shapes of distributions of sample means can be approximated by a Normal Distribution \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\)\n\nclass: inverse, center, middle #🦉 ## ICYI: Maximum Likelihood Estimation"
  },
  {
    "objectID": "slides/09-slides.html#maximum-likelihood-estimation-1",
    "href": "slides/09-slides.html#maximum-likelihood-estimation-1",
    "title": "Week 09:",
    "section": "🦉 Maximum Likelihood Estimation",
    "text": "🦉 Maximum Likelihood Estimation\nFormally, consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as\n\\[\\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta)\\]\nwhere \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\).\nMLE tries to find \\(\\hat{\\theta}_{MLE}\\) that maximizes \\(\\mathcal{L}(\\theta \\mid X)\\)"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-maximum-likelihood-estimators",
    "href": "slides/09-slides.html#properties-of-maximum-likelihood-estimators",
    "title": "Week 09:",
    "section": "🦉 Properties of Maximum Likelihood Estimators",
    "text": "🦉 Properties of Maximum Likelihood Estimators\nMLE Estimators are\n\nFunctionally Invariant (The “Plug in Principle”)\n\nIf \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\) than then the MLE of some function of \\(\\theta\\), \\(f(\\theta)\\) is \\(f(\\hat\\theta_{MLE})\\)\nIf we have the MLE of the variance, the square root of this will give us the MLE of the standard deviation\n\nConsistent (by the LLN)\n\n\\(\\hat\\theta_{MLE}\\) collapses to a spike over \\(\\theta\\) as \\(n \\to \\infty\\)\n\nAsympotically Normal (by the CLT)\n\nA \\(n \\to \\infty\\) the sampling distribution of \\(\\hat\\theta_{MLE}\\) becomes Normally distributed\nMakes calculating quantities for inference easy\n\nAsympotically Efficient\n\nAs \\(n \\to \\infty\\), \\(\\hat\\theta_{MLE}\\) tends to be the estimator with the lowest error\n\n\nclass: inverse, center, middle # 💡 # Generalized Linear Models"
  },
  {
    "objectID": "slides/09-slides.html#generalized-linear-models",
    "href": "slides/09-slides.html#generalized-linear-models",
    "title": "Week 09:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nOLS provides a linear estimate to the conditional mean function\n\n–\n\nIf the conditional mean function is linear and the errors are normally distributed, OLS is the MLE.\n\n–\n\nWhat if the conditional mean function is non-linear?\n\n–\n\nSometimes we can transform the mean function so that it is linear, and estimate a generalized linear model (GLM) using MLE\n\n–\n\nUsing a GLM often produces more “reasonable” estimates, and can make more efficient use of the data, although there are many cases where a linear estimate to conditional mean function works just fine (or better)"
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nWe can think some variable \\(y\\) as having a distribution \\(f\\) that contains both a stochastic (random) and systematic components\n\\[\\begin{aligned}\n\\text{Stochastic:    }&& y \\sim f(\\mu,\\alpha)\\\\\n\\text{Systematic:    }&&\\mu = g(X\\beta)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models-1",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models-1",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nIn the past we’ve described the process of modeling \\(y\\) using a linear regression:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nand with multiple predictors:\n\\[y = X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models-2",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models-2",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nWe haven’t really talked about the distribution of \\(\\epsilon\\), in part because OLS doesn’t require any distributional assumptions to be unbiased.\nBut if we assumed \\(\\epsilon\\) are normally distributed, with mean 0 and variance \\(\\sigma^2\\)\n\\[\\epsilon \\sim f_\\mathcal{N}(0,\\sigma^2)\\]\nThen we could write our model for \\(y\\) as follows:\n\\[\\begin{aligned} y &\\sim f_{\\mathcal{N}}(\\mu,\\sigma^2)\\\\\n\\mu &= X\\beta\\end{aligned}\\]\nWhere the systematic component of why is modeled by \\(X\\beta\\) (i.e. g() is the identity function), with errors that are Normally distributed.\nThe \\(\\beta\\)s that OLS estimates turn out to be the same values that would get by maximizing the likelihood of this function, given our data, \\(X\\), assuming normally distributed errors."
  },
  {
    "objectID": "slides/09-slides.html#generalized-linear-models-1",
    "href": "slides/09-slides.html#generalized-linear-models-1",
    "title": "Week 09:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nBut what if our outcome doesn’t follow a normal distribution?\nSay for example, we have a binary outcome,that we think follows a Bernoulli distribution with \\(\\pi\\) probability of success.\nWe could model the systematic portion of this using the logistic function, \\(g()\\)\n\\[\\begin{aligned}y &\\sim f_{Bern}(\\pi)\\\\\n\\pi &= \\frac{1}{1+\\exp(-{X\\beta})}\\end{aligned}\\]\nAgain, we could estimate \\(\\beta\\) using the MLE to fit a logistic regression."
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models-3",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models-3",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nOr if we had a count variable, we might use a Poisson distribution:\n\\[\\begin{aligned}y &\\sim f_{Pois}(\\lambda)\\\\\n\\lambda &= \\exp(X\\beta)\\end{aligned}\\]\nAgain estimating \\(\\beta\\) using MLE.\nIn this class, we’ll let R handle mechanics of actually fitting these models, and instead focus on interpreting their substantive differences"
  },
  {
    "objectID": "slides/09-slides.html#ols-vs-logistic-regression",
    "href": "slides/09-slides.html#ols-vs-logistic-regression",
    "title": "Week 09:",
    "section": "OLS vs Logistic Regression",
    "text": "OLS vs Logistic Regression\nOne situation where we’d use MLE is the case of binary responses variable coded using \\(0\\) and \\(1\\).\nIn practice, these \\(0\\) and \\(1\\)s will code for two classes such as yes/no, non-voter/voter,, etc.\nHow should we model this relationship?\nWe could use OLS to produce a linear estimate of the conditional mean function \\((\\text{E}[Y \\mid {\\bf X} = {\\bf x}])\\), by finding \\(\\beta\\)s that minimize the sum of squared errors\nOr\nWe could use a logistic regression, to produce a linear estimate of the “log-odds” of the conditional mean function of our binary variable by finding \\(\\beta\\)s that maximize the likelihood of this function.\nLet’s simulate data from the following model:\n\\[\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -2 + 3 x\\]\nWe’ll codify this into a function:\n\nsim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {\n  x = rnorm(n = sample_size)\n  eta = beta_0 + beta_1 * x\n  p = 1 / (1 + exp(-eta))\n  y = rbinom(n = sample_size, size = 1, prob = p)\n  data.frame(y, x)\n}\n\nAnd use it to generate some data\n\nset.seed(1)\nexample_data = sim_logistic_data()\nhead(example_data)\n\n  y          x\n1 0 -0.6264538\n2 1  0.1836433\n3 0 -0.8356286\n4 1  1.5952808\n5 0  0.3295078\n6 0 -0.8204684\n\n\nAfter simulating a dataset, we’ll then fit both ordinary linear regression and logistic regression.\n\n# ordinary linear regression\nfit_lm  = lm(y ~ x, data = example_data)\n# logistic regression\nfit_glm = glm(y ~ x, data = example_data, family = binomial)\n\nNotice that the syntax is extremely similar. What’s changed?\n\nlm() has become glm()\nWe’ve added family = binomial argument\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.31**\n\n\n-2.31*\n\n\n\n\n \n\n\n(0.08)\n\n\n(1.13)\n\n\n\n\nx\n\n\n0.30**\n\n\n3.66*\n\n\n\n\n \n\n\n(0.09)\n\n\n(1.65)\n\n\n\n\nR2\n\n\n0.34\n\n\n \n\n\n\n\nAdj. R2\n\n\n0.31\n\n\n \n\n\n\n\nNum. obs.\n\n\n25\n\n\n25\n\n\n\n\nAIC\n\n\n \n\n\n22.74\n\n\n\n\nBIC\n\n\n \n\n\n25.18\n\n\n\n\nLog Likelihood\n\n\n \n\n\n-9.37\n\n\n\n\nDeviance\n\n\n \n\n\n18.74\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nMaking predictions with an object of type glm is slightly different than making predictions after fitting with lm().\nIn the case of logistic regression, with family = binomial, we have:\n\n\n\n\n\n\n\ntype\nReturned\n\n\n\n\n\"link\" [default]\n\\(\\hat{\\eta}({\\bf x}) = \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right)\\)\n\n\n\"response\"\n\\(\\hat{p}({\\bf x})\\)\n\n\n\nThat is, type = \"link\" will get you the log odds, while type = \"response\" will return \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) for each observation.\n\nplot(y ~ x, data = example_data, \n     pch = 20, ylab = \"Estimated Probability\", \n     main = \"Ordinary vs Logistic Regression\")\nabline(fit_lm, col = \"darkorange\")\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \n      add = TRUE, col = \"dodgerblue\", lty = 2)\nlegend(\"topleft\", c(\"Ordinary\", \"Logistic\", \"Data\"), lty = c(1, 2, 0), \n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))"
  },
  {
    "objectID": "slides/09-slides.html#ols-vs-logistic-regression-1",
    "href": "slides/09-slides.html#ols-vs-logistic-regression-1",
    "title": "Week 09:",
    "section": "OLS vs Logistic Regression",
    "text": "OLS vs Logistic Regression\n\nOLS produces impossible predictions\nThe coefficients from logistic regression aren’t directly interpertable \\(\\to\\) need predicted values.\n\nCan also calculate things like odds-ratios but I find this convoluted.\n\nThe marginal effect of \\(X\\) varies in a logistic regression"
  },
  {
    "objectID": "slides/09-slides.html#interpreting-logistic-regression-coefficients",
    "href": "slides/09-slides.html#interpreting-logistic-regression-coefficients",
    "title": "Week 09:",
    "section": "Interpreting Logistic Regression Coefficients",
    "text": "Interpreting Logistic Regression Coefficients\nOur estimated model is then:\n\\[\\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right) = -2.3 + 3.7 x\\]\nBecause we’re not directly estimating the mean, but instead a function of the mean, we need to be careful with our interpretation of \\(\\hat{\\beta}_1 = 3.7\\).\nThis means that, for a one unit increase in \\(x\\), the log odds change (in this case increase) by \\(3.7\\). Also, since \\(\\hat{\\beta}_1\\) is positive, as we increase \\(x\\) we also increase \\(p({\\bf x})\\).\nFor example, we have:\n\\[\\hat{P}[Y = 1 \\mid X = -0.5] = \\frac{e^{-2.3 + 3.7 \\cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \\cdot (-0.5)}} \\approx 0.016\\]\n\npredict(fit_glm, newdata = data.frame(x=-0.5), type = \"response\")\n\n         1 \n0.01567416 \n\n\n\\[\\hat{P}[Y = 1 \\mid X = 0] = \\frac{e^{-2.3 + 3.7 \\cdot (0)}}{1 + e^{-2.3 + 3.7 \\cdot (0)}} \\approx 0.09\\]\n\npredict(fit_glm, newdata = data.frame(x=0), type = \"response\")\n\n         1 \n0.09016056 \n\n\n\\[\\hat{P}[Y = 1 \\mid X = 1] = \\frac{e^{-2.3 + 3.7 \\cdot (1)}}{1 + e^{-2.3 + 3.7 \\cdot (1)}} \\approx 0.38\\]\n\npredict(fit_glm, newdata = data.frame(x=.5), type = \"response\")\n\n        1 \n0.3814476 \n\n\nbackground-image:url(“https://resourcemoon.com/wp-content/uploads/2018/09/summery.png”) background-size:cover"
  },
  {
    "objectID": "slides/09-slides.html#summary-1",
    "href": "slides/09-slides.html#summary-1",
    "title": "Week 09:",
    "section": "Summary",
    "text": "Summary\n\nThe Law of Large Number’s says that as our sample size increases, our sample mean will converge to the population value\nThe Central Limit Theorem says that the distribution of those sample means will follow a normal distribution\nGeneralized Linear Models allow us to more accurately model different types of data-generating processes using Maxium Likelihood Estimation.\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/01-slides.html#class-plan",
    "href": "slides/01-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nLogistics (15 minutes)\n\nAnnouncements\nFeedback\n\nClass plan (60 minutes)\n\nIntroduction to R, R Studio and Quarto\nLoading and Looking at Data in R\nTransforming, Recoding, and Cleaning Data in R\nDescribing Data in R\nExploring Covid-19 Data for Lab"
  },
  {
    "objectID": "slides/01-slides.html#annoucements",
    "href": "slides/01-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nIf it’s your first time here you’ll need to work through Software Setup to follow along today\n\nTalk to me after class if you’re having installation issues\n\nIf you’re still on the waitlist on CAB, speak to me after class"
  },
  {
    "objectID": "slides/01-slides.html#announcements",
    "href": "slides/01-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\n\n“Uh ohhh, the Cavs are playing playoff basketball” pic.twitter.com/WrOrzeuEtW\n\n— Bottlegate ((Bottlegate?)) April 21, 2017"
  },
  {
    "objectID": "slides/01-slides.html#tutorials",
    "href": "slides/01-slides.html#tutorials",
    "title": "POLS 1600",
    "section": "Tutorials",
    "text": "Tutorials\nOnce you’ve done the following\n\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nYou can see the available problem sets by running the following code in your console:\n\nlearnr::run_tutorial(package = \"qsslearnr\")\n\nAvailable tutorials:\n* qsslearnr\n  - 00-intro          : \"QSS Tutorial 0: Introduction to R\"\n  - 01-measurement1   : \"QSS Tutorial 1: Measurment I\"\n  - 02-measurement2   : \"QSS Tutorial 2: Measurment II\"\n  - 03-causality1     : \"QSS Tutorial 3: Causality I\"\n  - 04-causality2     : \"QSS Tutorial 4: Causality II\"\n  - 05-prediction1    : \"QSS Tutorial 5: Prediction I\"\n  - 06-prediction2    : \"QSS Tutorial 6: Prediction II\"\n  - 07-probability1   : \"QSS Tutorial 7: Probability I\"\n  - 08-probability2   : \"QSS Tutorial 8: Probability II\"\n  - 09-uncertainty1   : \"QSS Tutorial 9: Uncertainty I\"\n  - 10-uncertainty2   : \"QSS Tutorial 10: Uncertainty II\"\n  - tidy-01-causality : \"QSS Review Tutorial for Tidyverse Users\""
  },
  {
    "objectID": "slides/01-slides.html#tutorials-1",
    "href": "slides/01-slides.html#tutorials-1",
    "title": "POLS 1600",
    "section": "Tutorials",
    "text": "Tutorials\nAnd start a specific tutorial by running:\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nPlease upload tutorials 00-intro and 01-measurement1 to Canvas by Friday"
  },
  {
    "objectID": "slides/01-slides.html#youve-committed-a-murder",
    "href": "slides/01-slides.html#youve-committed-a-murder",
    "title": "POLS 1600",
    "section": "You’ve committed a murder",
    "text": "You’ve committed a murder"
  },
  {
    "objectID": "slides/01-slides.html#why",
    "href": "slides/01-slides.html#why",
    "title": "POLS 1600",
    "section": "Why",
    "text": "Why"
  },
  {
    "objectID": "slides/01-slides.html#why-do-you-ask",
    "href": "slides/01-slides.html#why-do-you-ask",
    "title": "POLS 1600",
    "section": "Why do you ask?",
    "text": "Why do you ask?\nIt’s cousin Nick’s fault…\n\nFunny icebreaker, but lots of assumptions…\n\nYou’re not a murderer\nYou don’t know someone who’s committed a murder or been murdered\nYou’ve got a mom and dad"
  },
  {
    "objectID": "slides/01-slides.html#why-do-you-ask-1",
    "href": "slides/01-slides.html#why-do-you-ask-1",
    "title": "POLS 1600",
    "section": "Why do you ask?",
    "text": "Why do you ask?\n\nHow might we make this question better?\n\nUse a screener question\n\n“Would you feel comfortable…”\n\n“Pipe” in responses from a prior question\n\n“Who are two people who raised you…”\n\n\n\n\nWhat questions we ask and how we ask them matters"
  },
  {
    "objectID": "slides/01-slides.html#hopes-and-dreams-fears-and-worries",
    "href": "slides/01-slides.html#hopes-and-dreams-fears-and-worries",
    "title": "POLS 1600",
    "section": "Hopes and Dreams, Fears and Worries",
    "text": "Hopes and Dreams, Fears and Worries\n\n\nWhat are we excited about?\n\nEngaging with social science\nLearning statistics and math\nLearning to code\n\n\nWhat are weworried about?\n\nEngaging with social science\nLearning statistics and math\nLearning to code"
  },
  {
    "objectID": "slides/01-slides.html#overview-1",
    "href": "slides/01-slides.html#overview-1",
    "title": "POLS 1600",
    "section": "Overview",
    "text": "Overview\n\nR, R Studio and Quarto\nGetting set up to work in R\nBasic Programming in R"
  },
  {
    "objectID": "slides/01-slides.html#r-r-studio-and-quarto",
    "href": "slides/01-slides.html#r-r-studio-and-quarto",
    "title": "POLS 1600",
    "section": "R, R Studio and Quarto",
    "text": "R, R Studio and Quarto\n\nR is an open source statistical programming language (cheatsheet)\nR Studio is an integrated development environment (IDE) that makes working in R much easier (cheatsheet)\nQuarto is a publishing system that allows us to write and present code in different formats (cheatsheet)"
  },
  {
    "objectID": "slides/01-slides.html#general-tuesday-workflow",
    "href": "slides/01-slides.html#general-tuesday-workflow",
    "title": "POLS 1600",
    "section": "General Tuesday Workflow",
    "text": "General Tuesday Workflow\n\nGo to https://pols1600.paultesta.org\nGo to class content for current week\nOpen slides in browser\nOpen R Studio\nCreate .qmd file titled wk01-notes.qmd and save in course folder\nGet set up to work\nTake notes and follow along"
  },
  {
    "objectID": "slides/01-slides.html#lets-create-a-.qmd-file",
    "href": "slides/01-slides.html#lets-create-a-.qmd-file",
    "title": "POLS 1600",
    "section": " Let’s create a .qmd file",
    "text": "Let’s create a .qmd file"
  },
  {
    "objectID": "slides/01-slides.html#three-components-of-a-.qmd",
    "href": "slides/01-slides.html#three-components-of-a-.qmd",
    "title": "POLS 1600",
    "section": "Three components of a .qmd",
    "text": "Three components of a .qmd\n\n\nControl output with YAML header\n\n  ---\n  title: \"Title here\"\n  author: \"Your name\"\n  format:\n    html:\n      toc: true\n  ---\n\nWrite code Blocks/Chunks\n\n```{r}\n#| echo: true\n2+2\n```\n\nDescribe code using Markdown\n\nSee Help &gt; Markdown quick reference"
  },
  {
    "objectID": "slides/01-slides.html#the-basics-of-r",
    "href": "slides/01-slides.html#the-basics-of-r",
    "title": "POLS 1600",
    "section": "The Basics of R",
    "text": "The Basics of R\n\nR is an interpreter (&gt;)\n“Everything that exists in R is an object”\n“Everything that happens in R is the result of a function”\nData come in different types, shapes, and sizes\nPackages make R great"
  },
  {
    "objectID": "slides/01-slides.html#r-is-an-interpreter",
    "href": "slides/01-slides.html#r-is-an-interpreter",
    "title": "POLS 1600",
    "section": "R is an interpreter (>)",
    "text": "R is an interpreter (&gt;)\nEnter commands line-by-line in the console\n\nThe &gt; means R is a ready for a command\nThe + means your last command isn’t complete\n\nIf you get stuck with a + use your escape key!\n\nSend code from .qmd file to the console:\n\ncntrl + Enter (PC) | cmd + Return (Mac) -&gt; run current line\ncntrl + shift + Enter (PC) | cmd + shift  + Return (Mac) -&gt; run all code in current chunk"
  },
  {
    "objectID": "slides/01-slides.html#r-is-a-calculator",
    "href": "slides/01-slides.html#r-is-a-calculator",
    "title": "POLS 1600",
    "section": "R is a Calculator",
    "text": "R is a Calculator\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n+\naddition\nx + y\n\n\n-\nsubtraction\nx - y\n\n\n*\nmultiplication\nx * y\n\n\n/\ndivision\nx / y\n\n\n^\nraised to the power of\nx ^ y\n\n\nabs\nabsolute value\nabs(x)\n\n\n%/%\ninteger division\nx %/% y\n\n\n%%\nremainder after division\nx %% y"
  },
  {
    "objectID": "slides/01-slides.html#r-is-logical",
    "href": "slides/01-slides.html#r-is-logical",
    "title": "POLS 1600",
    "section": "R is logical",
    "text": "R is logical\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n&\nand\nx & y\n\n\n|\nor\nx | y\n\n\nxor\nexactly x or y\nxor(x, y)\n\n\n!\nnot\n!x"
  },
  {
    "objectID": "slides/01-slides.html#r-is-logical-1",
    "href": "slides/01-slides.html#r-is-logical-1",
    "title": "POLS 1600",
    "section": "R is logical",
    "text": "R is logical\n\nx &lt;- T; y &lt;- F\n\nx == T\n\n[1] TRUE\n\nx == T & y == T\n\n[1] FALSE\n\nx == T | y == T\n\n[1] TRUE\n\n!x\n\n[1] FALSE"
  },
  {
    "objectID": "slides/01-slides.html#r-can-make-comparisons",
    "href": "slides/01-slides.html#r-can-make-comparisons",
    "title": "POLS 1600",
    "section": "R can make comparisons",
    "text": "R can make comparisons\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n&lt;\nless than\nx &lt; y\n\n\n&lt;=\nless than or equal to\nx &lt;= y\n\n\n&gt;\ngreater than\nx &gt; y\n\n\n&gt;=\ngreater than or equal to\nx &gt;= y\n\n\n==\nexactly equal to\nx == y\n\n\n!=\nnot equal to\nx != y\n\n\n%in%\ngroup membership*\nx %in% y\n\n\nis.na\nis missing\nis.na(x)\n\n\n!is.na\nis not missing\n!is.na(x)"
  },
  {
    "objectID": "slides/01-slides.html#everything-that-exists-in-r-is-an-object",
    "href": "slides/01-slides.html#everything-that-exists-in-r-is-an-object",
    "title": "POLS 1600",
    "section": "Everything that exists in R is an object",
    "text": "Everything that exists in R is an object\n\n\nThe number 5 is an object in R\n\n\n5\n\n[1] 5\n\n\n\nWe can assign the object 5, the name x, using the assignment operator &lt;-+\n\n\nx &lt;- 5 # Read this as \"x gets 5\"\n\n\nNow if we tell R to show us x, we’ll get\n\n\nx\n\n[1] 5\n\nprint(x)\n\n[1] 5"
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-types",
    "href": "slides/01-slides.html#data-come-in-different-types",
    "title": "POLS 1600",
    "section": "Data come in different types",
    "text": "Data come in different types"
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-types-1",
    "href": "slides/01-slides.html#data-come-in-different-types-1",
    "title": "POLS 1600",
    "section": "Data come in different types",
    "text": "Data come in different types\n\n\n\n# Create some data\n\n# Numeric\nx &lt;- 2 # Double\ny &lt;- 6L # Integer\n\n# Logical\nonly_two_types_of_people &lt;- TRUE \n\n# Character\nme &lt;- \"Paul\"\n\n# Factor\ngrades = factor(c(\"A\",\"B\",\"C\"))\n\n\n\n# What type are they?\nclass(x)\n\n[1] \"numeric\"\n\nclass(y)\n\n[1] \"integer\"\n\nclass(only_two_types_of_people)\n\n[1] \"logical\"\n\nclass(me)\n\n[1] \"character\"\n\nclass(grades)\n\n[1] \"factor\""
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-shapes-and-sizes",
    "href": "slides/01-slides.html#data-come-in-different-shapes-and-sizes",
    "title": "POLS 1600",
    "section": "Data come in different “shapes” and “sizes”",
    "text": "Data come in different “shapes” and “sizes”\n\nSource: Gaurav Tiwari"
  },
  {
    "objectID": "slides/01-slides.html#section-1",
    "href": "slides/01-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Name\n“Size”\nType of Data\nR code\n\n\n\n\nscalar\n1\nnumeric, character, factor, logical\nx &lt;- 5\n\n\nvector\nN elements: length(x)\nall the same\nv &lt;- c(1, 2, T, \"false\")\n\n\nmatrix\nN rows by columns K: dim(x)\nall the same\nm &lt;- matrix(y,2,2)\n\n\narray\nN row by K column by J dimensions: dim(x)\nall the same\na &lt;- array(m,c(2,2,3))\n\n\ndata frames\nN row by K column matrix\ncan be different\nd &lt;-data.frame(x=x, y=y)\n\n\ntibbles\nN row by K column matrix\ncan be different\nd &lt;-tibble(x=x, y=y)\n\n\nlists\ncan vary\ncan be different\nl &lt;-list(x,y,m,a,d)"
  },
  {
    "objectID": "slides/01-slides.html#everything-that-happens-in-r-is-the-result-of-a-function",
    "href": "slides/01-slides.html#everything-that-happens-in-r-is-the-result-of-a-function",
    "title": "POLS 1600",
    "section": "Everything that happens in R is the result of a function",
    "text": "Everything that happens in R is the result of a function\n\nYou’ve already seen and used some R functions\n\nthe &lt;- is the assignement operator that assigns a value to a name\nc() is the combine function that combines elements together\ninstall.packages() installs packages\nlibrary() loads packages you’ve installed so you can use functions and data that are part of that package"
  },
  {
    "objectID": "slides/01-slides.html#three-sources-of-functions",
    "href": "slides/01-slides.html#three-sources-of-functions",
    "title": "POLS 1600",
    "section": "Three sources of functions",
    "text": "Three sources of functions\nThree sources of functions:\n\nbase R\n\n&lt;-; mean(x); library(\"package_name\")\n\npackages\n\ninstall.packages(\"packageName)\"\nremotes::intall_github(\"user/repository\")\n\nYou\n\nmy_function &lt;- function(x){x^2}"
  },
  {
    "objectID": "slides/01-slides.html#functions-are-like-recipes",
    "href": "slides/01-slides.html#functions-are-like-recipes",
    "title": "POLS 1600",
    "section": "Functions are like recipes",
    "text": "Functions are like recipes\nThey have:\n\n\n\nnames\ningredients (inputs)\nsteps that tell you what to do with the ingredients (statements/code)\ntasty results from applying those steps to given ingredients (outputs)\n\n\n (Source)"
  },
  {
    "objectID": "slides/01-slides.html#can-i-kick-it",
    "href": "slides/01-slides.html#can-i-kick-it",
    "title": "POLS 1600",
    "section": "Can I kick it?",
    "text": "Can I kick it?\n\ncan_x_kick_it &lt;- function(x){\n  # Determine if x can kick it\n  # If x in A Tribe Called Quest\n  if(x %in% c(\"Q-Tip\",\"Phife Dawg\",\n              \"Ali Shaheed Muhammad\", \n              \"Jarobi White\")){\n    return(\"Yes you can\")\n  }else{\n    return(\"Before this, did you really know what live was?\")\n  }\n\n}\ncan_x_kick_it(\"Q-Tip\")\n\n[1] \"Yes you can\"\n\ncan_x_kick_it(\"Paul\")\n\n[1] \"Before this, did you really know what live was?\""
  },
  {
    "objectID": "slides/01-slides.html#getting-setup-to-work-in-r",
    "href": "slides/01-slides.html#getting-setup-to-work-in-r",
    "title": "POLS 1600",
    "section": "Getting setup to work in R",
    "text": "Getting setup to work in R\nEach time you start a project in R, you will want to:\n\nSet your working directory\nLoad (and if needed, install) the R packages you will use\nSet any “global” options you want\nLoad the data you’ll be using"
  },
  {
    "objectID": "slides/01-slides.html#set-your-working-directory",
    "href": "slides/01-slides.html#set-your-working-directory",
    "title": "POLS 1600",
    "section": "Set your working directory",
    "text": "Set your working directory"
  },
  {
    "objectID": "slides/01-slides.html#load-and-if-needed-install-the-r-packages-you-will-use",
    "href": "slides/01-slides.html#load-and-if-needed-install-the-r-packages-you-will-use",
    "title": "POLS 1600",
    "section": "Load (and if needed, install) the R packages you will use",
    "text": "Load (and if needed, install) the R packages you will use\n\n\nInstall packages once1 with install.packages(\"package_name\")\nLoad packages every session with library(\"package_name\")\n\n\nOccasionally, you’ll have to update packages to newer versions and will likely need to reinstall when you upgrade R"
  },
  {
    "objectID": "slides/01-slides.html#install-packages-for-the-lab",
    "href": "slides/01-slides.html#install-packages-for-the-lab",
    "title": "POLS 1600",
    "section": " Install packages for the lab",
    "text": "Install packages for the lab"
  },
  {
    "objectID": "slides/01-slides.html#install-packages-for-the-lab-1",
    "href": "slides/01-slides.html#install-packages-for-the-lab-1",
    "title": "POLS 1600",
    "section": "Install packages for the lab",
    "text": "Install packages for the lab\nLet’s install the tidyverse and COVID19.\n\n\nCreate a new code chunk\nLabel it libraries\nCopy and paste the following into your console\n\n\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"COVID19\")\n\n\n\nOnce you’ve installed these packages comment out the code\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"COVID19\")\n\n\n\n\n\n\n\nKeyboard Shortcuts to toggle # comments\n\n\nmacOS: CMD + SHIFT + C\nPC: CTRL + SHIFT + C"
  },
  {
    "objectID": "slides/01-slides.html#loading-the-tidyverse-and-covid19-packages",
    "href": "slides/01-slides.html#loading-the-tidyverse-and-covid19-packages",
    "title": "POLS 1600",
    "section": "Loading the tidyverse and COVID19 packages",
    "text": "Loading the tidyverse and COVID19 packages\n\nType the following into your code chunk:\n\n\nlibrary(\"tidyverse\")\nlibrary(COVID19)"
  },
  {
    "objectID": "slides/01-slides.html#set-any-global-options-you-want",
    "href": "slides/01-slides.html#set-any-global-options-you-want",
    "title": "POLS 1600",
    "section": "Set any “global” options you want",
    "text": "Set any “global” options you want\nHere are the global options for these slides:1\n\n# Options for these slides\nknitr::opts_chunk$set(\n  warning = FALSE,       # Don't display warnings\n  message = FALSE,       # Don't display messages\n  comment = NA,          # No prefix before line of text\n  dpi = 300,             # Figure resolution\n  fig.align = \"center\",  # Figure alignment\n  out.width = \"80%\",     # Figure width\n  cache = FALSE          # Don't cache code chunks\n  )\n\nI’ll generally do this for you, but it’s useful to know that you can change options globally (for every chunk) and locallys (for specific chunks)"
  },
  {
    "objectID": "slides/01-slides.html#load-the-data-youll-be-using",
    "href": "slides/01-slides.html#load-the-data-youll-be-using",
    "title": "POLS 1600",
    "section": "Load the data you’ll be using",
    "text": "Load the data you’ll be using\nThere are three ways to load data.\n\nLoad a pre-existing dataset\n\ndata(\"dataset\") will load the dataset named “dataset”\ndata() will list all datasets\n\nLoad a .Rdata/.rda file using load(\"dataset.rda\")\nRead data of a different format (.csv, .dta, .spss) into R using specific functions from packages like haven and readr"
  },
  {
    "objectID": "slides/01-slides.html#overview-working-with-data-in-r",
    "href": "slides/01-slides.html#overview-working-with-data-in-r",
    "title": "POLS 1600",
    "section": "Overview: Working with Data in R",
    "text": "Overview: Working with Data in R\n\nLoading data into R\nLooking at your data\nCleaning and transforming your data"
  },
  {
    "objectID": "slides/01-slides.html#loading-data-into-r",
    "href": "slides/01-slides.html#loading-data-into-r",
    "title": "POLS 1600",
    "section": "Loading data into R",
    "text": "Loading data into R\nThere are three ways to load data.\n\nLoad a pre-existing dataset\n\ndata(\"dataset\") will load the dataset named “dataset”\ndata() will list all datasets\nUseful for tutorials, working through examples/help\n\nLoad a .Rdata/.rda file using load(\"dataset.rda\")\nRead data of a different format (.csv, .dta, .spss) into R using specific functions\n\nWe will use functions from the haven and readr packages to read data from the web and stored locally on your computer"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data",
    "href": "slides/01-slides.html#loading-state-level-covid-data",
    "title": "POLS 1600",
    "section": " Loading state-level Covid data",
    "text": "Loading state-level Covid data"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data-1",
    "href": "slides/01-slides.html#loading-state-level-covid-data-1",
    "title": "POLS 1600",
    "section": "Loading state-level Covid data",
    "text": "Loading state-level Covid data\nThe code below downloads two years of daily state-level Covid data:\n\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  start = \"2020-01-01\",\n  end = \"2022-12-31\",\n  level = 2,\n  verbose = F\n    \n)\n\nPlease run the following1\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n\n30 people trying to download this data live sometimes causes errors with the server"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data-2",
    "href": "slides/01-slides.html#loading-state-level-covid-data-2",
    "title": "POLS 1600",
    "section": "Loading state-level Covid data",
    "text": "Loading state-level Covid data\n\n\ncountry = US tells the function we want data for the US\nstart = \"2020-01-01\" sets the start date for the data\nstart = \"2020-01-01\" sets the end date for the data\nlevel = 2 tells the function we want state-level data\nverbose = F tells the function not to print other stuff\n\n\n\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  start = \"2020-01-01\",\n  end = \"2022-12-31\",\n  level = 2,\n  verbose = F\n    \n)"
  },
  {
    "objectID": "slides/01-slides.html#looking-at-your-data",
    "href": "slides/01-slides.html#looking-at-your-data",
    "title": "POLS 1600",
    "section": "Looking at your data",
    "text": "Looking at your data\nAnytime you load data into R, try some combination of the following to get a high-level overview (HLO) of the data\n\ndim(data) gives you the dimensions (# of rows and columns)\nView(data) opens data in a separate pane\nprint(data); data will display a truncated view of data in your console\nglimpse(data) will show a transposed (switch columns and rows) version of data with information on variable type\nhead(data) shows you the first 5 rows\ntail(data) shows you the last 5 rows\ndata$variable extracts variable from data\ntable(data$variable) creates a frequency table\n\nGood for categorical data\n\nsummary(data$variable) summary statistics\n\nGood for numeric data"
  },
  {
    "objectID": "slides/01-slides.html#hlos-allow-you-to",
    "href": "slides/01-slides.html#hlos-allow-you-to",
    "title": "POLS 1600",
    "section": "HLOs allow you to",
    "text": "HLOs allow you to\n\nDescribe the structure of your data:\n\nHow many observations (rows)\nHow many variables (columns)\n\nDescribe the unit of analysis\n\nWhat does a row in your data correspond to\n\nIdentify the class and type of variables (columns)\n\nNumeric, character, logical\nIs there missing data (NAs)\n\nFigure out what transformations, cleaning, and recoding you need to do"
  },
  {
    "objectID": "slides/01-slides.html#the-tidyverse",
    "href": "slides/01-slides.html#the-tidyverse",
    "title": "POLS 1600",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nFor more check out R for Data Science"
  },
  {
    "objectID": "slides/01-slides.html#tidy-data",
    "href": "slides/01-slides.html#tidy-data",
    "title": "POLS 1600",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data is a standard way of mapping the meaning of a dataset to its structure.\nA dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nMore"
  },
  {
    "objectID": "slides/01-slides.html#dplyr-functions-for-data-wrangling",
    "href": "slides/01-slides.html#dplyr-functions-for-data-wrangling",
    "title": "POLS 1600",
    "section": "dplyr functions for data wrangling",
    "text": "dplyr functions for data wrangling\nToday and this week will begin learning some tools for selecting and transforming data:\n\nselect() to select columns from a dataframe\nfilter() to select rows from a dataframe when some statement is TRUE\nmutate() to create new colums\n\ncase_when() to recode values when some statement is TRUE\n\nsummarise() to transform many values into one value\ngroup_by() to create a grouped table so that other functions are applied separately to each group and then combined"
  },
  {
    "objectID": "slides/01-slides.html#the-pipe-operator",
    "href": "slides/01-slides.html#the-pipe-operator",
    "title": "POLS 1600",
    "section": "The %>% (“pipe” operator)",
    "text": "The %&gt;% (“pipe” operator)\nThe %&gt;% lets us chain functions together so we can read left to right\n\nslice(filter(covid, administrative_area_level_2 == \"Rhode Island\"), 1)\n\nBecomes\n\ncovid %&gt;% \n  filter(administrative_area_level_2 == \"Rhode Island\") %&gt;% \n  slice(1)\n\n\n\n\n\n\n\nKeyboard Shortcuts for %&gt;%\n\n\nmacOS: CMD + SHIFT + M\nPC: CTRL + SHIFT + M"
  },
  {
    "objectID": "slides/01-slides.html#descriptive-statistics",
    "href": "slides/01-slides.html#descriptive-statistics",
    "title": "POLS 1600",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWhen social scientists talk about descriptive inference, we’re trying to summarize our data and make claims about what’s typical of our data\n\nWhat’s a typical value\n\nMeasures of central tendency\nmean, median, mode\n\nHow do our data vary around typical values\n\nMeasures of dispersion\nvariance, standard deviation, range, percentile ranges\n\nHow does variation in one variable relate to variation in another\n\nMeasures of association\ncovariance, correlation"
  },
  {
    "objectID": "slides/01-slides.html#using-r-to-summarize-data",
    "href": "slides/01-slides.html#using-r-to-summarize-data",
    "title": "POLS 1600",
    "section": "Using R to Summarize Data",
    "text": "Using R to Summarize Data\nHere are some common ways of summarizing data and how to calculate them with R\n\n\n\nDescription\nUsage\n\n\n\n\nsum\nsum(x)\n\n\nminimum\nmin(x)\n\n\nmaximum\nmax(x)\n\n\nrange\nrange(x)\n\n\nmean\nmean(x)\n\n\nmedian\nmedian(x)\n\n\npercentile\nquantile(x)\n\n\nvariance\nvar(x)\n\n\nstandard deviation\nsd(x)\n\n\nrank\nrank(x)\n\n\n\n\n\nAll of these functions have an argument called na.rm=F. If your data have missing values, you’ll need to set na.rm=F (e.g. mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/01-slides.html#exploring-covid-19-data-for-lab-1",
    "href": "slides/01-slides.html#exploring-covid-19-data-for-lab-1",
    "title": "POLS 1600",
    "section": "Exploring Covid-19 Data for Lab",
    "text": "Exploring Covid-19 Data for Lab\nLet’s spend the rest of class, exploring what seems like a simple question\n\nOn average, did states that adopted mask mandates have lower rates of new cases?"
  },
  {
    "objectID": "slides/01-slides.html#tasks",
    "href": "slides/01-slides.html#tasks",
    "title": "POLS 1600",
    "section": "Tasks",
    "text": "Tasks\n\nGet a high level overview of our data\nSubset the data to just U.S. States\nRecode our data to get a measure of new Covid cases and what face mask policy policy was in place\nSummarize the average number of new cases by face mask policy."
  },
  {
    "objectID": "slides/01-slides.html#get-a-high-level-overview-of-our-data",
    "href": "slides/01-slides.html#get-a-high-level-overview-of-our-data",
    "title": "POLS 1600",
    "section": "1. Get a high level overview of our data",
    "text": "1. Get a high level overview of our data\n\n\n\n\nCreate a new chunk\nLabel it #| label:\"HLO\"\nRun the following code\nComment code with #\n\n\n\n\ndim(covid)\nhead(covid)\nglimpse(covid)\ntable(covid$administrative_area_level_2)\nlength(unique(covid$administrative_area_level_2))\ncovid$confirmed[1:10]\ncovid %&gt;%\n  select(administrative_area_level_2, date, confirmed) %&gt;%\n  slice(10:20)\nsummary(covid$confirmed)\ntable(covid$facial_coverings)"
  },
  {
    "objectID": "slides/01-slides.html#answer-the-following",
    "href": "slides/01-slides.html#answer-the-following",
    "title": "POLS 1600",
    "section": "Answer the following",
    "text": "Answer the following\n\n\nHow many observations are there (rows)\nHow many variables (columns)\nWhat’s the unit of analysis?\n\nIn words, how would you describe what a row in your data set corresponds to?\n\nAre there any missing values for confirmed\nWhat range of values can facial_coverings take?1\n\n\nSee: https://covid19datahub.io/articles/docs.html"
  },
  {
    "objectID": "slides/01-slides.html#subsetting-our-data-to-only-u.s.-states",
    "href": "slides/01-slides.html#subsetting-our-data-to-only-u.s.-states",
    "title": "POLS 1600",
    "section": "Subsetting our data to only U.S. States",
    "text": "Subsetting our data to only U.S. States\nGoal: Subset our Covid data to include only the 50 states + DC\nSteps:\n\nCreate a vector of the territories we don’t want\nUse the filter() command to “filter” out these territories"
  },
  {
    "objectID": "slides/01-slides.html#create-a-vector-of-the-territories-we-dont-want",
    "href": "slides/01-slides.html#create-a-vector-of-the-territories-we-dont-want",
    "title": "POLS 1600",
    "section": "1. Create a vector of the territories we don’t want",
    "text": "1. Create a vector of the territories we don’t want\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )"
  },
  {
    "objectID": "slides/01-slides.html#use-the-filter-command-to-filter-out-these-territories",
    "href": "slides/01-slides.html#use-the-filter-command-to-filter-out-these-territories",
    "title": "POLS 1600",
    "section": "2. Use the filter() command to “filter” out these territories",
    "text": "2. Use the filter() command to “filter” out these territories\n\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories )\n\ndim(covid)\n\n[1] 58809    47\n\ndim(covid_us)\n\n[1] 53678    47"
  },
  {
    "objectID": "slides/01-slides.html#creating-new-variables-for-analysis",
    "href": "slides/01-slides.html#creating-new-variables-for-analysis",
    "title": "POLS 1600",
    "section": "Creating new variables for analysis",
    "text": "Creating new variables for analysis\n\nGoal: We need new variables that describe:\n\nthe number of new Covid-19 cases on a given date\nthe face mask policy in place\n\nSteps:\n\nUse mutate(), group_by() and lag() to calculate new_cases from total confirmed cases\nUse mutate(), case_when() and abs() to turn numeric facial_coverings into categorical factor variable"
  },
  {
    "objectID": "slides/01-slides.html#calculate-new-covid-19-cases",
    "href": "slides/01-slides.html#calculate-new-covid-19-cases",
    "title": "POLS 1600",
    "section": "Calculate new Covid-19 cases",
    "text": "Calculate new Covid-19 cases\nPlease run and comment the following code:\n\ndim(covid_us)\n\n[1] 53678    47\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases/population *100000\n    ) -&gt; covid_us\ndim(covid_us)\n\n[1] 53678    50"
  },
  {
    "objectID": "slides/01-slides.html#create-face-mask-policy-variable",
    "href": "slides/01-slides.html#create-face-mask-policy-variable",
    "title": "POLS 1600",
    "section": "Create Face Mask Policy variable",
    "text": "Create Face Mask Policy variable\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n      \n    ) \n  ) -&gt; covid_us\n\nlevels(factor(covid_us$face_masks))\n\n[1] \"No policy\"              \"Recommended\"            \"Required all times\"    \n[4] \"Required shared places\" \"Some requirements\""
  },
  {
    "objectID": "slides/01-slides.html#make-face_masks-a-factor-to-reflect-order-of-policies",
    "href": "slides/01-slides.html#make-face_masks-a-factor-to-reflect-order-of-policies",
    "title": "POLS 1600",
    "section": "Make face_masks a factor to reflect order of policies",
    "text": "Make face_masks a factor to reflect order of policies\n\nlevels(factor(covid_us$face_masks))\n\n[1] \"No policy\"              \"Recommended\"            \"Required all times\"    \n[4] \"Required shared places\" \"Some requirements\"     \n\ncovid_us %&gt;%\n  mutate(\n    face_masks = factor(\n      face_masks,\n      levels = c(\n        \"No policy\", \n        \"Recommended\", \n        \"Some requirements\",\n        \"Required shared places\",\n        \"Required all times\"\n        )\n    )\n  ) -&gt; covid_us\n\nlevels(covid_us$face_masks)\n\n[1] \"No policy\"              \"Recommended\"            \"Some requirements\"     \n[4] \"Required shared places\" \"Required all times\""
  },
  {
    "objectID": "slides/01-slides.html#calculate-the-average-number-of-covid-19-cases-by-face-mask-policy",
    "href": "slides/01-slides.html#calculate-the-average-number-of-covid-19-cases-by-face-mask-policy",
    "title": "POLS 1600",
    "section": "Calculate the Average Number of Covid-19 cases by Face Mask Policy",
    "text": "Calculate the Average Number of Covid-19 cases by Face Mask Policy\n\nGoal: On average, did states that adopted mask mandates have lower rates of new cases?\nSteps: use filter(), group_by() and summarise() and mean() to calculate the average number of cases for each level of the face_masks policy variable"
  },
  {
    "objectID": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k",
    "href": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k",
    "title": "POLS 1600",
    "section": "Face Masks and New Covid-19 Cases (per 100k)",
    "text": "Face Masks and New Covid-19 Cases (per 100k)\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(face_masks)%&gt;%\n  summarize(\n    `Average No. of New Cases` = round(mean(new_cases_pc, na.rm=T),2)\n  )%&gt;%\n  rename(\n    \"Face Mask Policy\" = face_masks\n  ) -&gt; face_mask_summary"
  },
  {
    "objectID": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k-1",
    "href": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k-1",
    "title": "POLS 1600",
    "section": "Face Masks and New Covid-19 Cases (per 100k)",
    "text": "Face Masks and New Covid-19 Cases (per 100k)\n\n\n\n\nWhat should we conclude?\nWhat’s wrong with this simple comparison?\nWhat’s a better comparison? (Thursday)\n\n\n\n\n\n\nFace Mask Policy\nAverage No. of New Cases\n\n\n\n\nNo policy\n10.26\n\n\nRecommended\n16.61\n\n\nSome requirements\n36.18\n\n\nRequired shared places\n29.38\n\n\nRequired all times\n32.18"
  },
  {
    "objectID": "slides/01-slides.html#commented-code",
    "href": "slides/01-slides.html#commented-code",
    "title": "POLS 1600",
    "section": "Commented Code",
    "text": "Commented Code\n\n# ---- Libraries ----\n## Uncomment to install\n# install.packages(\"tidyverse\")\n# install.packages(\"COVID19\")\n\nlibrary(\"tidyverse\")\nlibrary(\"COVID19\")\n\n# ---- Load data ----\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n\n# ---- Subset to US states and DC ----\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories )\n\n## Check subsetting\ndim(covid)[1] &gt; dim(covid_us)[1]\n\n# ---- Recode covid_us ----\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases/population *100000\n    ) %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\"\n      ) \n    ) %&gt;%\n  mutate(\n    face_masks = factor(\n      face_masks,\n      levels = c(\n        \"No policy\", \n        \"Recommended\", \n        \"Some requirements\",\n        \"Required shared places\",\n        \"Required all times\"\n        )\n      )\n    )-&gt; covid_us\n\n\n# ---- Calculate new cases per capita by facemask policy\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(face_masks)%&gt;%\n  summarize(\n    `Average No. of New Cases` = round(mean(new_cases_pc, na.rm=T),2)\n  )%&gt;%\n  rename(\n    \"Face Mask Policy\" = face_masks\n  ) -&gt; face_mask_summary\n\nface_mask_summary"
  },
  {
    "objectID": "slides/01-slides.html#summary-1",
    "href": "slides/01-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\nAfter today, you should have a better sense of\n\nHow to write R code using Quarto and R Markdown\nHow to install packages and load libraries\nSome of different types and shapes of data\nHow to get a high level overview of your data\nHow to transform, recode, and summarise data using dplyr and the tidyverse\nHow describe typical values and variation in data\nHow to explore substantive questions using these these typical values"
  },
  {
    "objectID": "slides/01-slides.html#congrats",
    "href": "slides/01-slides.html#congrats",
    "title": "POLS 1600",
    "section": "Congrats!",
    "text": "Congrats!\n\nWe covered A LOT\nIt’s OK to feel overwhelmed\n\nBut please don’t suffer in silence\n\nDon’t worry if everything didn’t make sense.\n\nEventually it will, but this takes time and practice\nTesta’s 50-50 rule\nFAAFO\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/02-slides.html#class-plan",
    "href": "slides/02-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nSetup (5 minutes)\nReview\n\nTroubleshooting Errors (5 min)\nData wrangling in R (20 min)\nDescriptive Statistics (10 min)\n\nData Visualization (40 min)\n\nThe grammar of graphics\nBasic plots to describe:\n\nDistributions\nAssociations"
  },
  {
    "objectID": "slides/02-slides.html#setup-for-today",
    "href": "slides/02-slides.html#setup-for-today",
    "title": "POLS 1600",
    "section": "Setup for today",
    "text": "Setup for today"
  },
  {
    "objectID": "slides/02-slides.html#libraries",
    "href": "slides/02-slides.html#libraries",
    "title": "POLS 1600",
    "section": "Libraries",
    "text": "Libraries\nThis week we’ll use the following libraries.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  \n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  \n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  \n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)\nthe_packages\n\n [1] \"tinytex\"    \"kableExtra\" \"tidyverse\"  \"lubridate\"  \"forcats\"   \n [6] \"haven\"      \"labelled\"   \"ggmap\"      \"ggrepel\"    \"ggridges\"  \n[11] \"ggthemes\"   \"ggpubr\"     \"GGally\"     \"COVID19\"    \"maps\"      \n[16] \"mapdata\"    \"DT\""
  },
  {
    "objectID": "slides/02-slides.html#installing-and-loading-new-packages",
    "href": "slides/02-slides.html#installing-and-loading-new-packages",
    "title": "POLS 1600",
    "section": "Installing and loading new packages",
    "text": "Installing and loading new packages\nNext we’ll create a function called ipak (thanks Steven) which:\n\nTakes a list of packages (pkg)\nChecks to see if these packages are installed\nInstalls any new packages\nLoads all the packages so we can use them\n\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nAgain, run this code on your machines"
  },
  {
    "objectID": "slides/02-slides.html#installing-and-loading-new-packages-1",
    "href": "slides/02-slides.html#installing-and-loading-new-packages-1",
    "title": "POLS 1600",
    "section": "Installing and loading new packages",
    "text": "Installing and loading new packages\nFinally, let’s use ipak to install and load the_packages\nWhat should we replace some_function and some_input with to do this?\n\nsome_function(some_input)\n\n\n\nipak(the_packages)\n\n   tinytex kableExtra  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally    COVID19 \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n      maps    mapdata         DT \n      TRUE       TRUE       TRUE \n\n\n\nR may ask you to install a package’s dependencies (other packages your package needs). Try entering the number 1 into your console\nR may tell you need to restart R Try saying yes. If it doesn’t start downloading, say no\nR may then ask if you want to compile some packages from source. Type Y into your console. If this doesn’t work, try again, but this time type N when asked"
  },
  {
    "objectID": "slides/02-slides.html#loading-the-covid-19-data",
    "href": "slides/02-slides.html#loading-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Loading the Covid-19 Data",
    "text": "Loading the Covid-19 Data\nLet’s load the Covid-19 data we worked with last week:\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/02-slides.html#section",
    "href": "slides/02-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "XKCD"
  },
  {
    "objectID": "slides/02-slides.html#two-kinds-of-errors",
    "href": "slides/02-slides.html#two-kinds-of-errors",
    "title": "POLS 1600",
    "section": "Two kinds of errors:",
    "text": "Two kinds of errors:\n\nSyntactic\n\nR doesn’t understand how to run your code\nMost common, easy to fix (eventually…)\n\n\n\n\nSemantic\n\nR runs your code but doesn’t give you the expected result\nLess common, harder to fix\n\n\n\n\nMost errors happen because R is looking for something that isn’t there.\nMore discussion here and here"
  },
  {
    "objectID": "slides/02-slides.html#common-syntactic-errors",
    "href": "slides/02-slides.html#common-syntactic-errors",
    "title": "POLS 1600",
    "section": "Common Syntactic Errors",
    "text": "Common Syntactic Errors\n\nUnmatched parentheses or brackets\nMisspelled a name\nForgot a comma\nForgot to install a package or load a library\nForgot to set the working directory/path to a file you want R to use.\nTried to select a column or row that doesn’t exist"
  },
  {
    "objectID": "slides/02-slides.html#fixing-syntactic-errors",
    "href": "slides/02-slides.html#fixing-syntactic-errors",
    "title": "POLS 1600",
    "section": "Fixing Syntactic Errors",
    "text": "Fixing Syntactic Errors\n\nR Studio’s script editor will show a red circle with a white x in next to a line of code it thinks has an error in it.\nHave someone else look at your code (Fresh eyes, paired programming)\nCopy and paste the “general part” of error message into Google.\nKnit your document after each completed code chunk\n\nThis will run the code from top to bottom, and stop when it encounters an error\nTry commenting out the whole chunk, and then uncommenting successive lines of code\n\nBe patient. Don’t be hard are yourself. Remember, errors are portals of discovery."
  },
  {
    "objectID": "slides/02-slides.html#semantic-errors",
    "href": "slides/02-slides.html#semantic-errors",
    "title": "POLS 1600",
    "section": "Semantic Errors",
    "text": "Semantic Errors\n\nYour code runs, but doesn’t produce what you expected.\nLess common; can be harder to identify and fix\nOne example: Two packages have a function with the same name that do different things\n\n\n\n# dplyr::summarize\n# Hmisc::summarize"
  },
  {
    "objectID": "slides/02-slides.html#semantic-errors-1",
    "href": "slides/02-slides.html#semantic-errors-1",
    "title": "POLS 1600",
    "section": "Semantic Errors",
    "text": "Semantic Errors\n\nSome general solutions/practices to avoid semantic errors:\n\nSpecify the package and the function you want: package_name::function_name()\nWrite helpful comments in your code.\nInclude “sanity” checks in your code.\nIf a function should produce an output that’s a data.frame, check to see if it is a data frame\n\n\n\n\n# Here's some pseudo code:\n\n# I expect my_function produces a data frame\nx &lt;- my_function(y) \n\n# Check to see if x is a data frame\n# If x is not a data frame, return an Error\nstopifnot(is.data.frame(x))"
  },
  {
    "objectID": "slides/02-slides.html#why-do-we-need-to-wrangle-data",
    "href": "slides/02-slides.html#why-do-we-need-to-wrangle-data",
    "title": "POLS 1600",
    "section": "Why do we need to “wrangle” data",
    "text": "Why do we need to “wrangle” data\n\nRarely, if ever, do we get data in the exact format we need.\nInstead, before we can get to work, we often need to transform our data in various ways\nSometimes called:\n\nData cleaning/recoding\nData wrangling\nData carpentry\n\nThe end goal is the same: make messy data tidy"
  },
  {
    "objectID": "slides/02-slides.html#tidy-data",
    "href": "slides/02-slides.html#tidy-data",
    "title": "POLS 1600",
    "section": "Tidy data",
    "text": "Tidy data\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value"
  },
  {
    "objectID": "slides/02-slides.html#tools-for-transforming-our-data",
    "href": "slides/02-slides.html#tools-for-transforming-our-data",
    "title": "POLS 1600",
    "section": "Tools for transforming our data",
    "text": "Tools for transforming our data\nLast week we used the following functions:\n\nread_csv() and data() to read and load data in R\nlogical operators like &, |, %in% ==, !=, &gt;,&gt;=,&lt;,&lt;= to make comparisons\nthe pipe command %&gt;% to “pipe” the output of one function into another\nfilter() to pick observations (rows) by their values\narrange() to reorder rows\nselect() to pick variables by their names\nmutate() and case_when() command to create new variables in our data set\nsummarise() to collapse many values into a single value (like a mean or median)\ngroup_by() to apply functions like mutate() and summarise() on a group-by-group basis"
  },
  {
    "objectID": "slides/02-slides.html#common-functions-for-transforming-data",
    "href": "slides/02-slides.html#common-functions-for-transforming-data",
    "title": "POLS 1600",
    "section": "Common functions for transforming data",
    "text": "Common functions for transforming data\nAll of these “verb” functions from the dplyr package (e.g. filter(),mutate()) follow a similar format:\n\nTheir first argument is a data frame\nThe subsequent arguments tell R what to do with the data frame, using the variable names (without quotes)\nThe output is a new data frame\n\nMore"
  },
  {
    "objectID": "slides/02-slides.html#you-trying-to-get-the",
    "href": "slides/02-slides.html#you-trying-to-get-the",
    "title": "POLS 1600",
    "section": "You trying to get the %>%?",
    "text": "You trying to get the %&gt;%?"
  },
  {
    "objectID": "slides/02-slides.html#the-pipe-command",
    "href": "slides/02-slides.html#the-pipe-command",
    "title": "POLS 1600",
    "section": "The pipe command %>%",
    "text": "The pipe command %&gt;%\n\nThe pipe command is way of “chaining” lines of code together, piping the results of one tidyverse function into the next function.\nThe pipe command works because these functions always expect a data frame as their first argument, and always produce a data frame as their output."
  },
  {
    "objectID": "slides/02-slides.html#the-pipe-command-1",
    "href": "slides/02-slides.html#the-pipe-command-1",
    "title": "POLS 1600",
    "section": "The pipe command %>%",
    "text": "The pipe command %&gt;%\n\nsummarise(\n  data = df,\n  mean = mean(var1, na.rm = T),\n  median = median(var1, na.rm = T)\n )\n# Rewrite with a pipe:\n\ndf %&gt;% \n  summarize(\n    mean = mean(var1, na.rm = T),\n    median = median(var1, na.rm = T)    \n  )"
  },
  {
    "objectID": "slides/02-slides.html#wrangling-the-covid-19-data",
    "href": "slides/02-slides.html#wrangling-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Wrangling the Covid-19 data",
    "text": "Wrangling the Covid-19 data\nTo work with the Covid-19 data we did the following:\n\nSubsetted/Filtered the data to exclude US Territories\nCreated new variables from existing variables in the data to use in our final analysis"
  },
  {
    "objectID": "slides/02-slides.html#wrangling-the-covid-19-data-1",
    "href": "slides/02-slides.html#wrangling-the-covid-19-data-1",
    "title": "POLS 1600",
    "section": "Wrangling the Covid-19 data",
    "text": "Wrangling the Covid-19 data\nSpecifically, we did the following:\n\nCreated an object called territories that is a vector containing the names of U.S. territories\nCreated a new dataframe, called covid_us, by filtering out observations from the U.S. territories\nCreated a state variable that is a copy of the administrative_area_level_2\nCreated a variable called new_cases from the confirmed. Create a variable called new_cases_pc that is the number of new Covid-19 cases per 100,000 citizens\nCreated a variable called face_masks from the facial_coverings variable.\nCalculated the average number of new cases, by different levels of face_masks\n\n\nLet’s take some time to make sure we understand everything that was happening."
  },
  {
    "objectID": "slides/02-slides.html#created-an-object-called-territories",
    "href": "slides/02-slides.html#created-an-object-called-territories",
    "title": "POLS 1600",
    "section": "Created an object called territories",
    "text": "Created an object called territories\n\n# - 1. Create territories object\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n\nThe object territories now exists in our environment."
  },
  {
    "objectID": "slides/02-slides.html#created-a-new-dataframe-called-covid_us",
    "href": "slides/02-slides.html#created-a-new-dataframe-called-covid_us",
    "title": "POLS 1600",
    "section": "Created a new dataframe, called covid_us",
    "text": "Created a new dataframe, called covid_us\n\n\nTaskCode\n\n\n\nUse the filter() command to select only the rows where the administrative_area_level_2 is not (!) in (%in%) the territories object\n\n\n\n\n# - 2. Create covid_us data frame\n# How many rows and columns in covid\ndim(covid)\n\n[1] 58809    47\n\n# Filter out obs from US territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\n# covid_us should have fewer rows than covid\ndim(covid_us)\n\n[1] 53678    47"
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-state",
    "href": "slides/02-slides.html#created-a-variable-called-state",
    "title": "POLS 1600",
    "section": "Created a variable called state",
    "text": "Created a variable called state\n\n\nTaskCode\n\n\nCopy administrative_area_level_2 into a new variable called state\n\n\n\n\n\n\nNote\n\n\nNote that we have to save the output of mutate back into covid_us for our state to exist as new column in covid_us\n\n\n\n\n\n\ndim(covid_us)\n\n[1] 53678    47\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2\n  ) -&gt; covid_us\ndim(covid_us)\n\n[1] 53678    48\n\nnames(covid_us)[48]\n\n[1] \"state\""
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-state-1",
    "href": "slides/02-slides.html#created-a-variable-called-state-1",
    "title": "POLS 1600",
    "section": "Created a variable called state",
    "text": "Created a variable called state\nNow there’s a new column in covid_us called state, that we can access by calling covid_us$state\n\ncovid_us$state[1:5] # Just show first 5 observations\n\n[1] \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\"\n\n\n\nWe could have done the same thing in “Base” R\n\ncovid_us$state &lt;- covid_us$administrative_area_level_2\n\n\n\nWhy didn’t we?\n\nConsistent preference for tidyverse &gt; base R\nSaves time when recoding lots of variables\nmutate() plays nicely with functions like group_by()"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases from the confirmed variable",
    "text": "Create a variable called new_cases from the confirmed variable\nThe confirmed variable contains a running total of confirmed cases in a given state on a given day.\nVizualing data helps us understand how we might need to transform our data"
  },
  {
    "objectID": "slides/02-slides.html#visualize-confirmed-variable-for-rhode-island",
    "href": "slides/02-slides.html#visualize-confirmed-variable-for-rhode-island",
    "title": "POLS 1600",
    "section": "Visualize confirmed variable for Rhode Island",
    "text": "Visualize confirmed variable for Rhode Island\n\nCodePlotData\n\n\n\noptions(scipen = 999) # No scientific notation\ncovid_us %&gt;% \n  filter(state == \"Rhode Island\") %&gt;% \n  ggplot(aes(\n    x = date,\n    y = confirmed\n  ))+\n  geom_point()+\n  theme_bw() +\n  labs(title = \"Total Covid-19 cases in Rhode Island\",\n       y = \"Total Cases\",\n       x = \"Date\") -&gt; fig_ri_covid"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable-1",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable-1",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases from the confirmed variable",
    "text": "Create a variable called new_cases from the confirmed variable\n\nTaskCodeData\n\n\nTake the difference between a given day’s value of confirmed and yesterday’s value of confirmed to create a measure of new_cases on a given date for each state\n\n\n\n\n\n\nNote\n\n\n\nUse lag() to shift values in a column down one row in the data\nUse group_by() to respect the state-date structure of the data\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed)\n  ) -&gt; covid_us"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases_pc",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases_pc",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases_pc",
    "text": "Create a variable called new_cases_pc\n\n\nTaskCode - WranglingCode - CheckingData\n\n\n\nScale new_cases by population to create a per capita measure (new_cases_pc)\n\n\n\n\n\n\n\nNote\n\n\nWe can create multiple variables in a single mutate() by separating lines of code with a ,\n\n\n\n\n\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000\n    ) -&gt;covid_us\n\n\n\n\n# Check recoding\ncovid_us %&gt;% \n  # Look at two states\n  filter(state == \"Rhode Island\" | state == \"New York\") %&gt;% \n  # In a small date range\n  filter(date &gt; \"2021-01-01\" & date &lt; \"2021-01-05\") %&gt;% \n  # Select only the columns we want\n  select(state, date, new_cases, new_cases_pc) -&gt; hlo_df\n# save to object hlo_df\n\n\n\n\nhlo_df\n\n# A tibble: 6 × 4\n# Groups:   state [2]\n  state        date       new_cases new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;         &lt;int&gt;        &lt;dbl&gt;\n1 Rhode Island 2021-01-02         0          0  \n2 Rhode Island 2021-01-03         0          0  \n3 Rhode Island 2021-01-04      4759        449. \n4 New York     2021-01-02     15849         81.5\n5 New York     2021-01-03     12232         62.9\n6 New York     2021-01-04     11242         57.8"
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-face_masks",
    "href": "slides/02-slides.html#created-a-variable-called-face_masks",
    "title": "POLS 1600",
    "section": "Created a variable called face_masks",
    "text": "Created a variable called face_masks\n\n\nTaskHLOCodeCheck\n\n\nCreate a variable called face_masks from the facial_coverings that describes the face mask policy experienced by most people in a given state on a given date.\n\n\n\n\n\n\nNote\n\n\n\nUse case_when() inside of mutate() to create a variable that takes certain values when certain logical statements are true\nSeting the levels = c(value1, value2, etc.) argument in factor() lets us control the ordering of categorical/character data.\n\n\n\n\n\n\nRecall, that the facial_coverings variable took on range of substantive values from 0 to 4, but empirically could take both positve and negative values\n\ntable(covid_us$facial_coverings)\n\n\n   -4    -3    -2    -1     0     1     2     3     4 \n  410  5897  7362   275  3893  8604 17424  9191   622 \n\n\n\n\n\ncovid_us %&gt;%\nmutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n\n\n\ncovid_us%&gt;%\n  filter(state == \"Illinois\", date &gt; \"2020-9-28\") %&gt;%\n  select(state, date, facial_coverings, face_masks) %&gt;% \n  slice(1:5)\n\n# A tibble: 5 × 4\n# Groups:   state [1]\n  state    date       facial_coverings face_masks        \n  &lt;chr&gt;    &lt;date&gt;                &lt;int&gt; &lt;fct&gt;             \n1 Illinois 2020-09-29                2 Some requirements \n2 Illinois 2020-09-30                2 Some requirements \n3 Illinois 2020-10-01               -4 Required all times\n4 Illinois 2020-10-02               -4 Required all times\n5 Illinois 2020-10-03               -4 Required all times"
  },
  {
    "objectID": "slides/02-slides.html#addtional-recoding",
    "href": "slides/02-slides.html#addtional-recoding",
    "title": "POLS 1600",
    "section": " Addtional recoding",
    "text": "Addtional recoding\nIn last week’s lab, we also added the following\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(\n      year, \n      str_pad(month, width = 2, pad=0), \n      sep = \"-\"\n      ),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/02-slides.html#working-with-dates",
    "href": "slides/02-slides.html#working-with-dates",
    "title": "POLS 1600",
    "section": " Working with dates",
    "text": "Working with dates\nR treat’s dates differently\n\ncovid_us$date[1:3]\n\n[1] \"2020-01-01\" \"2020-01-02\" \"2020-01-03\"\n\nclass(covid_us$date)\n\n[1] \"Date\"\n\n\nIf R knows a variable is a date, we can extract components of that date, using functions from the lubridate package\n\nyear(covid_us$date[1:3])\n\n[1] 2020 2020 2020\n\nmonth(covid_us$date[1:3])\n\n[1] 1 1 1"
  },
  {
    "objectID": "slides/02-slides.html#the-str_pad-and-paste-function",
    "href": "slides/02-slides.html#the-str_pad-and-paste-function",
    "title": "POLS 1600",
    "section": " The str_pad() and paste() function",
    "text": "The str_pad() and paste() function\n\nThe str_pad() function lets us ‘pad’ strings so that they’re all the same width\n\n\nmonth(covid_us$date[1:3])\n\n[1] 1 1 1\n\nstr_pad(month(covid_us$date[1:3]), width=2, pad = 0)\n\n[1] \"01\" \"01\" \"01\"\n\n\n\nThe paste function lets us paste objects together.\n\n\npaste(year(covid_us$date[1:3]),\n      str_pad(month(covid_us$date[1:3]), width=2, pad = 0),\n      sep = \"-\"\n      )\n\n[1] \"2020-01\" \"2020-01\" \"2020-01\""
  },
  {
    "objectID": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy",
    "href": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy",
    "title": "POLS 1600",
    "section": "Summarizing the averge number of new_cases by face_mask policy",
    "text": "Summarizing the averge number of new_cases by face_mask policy\n\nTaskCodeResults\n\n\nCalculate the mean (average) number of new_cases of Covid-19 when each type of face_mask policy was in effect\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() command will do each calculation inside of summarise() for each level of the grouping variable\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks)) %&gt;%\n  group_by(face_masks) %&gt;%\n  summarize(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  ) -&gt; face_mask_summary\n\n\n\n\nface_mask_summary\n\n# A tibble: 5 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2"
  },
  {
    "objectID": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy-by-month",
    "href": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy-by-month",
    "title": "POLS 1600",
    "section": "Summarizing the averge number of new_cases by face_mask policy by month",
    "text": "Summarizing the averge number of new_cases by face_mask policy by month\n:::: panel-tabset"
  },
  {
    "objectID": "slides/02-slides.html#task-6",
    "href": "slides/02-slides.html#task-6",
    "title": "POLS 1600",
    "section": "Task",
    "text": "Task\nCalculate the mean (average) number of new_cases of Covid-19 when each type of face_mask policy was in effect for each year_month in our dataset\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() command can group on multiple variables"
  },
  {
    "objectID": "slides/02-slides.html#code-6",
    "href": "slides/02-slides.html#code-6",
    "title": "POLS 1600",
    "section": "Code",
    "text": "Code\n\ncovid_us %&gt;%\n  group_by(face_masks, year_month) %&gt;%\n  summarize(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  ) -&gt; cases_by_month_and_policy"
  },
  {
    "objectID": "slides/02-slides.html#results-1",
    "href": "slides/02-slides.html#results-1",
    "title": "POLS 1600",
    "section": "Results",
    "text": "Results\n\ncases_by_month_and_policy\n\n# A tibble: 102 × 3\n# Groups:   face_masks [5]\n   face_masks year_month new_cases_pc\n   &lt;fct&gt;      &lt;chr&gt;             &lt;dbl&gt;\n 1 No policy  2020-01        0.000463\n 2 No policy  2020-02        0.00188 \n 3 No policy  2020-03        1.70    \n 4 No policy  2020-04        6.50    \n 5 No policy  2022-04       19.8     \n 6 No policy  2022-05       20.4     \n 7 No policy  2022-06       37.6     \n 8 No policy  2022-07       36.2     \n 9 No policy  2022-08       35.7     \n10 No policy  2022-09       19.0     \n# ℹ 92 more rows\n\n# In base R:\nmean(\n  covid_us$new_cases_pc[\n    covid_us$face_masks == \"No policy\" &\n      covid_us$year_month == \"2020-01\"], na.rm = T)\n\n[1] 0.0004626161"
  },
  {
    "objectID": "slides/02-slides.html#concept-check",
    "href": "slides/02-slides.html#concept-check",
    "title": "POLS 1600",
    "section": " Concept check",
    "text": "Concept check\nSuppose you want to do the following, what function or functions would you use:\n\nRead data into R\nLook at the data to get a high level overview of its structure\nSubset or filter the data to include just observations with certain values\nSelect specific columns from data\nAdd new columns to the data\nSummarize multiple values by collapsing them into a single value\nDoing some function group-by-group?"
  },
  {
    "objectID": "slides/02-slides.html#concept-check-1",
    "href": "slides/02-slides.html#concept-check-1",
    "title": "POLS 1600",
    "section": "Concept check",
    "text": "Concept check\nSuppose you want to do the following, what function or functions would you use:\n\nRead data into R\n\nread_xxx() (tidy), read.xxx() (base)\n\nLook at the data to get a high level overview of its structure\n\nhead(), tail(), glimpse(), table(), summary(), View()\n\nSubset the data to include just obersvations with certain values\n\ndata %&gt;% filter(x &gt; 0), data[data$x &gt; 0], subset(data, x &gt; 0)\n\nSelect specific columns from data\n\ndata$variable, data %&gt;% select(variable1, variable2), data[,c(\"x1\",\"x2\")]\n\nAdd new columns to the data\n\ndata %&gt;% mutate(x = y/10) data$x &lt;- data$y/10\n\nSummarize multiple values by collapsing them into a single value\n\ndata %&gt;% summarise(x_mn = mean(x, na.rm=T))\n\nDoing some function group-by-group?\n\ndata %&gt;% group_by(g) %&gt;% summarise(x_mn = mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/02-slides.html#concept-check-2",
    "href": "slides/02-slides.html#concept-check-2",
    "title": "POLS 1600",
    "section": "Concept check",
    "text": "Concept check\nShould you know exactly how to do all of this?\n\nNO! Of course not. For Pete’s sake, Paul, It’s only the second week\n\n\nWill you learn how to do much of this?\n\n\nMaybe, but I’m feeling pretty overwhelmed…\n\n\nHow will you learn how do these things?\n\n\nWith lots of practice, patience, and repetition motivated by a sense that these skills will help me learn about things I care about"
  },
  {
    "objectID": "slides/02-slides.html#advice-on-learning-how-to-code",
    "href": "slides/02-slides.html#advice-on-learning-how-to-code",
    "title": "POLS 1600",
    "section": "Advice on learning how to code",
    "text": "Advice on learning how to code\n\nIt takes lots of practice and lots of errors\n\nBreak long blocks of code into individual steps to see what’s happening\n\nCreate code chunks and FAFO\n\nJust clean up when you’re done…\n\nOnly dumb question is one you don’t ask\nGoogle, Stack Exchange are your friends\nTry writing out in comments what you want to do in code\nLearn to recognize patterns in the questions/tasks I give you:\n\nCopy and paste code I give\nChange one thing\nFix the error\nAdapt code from class to do a similar thing\n\nLearning to code is much less painful when you have a reason to do it\n\nLet me know what interests you"
  },
  {
    "objectID": "slides/02-slides.html#descriptive-statistics-1",
    "href": "slides/02-slides.html#descriptive-statistics-1",
    "title": "POLS 1600",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWhen social scientists talk about descriptive inference, we’re trying to summarize our data and make claims about what’s typical of our data\n\nWhat’s a typical value\n\nMeasures of central tendency\nmean, median, mode\n\nHow do our data vary around typical values\n\nMeasures of dispersion\nvariance, standard deviation, range, percentile ranges\n\nHow does variation in one variable relate to variation in another\n\nMeasures of association\ncovariance, correlation"
  },
  {
    "objectID": "slides/02-slides.html#using-r-to-summarize-data",
    "href": "slides/02-slides.html#using-r-to-summarize-data",
    "title": "POLS 1600",
    "section": "Using R to Summarize Data",
    "text": "Using R to Summarize Data\nHere are some common ways of summarizing data and how to calculate them with R\n\n\n\nDescription\nUsage\n\n\n\n\nsum\nsum(x)\n\n\nminimum\nmin(x)\n\n\nmaximum\nmax(x)\n\n\nrange\nrange(x)\n\n\nmean\nmean(x)\n\n\nmedian\nmedian(x)\n\n\npercentile\nquantile(x)\n\n\nvariance\nvar(x)\n\n\nstandard deviation\nsd(x)\n\n\nrank\nrank(x)\n\n\n\n\n\nAll of these functions have an argument called na.rm=F. If your data have missing values, you’ll need to set na.rm=F (e.g. mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of typical values\n\nMeans (mean()) all the time\nMedians (median()) useful for describing distributions of variables particularly those with extreme values\n**Mode* useful for characterizing categorical data\n\n\nMeasures of typical variation\n\nvar() important for quantifying uncertainty, but rarely will you be calculating this directly\nsd() a good summary of a typical change in the data.\nrange(), min(), max() useful for exploring data, detecting outliers and potential values that need to be recoded\n\nMeasures of association\n\nCovariance (var()) central to describing relationships but generally not something you’ll calculate or interpret directly\nCorrelation (cor()) useful for describing [bivariate] relationships (positive or negative relationships)."
  },
  {
    "objectID": "slides/02-slides.html#what-you-dont-really-need-to-know-for-pols-1600-smaller",
    "href": "slides/02-slides.html#what-you-dont-really-need-to-know-for-pols-1600-smaller",
    "title": "POLS 1600",
    "section": "What you don’t really need to know for POLS 1600 {smaller}",
    "text": "What you don’t really need to know for POLS 1600 {smaller}\nWe won’t spend much time on the formal definitions, math, and proofs\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\n\\[\nM_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx=\\int_{x_i}^\\infty f_x(X)dx=1/2\n\\]\n\nUseful eventually. Not necessary right now."
  },
  {
    "objectID": "slides/02-slides.html#data-visualizaiton",
    "href": "slides/02-slides.html#data-visualizaiton",
    "title": "POLS 1600",
    "section": "Data visualizaiton",
    "text": "Data visualizaiton\nData visualization is an incredibly valuable tool that helps us to\n\nExplore data, uncovering new relationships, as well as potential problems\nCommunicate our results clearly and precisely\n\nTake a look at how the BBC uses R to produce its graphics"
  },
  {
    "objectID": "slides/02-slides.html#data-visualization",
    "href": "slides/02-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data visualization",
    "text": "Data visualization\nToday, we will:\n\nIntroduce the grammar of graphics\nLearn how to apply this grammar with ggplot()\nIntroduce basic plots to describe\n\nUnivariate distributions\nBivariate relations"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics",
    "href": "slides/02-slides.html#the-grammar-of-graphics",
    "title": "POLS 1600",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nInspired by Wilkinson (2005)\n\nA statistical graphic is a mapping of data variables to aes thetic attributes of geom etric objects.\n\nAt a minimum, a graphic contains three core components:\n\ndata: the dataset containing the variables of interest.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars.\n\nIsmay and Kim (2022)"
  },
  {
    "objectID": "slides/02-slides.html#seven-layers-of-graphics",
    "href": "slides/02-slides.html#seven-layers-of-graphics",
    "title": "POLS 1600",
    "section": "Seven Layers of Graphics",
    "text": "Seven Layers of Graphics\nKesari (2018)"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics-in-r",
    "href": "slides/02-slides.html#the-grammar-of-graphics-in-r",
    "title": "POLS 1600",
    "section": "The grammar of graphics in R",
    "text": "The grammar of graphics in R\nIn R, we’ll implement this grammar of graphics using the ggplot package\n\nLet’s take a look at your feedback to last week’s survey and see how we can visualize some of the in formation you provided"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics-1",
    "href": "slides/02-slides.html#the-grammar-of-graphics-1",
    "title": "POLS 1600",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics"
  },
  {
    "objectID": "slides/02-slides.html#what-we-liked",
    "href": "slides/02-slides.html#what-we-liked",
    "title": "POLS 1600",
    "section": "What we liked",
    "text": "What we liked"
  },
  {
    "objectID": "slides/02-slides.html#what-we-disliked",
    "href": "slides/02-slides.html#what-we-disliked",
    "title": "POLS 1600",
    "section": "What we disliked",
    "text": "What we disliked"
  },
  {
    "objectID": "slides/02-slides.html#building-that-figure",
    "href": "slides/02-slides.html#building-that-figure",
    "title": "POLS 1600",
    "section": "Building that figure",
    "text": "Building that figure\n\nLook at the raw data\nRecode the raw data\nMake a basic plot, telling R the data, aesthetics, geometries, and statistics I want it to plot\nTinker with the data and plot’s scales, coordinates, labels and theme to make the figure look better"
  },
  {
    "objectID": "slides/02-slides.html#look-at-the-raw-data",
    "href": "slides/02-slides.html#look-at-the-raw-data",
    "title": "POLS 1600",
    "section": "1. Look at the raw data",
    "text": "1. Look at the raw data\n\ndf$trip\n\n&lt;labelled&lt;double&gt;[12]&gt;: You're on a road trip with friends. Who controls the music?\n [1] NA  3  1  2  3  2  2  2  1  2  2 NA\n\nLabels:\n value\n     1\n     2\n     3\n                                                                                                                               label\n                                                                                                                    The driver, duh.\n                                                                                                           The front seat, of course\n That jerk in the back who you don't even know but seems to have really strong feelings about Billy Joel's \"Only the good die young\""
  },
  {
    "objectID": "slides/02-slides.html#recode-the-raw-data",
    "href": "slides/02-slides.html#recode-the-raw-data",
    "title": "POLS 1600",
    "section": "2. Recode the raw data",
    "text": "2. Recode the raw data\n\ndf %&gt;%\n  mutate(\n    Playist = forcats::as_factor(trip)\n )%&gt;%\n  select(Playist)\n\n# A tibble: 12 × 1\n   Playist                                                                      \n   &lt;fct&gt;                                                                        \n 1  &lt;NA&gt;                                                                        \n 2 \"That jerk in the back who you don't even know but seems to have really stro…\n 3 \"The driver, duh.\"                                                           \n 4 \"The front seat, of course\"                                                  \n 5 \"That jerk in the back who you don't even know but seems to have really stro…\n 6 \"The front seat, of course\"                                                  \n 7 \"The front seat, of course\"                                                  \n 8 \"The front seat, of course\"                                                  \n 9 \"The driver, duh.\"                                                           \n10 \"The front seat, of course\"                                                  \n11 \"The front seat, of course\"                                                  \n12  &lt;NA&gt;"
  },
  {
    "objectID": "slides/02-slides.html#make-a-basic-plot",
    "href": "slides/02-slides.html#make-a-basic-plot",
    "title": "POLS 1600",
    "section": "3. Make a basic plot",
    "text": "3. Make a basic plot\n\nCodePlot\n\n\n\n#|\ndf %&gt;% #&lt;&lt; Raw data\n  mutate(\n    Playlist =forcats::as_factor(trip)\n  ) %&gt;% # Transformed data\n  ggplot(aes(x = Playlist, # Aesthetics\n             fill = Playlist))+\n  geom_bar( # Geometry\n    stat = \"count\" # Statistic\n    ) -&gt; fig_roadtrip\n\n\n\n\nfig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-data",
    "href": "slides/02-slides.html#tinker-with-data",
    "title": "POLS 1600",
    "section": "4.1 Tinker with data",
    "text": "4.1 Tinker with data\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\") -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-fill-aesthetic",
    "href": "slides/02-slides.html#tinker-with-fill-aesthetic",
    "title": "POLS 1600",
    "section": "4.2 Tinker with fill aesthetic",
    "text": "4.2 Tinker with fill aesthetic\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-coordinates",
    "href": "slides/02-slides.html#tinker-with-coordinates",
    "title": "POLS 1600",
    "section": "4.3 Tinker with coordinates",
    "text": "4.3 Tinker with coordinates\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer() +\n  coord_flip() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-labels",
    "href": "slides/02-slides.html#tinker-with-labels",
    "title": "POLS 1600",
    "section": "4.4 Tinker with labels",
    "text": "4.4 Tinker with labels\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")-&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-theme",
    "href": "slides/02-slides.html#tinker-with-theme",
    "title": "POLS 1600",
    "section": "4.4 Tinker with theme",
    "text": "4.4 Tinker with theme\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n   scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")+\n  theme_bw() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#the-final-code",
    "href": "slides/02-slides.html#the-final-code",
    "title": "POLS 1600",
    "section": "The final code",
    "text": "The final code\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist = str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  coord_flip()+\n  scale_fill_brewer()+\n  labs(title = \"Who controls the playlist on a roadtrip\")+\n  theme_bw()"
  },
  {
    "objectID": "slides/02-slides.html#describing-distributions-and-associations",
    "href": "slides/02-slides.html#describing-distributions-and-associations",
    "title": "POLS 1600",
    "section": "Describing Distributions and Associations",
    "text": "Describing Distributions and Associations\n\nIn the remaining slides, we’ see how to visualize some distributions and associations in the Covid data using:\n\nbarplots\nhistograms\ndensity plots\nboxplots\nline plots\nscatter plots"
  },
  {
    "objectID": "slides/02-slides.html#general-advice-for-making-figures",
    "href": "slides/02-slides.html#general-advice-for-making-figures",
    "title": "POLS 1600",
    "section": "General advice for making figures",
    "text": "General advice for making figures\n\nThink through conceptually how you want to figure to look\n\nDraw it out buy hand\n\nMake a basic plot and iterate\nDon’t let the perfect be the enemy of the good\nUse summarize() and other data wrangling skills to transform data for plotting\nUse factor() and related functions to control order of labels on axis"
  },
  {
    "objectID": "slides/02-slides.html#barplots",
    "href": "slides/02-slides.html#barplots",
    "title": "POLS 1600",
    "section": "Barplots",
    "text": "Barplots\n\nQuestionBasic CodeBetter CodeFigure\n\n\nWhat was the most common face mask policy in the data?\n\n\n\ncovid_us %&gt;% \n  ggplot(aes(x=face_masks))+\n  geom_bar(stat = \"count\")\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  ungroup() %&gt;% \n  mutate(\n    face_masks = forcats::fct_infreq(face_masks)\n  ) %&gt;% \n  ggplot(aes(x=face_masks,\n             fill = face_masks))+\n  geom_bar()+\n  geom_text(stat='count', aes(label=..count..), \n            hjust=.5,vjust=-.5)+\n  guides(fill = \"none\")+\n  theme_bw()+\n  labs(\n    x = \"Face Mask Policy \",\n    title = \"\"\n  ) -&gt; fig_barplot"
  },
  {
    "objectID": "slides/02-slides.html#histogram",
    "href": "slides/02-slides.html#histogram",
    "title": "POLS 1600",
    "section": "Histogram",
    "text": "Histogram\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat does the distribution new Covid-19 cases look like in June 2021\n\n\n\ncovid_us %&gt;% \n  filter(year_month == \"2021-06\") %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() -&gt; fig_hist1\n\n\n\n\nfig_hist1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(year_month == \"2021-06\") %&gt;% \n  filter(new_cases &gt; 0) %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() +\n  labs(\n    title = \"Exclude Negative Values\"\n  ) -&gt; fig_hist2a\n\ncovid_us %&gt;%\n  filter(year_month == \"2021-06\") %&gt;% \n  filter(new_cases &gt; 0) %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() +\n  scale_x_log10()+\n  labs(\n    title = \"Exclude Negative Values & Use log scale\"\n  ) -&gt; fig_hist2b\n\nfig_hist2 &lt;- ggarrange(fig_hist2a, fig_hist2b)"
  },
  {
    "objectID": "slides/02-slides.html#density-plots",
    "href": "slides/02-slides.html#density-plots",
    "title": "POLS 1600",
    "section": "Density Plots",
    "text": "Density Plots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat does the distribution of Covid-19 deaths look like?\n\n\n\ncovid_us %&gt;% \n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = deaths - lag(deaths)\n  ) %&gt;% \n  filter(new_deaths &gt; 0) %&gt;% \n  ggplot(aes(x=new_deaths_pc))+\n  geom_density() -&gt; fig_density1\n\n\n\n\nfig_hist1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;% \n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = deaths - lag(deaths),\n    year_f = factor(year)\n  ) %&gt;% \n  filter(new_deaths &gt; 0) %&gt;% \n  ggplot(aes(x=new_deaths_pc,\n             col = year_f))+\n  geom_density() +\n  geom_rug() +\n  scale_x_log10() +\n    facet_wrap(~month)+\n  theme(legend.position = \"bottom\")-&gt; \n  fig_density2"
  },
  {
    "objectID": "slides/02-slides.html#box-plots",
    "href": "slides/02-slides.html#box-plots",
    "title": "POLS 1600",
    "section": "Box plots",
    "text": "Box plots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nHow did the distribution of Covid-19 cases vary by face mask policy?\n\n\n\ncovid_us %&gt;%\n  filter(new_cases_pc &gt; 0) %&gt;% \n  ggplot(aes(x= face_masks, y=new_cases_pc))+\n  scale_y_log10()+\n  geom_boxplot() -&gt; fig_boxplot1\n\n\n\n\nfig_boxplot1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  mutate(\n    Month = lubridate::month(date, label = T)\n  ) %&gt;% \n  filter(new_cases_pc &gt; 0) %&gt;% \n  filter(year == 2020) %&gt;% \n ggplot(aes(x= face_masks, \n            y=new_cases_pc,\n            col = face_masks))+\n  scale_y_log10()+\n  coord_flip() +\n  geom_boxplot()  +\n    facet_wrap(~Month) +\n  theme(\n    legend.position = \"bottom\"\n  )-&gt; fig_boxplot2"
  },
  {
    "objectID": "slides/02-slides.html#line-graphs",
    "href": "slides/02-slides.html#line-graphs",
    "title": "POLS 1600",
    "section": "Line graphs",
    "text": "Line graphs\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nHow did vaccination rates vary by state?\n\n\n\ncovid_us %&gt;%\n  ggplot(\n    aes(x= date,\n        y=percent_vaccinated,\n        group = state\n        ))+\n  geom_line() -&gt; fig_line1\n\n\n\n\nfig_line1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Label = case_when(\n      date == max(date) & percent_vaccinated == max(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      date == max(date) & percent_vaccinated == median(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      date == max(date) & percent_vaccinated == min(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      TRUE ~ NA_character_\n    ),\n    line_alpha = case_when(\n      state %in% c(\"District of Columbia\", \"Nebraska\", \"Wyoming\") ~ 1,\n      T ~ .3\n    ),\n    line_col = case_when(\n      state %in% c(\"District of Columbia\", \"Nebraska\", \"Wyoming\") ~ \"black\",\n      T ~ \"grey\"\n    )\n  ) %&gt;%\n  ggplot(\n    aes(x= date,\n        y=percent_vaccinated,\n        group = state\n        ))+\n  geom_line(\n    aes(alpha = line_alpha,\n        col =line_col)) +\n  geom_text_repel(aes(label = Label),\n                  direction = \"x\",\n                  nudge_y = 2) +\n  guides(\n    alpha = \"none\",\n    col = \"none\"\n  )+\n  xlim(ym(\"2021-01\"), ym(\"2023-01\")) +\n  labs(\n    y = \"Percent Vacinated\",\n    x = \"Date\"\n  ) +\n  theme_bw()-&gt; fig_line2"
  },
  {
    "objectID": "slides/02-slides.html#scatterplots",
    "href": "slides/02-slides.html#scatterplots",
    "title": "POLS 1600",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat’s the relationship between vaccination rates and new cases of Covid-19?\n\n\n\ncovid_us %&gt;%\n  ggplot(\n    aes(x= percent_vaccinated,\n        y=new_cases_pc,\n        ))+\n  geom_point() -&gt; fig_scatter1\n\n\n\n\nfig_scatter1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(year &gt; 2020) %&gt;%\n  filter(month == 6) %&gt;%\n  filter(new_cases_pc &gt; 0) %&gt;%\n  ggplot(\n    aes(x= percent_vaccinated,\n        y=new_cases_pc,\n        ))+\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  facet_wrap(~year_month,ncol =1,\n             scales = \"free_y\")-&gt; fig_scatter2"
  },
  {
    "objectID": "slides/02-slides.html#summary-1",
    "href": "slides/02-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/11-slides.html#general-plan",
    "href": "slides/11-slides.html#general-plan",
    "title": "Week 11:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nReview: Confidence Intervals\nLecture : Hypothesis Testing\nDemo: Final Projects"
  },
  {
    "objectID": "slides/11-slides.html#course-plan",
    "href": "slides/11-slides.html#course-plan",
    "title": "Week 11:",
    "section": "Course Plan",
    "text": "Course Plan\n\nApril 18: Lecture – Hypothesis Testing\nApril 20 Lab – Hypothesis Testing and Interval Estimation\nApril 25: Lecture – Course Review\nApril 27: Workshop:\nApril 30: Take Home Final Exam\nMay ?: Tacos or Pizza with POLS 1140?\nMay 7: Take Home Final Exam due\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/11-slides.html#new-packages",
    "href": "slides/11-slides.html#new-packages",
    "title": "Week 11:",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anser)"
  },
  {
    "objectID": "slides/11-slides.html#packages-for-today",
    "href": "slides/11-slides.html#packages-for-today",
    "title": "Week 11:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"modelr\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 11:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/11-slides.html#load-packages-for-today",
    "href": "slides/11-slides.html#load-packages-for-today",
    "title": "Week 11:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         purrr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign        modelr           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, middle, center # 🔍 # Review ## Confidence Intervals"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals",
    "href": "slides/11-slides.html#confidence-intervals",
    "title": "Week 11:",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nStatistical inference involves quantifying uncertainty about what could have happened\nWe can describe this uncertainty in terms of a sampling distribution (what could have happened had we had a different sample)\nThe standard deviation of a sampling distribution describes its width (spread) and is called a standard error\nStandard errors decrease with by the \\(\\sqrt{N}\\) where \\(N\\) is the size of our sample\nWe can estimate standard errors via simulation or analytically.\n\nSimulations require fewer assumptions, but take more time.\nAnalytic estimates are are quick, but require more assumptions.\n\nWe use standard errors to construct confidence intervals which we interpret as describing a range of plausible values for the thing we’re trying to estimate.\nAny one 95% confidence interval may or may not contain the truth, but in repeated sampling, 95% of the intervals we construct would contain the truth."
  },
  {
    "objectID": "slides/11-slides.html#standard-errors-of-regression-coefficients",
    "href": "slides/11-slides.html#standard-errors-of-regression-coefficients",
    "title": "Week 11:",
    "section": "Standard Errors of Regression Coefficients",
    "text": "Standard Errors of Regression Coefficients\nLast class, we calculated the standard error of a sample mean using bootstrapping.\nWe can take the same approach for regression coefficients:\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\n# Fit model\nm1 &lt;- lm(support_war01 ~ age + education_n + sex, df_drww)\n# set seed\nset.seed(123)\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n# Estimate Boostrapped Models\nm1_bs &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + education_n + sex, data =.))\n# Tidy coefficients\nm1_bs_df &lt;- map_df(m1_bs, tidy, .id = \"id\")"
  },
  {
    "objectID": "slides/11-slides.html#bootstrapped-standard-errors",
    "href": "slides/11-slides.html#bootstrapped-standard-errors",
    "title": "Week 11:",
    "section": "Bootstrapped Standard Errors",
    "text": "Bootstrapped Standard Errors\nThen we simply calculate the standard error of for the sampling distribution of each coefficient\n\nm1_bs_df %&gt;%\n  group_by(term)%&gt;%\n  summarize(\n    bs_se = sd(estimate)\n  ) -&gt; m1_bs_se\nm1_bs_se\n\n# A tibble: 4 × 2\n  term           bs_se\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept) 0.0564  \n2 age         0.000717\n3 education_n 0.0108  \n4 sexMale     0.0223"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.",
    "href": "slides/11-slides.html#analytic-standard-errors.",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nAlternatively, we can derive the standard error of the sampling distribution analytically using asymptotic theory (e.g. the Central Limit Theorem).\nThe process starts by defining the quantity we want to know: the variance of our estimated coefficients \\(\\hat{\\beta}\\) around their true values in the population \\(\\beta\\):\n\\[V(\\hat{\\beta}|X) =(E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)' |X]\\]\nYour textbook walks through the math to estimate this quantity on pages 375-380 for simple bivariate regression (i.e. a regression with 1 predictor)\nIn the notes, you’ll find some further discussion of the math for the more general case of with multiple predictors.\nThis is also an excellent walk through of the linear algebra"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.-1",
    "href": "slides/11-slides.html#analytic-standard-errors.-1",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nEssentially, if you expand terms from:\n\\[V(\\hat{\\beta}|X) =(E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)' |X]\\]\nmake substitutions and some assumptions about the distribution of \\(\\epsilon\\), you arrive at a formula for the “Variance Covariance Matrix” of the model:\n\\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] Which is a function of\n\n\\(\\sigma^2\\) the “Sum of Square Errors”\n\\((X'X)^{-1}\\) roughly captures the underlying variance and covariance of the predictors in your model, where:\n\n\\(X\\) is a matrix of predictors (each row is an observation, each column a variable)\n\\(X'X\\) is a symmetric maxtrix (like squaring a variable, but in linear algerba)\n\\(X'X^{-1}\\) is the inverse of this matrix\n\\(sigma^2(X'X)^{-1}\\) is like dividing \\(\\sigma^2\\) by \\(X'X\\)"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.-2",
    "href": "slides/11-slides.html#analytic-standard-errors.-2",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nThe square root of the elements on the diagonal of \\(V(\\hat{\\beta}|X)\\) provides the standard error for each coefficient, which is larger if:\n\nif \\(\\sigma^2\\) is large (When there’s a lot of unexplained variance, our uncertainty is high)\nthe variance of \\(X\\) is small (When predictors don’t vary much our uncertainty increases)"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.-3",
    "href": "slides/11-slides.html#analytic-standard-errors.-3",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nIn practice, you will let your computer calculate these standard errors.\n\nsummary(m1)$coef\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.277540588 0.0529197543  5.244555 1.797036e-07\nage          0.009216952 0.0007129459 12.927982 2.827291e-36\neducation_n -0.015814493 0.0108357258 -1.459477 1.446491e-01\nsexMale      0.094320479 0.0225358310  4.185356 3.017253e-05"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors-by-hand",
    "href": "slides/11-slides.html#analytic-standard-errors-by-hand",
    "title": "Week 11:",
    "section": "Analytic Standard Errors by Hand",
    "text": "Analytic Standard Errors by Hand\nJust for fun:\n\n# Sum of Squared Residuals\nsigma_2 &lt;- sum(resid(m1)^2)/m1$df.residual\n# X transpose X\nXtXinv &lt;- solve(t(model.matrix(m1))%*%model.matrix(m1))\n# SE is square root of diagonal of Variance Covariance Matrix\nsqrt(diag(sigma_2*XtXinv))\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310 \n\nsummary(m1)$coef[,2]\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310"
  },
  {
    "objectID": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients",
    "href": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nEstimate the model to obtain coefficients \\(\\hat{\\beta}\\)\nCalculate Standard Errors using simulation or asymptotic theory\nChoose desired confidence level \\(\\alpha\\) with a corresponding critical value \\(z_{\\alpha/2}\\) derived from an approximation of the hypotehtical sampling distribution\nConstruct a \\((1-\\alpha)\\times 100 \\%\\) percent confidence interval:\n\n\\[CI(\\alpha) = [\\hat{\\beta} - z_{\\alpha/2} \\times \\text{standard error}, \\hat{\\beta} + z_{\\alpha/2} \\times \\text{standard error}]\\]\n\nEstimate the model to obtain coefficients \\(\\hat{\\beta}\\)\n\n\nbeta &lt;- coef(m1)\nbeta\n\n (Intercept)          age  education_n      sexMale \n 0.277540588  0.009216952 -0.015814493  0.094320479 \n\n\n\nCalculate Standard Errors using simulation or asymptotic theory\n\n\nse &lt;- summary(m1)$coef[,2]\nse\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310 \n\n# Similar to bs\nm1_bs_se\n\n# A tibble: 4 × 2\n  term           bs_se\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept) 0.0564  \n2 age         0.000717\n3 education_n 0.0108  \n4 sexMale     0.0223"
  },
  {
    "objectID": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-1",
    "href": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-1",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nCalculate critical value\n\n\nz_fs &lt;- abs(qt(.05/2, m1$df.residual))\nz_fs\n\n[1] 1.961591\n\n\n\nNote: For finite samples we use a \\(t\\) distribution…"
  },
  {
    "objectID": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-2",
    "href": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-2",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nll &lt;- beta - z_fs *se\nul &lt;- beta + z_fs *se\ncbind(ll,ul)\n\n                      ll          ul\n(Intercept)  0.173733660 0.381347516\nage          0.007818443 0.010615460\neducation_n -0.037069758 0.005440772\nsexMale      0.050114390 0.138526568\n\n# Compare to R:\nconfint(m1)\n\n                   2.5 %      97.5 %\n(Intercept)  0.173733660 0.381347516\nage          0.007818443 0.010615460\neducation_n -0.037069758 0.005440772\nsexMale      0.050114390 0.138526568"
  },
  {
    "objectID": "slides/11-slides.html#presenting-confidence-intervals-in-a-regression-table",
    "href": "slides/11-slides.html#presenting-confidence-intervals-in-a-regression-table",
    "title": "Week 11:",
    "section": "Presenting Confidence Intervals in a Regression Table:",
    "text": "Presenting Confidence Intervals in a Regression Table:\n\n```{r, results=\"asis\"}`r''`\ntexreg::htmlreg(m1, \n  digits = 3, \n  ci.force = T)\n```\n\ntexreg::htmlreg(m1,\n        digits = 3, \n        ci.force = T)\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.278*\n\n\n\n\n \n\n\n[ 0.174; 0.381]\n\n\n\n\nage\n\n\n0.009*\n\n\n\n\n \n\n\n[ 0.008; 0.011]\n\n\n\n\neducation_n\n\n\n-0.016\n\n\n\n\n \n\n\n[-0.037; 0.005]\n\n\n\n\nsexMale\n\n\n0.094*\n\n\n\n\n \n\n\n[ 0.050; 0.138]\n\n\n\n\nR2\n\n\n0.107\n\n\n\n\nAdj. R2\n\n\n0.106\n\n\n\n\nNum. obs.\n\n\n1463\n\n\n\n\n\n\n* 0 outside the confidence interval."
  },
  {
    "objectID": "slides/11-slides.html#tidying-regression-models",
    "href": "slides/11-slides.html#tidying-regression-models",
    "title": "Week 11:",
    "section": "Tidying Regression Models",
    "text": "Tidying Regression Models\n\nm1 %&gt;%\n  tidy(., conf.int =  T) -&gt; m1_df\nm1_df\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.278    0.0529        5.24 1.80e- 7  0.174     0.381  \n2 age          0.00922  0.000713     12.9  2.83e-36  0.00782   0.0106 \n3 education_n -0.0158   0.0108       -1.46 1.45e- 1 -0.0371    0.00544\n4 sexMale      0.0943   0.0225        4.19 3.02e- 5  0.0501    0.139"
  },
  {
    "objectID": "slides/11-slides.html#coefficient-plots-as-an-alternative-to-regression-tables",
    "href": "slides/11-slides.html#coefficient-plots-as-an-alternative-to-regression-tables",
    "title": "Week 11:",
    "section": "Coefficient Plots as an Alternative to Regression Tables",
    "text": "Coefficient Plots as an Alternative to Regression Tables\n\nm1_df %&gt;%\n  filter(term !=  \"(Intercept)\")%&gt;%\n  ggplot(aes(x = estimate, \n             y= term, \n             xmin = conf.low,\n             xmax = conf.high,\n             label = round(estimate,3)\n             ))+\n  geom_pointrange()+\n  geom_text(vjust=-1.5)+\n  geom_vline(xintercept = 0, linetype =2)+\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals-summary",
    "href": "slides/11-slides.html#confidence-intervals-summary",
    "title": "Week 11:",
    "section": "Confidence Intervals: Summary",
    "text": "Confidence Intervals: Summary\nWhat is a sampling distribution?\n\nA distribution of values we would have observed upon repeated sampling\nBootstrapping (sampling from a sample) approximates the width of the sampling distribution\n\nWhat is a standard error?\n\nStandard deviation of the sampling distribution\n\nDescribes the width or range of plausible observations we would see.\nDecreases as the sample size increases"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals-summary-1",
    "href": "slides/11-slides.html#confidence-intervals-summary-1",
    "title": "Week 11:",
    "section": "Confidence Intervals: Summary",
    "text": "Confidence Intervals: Summary\nWhat is a confidence interval\n\nCoverage interval for a sampling distribution\n\n“A confidence interval is a way of expressing the precision or repeatability of a statistic, how much variation would likely be present across the possible different random samples from the population”\n\nThree components:\n\nPoint Estimate (i.e. a mean, or coefficient)\nConfidence Level (Often 95 percent by convention)\nMargin of Error (+/- some range (typically 2*SD for 95 percent CI))\n\nConfidence is about the interval\n\n95 percent of the intervals construct in this manner would contain the truth.\n\n\nclass: inverse, center, middle # 💡 # Hypothesis Testing ## How likely is it that we would see what did if our hypothesis were true"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-hypothesis-test",
    "href": "slides/11-slides.html#what-is-a-hypothesis-test",
    "title": "Week 11:",
    "section": "What is a hypothesis test",
    "text": "What is a hypothesis test\n\nA formal way of assessing statistical evidence. Combines\n\nDeductive reasoning (distribution of a test statistic, if the a null hypothesis were true )\nInductive reasoning (based on the test statistic we observed, how likely is it that we would observe it if the null were true?)"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-test-statistic",
    "href": "slides/11-slides.html#what-is-a-test-statistic",
    "title": "Week 11:",
    "section": "What is a test statistic?",
    "text": "What is a test statistic?\n\nA way of summarizing data\n\ndifference of means\ncoefficients from a linear model\ncoefficients from a linear model divded by their standard errors\nR^2"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-null-hypothesis",
    "href": "slides/11-slides.html#what-is-a-null-hypothesis",
    "title": "Week 11:",
    "section": "What is a null hypothesis?",
    "text": "What is a null hypothesis?\n\nA statement about the world\n\nOnly interesting if we reject it\nWould yield a distribution of test statistics “under the null”\nTypically something like “X has no effect on Y” (Null = no effect)\nNever accept the null can only reject"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-p-value",
    "href": "slides/11-slides.html#what-is-a-p-value",
    "title": "Week 11:",
    "section": "What is a p-value?",
    "text": "What is a p-value?\n\nA p-value is a conditional probability summarizing the likelihood of observing a test statistic as far from our hypothesis or farther, if our hypothesis were true.\nIt’s the area in the “tails of the curve” of the distribution of the test statistic under the null."
  },
  {
    "objectID": "slides/11-slides.html#how-do-we-do-hypothesis-testing",
    "href": "slides/11-slides.html#how-do-we-do-hypothesis-testing",
    "title": "Week 11:",
    "section": "How do we do hypothesis testing?",
    "text": "How do we do hypothesis testing?\n\nPosit a hypothesis (e.g. \\(\\beta = 0\\))\n\n–\n\nCalculate the test statistic (e.g. \\((\\hat{\\beta}-\\beta)/se_\\beta\\))\n\n–\n\nDerive the distribution of the test statistic under the null via\n\n\nSimulation\nAsymptotic theory\n\n–\n\nCompare the test statistic to the distribution under the null\n\nIf it’s in the tails \\(\\to\\) very unlikely that we would observe what we did if our hypothesis were true\n\n\n–\n\nCalculate p-value\n\nQuantify how often we would see test statistics as big or bigger\nTwo-side tests: how often do we see test statics as big or bigger in absolute value as our observed test statistic\nOne-side test: how often do we see test statistics as extreme as our observed statistic in a particular direction (positive/negative)\n\n\n–\n\nReject or fail to reject/retain our hypothesis based on some threshold of statistical significance (e.g. p &lt; 0.05)"
  },
  {
    "objectID": "slides/11-slides.html#outcomes-of-hypothesis-tests",
    "href": "slides/11-slides.html#outcomes-of-hypothesis-tests",
    "title": "Week 11:",
    "section": "Outcomes of Hypothesis Tests",
    "text": "Outcomes of Hypothesis Tests\n\nTwo conclusions from of a hypothesis test: we can reject or fail to reject a hypothesis test.\nWe never “accept” a hypothesis, since there are, in theory, an infinite number of other hypotheses we could have tested.\n\nOur decision can produce four outcomes and two types of error:\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative"
  },
  {
    "objectID": "slides/11-slides.html#outcomes-of-hypothesis-tests-1",
    "href": "slides/11-slides.html#outcomes-of-hypothesis-tests-1",
    "title": "Week 11:",
    "section": "Outcomes of Hypothesis Tests",
    "text": "Outcomes of Hypothesis Tests\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative\n\n\n\nSuppose we chose to reject a hypothesis if our p-value was less than 0.05.\nWhat we’re saying is that we’re willing to falsely reject our hypothesis 5 times out of 100.\nTypically we want to minimize this false positive rate (Type 1 error), but there’s a trade off:\n\nReducing Type 1 error means, we’re more likely to make a Type 2 error – failing to reject when our null is false.\n\nclass:inverse, middle, center # 💪 ## Application: Hypothesis Testing for Linear Models"
  },
  {
    "objectID": "slides/11-slides.html#posit-a-null-hypothesis",
    "href": "slides/11-slides.html#posit-a-null-hypothesis",
    "title": "Week 11:",
    "section": "0. Posit a Null Hypothesis",
    "text": "0. Posit a Null Hypothesis\n\nTypically, we will test a null hypothesis that the coefficient for variable \\(X\\) equals 0 (e.g. \\(H_0: \\beta_x = 0\\) )\nOur alternative hypothesis in a two sided test then is that \\(\\beta_0\\) doesn’t equal 0 (\\(H_1: \\beta_x \\neq 0\\))\nIn a one-sided test, our alternative is directional (\\(H_1: \\beta_x &gt; 0\\) or \\(H_1: \\beta_x &lt; 0\\) )\n\nOne-sided tests are rare in practice – need a strong substantive reason for directional expectation"
  },
  {
    "objectID": "slides/11-slides.html#calculate-the-test-statistic",
    "href": "slides/11-slides.html#calculate-the-test-statistic",
    "title": "Week 11:",
    "section": "1. Calculate the test statistic",
    "text": "1. Calculate the test statistic\nFor a linear regression, we could use the \\(\\hat{\\beta}\\) as our test statistic.\nIn practice, we we use a “t-stat” which is our observed coefficiet, \\(\\hat{\\beta}\\) minus our hypothesized value \\(\\beta\\) (e.g. 0), divided by the standard error of \\(\\hat{\\beta}\\).\n\\[t= \\frac{\\hat\\beta-\\beta}{\\widehat{SE}_{\\hat{\\beta}}} \\sim \\text{Students's } t \\text{ with } n-k \\text{ degrees of freedom}\\] Fisher showed that this statistic from a regression follows a \\(t\\) distribution – which looks like a normal distribution but with “fatter tails” (e.g. more probability assigned to extreme values)"
  },
  {
    "objectID": "slides/11-slides.html#calculate-the-test-statistic-in-r",
    "href": "slides/11-slides.html#calculate-the-test-statistic-in-r",
    "title": "Week 11:",
    "section": "1. Calculate the test statistic in R",
    "text": "1. Calculate the test statistic in R\nWe can caclulate test statistics from our model by dividing the coefficients by their standard errors\n\nt_stat &lt;- coef(m1) / summary(m1)$coef[,2]\nt_stat\n\n(Intercept)         age education_n     sexMale \n   5.244555   12.927982   -1.459477    4.185356 \n\n\nWhich is exactly what the third column of summary() shows\n\nsummary(m1)$coef\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.277540588 0.0529197543  5.244555 1.797036e-07\nage          0.009216952 0.0007129459 12.927982 2.827291e-36\neducation_n -0.015814493 0.0108357258 -1.459477 1.446491e-01\nsexMale      0.094320479 0.0225358310  4.185356 3.017253e-05"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-via",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-via",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null via",
    "text": "2. Derive the distribution of the test statistic under the null via\nTwo approaches:\n\nSimulation: Calculate test statistics in a world where \\(H_0\\) is true\nAsymptotic theory: Use statistics to approximate this distribution"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nWe can make the null true, by randomly permuting (sampling without replacement) the outcome of our model:\n\n# One perumtation\nset.seed(123)\ndf_drww$support_war01_null &lt;- sample(df_drww$support_war01)\n\nOur permuted outcome is now uncorrelated with any predictors, it’s just a random sample of 0s, and 1s.\n\ncor(df_drww$support_war01, df_drww$age, use = \"complete.obs\")\n\n[1] 0.3093656\n\ncor(df_drww$support_war01_null, df_drww$age, use = \"complete.obs\")\n\n[1] -0.0005132924"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-1",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-1",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nAs such when we estimate a model with data where we have made our null hypothesis true (e.g \\(\\beta = 0\\) for all predictors), we get coefficients that are close to 0. Of course, by chance some might be a little negative, or a little positive.\n\nm1_null &lt;- lm(support_war01_null ~ age + education_n + sex, df_drww)\n\ntidy(m1_null)%&gt;%mutate_if(is.numeric, round,2)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     0.74      0.05     13.6     0   \n2 age             0         0        -0.07    0.94\n3 education_n    -0.01      0.01     -0.8     0.43\n4 sexMale         0.02      0.02      0.65    0.52"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-2",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-2",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nThe logic of hypothesis testing is to compare what we observed, to what we could have observed when our null is true.\nBy repeatedly permuting our outcome, estimating models, caclulating test statistics to generate the distributions under the null\nTo do this, we’ll write a function to make our life easier:\n\nmy_null_fn &lt;- function(\n  df=df_drww, \n  y = \"support_war01\", \n  f = formula(m1)\n  ){\n  df[, y] &lt;- sample(df[,y])\n  m &lt;- lm(f, df)\n  stat &lt;- summary(m)$coef[,3]\n  return(stat)\n}\nmy_null_fn()\n\n(Intercept)         age education_n     sexMale \n 11.0493734   1.8113230   0.9787517   0.6637328 \n\nmy_null_fn()\n\n(Intercept)         age education_n     sexMale \n 13.8126681   0.3840811  -1.4405531  -0.9781048"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-3",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-3",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nThen we’ll use the replicate() function to repeat this process 1000, saving the distribution of test statistics \\(((\\hat{\\beta}-\\beta)/se_\\beta)\\) to an object called m1_null\n\nset.seed(123)\nm1_null &lt;- data.frame(t(\n  replicate(1000, my_null_fn(df_drww, \"support_war01\",formula(m1)))))\nhead(m1_null)\n\n  X.Intercept.         age education_n    sexMale\n1     13.58601 -0.06917489  -0.7952217  0.6460818\n2     11.04937  1.81132298   0.9787517  0.6637328\n3     13.81267  0.38408113  -1.4405531 -0.9781048\n4     13.15015  0.04525689  -0.9986349  1.0681923\n5     13.41492 -0.59487147   0.4026906 -1.5637179\n6     13.43857 -1.11574477   0.3126860 -0.7188458"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-4",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-4",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nWe can visualize this distribution of test statistics for the education_n:\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#compare-permutation-distribution-to-asymptotic-t-distribution",
    "href": "slides/11-slides.html#compare-permutation-distribution-to-asymptotic-t-distribution",
    "title": "Week 11:",
    "section": "2. Compare Permutation Distribution to Asymptotic t-Distribution",
    "text": "2. Compare Permutation Distribution to Asymptotic t-Distribution\nThis simulated distribution (grey) can be approximated by a \\(t\\) distribution (black), with \\(n\\) (the number of observations) minus \\(k\\) (the number of coefficients in our model) degrees of freedom.\n\nRoughly, you can think of “degrees of freedom” as a way of reflecting the fact that we have to estimate parameters of the distribution, and so we use up pieces of information (degrees of freedom),\nBecause we’re estimating parameters our uncertainty (the spread of the distribution) increases as the degrees of freedom decreases\nFor a small \\(N\\) reducing degrees of freedom, can increase uncertainty a lot\nFor a large \\(N\\) the differences are minute\n\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col = \"grey\")+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  stat_function(fun =dt,args =list(df = m1$df.residual), #&lt;&lt;\n                xlim =c(-3.5,3.5))+ #&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#compare-the-test-statistic-to-the-distribution-assuming-the-null-were-true",
    "href": "slides/11-slides.html#compare-the-test-statistic-to-the-distribution-assuming-the-null-were-true",
    "title": "Week 11:",
    "section": "3. Compare the test statistic to the distribution assuming the null were true",
    "text": "3. Compare the test statistic to the distribution assuming the null were true\nNext we’ll compare our observed test statistic -1.45 to its hypothesized distribution under the null\n\nFor a two sided test, we interested in absolute distance from the null, so we’ll put lines at both -1.45 and 1.45\n\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col = \"grey\")+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  stat_function(fun =dt,args =list(df = m1$df.residual),\n                xlim =c(-3.5,3.5))+\n  geom_vline(xintercept =t_stat[\"education_n\"], linetype = 3)+ #&lt;&lt;\n  geom_vline(xintercept =t_stat[\"education_n\"]*-1, linetype = 3)+ #&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#calculate-p-value",
    "href": "slides/11-slides.html#calculate-p-value",
    "title": "Week 11:",
    "section": "4. Calculate p-value",
    "text": "4. Calculate p-value\nTo quantify how often we would see test statistics as big or bigger, we can simply take the mean of a logical comparison of the absolute value of the test statistics under the null greater than the absolute value of our observed test statistic\n\n# Simulation\nmean(abs(m1_null$education_n) &gt; abs(t_stat[\"education_n\"]))\n\n[1] 0.145\n\n\nOr, we can calculate the area under the curve for a \\(t\\)-distribution, as with 1459 degrees of freedom, that is less than -1.45 and multiply this by 2 (because the distribution is symetric), to get the p-value\n\n# Asymptotic\n2*pt(t_stat[\"education_n\"], m1$df.residual)\n\neducation_n \n  0.1446491"
  },
  {
    "objectID": "slides/11-slides.html#calculate-p-value-1",
    "href": "slides/11-slides.html#calculate-p-value-1",
    "title": "Week 11:",
    "section": "4. Calculate p-value",
    "text": "4. Calculate p-value\n\n# Asymptotic\n2*pt(t_stat[\"education_n\"], m1$df.residual)\n\neducation_n \n  0.1446491 \n\n\nOs exactly what R’s summary() function returns:\n\n# R's summary\nsummary(m1)$coef[4,]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n9.432048e-02 2.253583e-02 4.185356e+00 3.017253e-05 \n\n\nVisually, these calculations look like this:\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col=\"grey\")+\n  geom_rug(col = ifelse(abs(m1_null$education_n) &gt;= abs(t_stat[\"education_n\"]), \"red\",\"black\"))+ #&lt;&lt;\n  geom_vline(xintercept =0, linetype = 2)+\n  geom_vline(xintercept =t_stat[\"education_n\"], linetype = 3)+\n  geom_vline(xintercept =t_stat[\"education_n\"]*-1, linetype = 3)+\n  stat_function(fun =dt,args =list(df = m1$df.residual),\n                xlim =c(-3.5,3.5))+\n  stat_function(fun =dt,args =list(df = m1$df.residual), #&lt;&lt;\n                geom =\"area\", xlim = c(-3.5,t_stat[\"education_n\"]),#&lt;&lt;\n                alpha = .5,fill =\"red\")+#&lt;&lt;\n  stat_function(fun =dt, args =list(df = m1$df.residual), #&lt;&lt;\n                geom =\"area\", xlim = c(abs(t_stat[\"education_n\"]),3.5),#&lt;&lt;\n                alpha = .5, fill =\"red\")+#&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#reject-or-fail-to-rejectretain-our-hypothesis",
    "href": "slides/11-slides.html#reject-or-fail-to-rejectretain-our-hypothesis",
    "title": "Week 11:",
    "section": "Reject or fail to reject/retain our hypothesis",
    "text": "Reject or fail to reject/retain our hypothesis\nFor an estimated p-value of 0.145 for coefficient on education_n and a significance threshold of p &lt; 0.05, we would…\n–\nFail to reject the null hypothesis that \\(H_0: \\beta_{education} = 0\\)\nBecause in a world where the truth was \\(\\beta_{education} = 0\\) test statistics reflecting coefficients as far as \\(t-stat = |-1.459|\\) about 14.5 percent of the time.\nIf we were to reject the null, 14.5 percent of the time, we would be making a Type-1 error | False Positive | concluding there was a relationship when in fact their wasn’t.\nclass:inverse, middle, center # Break\nclass:inverse, middle, center # Final Projects"
  },
  {
    "objectID": "slides/11-slides.html#timelines-this-week",
    "href": "slides/11-slides.html#timelines-this-week",
    "title": "Week 11:",
    "section": "Timelines: This Week",
    "text": "Timelines: This Week\nNOTE: I’m leaving these slides in for nw\n\nFeedback on Data today\nQuestions/office hours today tomorrow/by appointment\nThursday: Work on analyzing your data\nDrafts are still due April 24th.\nYour draft need not, and probably will not, be a completed project.\nAt a minimum what you want is:\n\nA clean data set\nExploratory descriptives\nInitial results\n\nThe rest can come later. Our goal on Thursday is to move you from the data portions of your projects, into the analysis."
  },
  {
    "objectID": "slides/11-slides.html#timelines-next-week",
    "href": "slides/11-slides.html#timelines-next-week",
    "title": "Week 11:",
    "section": "Timelines: Next Week",
    "text": "Timelines: Next Week\n\nTuesday, April 26: Workshop: Review and Presentations\nThursday April 28: Workshop: Producing Presentations\n\n7-10 Slides\nDrafts/Analysis, just need to fill those slides\n\nSunday, May 1: Presentations due (Monday at the latests)"
  },
  {
    "objectID": "slides/11-slides.html#timelines-first-week-of-may",
    "href": "slides/11-slides.html#timelines-first-week-of-may",
    "title": "Week 11:",
    "section": "Timelines: First Week of May:",
    "text": "Timelines: First Week of May:\n\nTuesday, May 3: Presentations\nThursday, May 5: Last class and Food?\n\nClass at Dolores at 5pm?\nBagels/Treats in class\n\nSunday May 8: Final Papers Due"
  },
  {
    "objectID": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "href": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "title": "Week 11:",
    "section": "Strucutre of Final Paper and Drafts:",
    "text": "Strucutre of Final Paper and Drafts:\nSeven sections:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides.html#focus-on-for-sunday",
    "href": "slides/11-slides.html#focus-on-for-sunday",
    "title": "Week 11:",
    "section": "Focus on for Sunday",
    "text": "Focus on for Sunday\nSeven sections:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides.html#motivating-questions",
    "href": "slides/11-slides.html#motivating-questions",
    "title": "Week 11:",
    "section": "Motivating Questions:",
    "text": "Motivating Questions:\nIn the reset of today’s class, we’ll get some practice putting together the various skills you need for your drafts by exploring the following:\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\nHave these perceptions about vaccines changed over time?"
  },
  {
    "objectID": "slides/11-slides.html#tasks",
    "href": "slides/11-slides.html#tasks",
    "title": "Week 11:",
    "section": "Tasks:",
    "text": "Tasks:\nTo explore these questions, we need to\n\nGet setup to work\nLoad our data\nRecode our data\nSummarize our data\nSpecify our expectations\nEstimate models to test these expectations\nPresent and interpret results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/11-slides.html#new-packages-1",
    "href": "slides/11-slides.html#new-packages-1",
    "title": "Week 11:",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anesr)"
  },
  {
    "objectID": "slides/11-slides.html#packages-for-today-1",
    "href": "slides/11-slides.html#packages-for-today-1",
    "title": "Week 11:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"modelr\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages-1",
    "href": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages-1",
    "title": "Week 11:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/11-slides.html#load-packages-for-today-1",
    "href": "slides/11-slides.html#load-packages-for-today-1",
    "title": "Week 11:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         purrr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign        modelr           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Loading Data"
  },
  {
    "objectID": "slides/11-slides.html#data",
    "href": "slides/11-slides.html#data",
    "title": "Week 11:",
    "section": "Data",
    "text": "Data\nNow that we have anesr installed, let’s load data from the 2016 and 2020 National Election Studies:\n\n# Load data\ndata(timeseries_2016, package = \"anesr\")\ndata(timeseries_2020, package = \"anesr\")\n\nAnd copy those data frames into new dataframes with shorter names\n\n# Rename datasets\nnes16 &lt;- timeseries_2016\nnes20 &lt;- timeseries_2020\n\nclass:inverse, center, middle # 💪 ## Recoding Data"
  },
  {
    "objectID": "slides/11-slides.html#finding-variables-of-interest",
    "href": "slides/11-slides.html#finding-variables-of-interest",
    "title": "Week 11:",
    "section": "Finding variables of interest",
    "text": "Finding variables of interest\nOur primary outcome of interest are beliefs about vaccines.\nVariables V162162x in the 2016 NES and V202383x in the 2020 NES will serve as our primary outcome of interest, summarizing respondents answer to the following question:\n\nDo the health benefits of vaccinations generally outweigh the risks, do the risks outweigh the benefits, or is there no difference?\n\nSimilarly, V161158x in the 2016 NES and V201231x in the 2020 NES will serve our key predictor (respondent’s partisanship).\nFinally, we’ll control respondents’ age, using V161267 in the 2016 NES and V201507x in the 2020 NES\nLet’s take a look at the values and distributions of these raw variables and think about what we need to do to recode these data so that they are suitable for analysis"
  },
  {
    "objectID": "slides/11-slides.html#look-at-the-distribution-and-coding-of-our-outcome-vaccine-beliefs",
    "href": "slides/11-slides.html#look-at-the-distribution-and-coding-of-our-outcome-vaccine-beliefs",
    "title": "Week 11:",
    "section": "Look at the distribution and coding of our outcome: Vaccine Beliefs",
    "text": "Look at the distribution and coding of our outcome: Vaccine Beliefs\nThe variables in the NES datasets are of a class labelled which allows numeric values to have substantive labels\n\nclass(nes16$V162162x)\n\n[1] \"haven_labelled\"\n\n\nOur outcome variable has the following labels:\n\nlabelled::val_labels(nes16$V162162x)\n\n                                   -9. Refused \n                                            -9 \n                                -8. Don't know \n                                            -8 \n-7. No post data, deleted due to incomplete IW \n                                            -7 \n                -6. No post-election interview \n                                            -6 \n                      1. Benefits much greater \n                                             1 \n                2. Benefits moderately greater \n                                             2 \n                  3. Benefits slightly greater \n                                             3 \n                              4. No difference \n                                             4 \n                     5. Risks slightly greater \n                                             5 \n                   6. Risks moderately greater \n                                             6 \n                         7. Risks much greater \n                                             7 \n\n\nAnd distribution of responses:\n\ntable(nes16$V162162x)\n\n\n  -9   -8   -7   -6    1    2    3    4    5    6    7 \n  21   28   86  536 1687  726  258  539   96  211   82"
  },
  {
    "objectID": "slides/11-slides.html#recoding-our-outcome-variable",
    "href": "slides/11-slides.html#recoding-our-outcome-variable",
    "title": "Week 11:",
    "section": "Recoding our outcome variable",
    "text": "Recoding our outcome variable\nWhat transformations do we need to make to V162162x in nes16 and V202383x in nes20 so that these variables are suitable for analysis?\n–\n\nRecode negative values to be NA\n\n–\n\nReverse code so that higher values indicate greater belief in the benefits of vaccines\n\n–\n\nCreate an indicator of people who are skeptical of the benefits of vaccines"
  },
  {
    "objectID": "slides/11-slides.html#recoding-v162162x-in-2016-nes",
    "href": "slides/11-slides.html#recoding-v162162x-in-2016-nes",
    "title": "Week 11:",
    "section": "Recoding V162162x in 2016 NES",
    "text": "Recoding V162162x in 2016 NES\n\nnes16 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V162162x &lt; 0, NA, (V162162x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes16 # Save recodes to nes16"
  },
  {
    "objectID": "slides/11-slides.html#recoding-v202383x-in-2020-nes",
    "href": "slides/11-slides.html#recoding-v202383x-in-2020-nes",
    "title": "Week 11:",
    "section": "Recoding V202383x in 2020 NES",
    "text": "Recoding V202383x in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V202383x &lt; 0, NA, (V202383x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes20 # Save recodes to nes20"
  },
  {
    "objectID": "slides/11-slides.html#recoding-predictors",
    "href": "slides/11-slides.html#recoding-predictors",
    "title": "Week 11:",
    "section": "Recoding Predictors",
    "text": "Recoding Predictors\nNow we repeat this process for our key predictor, partisanship.\n\nRecode the the summary partisanship variables V161158x in nes16 and V201231x in nes20\nCreate indicators from this recoded variable that classify partisanship as categorical variable (with Democrats as the reference category)\n\nAnd our covariate, age variables V161267 in nes16 and V201507x in nes20\n\nRecode negative values to be NA"
  },
  {
    "objectID": "slides/11-slides.html#recoding-partisanship-v161158x-in-2016-nes",
    "href": "slides/11-slides.html#recoding-partisanship-v161158x-in-2016-nes",
    "title": "Week 11:",
    "section": "Recoding Partisanship (V161158x) in 2016 NES",
    "text": "Recoding Partisanship (V161158x) in 2016 NES\n\nnes16 %&gt;%\n  mutate(\n    pid = ifelse(V161158x &lt; 0, NA, V161158x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V161267 &lt; 0, NA, V161267)\n  ) -&gt; nes16"
  },
  {
    "objectID": "slides/11-slides.html#recoding-partisanship-v201231x-in-2020-nes",
    "href": "slides/11-slides.html#recoding-partisanship-v201231x-in-2020-nes",
    "title": "Week 11:",
    "section": "Recoding Partisanship (V201231x) in 2020 NES",
    "text": "Recoding Partisanship (V201231x) in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    pid = ifelse(V201231x &lt; 0, NA, V201231x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V201507x &lt; 0, NA, V201507x)\n  ) -&gt; nes20"
  },
  {
    "objectID": "slides/11-slides.html#progress-report",
    "href": "slides/11-slides.html#progress-report",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data📥\nSpecify our expectations\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Summarizing data"
  },
  {
    "objectID": "slides/11-slides.html#tables-of-descriptive-statistics",
    "href": "slides/11-slides.html#tables-of-descriptive-statistics",
    "title": "Week 11:",
    "section": "Tables of descriptive statistics",
    "text": "Tables of descriptive statistics\n\nCreate a object with the names of the variables you want to summarize\nSelect these variables\nPivot the data\nCalculate summary statistics\nFormat as an html table"
  },
  {
    "objectID": "slides/11-slides.html#tables-of-descriptive-statistics-1",
    "href": "slides/11-slides.html#tables-of-descriptive-statistics-1",
    "title": "Week 11:",
    "section": "Tables of descriptive statistics",
    "text": "Tables of descriptive statistics\n\n# 1. Create a object with the names of the variables you want to summarize\nthe_vars &lt;- c(\"vaccine_skeptic01\",\"pid\",\"age\")\n# 2. Select these variables\nnes16 %&gt;%\n  select(all_of(the_vars)) %&gt;%\n# 3. Pivot the data\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  # 3. Calculate summary statistics\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) -&gt; sum_tab"
  },
  {
    "objectID": "slides/11-slides.html#format-table",
    "href": "slides/11-slides.html#format-table",
    "title": "Week 11:",
    "section": "Format Table",
    "text": "Format Table\n\nknitr::kable(sum_tab,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =2) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 3, end_row =3)\n\n\n\n\nDescriptive Statistics\n\n\nVariable\nmin\np25\nMedian\nmean\np75\nmax\nmissing\n\n\n\n\nOutcome\n\n\nvaccine_skeptic01\n0\n0\n0\n0.26\n0\n1\n671\n\n\nKey Predictors\n\n\npid\n1\n2\n4\n3.86\n2\n7\n23\n\n\nCovariates\n\n\nage\n18\n34\n50\n49.58\n34\n90\n121"
  },
  {
    "objectID": "slides/11-slides.html#progress-report-1",
    "href": "slides/11-slides.html#progress-report-1",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data ✅\nSpecify our expectations 📥\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Specificying Expecations"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations",
    "href": "slides/11-slides.html#specificying-expecations",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nConsider our first two motivating questions\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\n\nAnd some illustrative stereotypes:\n\n“Republicans are anti-science”\n“Liberal always for Goopy pseudo-science”\n“Independents love to do their own research”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-1",
    "href": "slides/11-slides.html#specificying-expecations-1",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nSimilarly, consider our third question:\n\nHave these perceptions about vaccines changed over time?\n\nAnd some similar simplified claims:\n\n“The Covid-19 vaccine is a miracle of modern science”\n“Social media is rife with misinformation about the Covid-19 vaccine”\n“Politicians are politicizing vaccine politics for political benefits”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-2",
    "href": "slides/11-slides.html#specificying-expecations-2",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nOur goal is to take claims/conventional wisdom/theories, and derive their empirical implications:\n\nH1: Partisan Differences in Vaccine Skepticism\n\nH1a: Republicans will be the most skeptical of vaccines\nH1b: Democrats will be the most skeptical of vaccines\nH1a: Independents will be the most skeptical of vaccines\n\nH2: Temporal Differences in Vaccine Skepticism\n\nH2a: Vaccine skepticism will decrease from 2016 to 2020 with the widespread roll out of the Covid-19 vaccine\nH2b: Vaccine skepticism will increase from 2016 to 2020 with increased amounts of misinformation about the Covid-19 vaccine\n\nH3: Partisan Difference in Vaccine Skepticism Over Time Partisan differences in Vaccine Skepticism will increase from 2016 to 2020 with the politicization of Covid-19 policies"
  },
  {
    "objectID": "slides/11-slides.html#motivating-your-expectations.",
    "href": "slides/11-slides.html#motivating-your-expectations.",
    "title": "Week 11:",
    "section": "Motivating your expectations.",
    "text": "Motivating your expectations.\nIn your papers, unlike in these slides, your expectations should be grounded in existing theory, research, and evidence. For the present question, we might cite sources such as:\n\nEnders, Adam M., and Steven M. Smallpage. “Informational cues, partisan-motivated reasoning, and the manipulation of conspiracy beliefs.” Political Communication 36.1 (2019): 83-102.\nStecula, Dominik A., and Mark Pickup. “How populism and conservative media fuel conspiracy beliefs about COVID-19 and what it means for COVID-19 behaviors.” Research & Politics 8.1 (2021): 2053168021993979.\nJennings, Will, et al. “Lack of trust, conspiracy beliefs, and social media use predict COVID-19 vaccine hesitancy.” Vaccines 9.6 (2021): 593.\nHollander, Barry A. “Partisanship, individual differences, and news media exposure as predictors of conspiracy beliefs.” Journalism & Mass Communication Quarterly 95.3 (2018): 691-713."
  },
  {
    "objectID": "slides/11-slides.html#model-specification",
    "href": "slides/11-slides.html#model-specification",
    "title": "Week 11:",
    "section": "Model Specification",
    "text": "Model Specification\nTranslate these expectations into empirical models requires choices about how to specify our models\n–\n\nHow should we measure/operationalize our outcome\n\nShould we measure beliefs about vaccines with 7-point ordinal scale or as a binary indicator of vaccine skepticism\n\nHow should we measure/operationalize our key predictor(s)\n\nShould we measure partisanship using a 7 point scale or as categorical variable?\n\nWhat should we control for in our model?\n\nFactors likely to predict both our outcome and our key predictor of interest\n\n\n–\n\nThere are rarely definitive answers to these questions.\nIn practice, we will often estimate multiple models to try and show that our findings are robust to alternative modeling strategies/specifications"
  },
  {
    "objectID": "slides/11-slides.html#model-specification-1",
    "href": "slides/11-slides.html#model-specification-1",
    "title": "Week 11:",
    "section": "Model Specification",
    "text": "Model Specification\nFor your projects, every group will almost surely estimate some form of the following:\n\nBaseline bivariate model: The simplest test of the relationship between your outcome and key predictor\nMultiple regression model: A test of the robustness of this relationship, controlling for alternative explanations\n\nIn practice, I suspect you may estimate multiple regression models such as:\n\nAlternative specifications/operationalizations of outcomes and predictors\nInteraction models to test conditional relationships\nPolynomial models to test non-linear relationships"
  },
  {
    "objectID": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models",
    "href": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models",
    "title": "Week 11:",
    "section": "Translating Theoretical Expectations into Empirical Models",
    "text": "Translating Theoretical Expectations into Empirical Models\nBefore we estimate our models in R, we will write down our models formally and empirical implications of our theoretical expectations in terms of the coefficients of our model.\nFor example, test for partisan differences in vaccine skepticism, we might fit the following baseline model:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} + X\\beta + \\epsilon\\] - If \\(\\beta_1\\) is positive this is consistent with H1a (greater skepticism among Republicans), - If \\(\\beta_2\\) is negative this is consistent with H1b (greater skepticism among Democrats),\nBut how could we test H1c – greater skepticism among Independents, who are “4s” on \\(\\text{PID}_{7pt}\\)?"
  },
  {
    "objectID": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models-1",
    "href": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models-1",
    "title": "Week 11:",
    "section": "Translating Theoretical Expectations into Empirical Models",
    "text": "Translating Theoretical Expectations into Empirical Models\nWe could fit a polynomial regression, including both partisanship and partinaship squared to allow the relationship between partisanship and vaccine skepticism to vary non-linearly\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} +  \\beta_2 \\text{PID}_{7pt}^2+ X\\beta+ \\epsilon\\] Or we could estimate a model treating Partisanship as a categorical variable rather than an ordinal interval variable. In our recoding, we set \"Democrat\" to be the first level of the variable pid3cat, so the model R will estimate by default is:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{Ind} +  \\beta_2 \\text{PID}_{Rep}+ X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides.html#testing-differences-over-time.",
    "href": "slides/11-slides.html#testing-differences-over-time.",
    "title": "Week 11:",
    "section": "Testing differences over time.",
    "text": "Testing differences over time.\nTesting Hypotheses 2 and 3 involve making comparisons across models estimated on data from different surveys.\nFormally, testing these expectations is a little more complicated\n\nwe could pool our two surveys together include an interaction term for survey year\n\nFor our purposes, we’ll treat these as more qualitative/exploratory hypotheses:\n\nH2a/b implies overall rates of vaccine skepticism will be lower/higher in 2020 compared to 2016\nH3 implies that whatever partisan differences we find in 2016 should be larger in 2020."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-2",
    "href": "slides/11-slides.html#progress-report-2",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations 📥\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Estimating Empirical Models"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models",
    "href": "slides/11-slides.html#estimating-empirical-models",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nHaving derived empirical implications of our theoretical expectations expressed in terms of linear regressions, now we simply have to estimate our models in R.\nWhen estimating the same model on different datasets we can write the formulas once\n\nf1 &lt;- formula(vaccine_skeptic01 ~ pid + age)\nf2 &lt;- formula(vaccine_skeptic01 ~ pid + I(pid^2) + age)\nf3 &lt;- formula(vaccine_skeptic01 ~ pid3cat + age)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-1",
    "href": "slides/11-slides.html#estimating-empirical-models-1",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nAnd then pass it to lm() with different data arguments:\n\nm1_2016 &lt;- lm(formula = f1, data = nes16)\nm1_2020 &lt;- lm(formula = f1, data = nes20)\nm2_2016 &lt;- lm(formula = f2, data = nes16)\nm2_2020 &lt;- lm(formula = f2, data = nes20)\nm3_2016 &lt;- lm(formula = f3, data = nes16)\nm3_2020 &lt;- lm(formula = f3, data = nes20)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-2",
    "href": "slides/11-slides.html#estimating-empirical-models-2",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nIf you’ve\n\ncoded your data correctly\ndeveloped clear testable implications from your theoretical expectations\n\nSpecifying and estimating empirical models is straightforward. Literally a few lines of code."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-3",
    "href": "slides/11-slides.html#progress-report-3",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations ✅\nPresent our results 📥\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis testing (new!)\n\n\nclass:inverse, center, middle # 💪 ## Presenting and Interpreting Your Results"
  },
  {
    "objectID": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "href": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "title": "Week 11:",
    "section": "Presenting and Interpreting Your Results",
    "text": "Presenting and Interpreting Your Results\nPresenting and interpreting your results is requires both art and science.\nYour goal is to tell a story with your results, walking your reader through the substantive and statsitical interpretation of tables and figures.\nLet’s start by producing a regression table, which provides a concise summary of multiple regression models.\nLike figures, producing a good regression table is an interactive process."
  },
  {
    "objectID": "slides/11-slides.html#regression-tables-with-htmlreg",
    "href": "slides/11-slides.html#regression-tables-with-htmlreg",
    "title": "Week 11:",
    "section": "Regression Tables with htmlreg",
    "text": "Regression Tables with htmlreg\nThe following code produces a basic regression table.\n\ntexreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020)\n)"
  },
  {
    "objectID": "slides/11-slides.html#regression-tables-with-htmlreg-1",
    "href": "slides/11-slides.html#regression-tables-with-htmlreg-1",
    "title": "Week 11:",
    "section": "Regression Tables with htmlreg",
    "text": "Regression Tables with htmlreg\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.46***\n\n\n0.35***\n\n\n0.42***\n\n\n0.34***\n\n\n0.32***\n\n\n0.35***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\npid\n\n\n-0.00\n\n\n0.06***\n\n\n \n\n\n0.02***\n\n\n0.04***\n\n\n \n\n\n\n\n \n\n\n(0.00)\n\n\n(0.02)\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n \n\n\n\n\nage\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\npid^2\n\n\n \n\n\n-0.01***\n\n\n \n\n\n \n\n\n-0.00\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n\n\npid3catIndependent\n\n\n \n\n\n \n\n\n0.17***\n\n\n \n\n\n \n\n\n0.20***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\npid3catRepublican\n\n\n \n\n\n \n\n\n-0.02\n\n\n \n\n\n \n\n\n0.10***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#customizing-our-regression-table",
    "href": "slides/11-slides.html#customizing-our-regression-table",
    "title": "Week 11:",
    "section": "Customizing our Regression Table",
    "text": "Customizing our Regression Table\nLet’s make this regression table more reader friendly and informative by:\n\nGiving the variables in substantive names\nReporting coefficients to 3 decimal places\nUsing a single significance threshold of \\(p &lt; 0.05\\)\nGiving the models custom names\nAdding a header to group models by year\nChanging the caption of the table\n\n\ntexreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020),\n  # Reporting coefficients to 3 decimal places\n  digits = 3,\n  # Using a single significance threshold \n  stars = 0.05,\n  # Giving the variables in substantive names\n  custom.coef.names = c(\n    \"(Intercept)\",\n    \"PID (7pt)\",\n    \"Age\",\n    \"PID&lt;sup&gt;2&lt;/sup&gt; (7pt)\",\n    \"Independent\",\n    \"Republican\"\n  ),\n  # Giving the models custom names\n  custom.model.names = paste(\"Model\",c(1:3,1:3)),\n  # Adding a header to group models by year\n  custom.header = list(\"NES 2016\" = 1:3, \"NES 2020\" = 4:6),\n  # Changing the caption of the table\n  caption = \"Partisanship and Vaccine Skepticism\"\n)\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#telling-a-story-with-regression",
    "href": "slides/11-slides.html#telling-a-story-with-regression",
    "title": "Week 11:",
    "section": "Telling a Story with Regression",
    "text": "Telling a Story with Regression\nFirst, provide an overview the models presented in the table\n\nExplain what each model is doing conceptually\n\nThen, start with your simplest model (Typically the first column in your table).\n\nUse this as a chance to explain core concepts from the course\n\nWhat is regression\nHow should I interpret a coefficient substantively\nHow should I interepret the statistical signficance of a give coefficient\n\nAs you move from left to right (simple to more complex)\n\nyou need not interpret every single coefficient in the model\ninstead highlight the factors that are important for the reader to note (e.g. a comparison between one coefficient in model or another.)\n\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#overview",
    "href": "slides/11-slides.html#overview",
    "title": "Week 11:",
    "section": "Overview",
    "text": "Overview\n\nTable 1 presents the results of three specifications exploring the relationship between partisanship and vaccine skepticism using data from the 2016 (Models 1-3) and 2020 (Models 4-5) National Election Studies.\nModels 1 and 4 operationalize partisanship as a 7-point scale, where 1 corresponds to Strong Democrats, 4 to Indepndents, and 7 to Strong Republicans in the 2016 (Model 1) and 2020 (Model 2) surveys.\nModels 2 and 5 allow the relationship between partisanship and vaccine skepticism to vary non-linear again for the 2016 (Model 2) and 2020 (Model 5) elections.\nModels 3 and 6 treat partisanship as categorical variable, describing how Independents and Republicans differ from Democrats, the reference category in these models.\nAll models control age, since (put in substantive justification for controlling for age here)"
  },
  {
    "objectID": "slides/11-slides.html#story-testing-for-partisan-differences",
    "href": "slides/11-slides.html#story-testing-for-partisan-differences",
    "title": "Week 11:",
    "section": "Story: Testing for Partisan Differences",
    "text": "Story: Testing for Partisan Differences\n\nThe results from Model 1 provide little initial evidence for partisan differences in vaccine skepticism in the 2016 Election.\n\nThe coefficient on the partisanship variable is -0.005, suggesting that a unit increase in partisanship (going from being a Strong Democrat to just a Democrat, or an Independent to an independent who leans Republican), is associated with just a 0.5 percentage point increase in the probability of being a vaccine skeptic (believing that the risks of vaccination outweigh the benefits or that their is no difference in the risks versus benefits).\n\nFurthermore the 95-percent confidence interval for this estimate (-0.011, 0.002) brackets 0, suggesting the true population estimate from this model could be either positive or negative. Similarly, we fail to reject the null hypothesis that the true coefficient on partisanship in this model is 0 as the test statistic for this estimate ( -1.38) corresponds to a p-value of 0.168 suggesting that we would see test statistics this large or larger fairly often when the true relationship was 0.\nIn some the results from Model 1 provide little support for any of the expectations described by H1"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "href": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "title": "Week 11:",
    "section": "Testing for Partisan Differences: Model 2",
    "text": "Testing for Partisan Differences: Model 2\n\nWhile coefficients from Model 1 suggest little evidence of partisan differences in vaccine skepticism, the coefficients on both partisanship, and partisanship squared are statistically significant (p &lt; 0.05)."
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2",
    "href": "slides/11-slides.html#interpreting-model-2",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nThe coefficients from polynomial regressions can be difficult to interpret jointly and so Figure 1 presents the predicted values from Model 2, holding age constant at its sample mean.\n\n\npred_df_m2 &lt;- expand_grid(\n  pid = 1:7,\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m2 &lt;- cbind(pred_df_m2, predict(m2_2016,pred_df_m2, interval =\"confidence\"))\npred_df_m2\n\n  pid      age       fit       lwr       upr\n1   1 49.58231 0.2366157 0.2082979 0.2649335\n2   2 49.58231 0.2743408 0.2551659 0.2935157\n3   3 49.58231 0.2945841 0.2729532 0.3162151\n4   4 49.58231 0.2973457 0.2738012 0.3208902\n5   5 49.58231 0.2826255 0.2611041 0.3041469\n6   6 49.58231 0.2504236 0.2300546 0.2707925\n7   7 49.58231 0.2007398 0.1688900 0.2325897"
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2-1",
    "href": "slides/11-slides.html#interpreting-model-2-1",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\npred_df_m2 %&gt;%\n  ggplot(aes(pid, fit, ymin =lwr, ymax =upr))+\n  geom_line()+\n  geom_ribbon(alpha=.2, fill=\"grey\")+\n  theme_bw()+\n  labs(x = \"Partisanship\",\n       y = \"Predicted Vaccine Skepticism\",\n       title = \"Independents are the most skeptical of vaccines\",\n       subtitle = \"Data: 2016 NES\"\n       )"
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2-2",
    "href": "slides/11-slides.html#interpreting-model-2-2",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nWe see from Model 2 that 29.7 percent [27.3%, 32.1%] of Independents in the 2016 NES were predicted to be vaccine skeptics compared to 23.7 percent [20.8%, 26.5%] of Strong Democrats and only 20.1 percent [16.9%, 23.3%] of Strong Republicans."
  },
  {
    "objectID": "slides/11-slides.html#testing-for-partisan-differences-model-3",
    "href": "slides/11-slides.html#testing-for-partisan-differences-model-3",
    "title": "Week 11:",
    "section": "Testing for Partisan Differences: Model 3",
    "text": "Testing for Partisan Differences: Model 3\nModel 3 tells a similar story to model 2. Again, adjusting for differences in vaccine skepticism explained by age, Model 3 predicts that 41.7 percent [37.7%, 45.6%] of Independents in the 2016 NES are vaccine skeptics compared to 24.2 percent [22.1%, 26.2%] of Democrats, and 22.6 percent [20.4%, 24.8%] of Republicans.\nNote the coefficients from Model 3 imply that the differences between Independents and Democrats are statistically significant (\\(\\beta_{Ind} = 0.175, p &lt; 0.05\\)), the differences between Republicans and Democrats are not (\\(\\beta_{Rep} = -0.004, p = 0.31\\))\n\npred_df_m3 &lt;- expand_grid(\n  pid3cat = c(\"Democrat\", \"Independent\",\"Republican\"),\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m3 &lt;- cbind(pred_df_m3, predict(m3_2016,pred_df_m3, interval =\"confidence\"))\npred_df_m3\n\n      pid3cat      age       fit       lwr       upr\n1    Democrat 49.58231 0.2419547 0.2211228 0.2627867\n2 Independent 49.58231 0.4169043 0.3773539 0.4564547\n3  Republican 49.58231 0.2261496 0.2038046 0.2484947"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-differences-over-time",
    "href": "slides/11-slides.html#testing-for-differences-over-time",
    "title": "Week 11:",
    "section": "Testing for Differences Over Time",
    "text": "Testing for Differences Over Time\nThe results for the 2016 NES suggest political independents are most skeptical of vaccines.\nThe results for 2020 suggest the relationship between partisanship and vaccine skepticism has changed overtime.\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05\n\n\n\n\n\n\nThe coefficient on partisanship in model 4 is now positive and statistically significant (p &lt; 0.05), suggesting that as respondents become more Republican, they are more likely to be skeptical of vaccines\nThe coefficients from Model 5 suggest the relationship between partisanship skepticism is non linear, which is confirmed by model 6.\nIn Model 6, we see that independents remain the most skeptical of vaccines in 2020 \\((\\beta = 0.20,\\, p &lt;0.05)\\), but that Republicans now tend to be more skeptical of vaccines than Democrats \\((\\beta = 0.10,\\, p &lt;0.05)\\)\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/03-slides-old.html#general-plan",
    "href": "slides/03-slides-old.html#general-plan",
    "title": "Week 03:",
    "section": "General Plan",
    "text": "General Plan\n\nGroups, Labs & Tutorials\nFeedback\nReview\nLecture:\n\nCausal inference\nNotations to describe causal claims\nCausal Identification\nCausal Identification in experimental designs"
  },
  {
    "objectID": "slides/03-slides-old.html#group-assignments",
    "href": "slides/03-slides-old.html#group-assignments",
    "title": "Week 03:",
    "section": "Group Assignments",
    "text": "Group Assignments"
  },
  {
    "objectID": "slides/03-slides-old.html#labs-tutorials",
    "href": "slides/03-slides-old.html#labs-tutorials",
    "title": "Week 03:",
    "section": "Labs & Tutorials",
    "text": "Labs & Tutorials\n\nUpload Lab 2 if you haven’t done so already\nSame for Tutorials 0, 1, 2.\n\nSome confusion on what to upload, so let’s do a quick demo.\n\n\nclass: inverse, center, middle background-image:url(“https://m.media-amazon.com/images/I/51vs7dtZnZL._AC_.jpg”) background-size:contain"
  },
  {
    "objectID": "slides/03-slides-old.html#what-do-we-like",
    "href": "slides/03-slides-old.html#what-do-we-like",
    "title": "Week 03:",
    "section": "What do we like",
    "text": "What do we like"
  },
  {
    "objectID": "slides/03-slides-old.html#what-do-we-dislike",
    "href": "slides/03-slides-old.html#what-do-we-dislike",
    "title": "Week 03:",
    "section": "What do we dislike",
    "text": "What do we dislike"
  },
  {
    "objectID": "slides/03-slides-old.html#learning-to-code",
    "href": "slides/03-slides-old.html#learning-to-code",
    "title": "Week 03:",
    "section": "Learning to code:",
    "text": "Learning to code:\n\nThinking programattically\n\nChunk big problems into concrete tasks\n\nUse R Markdown to organize your code\nLearn to troubleshoot errors\nDon’t be afraid to FAAFO"
  },
  {
    "objectID": "slides/03-slides-old.html#whos-got-the-right-of-way",
    "href": "slides/03-slides-old.html#whos-got-the-right-of-way",
    "title": "Week 03:",
    "section": "Who’s got the right of way?",
    "text": "Who’s got the right of way?\n\n# Data\ndf %&gt;%\n  mutate(\n    Turn = str_wrap(as_factor(Turn), width = 15)\n  )%&gt;%\n  # Aesthetics\n  ggplot(aes(Turn,fill=Turn))+\n  # Geometries\n  geom_bar(stat=\"count\")+\n  # Other layers\n  coord_flip()+\n  guides(fill =F)+\n  theme_minimal()+\n  labs(y=\"\",\n       x=\"\",\n       title = \"Another car is turning left, who goes first at a red light?\")"
  },
  {
    "objectID": "slides/03-slides-old.html#whos-got-the-right-of-way-1",
    "href": "slides/03-slides-old.html#whos-got-the-right-of-way-1",
    "title": "Week 03:",
    "section": "Who’s got the right of way?",
    "text": "Who’s got the right of way?"
  },
  {
    "objectID": "slides/03-slides-old.html#review",
    "href": "slides/03-slides-old.html#review",
    "title": "Week 03:",
    "section": "Review",
    "text": "Review\n\nSet up\nData wrangling\nData visualization"
  },
  {
    "objectID": "slides/03-slides-old.html#setup",
    "href": "slides/03-slides-old.html#setup",
    "title": "Week 03:",
    "section": "Setup",
    "text": "Setup\nEvery time you work in R\n\nSave your file to your course or project folder\nSet your working directory\nLoad, and if needed, install packages\nMaybe change some global options in your .Rmd file"
  },
  {
    "objectID": "slides/03-slides-old.html#setting-your-working-directory",
    "href": "slides/03-slides-old.html#setting-your-working-directory",
    "title": "Week 03:",
    "section": "Setting your working directory:",
    "text": "Setting your working directory:\n\nMy default code for setting a working directory is:\n\n\n# Set working directory\nwd &lt;- \".\" # Change to file path on your computer\nsetwd(wd)\n\n\nThis is really just a reminder to someone else using my code that they need to have their working directories set up correctly\nR Studio sets the working directory automatically, when you knit the file\nWhen I work on a file, I set the working directory manually"
  },
  {
    "objectID": "slides/03-slides-old.html#setting-your-working-directory-when-working-live",
    "href": "slides/03-slides-old.html#setting-your-working-directory-when-working-live",
    "title": "Week 03:",
    "section": "Setting your working directory when working “Live”",
    "text": "Setting your working directory when working “Live”"
  },
  {
    "objectID": "slides/03-slides-old.html#packages-for-today",
    "href": "slides/03-slides-old.html#packages-for-today",
    "title": "Week 03:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\n  \n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \n  \"haven\", \"labelled\",\n  \n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \n  \"ggthemes\", \"ggpubr\", \"GGally\",\n  \"scales\", \"dagitty\", \"ggdag\", #&lt;&lt;\n  \n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\n  \"qss\" #&lt;&lt;\n)"
  },
  {
    "objectID": "slides/03-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/03-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 03:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/03-slides-old.html#load-packages-for-today",
    "href": "slides/03-slides-old.html#load-packages-for-today",
    "title": "Week 03:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\nkableExtra         DT  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally     scales \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n   dagitty      ggdag    COVID19       maps    mapdata        qss \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load the Covid-19 Data"
  },
  {
    "objectID": "slides/03-slides-old.html#load-the-covid-19-data",
    "href": "slides/03-slides-old.html#load-the-covid-19-data",
    "title": "Week 03:",
    "section": "Load the Covid-19 Data",
    "text": "Load the Covid-19 Data\n\n# covid &lt;- COVID19::covid19(\n#   country = \"US\",\n#   level = 2,\n#   verbose = F\n# )\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/03-slides-old.html#filter-the-covid-19-data-to-include-just-us-states",
    "href": "slides/03-slides-old.html#filter-the-covid-19-data-to-include-just-us-states",
    "title": "Week 03:",
    "section": "Filter the Covid-19 Data to include Just US States",
    "text": "Filter the Covid-19 Data to include Just US States\n\n# Vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )"
  },
  {
    "objectID": "slides/03-slides-old.html#setting-global-options",
    "href": "slides/03-slides-old.html#setting-global-options",
    "title": "Week 03:",
    "section": "Setting global options",
    "text": "Setting global options\n\nLets you control the default behavior of R and R Markdown\n\nDo you want to print warnings when you knit?\nHow big should your figures be\n\nI’ll typically set these for you"
  },
  {
    "objectID": "slides/03-slides-old.html#the-global-options-for-these-slides",
    "href": "slides/03-slides-old.html#the-global-options-for-these-slides",
    "title": "Week 03:",
    "section": "The global options for these slides",
    "text": "The global options for these slides\n\noptions(htmltools.dir.version = FALSE)\n\nknitr::opts_chunk$set(\n  warning = FALSE, \n  message = FALSE, \n  comment = NA, \n  dpi = 300, fig.align = \"center\", out.width = \"80%\", \n  cache = FALSE) #&lt;&lt;"
  },
  {
    "objectID": "slides/03-slides-old.html#cacheing",
    "href": "slides/03-slides-old.html#cacheing",
    "title": "Week 03:",
    "section": "Cacheing",
    "text": "Cacheing\n\nSometimes I will turn cacheing on (cache=TRUE)\nWhen you knit with cache-ing, R saves the output of each codechunk\nThe next time you knit, if the code in a code chunk hasn’t changed, R just loads the saved output from the previous session, rather than re-running the code.\nThis is useful when a chunk of code takes a long time to run\nThis can potentially create errors, if you change something in one code chunk, but subsequent code are unchanged and load older versions of your code, that don’t have the thing you changed.\nIf this happens, try turning off the cacheing (cache=F) and rerunning your code. If your code works, can turn cache-ing back on."
  },
  {
    "objectID": "slides/03-slides-old.html#data-transformations",
    "href": "slides/03-slides-old.html#data-transformations",
    "title": "Week 03:",
    "section": "Data transformations",
    "text": "Data transformations\nYou want to:\n\nLoad some data\nCombine multiple functions\nLook at your data\nRecode your data\nTransform your data"
  },
  {
    "objectID": "slides/03-slides-old.html#data-transformations-1",
    "href": "slides/03-slides-old.html#data-transformations-1",
    "title": "Week 03:",
    "section": "Data transformations",
    "text": "Data transformations\n.pull-left[ You want to:\n\nLoad some data\nCombine multiple functions\nLook at your data\nRecode your data\nTransform your data ]\n\n.pull-right[ You could use\n\nread_* functions\n%&gt;% the “pipe” operator\nglimpse() head(), filter(), select(), arrange()\nmutate(), case_when(), ifelse()\nsummarize(), group_by()\n\n]"
  },
  {
    "objectID": "slides/03-slides-old.html#why-are-we-doing-this",
    "href": "slides/03-slides-old.html#why-are-we-doing-this",
    "title": "Week 03:",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\nWhen we looked at the raw data, facial_coverings was a numeric variable with both positive and negative values\n\ntable(covid_us$facial_coverings)\n\n\n   -4    -3    -2    -1     0     1     2     3     4 \n  410  5897  7362   275  3893  8604 17424  9191   622"
  },
  {
    "objectID": "slides/03-slides-old.html#why-are-we-doing-this-1",
    "href": "slides/03-slides-old.html#why-are-we-doing-this-1",
    "title": "Week 03:",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\nSubstantively, the absolute value of these numeric values correspond to increasingly restrictive face mask policies:\n\n0 - No policy\n1 - Recommended\n2 - Required in some specified shared/public spaces outside the home with other people present, or some situations when social distancing not possible\n3 - Required in all shared/public spaces outside the home with other people present or all situations when social distancing not possible\n4 - Required outside the home at all times regardless of location or presence of other people\n\nWith negative values denoting “best guesses” of the policy in effect for most people in a state - Example: Illinois is coded as a negative 4 when Chicago adopts a stringent face mask policy"
  },
  {
    "objectID": "slides/03-slides-old.html#why-are-we-doing-this-2",
    "href": "slides/03-slides-old.html#why-are-we-doing-this-2",
    "title": "Week 03:",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\n\nThe numbers for these policies reflect an ordinal ranking:\n\nNo policy is less restrictive than Recommendations is less restrictive than requirements …\n\nBeyond this ordering, the numbers themselves don’t convey any additional meaning:\n\nA partial requirement (2) is not “twice as restrictive” as a recommendation (1).\n\nHow to treat statewide policies (positive numbers) vs mixed regimes (negative) is tricky\n\nIdeally, we’d use more granular data (e.g. counties)\nIn practice, we’ll collapse this distinction in our code\nAs social scientists, we’d then want explore how our results change using alternative coding rules\n\nBy converting the numeric facial_coverings variable into the categorical, ordered factor face_masks we get a variable that:\n\nHas meaningful labels\nRetains the substantive ordering of policies"
  },
  {
    "objectID": "slides/03-slides-old.html#creating-the-face_masks-variable",
    "href": "slides/03-slides-old.html#creating-the-face_masks-variable",
    "title": "Week 03:",
    "section": "Creating the face_masks variable",
    "text": "Creating the face_masks variable\nHere’s the snippet of code I’ve given you in the past:\n\ncovid_us %&gt;%\nmutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/03-slides-old.html#creating-the-face_masks-variable-1",
    "href": "slides/03-slides-old.html#creating-the-face_masks-variable-1",
    "title": "Week 03:",
    "section": "Creating the face_masks variable",
    "text": "Creating the face_masks variable\nHere’s a slightly clearer way of doing the same thing\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/03-slides-old.html#understanding-what-case_when-does",
    "href": "slides/03-slides-old.html#understanding-what-case_when-does",
    "title": "Week 03:",
    "section": "Understanding what case_when() does",
    "text": "Understanding what case_when() does\nLet’s take a (pseudo) random slice of our data:\n\nset.seed(123)\ncovid_us %&gt;%\n  select(date, state, facial_coverings, face_masks)%&gt;%\n  slice(sample(1:dim(covid_us)[1], size=10, replace = F))\n\n         date         state facial_coverings             face_masks\n1  2020-04-26      Maryland                2      Some requirements\n2  2022-04-25       Florida                1            Recommended\n3  2020-12-16     Tennessee               -3 Required shared places\n4  2020-05-15     Tennessee                1            Recommended\n5  2021-09-18      Arkansas               -2      Some requirements\n6  2021-09-08       Florida               -2      Some requirements\n7  2020-07-11        Alaska               -3 Required shared places\n8  2020-07-07 New Hampshire               -2      Some requirements\n9  2022-09-11      Nebraska                2      Some requirements\n10 2020-04-06        Oregon                1            Recommended"
  },
  {
    "objectID": "slides/03-slides-old.html#understanding-all-that-other-stuff-did",
    "href": "slides/03-slides-old.html#understanding-all-that-other-stuff-did",
    "title": "Week 03:",
    "section": "Understanding all that other stuff did",
    "text": "Understanding all that other stuff did\n.pull-left[ - set.seed(123) sets the “seed” that your computer uses to generate pseudo random numbers - select() selects just the date, state, facial_coverings, and face_masks columns from the data - slice() selects a slice of rows from the data - sample() takes random sample from the sequence of numbers 1 to 40,889 (dim(covid_us)) of size 10, without replacement (no duplicates), which slice() interprets as the rows you want to select\n]\n.pull-right[\n\nset.seed(123)\ncovid_us %&gt;%\n  select(date, state, facial_coverings, face_masks)%&gt;%\n  slice(sample(1:dim(covid_us)[1], size=10, replace = F))\n\n]"
  },
  {
    "objectID": "slides/03-slides-old.html#data-visualization",
    "href": "slides/03-slides-old.html#data-visualization",
    "title": "Week 03:",
    "section": "Data visualization",
    "text": "Data visualization\nA basic graphic requires at minimum:\n\ndata: the dataset containing the variables of interest.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars."
  },
  {
    "objectID": "slides/03-slides-old.html#data-visualization-1",
    "href": "slides/03-slides-old.html#data-visualization-1",
    "title": "Week 03:",
    "section": "Data visualization",
    "text": "Data visualization\nBasic graphics are made even better with:\n\nfacets\nstatistics\ncoordinates\nthemes"
  },
  {
    "objectID": "slides/03-slides-old.html#topics",
    "href": "slides/03-slides-old.html#topics",
    "title": "Week 03:",
    "section": "Topics",
    "text": "Topics\n\nCausal inference\nNotations to describe causal claims\n\nPotential Outcomes\nDirected Acyclic Graphs\n\nCausal Identification\n\nExperimental designs (this week)\nObservational designs (next week)\n\nCausal Identification in Experimental Designs\n\nHow random assignment creates credible counter factual comparisons"
  },
  {
    "objectID": "slides/03-slides-old.html#exercises",
    "href": "slides/03-slides-old.html#exercises",
    "title": "Week 03:",
    "section": "Exercises",
    "text": "Exercises\n\nWhat kinds questions have causal interpretations?\nEstimating an average treatment effect with the resume data\nExploring the data for Broockman and Kalla 2016\n\nclass: inverse, center, middle # 💡 # Causal Inference ## Causal claims imply counterfactual comparisons"
  },
  {
    "objectID": "slides/03-slides-old.html#causal-claims-imply-counterfactual-comparisons",
    "href": "slides/03-slides-old.html#causal-claims-imply-counterfactual-comparisons",
    "title": "Week 03:",
    "section": "Causal claims imply counterfactual comparisons",
    "text": "Causal claims imply counterfactual comparisons\n\nCausal claims imply claims about counterfactuals\n\nWhat would have happened if we were to change some aspect of the world?\n\n\n–\nWhat’s the counter factual for these claims:\n–\n\nForeign aid increases develop\n\n–\n\nWikileaks cost Hillary Clinton the 2016 election\n\n–\n\nDemocracies don’t fight wars with other democracies\n\n–\n\nUniversal Pre-K improves child development"
  },
  {
    "objectID": "slides/03-slides-old.html#casual-claims-are-are-all-around-us",
    "href": "slides/03-slides-old.html#casual-claims-are-are-all-around-us",
    "title": "Week 03:",
    "section": "Casual claims are are all around us",
    "text": "Casual claims are are all around us\n.pull-left[\n\n\n\n\n\n\n\n\n\n]\n–\n.pull-right[\n\n\n\n\n\n\n\n\n\n]"
  },
  {
    "objectID": "slides/03-slides-old.html#casual-claims-are-are-all-around-us-1",
    "href": "slides/03-slides-old.html#casual-claims-are-are-all-around-us-1",
    "title": "Week 03:",
    "section": "Casual claims are are all around us",
    "text": "Casual claims are are all around us\nWhat are some questions that interest you?\nWhat are the counterfactual comparisons they imply?\nclass: inverse, center, middle # 💡 # Notations to describe causal claims\nIn this course, we will use two forms of notation to describe our causal claims.\n\nDirected Acyclic Graphs\nPotential Outcomes Notation"
  },
  {
    "objectID": "slides/03-slides-old.html#directed-acyclic-graphs",
    "href": "slides/03-slides-old.html#directed-acyclic-graphs",
    "title": "Week 03:",
    "section": "Directed Acyclic Graphs",
    "text": "Directed Acyclic Graphs\n\nDirected Acyclic Graphs provide a way of encoding assumptions about casual relationships\n\nDirected Arrows \\(\\to\\) describe a direct causal effect\nArrow from \\(D\\to Y\\) means \\(Y_i(d) \\neq Y_i(d^\\prime)\\) “The outcome ( \\(Y\\)) for person \\(i\\) when D happens ( \\(Y_i(d)\\) ) is different than the the outcome when \\(D\\) doesn’t happen ( \\(Y_i(d^\\prime)\\) )\nNo arrow = no effect ( \\(Y_i(d) = Y_i(d^\\prime)\\) )\nAcyclic: No cycles. A variable can’t cause itself\n\nUsed by Pearl (2009, 2017) to describe and assess causal structural models\n\n\nBasically, they’re math’s fancy pants version of a flow chart"
  },
  {
    "objectID": "slides/03-slides-old.html#why-we-use-directed-acyclic-graphs",
    "href": "slides/03-slides-old.html#why-we-use-directed-acyclic-graphs",
    "title": "Week 03:",
    "section": "Why we use Directed Acyclic Graphs",
    "text": "Why we use Directed Acyclic Graphs\n\nDirected Acyclic Graphs are flexible ways of representing complex relationships\nThey’re great for illustrating sources of potential bias in our models:\nTwo types of bias we’ll talk about today:\n\nConfounder bias: Bias that exists because two variables have a common cause\nCollider bias: Bias we create when we condition on a common consequence"
  },
  {
    "objectID": "slides/03-slides-old.html#what-do-we-mean-by-bias",
    "href": "slides/03-slides-old.html#what-do-we-mean-by-bias",
    "title": "Week 03:",
    "section": "What do we mean by bias",
    "text": "What do we mean by bias\nWe’ll talk about lots of types of bias throughout this course.\nFormally, we’ll say an estimate, \\(\\hat{\\theta}\\) (“theta hat”) is an unbiased estimator of a parameter, \\(\\theta\\) (“theta”) if:\n\\[\nE[\\hat{\\theta}] = \\theta\n\\]\nBias or error, \\(\\epsilon\\), is the difference between our estimate and the truth\n\\[\n\\epsilon = \\hat{\\theta} -\\theta\n\\]\nAn estimator is unbiased if, on average, the errors equal 0\n\\[\nE[\\epsilon] = E[\\hat{\\theta} -\\theta] = 0\n\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#bias-vs.-variance",
    "href": "slides/03-slides-old.html#bias-vs.-variance",
    "title": "Week 03:",
    "section": "Bias vs. variance",
    "text": "Bias vs. variance"
  },
  {
    "objectID": "slides/03-slides-old.html#the-bias-variance-tradeoff",
    "href": "slides/03-slides-old.html#the-bias-variance-tradeoff",
    "title": "Week 03:",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff"
  },
  {
    "objectID": "slides/03-slides-old.html#drawing-dags-in-r",
    "href": "slides/03-slides-old.html#drawing-dags-in-r",
    "title": "Week 03:",
    "section": "Drawing DAGs in R",
    "text": "Drawing DAGs in R\n\nLet’s draw some dags to consider some simple examples:\n\nDoes drinking coffee cause lung cancer? (Confounding bias)\nDoes chicken pox cause influenza? (Collider bias)\n\nNice examples of DAGs from ggdag\n\nConfounding = Common Causes\n\n\n\n\n\n\n\n\n\nControlling for a confounding variable\n\n\n\n\n\n\n\n\n\nColliders = Common Consequence\n\n\n\n\n\n\n\n\n\nDon’t Condition on a Collider\n\n\n\n\n\n\n\n\n\nDon’t Condition on a Collider"
  },
  {
    "objectID": "slides/03-slides-old.html#potential-outcomes",
    "href": "slides/03-slides-old.html#potential-outcomes",
    "title": "Week 03:",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nDAGs are great for representing a wide array of relationships\nDoing causal inference with DAGs is a bit beyond the scope of this class\nInstead, we’ll typically talk about causal claims using the logic of potential outcomes to describe the counterfactual comparisons we want to make.\nTo do this, we’ll need to get comfortable with some basic notation."
  },
  {
    "objectID": "slides/03-slides-old.html#some-general-notes-on-notation",
    "href": "slides/03-slides-old.html#some-general-notes-on-notation",
    "title": "Week 03:",
    "section": "Some general notes on notation",
    "text": "Some general notes on notation\n\nNotation can seem scary, overwhelming, confusing.\nNotation can be incredibly helpful for communicating precisely and clearly\nOur goal is to demystify notation"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-populations-and-samples",
    "href": "slides/03-slides-old.html#general-notation-populations-and-samples",
    "title": "Week 03:",
    "section": "General Notation: Populations and Samples",
    "text": "General Notation: Populations and Samples\n\n\\(U\\): Population of units:\n\nFinite population \\(U = {1,2,3,\\dots,N}\\)\nInfinite super population \\(U = {1,2,3,\\dots,\\infty}\\)\n\n\\(S\\): sample of size \\(n\\) from population \\(U\\)"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-outcome-variables",
    "href": "slides/03-slides-old.html#general-notation-outcome-variables",
    "title": "Week 03:",
    "section": "General Notation: Outcome Variables",
    "text": "General Notation: Outcome Variables\n\n\\(Y_i\\): Observed outcome from unit \\(i\\)\n\n\\(Y_i\\) might be whether persion \\(i\\), call them Paul has Covid while \\(Y_j\\) would be whether person \\(j\\), call them Aleks has Covid"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-treatment-variables",
    "href": "slides/03-slides-old.html#general-notation-treatment-variables",
    "title": "Week 03:",
    "section": "General Notation: Treatment Variables",
    "text": "General Notation: Treatment Variables\n\n\\(D_i\\): An indicator for the receipt of treatment. \\(D_i\\) = 1 if treated, \\(D_i\\) = 0 if untreated,\n\n\\(D_i = 0\\) means Paul got the placebo in a vaccine trial\n\\(D_i = 1\\) means Paul got the vaccine\nGeneralizes to multiple treatments \\(D \\in \\{0,1,2,3, \\dots\\}\\)\n\n\\(Z_i\\): An indicator for the assignment of treatment. \\(Z_i\\) = 1 if assigned to treatment, \\(Z_i\\) = 0 if assigned to control,\n\n\\(Z_i = 1\\) means Paul was assigned to receive the vaccine. Whether Paul got the vaccine depends on \\(D_i\\)"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-observed-and-unobserved-covariates",
    "href": "slides/03-slides-old.html#general-notation-observed-and-unobserved-covariates",
    "title": "Week 03:",
    "section": "General Notation: Observed and Unobserved Covariates",
    "text": "General Notation: Observed and Unobserved Covariates\n\n\\(X_i\\) pre-treatment covariates.\n\n\\(X_i\\) might be Paul’s age, or gender, or partisan affiliation\nPre-treatment refers to things that are determined before the treatment is assigned and therefore cannot be influenced by the treatment\n\n\\(U_i\\) unobserved potential confounds\n\n\\(_i\\) might be something we didn’t or can’t measure like Paul’s genome or general crankiness"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-expected-values",
    "href": "slides/03-slides-old.html#general-notation-expected-values",
    "title": "Week 03:",
    "section": "General Notation: Expected Values",
    "text": "General Notation: Expected Values\n\nThe \\(E[Y]\\) reads as “the expected value of Y”\n\\(E[Y]\\) is defined as a probability weighted average based on the uncoditional probability of Y ( \\(f(y)\\) )\n\n\\[\\operatorname{E}[Y] = \\int_{-\\infty}^\\infty y f(y)\\, dy\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-conditional-expectations",
    "href": "slides/03-slides-old.html#general-notation-conditional-expectations",
    "title": "Week 03:",
    "section": "General Notation: Conditional Expectations",
    "text": "General Notation: Conditional Expectations\n\nThe \\(E[Y|X=x]\\) reads as “the expected value of Y conditional on the value of X”\n\\(E[Y|X=x]\\) is defined as a probability weighted average of Y based on the conditional probability of Y given X ( \\(y f_{Y|X}(y|x)\\) )\n\n\\[\\operatorname{E}[Y \\vert X=x] = \\int_{-\\infty}^\\infty y f (y\\vert x) \\, dy\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#general-notation-expected-values-1",
    "href": "slides/03-slides-old.html#general-notation-expected-values-1",
    "title": "Week 03:",
    "section": "General Notation: Expected Values",
    "text": "General Notation: Expected Values\n\nDon’t worry about the math in this course.\nWe’ll talk more expected values during our discussion of probability\nFor now, just think of \\(E[Y]\\) and \\(E[Y|X=x]\\) as a theoretical averages which we can estimate from the empirical means in our data (mean(data$y), mean(data$y[data$x == 1]))"
  },
  {
    "objectID": "slides/03-slides-old.html#potential-outcomes-notation",
    "href": "slides/03-slides-old.html#potential-outcomes-notation",
    "title": "Week 03:",
    "section": "Potential Outcomes Notation:",
    "text": "Potential Outcomes Notation:\n\nPotential outcomes notation is a way of describing counterfactuals\n\n–\n\n\\(Y_i(d)\\) is the value of the outcome if \\(D_i\\) was \\(d\\)\n\n\\(Y_i(1)\\) is the vote share of Rep. Smith when they support #MeToo\n\\(Y_i(0)\\) is the vote share of Rep. Smith when they do not support #MeToo.\n\n\n–\n\nPotential outcomes are fixed, but we only observe one (of many) potential outcomes \\(\\to\\) Fundamental Problem of Causal Inferenece"
  },
  {
    "objectID": "slides/03-slides-old.html#fundamental-problem-of-causal-infernece",
    "href": "slides/03-slides-old.html#fundamental-problem-of-causal-infernece",
    "title": "Week 03:",
    "section": "Fundamental Problem of Causal Infernece",
    "text": "Fundamental Problem of Causal Infernece\nThe individual causal effect (ICE), \\(\\tau_i\\), of some treatment \\(d_i\\) on some observation \\(i\\) is\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\n\nThe fundamental problem of causal inference is that we only ever see one potential outcome for an individual, and so it’s impossible to know the causal effect of some intervention for that individual\nThe ICE is unidentified\nExample:\n\nPaul got the vaccine. The individual effect of the vaccine on Paul is the difference in his health status when he got the vaccine compared to his health status when he got the placebo. But we only observe one of these two potential outcomes.\n\n\nclass: inverse, center, middle # 💡 # Causal Identification ## What assumptions do we need to make for a causal claim to be credible"
  },
  {
    "objectID": "slides/03-slides-old.html#identification",
    "href": "slides/03-slides-old.html#identification",
    "title": "Week 03:",
    "section": "Identification",
    "text": "Identification\n\nIdentification refers to what we can learn from the data available\nA quantity of interest is identified if, with infinite data it can only take one value\nMathematically, we’ll sometimes say a coefficient in an equation is unidentified if\n\nWe have more predictors than observations, or\nSome of predictors are linear combinations of other predictors."
  },
  {
    "objectID": "slides/03-slides-old.html#causal-identification",
    "href": "slides/03-slides-old.html#causal-identification",
    "title": "Week 03:",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCasual Identification refers to “the assumptions needed for statistical estimates to be given a causal interpretation” Keele (2015)\nWhat’s Your Casual Identification Strategy: What are the assumptions that make your research design credible?\nIdentification &gt; Estimation"
  },
  {
    "objectID": "slides/03-slides-old.html#observational-vs-experimental-designs",
    "href": "slides/03-slides-old.html#observational-vs-experimental-designs",
    "title": "Week 03:",
    "section": "Observational vs Experimental Designs",
    "text": "Observational vs Experimental Designs\n\nExperimental designs are studies in which a causal variable of interest, the treatement, is manipulated by the researcher to examine its causal effects on some outcome of interest\nObservational designs are studies in which a causal variable of interest is assigned by someone other than the researcher (nature, governments, people)\n\nclass: inverse, center, middle # 💡 # Causal Identification in Experimental Designs"
  },
  {
    "objectID": "slides/03-slides-old.html#the-fpoci-is-a-problem-of-missing-data",
    "href": "slides/03-slides-old.html#the-fpoci-is-a-problem-of-missing-data",
    "title": "Week 03:",
    "section": "The FPoCI is a problem of missing data",
    "text": "The FPoCI is a problem of missing data\nThat an individual causal effect \\(\\tau_i\\), is defined as:\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\nThe problem is that for any one individual, we only observe \\(Y_i(1)\\) or \\(Y_i(0)\\), but never both.\n\nIf Paul got the vaccine \\((Y_{Paul}(Vaxxed)=\\text{Covid Free})\\), then we don’t know what Paul’s health status would have been, had he not got the vaccine \\((Y_{Paul}(Unvaxxed) =???)\\)"
  },
  {
    "objectID": "slides/03-slides-old.html#a-statistical-solution-to-the-fpoci",
    "href": "slides/03-slides-old.html#a-statistical-solution-to-the-fpoci",
    "title": "Week 03:",
    "section": "A statistical solution to the FPoCI",
    "text": "A statistical solution to the FPoCI\nRather than individual causal effects:\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\nFocus on average causal effects\n\\[\nE[\\tau_i] = \\overbrace{E[Y_i(1) - Y_i(0)]}^{\\text{Average of a difference}} = \\overbrace{E[Y_i(1)] - E[Y_i(0)]}^{\\text{Difference of Averages}}\n\\]\nWhen does the difference of averages provide us with a good estimate of the average difference?"
  },
  {
    "objectID": "slides/03-slides-old.html#the-hospital-example",
    "href": "slides/03-slides-old.html#the-hospital-example",
    "title": "Week 03:",
    "section": "The hospital example",
    "text": "The hospital example"
  },
  {
    "objectID": "slides/03-slides-old.html#the-hospital-example-1",
    "href": "slides/03-slides-old.html#the-hospital-example-1",
    "title": "Week 03:",
    "section": "The hospital example",
    "text": "The hospital example\nWant to know the effect of going to the hospital on people need to go to the hospital\n\\[\n\\text{What we want} = E[Y(1|D=1) - Y(0|D=1)]\n\\]\n\\[\n\\text{What we want} = E[\\text{Mortality}(\\text{Hospital}|\\text{Sick}) - \\text{Mortality}(\\text{No Hospital}|\\text{Sick})]\n\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#the-hospital-example-2",
    "href": "slides/03-slides-old.html#the-hospital-example-2",
    "title": "Week 03:",
    "section": "The hospital example",
    "text": "The hospital example\nInstead we might end up comparing outcomes among the sick and non-sick\n\\[\\text{What we estimate} = E[Y(1|D=1)] - E[Y(0|D=0)]\\]\n\\[\\text{What we estimate} = E[\\text{Mortality}(\\text{Hospital})] - E[\\text{Mortality}(\\text{No Hospital})]\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#selection-bias",
    "href": "slides/03-slides-old.html#selection-bias",
    "title": "Week 03:",
    "section": "Selection bias",
    "text": "Selection bias\nThe hospital example illustrates the general problem of selection bias\n\\[\\widehat{SATE}=\\overbrace{\\textbf{E[Y(1)|D=1]} - E[Y(0)|D=1]}^{\\text{Average Effect of Treatment on Treated}}\\\\\n\\underbrace{+ E[Y(0)|D=1]- \\textbf{E[Y(0)|D=0]}}_{\\text{Selection Bias}}\\]\n\nThe bold quantities are things we can observe (estimate)\n\\(E[Y(0)|D=1]\\) is a an unobservable counterfactual.\nIt’s the average health outcomes of people who went to the hospital \\((D=1)\\) if they hadn’t \\((Y(0)|D=1)\\)"
  },
  {
    "objectID": "slides/03-slides-old.html#selection-bias-1",
    "href": "slides/03-slides-old.html#selection-bias-1",
    "title": "Week 03:",
    "section": "Selection bias",
    "text": "Selection bias\n\nAdding and subtracting \\(E[Y(0)|D=1]\\) thing doesn’t change the equation but it allows us to write what we can estimate in terms of two quantities:\nThe Average Treatment Effect on the Treated: \\(\\mathbf{E[Y(1)|D=1]} - E[Y(0)|D=1]\\)\n\nThis is what we want to know if we want to make causal claims about the effect of health care\n\nSelection Bias: \\(E[Y(0)|D=1] - \\mathbf{E[Y(0)|D=0]}\\),\n\nThis is the difference in health outcomes between healthy folks who didn’t go to the hospital \\((Y(0)|D=0)\\) and sick folks who went to the hopsital had they not gone \\((Y(0)|D=1)\\)\nThis what confounds our causal claims if we just compare outcomes among people who went to the hospital and people who didn’t\nSelection bias equals 0 if \\(E[Y(0)|D=1] = \\mathbf{E[Y(0)|D=0]}\\)\n\\(E[Y(0)|D=1] = \\mathbf{E[Y(0)|D=0]}\\) if D has been randomly assigned"
  },
  {
    "objectID": "slides/03-slides-old.html#random-assignment-solves-the-problem-of-selection-bias",
    "href": "slides/03-slides-old.html#random-assignment-solves-the-problem-of-selection-bias",
    "title": "Week 03:",
    "section": "Random Assignment “Solves” the Problem of Selection Bias",
    "text": "Random Assignment “Solves” the Problem of Selection Bias\nRandomly assigning treatments creates statistical independence \\((\\unicode{x2AEB})\\) between treatment ( \\(D\\) ) and potential outcomes ( \\(Y(1),Y(0)\\) ) as well as any observed ( \\(X\\) ) or unobserved confounders ( \\(U\\) ):\n\\[Y_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#random-assignment-solves-the-problem-of-selection-bias-1",
    "href": "slides/03-slides-old.html#random-assignment-solves-the-problem-of-selection-bias-1",
    "title": "Week 03:",
    "section": "Random Assignment “Solves” the Problem of Selection Bias",
    "text": "Random Assignment “Solves” the Problem of Selection Bias\nWhen treatment has been randomly assigned, what we can observe ( \\(E[Y_i|D=0], E[Y_i|D=1]\\) ), provides good (unbiased) estimates of theoretical quantities we want observe\n\\[E[Y_i|D=0] = E[Y_i(0)|D=0] = E[Y_i(0)] = E[Y_i(0)|D=1]\\]\n\\[E[Y_i|D=1] = E[Y_i(1)|D=1] = E[Y_i(1)] = E[Y_i(1)|D=0]\\]"
  },
  {
    "objectID": "slides/03-slides-old.html#estimating-an-average-treatment-effect",
    "href": "slides/03-slides-old.html#estimating-an-average-treatment-effect",
    "title": "Week 03:",
    "section": "Estimating an Average Treatment Effect",
    "text": "Estimating an Average Treatment Effect\nIf we treatment as been randomly assigned, we can estimate the ATE by taking the difference of means between treatment and control:\n\\[\n\\begin{align*}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{align*}\n\\]\nThat is, the ATE is causally identified by the difference of means estimator in an experimental design"
  },
  {
    "objectID": "slides/03-slides-old.html#causal-identification-with-experimental-designs",
    "href": "slides/03-slides-old.html#causal-identification-with-experimental-designs",
    "title": "Week 03:",
    "section": "Causal Identification with Experimental Designs",
    "text": "Causal Identification with Experimental Designs\nCausal identification for an experiment, requires very few assumptions:\n–\n\nIndependence (Satisfied by Randomization)\n\n\\(Y(1), Y(0),X,U, \\perp D\\)\n\nSUTVA Stable Unit Treatment Value Assumption (Depends on features of the design)\n\nNo interference between units \\(Y_i(d_1, d_2, \\dots, d_N) = Y_i(d_i)\\)\nNo hidden values of the treatment/Variation in the treatment"
  },
  {
    "objectID": "slides/03-slides-old.html#random-assignment-creates-testable-implications",
    "href": "slides/03-slides-old.html#random-assignment-creates-testable-implications",
    "title": "Week 03:",
    "section": "Random Assignment creates testable implications",
    "text": "Random Assignment creates testable implications\n\nIf treatment has been randomly assigned, we would expect that there should be few differences between pre-treatment covariates in our treatment and control groups.\n\n–\n\nThat is, on average, the only the thing that should differ between these groups is that one group got the treatment and one group did not.\n\n–\n\nIf the treatment had an effect, than we can credibly claim that that effect was due to the presence or absence of the treatment, and not some alternative explanation.\n\n–\n\nThis type of clean counterfactual comparison is what people mean when they talk about an experimental ideal"
  },
  {
    "objectID": "slides/03-slides-old.html#no-causation-without-manipulation",
    "href": "slides/03-slides-old.html#no-causation-without-manipulation",
    "title": "Week 03:",
    "section": "No Causation without Manipulation?",
    "text": "No Causation without Manipulation?\n\n“No causation without manipulation” - Holland (1986)\nCausal effects are well defined when we can imagine manipulating (changing) the value of \\(D_i\\) and only \\(D_i\\)\nBut what about the “effects” of things like:\n\nRace\nSex\nDemocracy\n\nStudying the effects of these factors requires strong theory and clever design Sen and Wasow (2016)\n\nclass:inverse, center, middle"
  },
  {
    "objectID": "slides/03-slides-old.html#the-resume-experiment-p.-33",
    "href": "slides/03-slides-old.html#the-resume-experiment-p.-33",
    "title": "Week 03:",
    "section": "The Resume Experiment (p. 33)",
    "text": "The Resume Experiment (p. 33)\nLet’s take a look at the resume experiment from your text book and compare some of Imai’s code to its tidyverse equivalent\n\n# make sure qss package is loaded\nlibrary(qss)\ndata(\"resume\")"
  },
  {
    "objectID": "slides/03-slides-old.html#high-level-overview-p.-34",
    "href": "slides/03-slides-old.html#high-level-overview-p.-34",
    "title": "Week 03:",
    "section": "High level Overview (p. 34)",
    "text": "High level Overview (p. 34)\n\ndim(resume)\n\n[1] 4870    4\n\nhead(resume)\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n5    Carrie female white    0\n6       Jay   male white    0"
  },
  {
    "objectID": "slides/03-slides-old.html#high-level-overview-p.-34-1",
    "href": "slides/03-slides-old.html#high-level-overview-p.-34-1",
    "title": "Week 03:",
    "section": "High level Overview (p. 34)",
    "text": "High level Overview (p. 34)\n\nsummary(resume)\n\n  firstname             sex                race                call        \n Length:4870        Length:4870        Length:4870        Min.   :0.00000  \n Class :character   Class :character   Class :character   1st Qu.:0.00000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.00000  \n                                                          Mean   :0.08049  \n                                                          3rd Qu.:0.00000  \n                                                          Max.   :1.00000"
  },
  {
    "objectID": "slides/03-slides-old.html#cross-tabs",
    "href": "slides/03-slides-old.html#cross-tabs",
    "title": "Week 03:",
    "section": "Cross Tabs",
    "text": "Cross Tabs\n\nrace.call.tab &lt;- table(race = resume$race,\n                       call = resume$call)\nrace.call.tab\n\n       call\nrace       0    1\n  black 2278  157\n  white 2200  235\n\naddmargins(race.call.tab)\n\n       call\nrace       0    1  Sum\n  black 2278  157 2435\n  white 2200  235 2435\n  Sum   4478  392 4870"
  },
  {
    "objectID": "slides/03-slides-old.html#tidy-cross-tab",
    "href": "slides/03-slides-old.html#tidy-cross-tab",
    "title": "Week 03:",
    "section": "Tidy cross tab",
    "text": "Tidy cross tab\n\nresume %&gt;%\n  group_by(race, call)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  pivot_wider(names_from = call, values_from = n)\n\n# A tibble: 2 × 3\n# Groups:   race [2]\n  race    `0`   `1`\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 black  2278   157\n2 white  2200   235"
  },
  {
    "objectID": "slides/03-slides-old.html#calculating-call-back-rates",
    "href": "slides/03-slides-old.html#calculating-call-back-rates",
    "title": "Week 03:",
    "section": "Calculating Call Back Rates",
    "text": "Calculating Call Back Rates\n\n# Overall\nsum(race.call.tab[,2])/nrow(resume)\n\n[1] 0.08049281\n\n# Black names\ncb_bl &lt;- sum(race.call.tab[1,2])/sum(race.call.tab[1,])\n# White Names\ncb_wh &lt;- sum(race.call.tab[2,2])/sum(race.call.tab[2,])\n\n# ATE\ncb_wh - cb_bl\n\n[1] 0.03203285"
  },
  {
    "objectID": "slides/03-slides-old.html#calculating-call-back-rates-with-group_by",
    "href": "slides/03-slides-old.html#calculating-call-back-rates-with-group_by",
    "title": "Week 03:",
    "section": "Calculating Call Back Rates with group_by()",
    "text": "Calculating Call Back Rates with group_by()\n\nresume %&gt;%\n  group_by(race)%&gt;%\n  summarise(\n    call_back = mean(call)\n  )\n\n# A tibble: 2 × 2\n  race  call_back\n  &lt;chr&gt;     &lt;dbl&gt;\n1 black    0.0645\n2 white    0.0965"
  },
  {
    "objectID": "slides/03-slides-old.html#factor-variables-in-base-r",
    "href": "slides/03-slides-old.html#factor-variables-in-base-r",
    "title": "Week 03:",
    "section": "Factor variables in Base R",
    "text": "Factor variables in Base R\n\nresume$type &lt;- NA\nresume$type[resume$race == \"black\" & resume$sex == \"female\"] &lt;- \"BlackFemale\"\nresume$type[resume$race == \"black\" & resume$sex == \"male\"] &lt;- \"BlackMale\"\nresume$type[resume$race == \"white\" & resume$sex == \"female\"] &lt;- \"WhiteFemale\"\nresume$type[resume$race == \"white\" & resume$sex == \"male\"] &lt;- \"WhiteMale\""
  },
  {
    "objectID": "slides/03-slides-old.html#factor-variables-in-tidy-r",
    "href": "slides/03-slides-old.html#factor-variables-in-tidy-r",
    "title": "Week 03:",
    "section": "Factor variables in Tidy R",
    "text": "Factor variables in Tidy R\n\nresume %&gt;%\n  mutate(\n    type_tidy = case_when(\n      race == \"black\" & sex == \"female\" ~ \"BlackFemale\",\n      race == \"black\" & sex == \"male\" ~ \"BlackMale\",\n      race == \"white\" & sex == \"female\" ~ \"WhiteFemale\",\n      race == \"white\" & sex == \"male\" ~ \"WhiteMale\"\n    )\n  ) -&gt; resume"
  },
  {
    "objectID": "slides/03-slides-old.html#comparing-approaches",
    "href": "slides/03-slides-old.html#comparing-approaches",
    "title": "Week 03:",
    "section": "Comparing approaches",
    "text": "Comparing approaches\n\ntable(base= resume$type, tidy= resume$type_tidy)\n\n             tidy\nbase          BlackFemale BlackMale WhiteFemale WhiteMale\n  BlackFemale        1886         0           0         0\n  BlackMale             0       549           0         0\n  WhiteFemale           0         0        1860         0\n  WhiteMale             0         0           0       575"
  },
  {
    "objectID": "slides/03-slides-old.html#visualizing-call-back-rates-by-name",
    "href": "slides/03-slides-old.html#visualizing-call-back-rates-by-name",
    "title": "Week 03:",
    "section": "Visualizing Call Back Rates by Name",
    "text": "Visualizing Call Back Rates by Name\n\nresume %&gt;%\n  group_by(race, sex,firstname)%&gt;%\n  summarize(\n    Y = mean(call),\n    n = n()\n  )%&gt;%\n  arrange(desc(Y)) %&gt;%\n  mutate(\n    firstname = forcats::fct_reorder(firstname,Y)\n  )%&gt;%\n  ggplot(aes(Y, firstname,col=race, size=n))+\n  geom_point() + \n  facet_grid(sex~.,scales = \"free_y\")\n\n\nclass:inverse, center, middle # 💪\n# Exploring the data for Broockman and Kalla 2016"
  },
  {
    "objectID": "slides/03-slides-old.html#broockman-and-kalla-2016",
    "href": "slides/03-slides-old.html#broockman-and-kalla-2016",
    "title": "Week 03:",
    "section": "Broockman and Kalla (2016)",
    "text": "Broockman and Kalla (2016)\n\nWhat’s the research question?\nWhat’s the theoretical framework?\nWhat’s the empirical design?\nWhat’s are the main results?"
  },
  {
    "objectID": "slides/03-slides-old.html#study-design-a-placebo-controlled-field-experiment",
    "href": "slides/03-slides-old.html#study-design-a-placebo-controlled-field-experiment",
    "title": "Week 03:",
    "section": "Study Design :A placebo-controlled field experiment",
    "text": "Study Design :A placebo-controlled field experiment\n\nRecruited from voter files to complete a baseline survey\nAmong those who complete the survey, half are assigned to receive an intervention and half are assigned to receive a placebo\nOnly some are actually home or open the door when the canvassers knock.\nThese people are then recruited to participate in a series of surveys 3 days, 3 weeks, 6 weeks, and 3 months after the initial intervention."
  },
  {
    "objectID": "slides/03-slides-old.html#data-for-thursday",
    "href": "slides/03-slides-old.html#data-for-thursday",
    "title": "Week 03:",
    "section": "Data for Thursday",
    "text": "Data for Thursday\nLet’s load the data from the orginal study\n\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))"
  },
  {
    "objectID": "slides/03-slides-old.html#codebook",
    "href": "slides/03-slides-old.html#codebook",
    "title": "Week 03:",
    "section": "Codebook",
    "text": "Codebook\n\ncompleted_baseline whether someone completed the baseline survey (“Survey”) or not (“No Survey”)\ntreatment_assigned what intervention someone who completed the baseline survey was assigned two (treatment= “Trans-Equality”, placebo = “Recycling”)\nanswered_door whether someone answered the door (“Yes”) or not (“No”) when a canvasser came to their door\ntreatment_group the treatment assignments of those who answered the door (treatment= “Trans-Equality”, placebo = “Recycling”)\nvf_age the age of the person in years\nvf_female the respondent’s sex (female = 1, male = 0)\nvf_democrat whether the person was a registered Democract (Democrat=1, 0 otherwise)\nvf_white whether the person was white (White=1, 0 otherwise)\nvf_vg_12 whether the person voted in the 2012 general election (voted = 1, 0 otherwise)"
  },
  {
    "objectID": "slides/03-slides-old.html#hlo",
    "href": "slides/03-slides-old.html#hlo",
    "title": "Week 03:",
    "section": "HLO",
    "text": "HLO\n\nglimpse(df)\n\nRows: 68,378\nColumns: 14\n$ completed_baseline &lt;chr&gt; \"No Survey\", \"No Survey\", \"No Survey\", \"No Survey\",…\n$ treatment_assigned &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ answered_door      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ treatment_group    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ vf_age             &lt;dbl&gt; 23.00000, 38.00000, 48.00000, 49.20192, 49.20192, 4…\n$ vf_female          &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, …\n$ vf_democrat        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ vf_white           &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ vf_vg_12           &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ therm_trans_t0     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t1     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t2     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t3     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t4     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…"
  },
  {
    "objectID": "slides/03-slides-old.html#study-design",
    "href": "slides/03-slides-old.html#study-design",
    "title": "Week 03:",
    "section": "Study Design",
    "text": "Study Design\n\ntable(df$completed_baseline)\n\n\nNo Survey    Survey \n    66553      1825 \n\ntable(df$treatment_assigned)\n\n\n     Recycling Trans-Equality \n           913            912 \n\ntable(df$answered_door)\n\n\n  No  Yes \n1324  501 \n\ntable(df$treatment_group)\n\n\n     Recycling Trans-Equality \n           255            246"
  },
  {
    "objectID": "slides/03-slides-old.html#assessing-balance-in-covariates",
    "href": "slides/03-slides-old.html#assessing-balance-in-covariates",
    "title": "Week 03:",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\ndf %&gt;%\n  filter(completed_baseline == \"Survey\") %&gt;%\n  select(treatment_assigned,starts_with(\"vf_\"))%&gt;%\n  group_by(treatment_assigned)%&gt;%\n  summarise(across(starts_with(\"vf_\"), mean))-&gt;pretreatment_balance"
  },
  {
    "objectID": "slides/03-slides-old.html#assessing-balance-in-covariates-1",
    "href": "slides/03-slides-old.html#assessing-balance-in-covariates-1",
    "title": "Week 03:",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\npretreatment_balance\n\n# A tibble: 2 × 6\n  treatment_assigned vf_age vf_female vf_democrat vf_white vf_vg_12\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Recycling            46.3     0.593       0.463    0.209    0.757\n2 Trans-Equality       47.7     0.582       0.488    0.217    0.719"
  },
  {
    "objectID": "slides/03-slides-old.html#assessing-balance-in-covariates-2",
    "href": "slides/03-slides-old.html#assessing-balance-in-covariates-2",
    "title": "Week 03:",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\npretreatment_balance %&gt;%\n  gather(.,key = covariate, value = value, -treatment_assigned) %&gt;%\n  spread(treatment_assigned, value) %&gt;%\n  mutate(\n    Difference = `Trans-Equality` - Recycling\n  )\n\n# A tibble: 5 × 4\n  covariate   Recycling `Trans-Equality` Difference\n  &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 vf_age         46.3             47.7      1.40   \n2 vf_democrat     0.463            0.488    0.0246 \n3 vf_female       0.593            0.582   -0.0103 \n4 vf_vg_12        0.757            0.719   -0.0375 \n5 vf_white        0.209            0.217    0.00790\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/00-slides-template.html#class-plan",
    "href": "slides/00-slides-template.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/00-slides-template.html#annoucements",
    "href": "slides/00-slides-template.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/00-slides-template.html#feedback",
    "href": "slides/00-slides-template.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/00-slides-template.html#concepts",
    "href": "slides/00-slides-template.html#concepts",
    "title": "POLS 1600",
    "section": " Concepts",
    "text": "Concepts"
  },
  {
    "objectID": "slides/00-slides-template.html#code",
    "href": "slides/00-slides-template.html#code",
    "title": "POLS 1600",
    "section": " Code",
    "text": "Code"
  },
  {
    "objectID": "slides/00-slides-template.html#review-1",
    "href": "slides/00-slides-template.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "slides/00-slides-template.html#concept-1",
    "href": "slides/00-slides-template.html#concept-1",
    "title": "POLS 1600",
    "section": "Concept",
    "text": "Concept"
  },
  {
    "objectID": "slides/00-slides-template.html#code-2",
    "href": "slides/00-slides-template.html#code-2",
    "title": "POLS 1600",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "slides/00-slides-template.html#summary-1",
    "href": "slides/00-slides-template.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "POLS 1600",
    "section": "",
    "text": "This course provides a foundation in the principles and practice of quantitative social science, with a focus on building tools to make descriptive, causal, and predictive inferences.\n\nLogistics\nWe meet twice a week, alternating lectures on Tuesdays and labs on Thursday. Both sessions require laptops that run R and RStudio\n\n\n\n\nLecture\nTuesdays 9-10:20 am\n\n\n\n\n\nLabs\nThursdays 9-10:20 am\n\n\n\n\n\nAssignments\nDue on Canvas\n\n\n\n\n\nLocation\nBarus & Holley 751\n\n\n\n\n\nZoom\n\n\n\n\n\nOffice Hours\n111 Thayer St Rm 339\n\n\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\n\n\nLecture\nLabs\nSolutions\nAssignments\n\n\n\n\nWeek 0\n\n\n\n\n\n\nIntroductions\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nData & Measurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\nCausation -- Experiments\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\n\nCausation -- Observational Studies\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\nBivariate Regression\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\n\nMultiple Regression\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\nRegression Extensions\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\n\nProbability - Random Variables & Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n\n\nProbability - Limit Theorems\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\n\n\nInference -- Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\n\n\nInference -- Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\n\n\nPresentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructors\n\n\nPaul Testa\nAssistant Professor\nDepartment of Political Science\nBrown University &lt;paul_testa@brown.edu&gt;\n\n\n\n\n\nManuel Moscoso Rojas\nTeaching Assistant\nDepartment of Political Science\nBrown University &lt;Manuel_Moscoso_Rojas@Brown.edu&gt;"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html",
    "href": "assignments/A1_Research_Questions.html",
    "title": "POLS 2580 Assignment 1",
    "section": "",
    "text": "Asking a good research question is one of the most important skills you will develop in your academic careers. It’s also one of the hardest.\nWe often think we’re asking one question, when in fact the study we conduct really addresses a related but distinct question. When a priest asked Willie Sutton why he robbed banks, he replied the “Well, that’s where the money is”. The priest’s question was about why rob at all, while Sutton answered the different question “Given one robs, why rob banks?” Similarly, Medieval philosophers might ask why objects stay in motion, while Newton suggests what really need is not an explanation of motion itself but of changes in motion.\nThe object of our question shapes the form of our explanation.\nIn this assignment, I would like your group to craft three potential research questions that we might explore in our research project for this class. Each question, should be a single sentence, with a few sentences answering the following questions (More details below):\n\nWhy do we care about the answer to this research question?\nWhat’s would a hypothetical “ideal experiment” to answer this question look like?\nWhat would a study with observational data look like?\nA published study that relates to this question\nHow feasible would it be to do a study like this for the course\n\nYou may use this Rmd file as a template (click here to download) or create your own file. Please submit your responses to Canvas.\nYou might start by writing down several questions of different forms about the same topic:\n\nWhy do people vote?\nWhy do people not vote?\nWhy do the rich vote at higher rates than the poor?\nWhen might people who don’t vote, be motivated to vote?\nWhat is the effect of encouraging someone to vote via a phone call?\nAre phone calls more or less effective than in-person contact for get-out the vote efforts?\n\nEach of these questions addresses a general topic that political scientists seem to think is important. Each carries some suppositions and assumptions that in turn influence the type of explanation we might find convincing. Why do people vote feels a bit broad to me. People probably vote for many reasons. How can we hope to adjudicate between all the possible reasons for voting? (Further are these the same reasons for not voting or do we need another set of explanations altogether?)\nWhether phone calls are more or less effective than in-person contacts for GOTV efforts seems more tractable, but also perhaps to narrow. Do we really care? If we’re confident we can identify an effect or difference in one study, are we sure we’ll see similar effects in a different study conducted under different circumstances?\nIn crafting your research questions, you want to strike a balance between things we actually care about (why do people vote) and things we can actually assess (what’s effect of a particularly type of encouragement to vote). A few thoughts on this process:\n\n“Why” questions tend to be more compelling than “What” or “How” or “Do” questions, I think in part because “why” questions often imply a theory and suggest a counterfactual (why this and not that), while other ways of asking questions feel more descriptive. For example, why do the rich vote at higher rates than the poor. Well, one explanation may be that their relative social and economic status means they are more likely to be targets of mobilization efforts by campaigns (among many things). So a natural follow up to this larger question might be, what’s the effect of providing similar mobilization efforts to the poor. Would they vote at similar rates to the rich? If so, then we’ve learned something about how mobilization explains class differences in participation.\nThinking about questions in terms of puzzles is another useful trick. Why do parties exist when politicians’ ideological preferences can explain the vast majority of their legislative behavior? Note this type of question contains a lot of presuppositions (how do we measure ideological preferences? Do they really explain legislative behavior? Is that what we care about?), but as point of departure for a study these type arguments can be useful\nTry to be simple and clear. Don’t worry about asking the perfect question right away. Your questions can and should evolve over time, and I suspect some of you will write a paper that has nothing to do with the questions you posed here.\n\nFor each question, please discuss the following:\n\nWhy do we care? Why we should care about the answer to this question. A strong justification is often that existing theories yield conflicting predictions and so your study will offer some insight into how to adjudicate betweeen these theories. A less strong justification is that no one has ever studied this before. Even if this is true (and it’s often not) it may be true for good reason. No need for formal citations, but if there are specific theories or claims your addressing feel free to name names.\nThe ideal experiment Please describe an “ideal” experiment that you could run that would give you some purchase on your question. Note the key feature of an experiment, is that you the researcher are able to manipulate (through random assignment) some facet of the world. Assume money, resources, physics, and even ethics are not an object. If you could randomly assign anything, what would you manipulate. At what level of analysis would your manipulation occur (i.e. are your units of analysis individuals or countries or something else). How would you measure your outcome, again assuming you were all power and all-seeing. If that manipulation isn’t feasible, what does that say about the ability to make a causal claim about your question?\nThe observational study Finally, considering some of the potential limitations that might prevent you from implementing your ideal experiment (it’s hard to randomly assign democratic government), what is one way you might address your research question with observational data. Would your study use cross-sectional or longitudinal data. What are some of the concerns (selection on observables) that arise in this setting. Is there a natural experiment or some sort of discontinuity you might leverage to approximate this experimental ideal.\n\nAgain, each paragraph should be brief and to the point. No need to specify a full research design–just give me the broad strokes. You’re writing for each question should not exceed a page.\nAfter you’ve thought through how you might go about answering your question, please find\n\nA published study that relates to this question. It need not be exactly your question as posed, but it should be in a similar area. Include a full citation, and link to the study. Then in a paragraph sentences try to summarize:\n\n\nThe study’s research question\nEmprical design\nCore findings.\n\nFinally on a scale of 1 (least feasible) to 10 (most feasible), please evaluate how likely you think it is you could write an empirical paper on this question for this course.\nDon’t worry about getting everything right. Your final projects can, will, and probably should change. The point of this exercise is to get some practice thinking about questions that interest you in the language of causal inference and potential outcomes."
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study",
    "href": "assignments/A1_Research_Questions.html#the-observational-study",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study",
    "href": "assignments/A1_Research_Questions.html#a-published-study",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care-1",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care-1",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment-1",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment-1",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study-1",
    "href": "assignments/A1_Research_Questions.html#the-observational-study-1",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study-1",
    "href": "assignments/A1_Research_Questions.html#a-published-study-1",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10-1",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10-1",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care-2",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care-2",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment-2",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment-2",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study-2",
    "href": "assignments/A1_Research_Questions.html#the-observational-study-2",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study-2",
    "href": "assignments/A1_Research_Questions.html#a-published-study-2",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10-2",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10-2",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/a5.html",
    "href": "assignments/a5.html",
    "title": "A5: Presentation",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A5: Presentations"
    ]
  },
  {
    "objectID": "assignments/final.html",
    "href": "assignments/final.html",
    "title": "A5: Presentation",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "Final Paper"
    ]
  },
  {
    "objectID": "assignments/a3.html",
    "href": "assignments/a3.html",
    "title": "A3: Initial Analyses",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A3: Exploratory Analysis"
    ]
  },
  {
    "objectID": "labs/07-lab.html",
    "href": "labs/07-lab.html",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "",
    "text": "Today, we’ll explore Russians’ support the war in Ukraine using a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project.\nThe survey was conducted by phone using a random sample of mobile phone numbers to produce a sample of respondents representative of the population in terms of age, sex, and geography. It was in the field from February 28 to March 2.\nWe will look at how support for the war varies with the demographic predictors age, sex and education. We will see how multiple regression can be used to describe more complex relationships. From our baseline model, we will ask:\n\nDoes the relationship between age and support for the war vary among male and female respondents (Interaction model)\nDoes the relationship between age and support vary at different levels of age (Polynomial regression model)\nDoes the relationship between age and support vary at different levels of age and does the nature of this variation differ among male and female respondents (Polynomial regression model with an interaction term)\n\nTo accomplish this, we will do the following:\n\nGet set up to work and describe our data (10 minutes)\nGet practice interpreting long chunks of codes with lots of %&gt;%s (10 minutes)\nEstimate four models of increasing complexity (15 minutes)\nPresent these models in a regression table and interpret the results (10 minutes)\nEvaluate the relative performance of these models in terms of their variance explained \\(R^2\\)’s (15 minutes)\nProduce predicted values to help us interpret and compare our baseline model to a model where the “effect” of an increase in age changes with age (10 minutes)\nProduce predicted values to help us interpret and compare models where the “effect” of age is allowed to vary with respondent’s sex (10 minutes)\n\nFinally, if there’s time, we will:\n\nExplore additional questions of our choosing in the data\n\nOne of questions 1-7 will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file produced by knitting the lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nYou can find your assigned groups in previous labs"
  },
  {
    "objectID": "labs/07-lab.html#load-packages",
    "href": "labs/07-lab.html#load-packages",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  #\"flair\", # Comments only\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "labs/07-lab.html#load-the-data",
    "href": "labs/07-lab.html#load-the-data",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the recoded data for the lab.\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))"
  },
  {
    "objectID": "labs/07-lab.html#describe-the-data",
    "href": "labs/07-lab.html#describe-the-data",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nAs always, it’s important to get a high level overview of data when we first load it into R.\nBelow we take a look at the first few values of all the data. You’ll see that df_drww includes both the Russian data and recoded revisions of the data (which are typically appended with _n for numeric data or _f for factor data).\n\nglimpse(df_drww)\n\nRows: 1,807\nColumns: 42\n$ sex                          &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Male…\n$ age                          &lt;dbl&gt; 99, 78, 73, 73, 69, 69, 59, 54, 49, 48, 4…\n$ support_war                  &lt;fct&gt; \"(НЕ ЗАЧИТЫВАТЬ) Затрудняюсь ответить\", \"…\n$ trust_gov                    &lt;fct&gt; (НЕ ЗАЧИТЫВАТЬ) Затрудняюсь ответить, Дов…\n$ employ_working               &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,…\n$ employ_student               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_retired               &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_maternity_leave       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_homemaker             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_unemployed            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,…\n$ employ_other_employment      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ other_employ_open_response   &lt;chr&gt; \"                                        …\n$ education                    &lt;fct&gt; (НЕ ЗАЧИТЫВАТЬ) Затрудняюсь ответить, Выс…\n$ social_classmates            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_in_contact_with       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_facebook              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_instagram             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_twitter               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_telegram              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_whatsapp              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_viber                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_tiktok                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,…\n$ social_other_social_networks &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ none_social                  &lt;fct&gt; НЕ ВЫБРАН, ВЫБРАН, НЕ ВЫБРАН, ВЫБРАН, НЕ …\n$ dk_social                    &lt;fct&gt; ВЫБРАН, НЕ ВЫБРАН, НЕ ВЫБРАН, НЕ ВЫБРАН, …\n$ other_social_open_response   &lt;chr&gt; \"                                        …\n$ other_social_group           &lt;fct&gt; NA, NA, Ютуб, NA, Ютуб, NA, NA, NA, NA, N…\n$ geo_urban_rural              &lt;fct&gt; NA, NA, \"город, поселок городского типа\",…\n$ geo_district                 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ weight                       &lt;dbl&gt; 0.000, 0.000, 1.208, 0.000, 1.231, 1.208,…\n$ support_war_f                &lt;fct&gt; NA, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes…\n$ support_war01                &lt;dbl&gt; NA, 1, 1, 1, 1, 1, 0, 1, 1, NA, 1, 1, 1, …\n$ trust_gov_f                  &lt;fct&gt; NA, Partly, Largely, Fully, Partly, Fully…\n$ trust_gov_n                  &lt;dbl&gt; NA, 1, 2, 3, 1, 3, 0, NA, 2, NA, NA, 0, 3…\n$ education_f                  &lt;fct&gt; NA, College or some college, Vocational, …\n$ education_n                  &lt;dbl&gt; NA, 4, 3, 4, 1, 3, 4, NA, 4, NA, 3, 3, 3,…\n$ geo_urban_rural_f            &lt;fct&gt; NA, NA, Urban, NA, Rural, Urban, Rural, N…\n$ geo_district_f               &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ social_youtube               &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ social_yandex                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ total_social_media_use       &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 9, 0, 1, 0, 1, 0,…\n$ no_social_media              &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n\n\nIn the first part of this lab, we’ll work with the following variables\n\nsupport_war01 “Please tell me, do you support or do not support Russia’s military actions on the territory of Ukraine?” (1=yes, 0 = no)\nage “How old are you?”\nsex “Gender of respondent” (As assessed by the interviewer)\neducation_n “What is your highest level of education (confirmed by a diploma, certificate)?” (1 = Primary school, 2 = “High School”, 3 = “Vocational School” 4 = “College”, 5 = Graduate Degree)3\n\nIn the code chunk below, I create a data frame of summary statistics:\n\nthe_vars &lt;- c(\"support_war01\",\"age\", \"is_female\", \"education_n\")\n\ndf_drww %&gt;%\n  mutate(\n    is_female = ifelse(sex == \"Female\",1, 0)\n  ) %&gt;%\n  select(all_of(the_vars)) %&gt;%\n  pivot_longer(\n    cols = all_of(the_vars ),\n    names_to = \"Variable\",\n    values_to = \"Value\"\n  ) %&gt;%\n  group_by(Variable) %&gt;%\n  summarise(\n    `N obs` = n(),\n    Missing = sum(is.na(Value)),\n    Min = min(Value, na.rm = T),\n    `25th perc` = quantile(Value, .25, na.rm=T),\n    Mean = mean(Value, na.rm=T),\n    Median = median(Value, na.rm = T),\n    `75th perc` = quantile(Value, .75, na.rm=T),\n    Max = max(Value, na.rm = T)\n  ) %&gt;%\n  mutate(\n    Variable = case_when(\n      Variable == \"age\" ~ \"Age\",\n      Variable == \"education_n\" ~ \"Education\",\n      Variable == \"is_female\" ~ \"Female\",\n      Variable == \"support_war01\" ~ \"Support for War\",\n      \n    ),\n    Variable = factor(Variable, levels = c(\"Support for War\",\"Age\",\"Female\",\"Education\"))\n    ) %&gt;%\n  arrange(Variable) -&gt;  summary_table\n\nsummary_table\n\n# A tibble: 4 × 9\n  Variable     `N obs` Missing   Min `25th perc`   Mean Median `75th perc`   Max\n  &lt;fct&gt;          &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Support for…    1807     334     0           0  0.720      1           1     1\n2 Age             1807       0    18          34 46.6       45          60    99\n3 Female          1807       0     0           0  0.470      0           1     1\n4 Education       1807      13     1           3  3.17       3           4     5\n\n\nWhich we can then format into a table of summary statistics:\n\nknitr::kable(summary_table,\n      digits = 2) %&gt;% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  kableExtra::pack_rows(\n    group_label = \"Outcome\",\n    start_row = 1,\n    end_row = 1\n  )%&gt;%\n  kableExtra::pack_rows(\n    group_label = \"Predictors\",\n    start_row = 2,\n    end_row = 4\n  )\n\n\n\n\nVariable\nN obs\nMissing\nMin\n25th perc\nMean\nMedian\n75th perc\nMax\n\n\n\n\nOutcome\n\n\nSupport for War\n1807\n334\n0\n0\n0.72\n1\n1\n1\n\n\nPredictors\n\n\nAge\n1807\n0\n18\n34\n46.65\n45\n60\n99\n\n\nFemale\n1807\n0\n0\n0\n0.47\n0\n1\n1\n\n\nEducation\n1807\n13\n1\n3\n3.17\n3\n4\n5\n\n\n\n\n\n\n\nPlease use this table to describe a typical respondent to the survey\nYOUR DESCRIPTION HERE"
  },
  {
    "objectID": "labs/07-lab.html#describe-your-expectations-for-these-models",
    "href": "labs/07-lab.html#describe-your-expectations-for-these-models",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "3.1 Describe your expectations for these models",
    "text": "3.1 Describe your expectations for these models\nBefore you estimate these models, please answer the following:\nIn the baseline model, m1 what do you expect the sign of the coefficient for each predictor to be:\n\nAge (Positive/Negative)\nSex (Positive/Negative)4\nEducation (Positive/Negative)\n\nIn the interaction model, m2\n\nDo you think the relationship between age and support will vary by sex (Yes/No)\nIf you said yes, do you think the coefficient on the interaction between age and sex will be positive or negative (Positive/Negative)\n\nIn the polynomial model, m3\n\nIf the coefficient on age is positive and the coefficient on age^2 is positive, then as age increases, the increase in the predicted level of support will be (increasing/decreasing)\nIf the coefficient on age is positive and the coefficient on age^2 is negative, then as age increases, the increase in the predicted level of support will be (increasing/decreasing)\n\nIn m4\n\nIf the coefficients on the interaction terms between the sex variable and the age variables (age and age^2) is statistically significant, this implies that the relationship between age and support for the war is (similar/different) for male and female respondents."
  },
  {
    "objectID": "labs/07-lab.html#estimate-the-regression-models",
    "href": "labs/07-lab.html#estimate-the-regression-models",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "3.2 Estimate the regression models",
    "text": "3.2 Estimate the regression models\nUncomment the code below, and replace the ??? with the appropriate terms to fit the following models.\n\n# Baseline Model\n# m1 &lt;- lm(support_war01 ~ ??? + ??? + ???, ???)\n\n# Interaction model: Allow coefficient for age to vary with sex\n# m2 &lt;- lm(support_war01 ~ age*??? + education_n, df_drww)\n\n# Polynomial model: Allow coefficient for age to vary by age\n# m3 &lt;- lm(support_war01 ~ age + I(???^2) + sex + education_n, df_drww)\n\n# Separate Polynomial: Allow coefficient for age to vary by age separately  by sex\n# m4 &lt;- lm(support_war01 ~ (age + I(age^2))*??? + education_n, df_drww)"
  },
  {
    "objectID": "labs/07-lab.html#footnotes",
    "href": "labs/07-lab.html#footnotes",
    "title": "Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMathematically, recall that the slope/first derivative of the line \\(y = f(x) = 2x\\) is constant \\((f'(x) = 2)\\). If we increase x by 1, we expect y to increase by 2, while the derivative of a parabola \\(y = f(z) = z^2\\) varies with \\(z\\) \\((f'(z) = 2x)\\). Going from z= 2 to z= 3 is associated with a greater increase in y, then going from z=1 to z=2. Our model, however, is still linear in parameters \\(\\beta\\). That is, it is still a linear regression. If our model included some parameter \\(\\theta^2\\), then it would be a non-linear regression.↩︎\nIn a machine learning framework, we trying to find an optimal tradeoff between reducing bias in our predictors by including more predictors and minimizing variance in predictions by not overfitting the data.↩︎\nI think, google translate was a bit unclear. But higher numbers equal more education.↩︎\nThis is tricky, you need to know what the reference (excluded) category will be.↩︎\nBasically, I’m asking whether the coefficient on I(age^2) is statistically significant. If it is, then the change in predicted support for the war among say 20-year-olds compared to 30-year-olds, would be different than change between 30- to 40-year-olds. Interpreting polynomials terms and interaction models is much easier if, as we do later, we simply obtain and plot the predicted values from this model↩︎\nAs in the previous question, basically you need to look at the table and see if the coefficients on the interaction terms are statistically significant. It’s a little more complicated than this, but if they are significant, this is evidence of differences across Sex.↩︎"
  },
  {
    "objectID": "labs/04-lab.html",
    "href": "labs/04-lab.html",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "In this assignment, we’ll walk through the logic and design of Ferwerda and Miller (2014).\n\n\nConceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/04-lab.html#goals",
    "href": "labs/04-lab.html#goals",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "Conceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/10-lab.html",
    "href": "labs/10-lab.html",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "",
    "text": "In our final lab, you will apply concepts and skills from this course to explore data from the 2020 American National Election Study. Specifically you will\n\nIdentify an outcome of interest (5-10 minutes)\nIdentify key predictors and covariates (5-10 minutes)\nRecode your data (20 minutes)\nDescribe your data (20 minutes)\nDescribe your question, expectations, and models (10 minutes)\nEstimate, present, and interpret your models (20 minutes, Graded Question)\n\nIdeally, each group will pursue a question that interests them. I will also complete these tasks live, so, if you’re not feeling confident, you can follow along with me and submit the code I demo in class as your lab for a grade of 85.."
  },
  {
    "objectID": "labs/10-lab.html#summary-statistics",
    "href": "labs/10-lab.html#summary-statistics",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "4.1 Summary statistics:",
    "text": "4.1 Summary statistics:\nProducing a table of summary statistics requires a little foresight.\nEssentially you want to make a data frame where each row is a (numeric) variable, and each column is a statistic (minimum, 25th percentile, median, mean, 75th percentile, max, Number of missing).\nTo do this, I would:\n\ncreate a object called the_vars which contains the names (in quotation marks) of the variables you want to summarize.\nSelect these variables from your data set. using df%&gt;%select(all_of(the_vars))\nUse %&gt;%pivot_longer() specifying cols=select(all_of(the_vars)), and names_to equals \"Variable\" and values_to = \"value\" to transform this wide dataset into a long dataset\nThen use %&gt;%group_by(Variable)%&gt;% and summarise() to calculate the statistics for each variable of interest (e.g. %&gt;%summarise(Mean = mean(value, na.rm=T))))\nSave the output to an object called something like sum_df\nIn a new chunk use knitr::kable(sum_df) %&gt;% kableExtra::kable_styling() to format your table. Set echo=F in the code chunk head\n\n\n# Summarise data\n\n# Display results"
  },
  {
    "objectID": "labs/10-lab.html#descriptive-figures",
    "href": "labs/10-lab.html#descriptive-figures",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "4.2 Descriptive Figures",
    "text": "4.2 Descriptive Figures\nTo create a figure, you’ll need to specificy the following\n\ndata (e.g. df %&gt;%)\naesthetic mappings, ggplot(aes(x = predictor, y = outcome))\ngeometries\n\nUnivariate: geom_density(), geom_boxplot() geom_histogram()\nBivariate: geom_point() (for a scatterplot), geom_line() for a trend.\n\n\nOnce you have a minimal working example, play around with other grammars of graphics:\n\nlabs() for custom labels\ntheme_XXX for custom themes\nfacet_wrap(~group) to produce the same plot facetted by some categorical grouping variable\n\nWhen you’re happy with your figure, save it as object in R (e.g. fig1 &lt;- df %&gt;% ggplot(aes(predictor, outcome))+geom_point()). Put that object in its own chunk to display it in your document.\nDon’t let the perfect be the enemy of the good.\n\n# Descriptive figures"
  },
  {
    "objectID": "labs/10-lab.html#descrptive-interpretation",
    "href": "labs/10-lab.html#descrptive-interpretation",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "4.3 Descrptive Interpretation:",
    "text": "4.3 Descrptive Interpretation:\nPlease provide an overview of the data (source, number of observations, unit of analysis).\nDescribe a typical observation, making reference to the statistics in your summary table.\nOffer a substantive interpretation of your descriptive figure(s). What do they tell us about the distribution of a key variable, or the relationship between two variables."
  },
  {
    "objectID": "labs/10-lab.html#fit-the-models",
    "href": "labs/10-lab.html#fit-the-models",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "6.1 Fit the models",
    "text": "6.1 Fit the models\n\n# Model 1: Bivariate Model\n\n# Model 2: Multiple Regression"
  },
  {
    "objectID": "labs/10-lab.html#display-the-models-in-a-regression-table",
    "href": "labs/10-lab.html#display-the-models-in-a-regression-table",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "6.2 Display the models in a regression table",
    "text": "6.2 Display the models in a regression table\n\n# Regression table"
  },
  {
    "objectID": "labs/10-lab.html#interpet-your-models",
    "href": "labs/10-lab.html#interpet-your-models",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "6.3 Interpet your models",
    "text": "6.3 Interpet your models\nPlease write a 1 paragraph summary interpreting your results in terms of both their statistical and substantive significance. Assume your audience is smart, but has never taken POLS 1600. Explain to them what a regression model is, what a standard error, p-value, and/or confidence interval is. How should they interepret the substantive findings of your model. How should they assess the statistical uncertainty around these results?\nPerhaps you might reade create a plot of predicted values from a model to help facilitate the substantive interpretation of your results. If so, here’s a code chunk for you:\n\n# Additional code chunk to facilitate interpretation of models"
  },
  {
    "objectID": "labs/08-lab.html",
    "href": "labs/08-lab.html",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Today we’ll work through exercise 6.6.2 A Probability Model for Betting Market Election Prediction from QSS (pp. 309-310). We will use the daily data from the Intrade betting market to derive probabilities from degenerate gamblers about the likelihood that Obama would win the 2008 presidential election. We will use these probabilities to simulate possible elections and the summarize outcomes of these simulations graphically.\nPlan to spend the following amount of time\n\nGet set up to work (5 minutes)\nCalculate Obama’s expected electoral vote share on November 3, 2008 (the day before the election) (10 minutes)\nSimulate a 1000 elections for November 3, 2008 using the betting market prices a measure of the probability that Obama wins or loses a state (10 minutes)\nDisplay the results of your simulation with a histogram. (5 minutes)\nTransform these probabilities to reduce the likelihood that Obama wins states like Alabama and increase the likelihood that Obama wins states like California (5 minutes)\nSimulate another 1000 elections using these transformed probabilities. Compare the results to your initial simulation. (10)\nCalculate Obama’s expected total number of votes for each day in the 120 days before the 2008 election (15 minutes)\nSimulate 100 elections for each of the 120 days before the election, plot the results of your simulation. (20 minutes)\n\n\n\nConceptually, the main goals of this lab are to\n\ngive you some practice working with probabilities in an application to the real world (predicting elections)\nintroduce the idea of simulations as a tool for understanding probability and describing our uncertainty about what could have happened\n\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)\n\n\n\n\n\n\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab.html#goals",
    "href": "labs/08-lab.html#goals",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Conceptually, the main goals of this lab are to\n\ngive you some practice working with probabilities in an application to the real world (predicting elections)\nintroduce the idea of simulations as a tool for understanding probability and describing our uncertainty about what could have happened\n\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)"
  },
  {
    "objectID": "labs/08-lab.html#workflow",
    "href": "labs/08-lab.html#workflow",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "As with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab.html#load-packages",
    "href": "labs/08-lab.html#load-packages",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst we’ll load the pacakges we need for today:\n\nlibrary(tidyverse)\nlibrary(qss)\n\nNow we’ll load two data sets from the qss package: pres08 and intrade08\n\npres08 contains the 2008 US presidential election outcomes by state.\nintrade08 contains from Intrade, an online prediction market, in days leading up to the 2008 United States Presidential Election.\n\n\n# Results from 2008 election\ndata(\"pres08\")\n\n# Daily betting market data\ndata(\"intrade08\")"
  },
  {
    "objectID": "labs/08-lab.html#provide-a-high-level-overview-each-data-set",
    "href": "labs/08-lab.html#provide-a-high-level-overview-each-data-set",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.2 Provide a high level overview each data set",
    "text": "1.2 Provide a high level overview each data set\nIn the code chunk below, please write some code to provide a high level overview of each data set.\nWe will primarily work with the EV variable from pres08 which contains the electoral votes for each state, and the PriceD variable from intrade08 from which we will construct a probability that Obama wins that state.\n\n# HLO\n# pres08\n\n# intrade08\n\nBriefly describe each data set\n\nThe pres08 data set contain …\nThe intrade08 data contain …"
  },
  {
    "objectID": "labs/08-lab.html#re-arrange-the-intrade",
    "href": "labs/08-lab.html#re-arrange-the-intrade",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.3 Re-Arrange the Intrade",
    "text": "1.3 Re-Arrange the Intrade\nIf you look closely at the data, you’d see that both are arranged alphabetically by state name, but in the pres08 data, the District of Columbia is named “D.C.”, while in the intrade08 data it is called “District of Columbia”.\n\npres08$state.name[8]\n\n[1] \"D.C.\"\n\nintrade08$statename[9]\n\n[1] \"District of Columbia\"\n\n\nAs a result, D.C. comes before Delaware in pres08 but after Delaware in intrade08. It will be useful below, for the states to be in the same order in both data sets.\n\n# DC and DE are reversed\npres08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\"\n\nintrade08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" \"DC\"\n\n# Same abbrevs\nsum(pres08$state %in% intrade08$state)\n\n[1] 51\n\n# Different namings of DC\nsum(pres08$state.name %in% intrade08$statename)\n\n[1] 50\n\n\nPlease use the arrange() function to re-arrange both data sets using the state variable which is the postal abbreviation code for each state and is the same in both data sets.\nFor the pres08 data arrange() by state and intrade08 by day and then by state\n\nRemember to save the output of arrange() back into each respective data frame\n\n\n# arrange pres08\n\n\n# arrange intrade08\n\nIf your code was correct, DC should now come before DE in intrade08\n\n# pres08$state[1:9]\n# intrade08$state[1:9]\n\nAnd the following code should return TRUE\n\n# all.equal(pres08$state[1:51], intrade08$state[intrade08$day==\"2008-11-03\"])"
  },
  {
    "objectID": "labs/08-lab.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "href": "labs/08-lab.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.4 Convert the PriceD variable in intrade08 to a probability",
    "text": "1.4 Convert the PriceD variable in intrade08 to a probability\nThe intrade08 data contain a variable called PriceD which we will treat as the probability that Obama will win the presidential election.\nRecall that probabilities must be between 0 and 1. Please create a variable called prob_obama_win in intrade08 by dividing PriceD by 100.\n\n# Create variable prob_obama_win from PriceD in intrade08"
  },
  {
    "objectID": "labs/08-lab.html#create-a-subset-of-intrade08-called-df_nov3",
    "href": "labs/08-lab.html#create-a-subset-of-intrade08-called-df_nov3",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.5 Create a subset of intrade08, called df_nov3",
    "text": "1.5 Create a subset of intrade08, called df_nov3\nNext we’ll create a subset of the data, called df_nov3 that contains just the data from the day before the election (that is Monday, November 3, 2008)\n\nHint: try filter()\n\n\n# create df_nov3"
  },
  {
    "objectID": "labs/08-lab.html#footnotes",
    "href": "labs/08-lab.html#footnotes",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we’re assuming these probabilities are independent. Actual forecasting models like 538 or the NYT generally assume the probabilities of winning some states correlate with the probability of winning other neighboring states.↩︎"
  },
  {
    "objectID": "labs/01-lab-comments.html",
    "href": "labs/01-lab-comments.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/01-lab-comments.html#uncomment-and-run-the-following-code",
    "href": "labs/01-lab-comments.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/01-lab-comments.html#uncomment-and-run-the-code-below",
    "href": "labs/01-lab-comments.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\ncovid_us %&gt;%\n  filter(facial_coverings == -4) %&gt;%\n  select(date, state) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    n = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n  )%&gt;%\n  arrange(earliest_date)\n\n# A tibble: 4 × 4\n  state              n earliest_date latest_date\n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;        &lt;date&gt;     \n1 Illinois         156 2020-10-01    2021-05-15 \n2 Massachusetts     35 2020-10-02    2020-11-05 \n3 South Carolina    61 2020-10-13    2020-12-12 \n4 Maryland         158 2020-11-06    2021-04-12"
  },
  {
    "objectID": "labs/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "href": "labs/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explain in words your best understanding of what each line of code is doing:",
    "text": "6.2 Please explain in words your best understanding of what each line of code is doing:\n\ncovid_us %&gt;% tells R that every line of code after will use covid_us dataframe\nfilter(facial_coverings == -4) %&gt;% tells R to filter out only the rows where the facial coverings variable equals -4\nselect(date, state) %&gt;% tells R to select the columns named date and state\ngroup_by(state) %&gt;% tells R that subsequent commands should be done separately for each unique value of state\nsummarize( tells R we want to summarize the output of susequent commands\nn = n(), tells R to count the number of observations (state-dates) for each state that had a value of -4 on the facial_coverings variable\nearliest_date = min(date), tells R to report the earliest date that each state had a value of -4\nlatest_date = max(date), tells R to report the last date that each state had a value of -4\n)%&gt;% tells R we’re finished with the summarize() function\narrange(earliest_date) arranges the data in asscending order from earliest to latest start date\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\nSubstantively, what does the previous chunk of code tell us?\n\nSo there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. Illinois was the first state where this code appears, and it appears present in 156 observations while Montana was the last adopting a policy code -4 on March 25, 2021\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/01-lab-comments.html#please-run-the-following-code",
    "href": "labs/01-lab-comments.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  )%&gt;%\n  filter(RegionName == \"Illinois\", \n         date &gt; \"2020-08-01\", \n         date &lt; \"2021-01-01\",\n         !is.na(H6_Notes)) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\n# A tibble: 8 × 4\n  date       `H6_Facial Coverings` H6_Flag H6_Notes                             \n  &lt;date&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                \n1 2020-08-21                     2       1 \"In Executive Order 2020-52, Executi…\n2 2020-08-26                     2       1 \"Effective from 26 August 2020, the …\n3 2020-09-18                     2       1 \"On 18 September, in Executive Order…\n4 2020-10-01                     4       0 \"Originally coded a 3T, but looking …\n5 2020-10-16                     4       0 \"In Executive Order (EO) 2020-59, Go…\n6 2020-11-13                     4       0 \"Noting that Executive Order 2020-71…\n7 2020-11-20                     4       0 \"Executive Order 2020-73 requires pe…\n8 2020-12-01                     3       1 \"Chicago seems to have changed its g…"
  },
  {
    "objectID": "labs/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;% Tells R to use the Oxford policy data\nmutate(date = ymd(Date))%&gt;% Creates a date variable of class date from the original Date variable (which was class numeric)\nfilter(RegionName == \"Illinois\", subsets the data to just Illinois\ndate &gt; \"2020-08-01\", filters out dates before August 1, 2020\ndate &lt; \"2021-01-01\", filters out observations with dates after January 1,2021\n!is.na(H6_Notes)) %&gt;% filters out observations without notes (which appear in the data when policy changes)\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks Selects just the date and notes variables and saves them to an object called il_facemasks\nil_facemasks prints the obejct in the console\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\n[1] \"On 18 September, in Executive Order 2020-55, the Governor reissued most executive orders, extending a majority of the provisions through 17 October 2020. This includes mask requirements.      https://web.archive.org/web/20200922144918/https://www2.illinois.gov/Pages/Executive-Orders/ExecutiveOrder2020-55.aspx\"\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\nil_facemasks$H6_Notes[il_facemasks$date == \"2020-10-01\"]\n\n[1] \"Originally coded a 3T, but looking at the below description, which includes even residential buildings, it is hard to conceive of a time outside the home when a Chicago resident would not be required to wear a mask. The Phase IV \\\"Gradually Resume\\\" guidelines seem not to provide any significant exemption (https://archive.fo/dOyY9). Hence code moves up to 4T.    Effective October 1, 2020, residents of Chicago are required to wear masks in all public places.     “Any individual who is over age two and able to medically tolerate a mask shall be required to wear a mask when in a public place, which for purposes of this Order includes any common or shared space in: (1) a residential multi-unit building or (2) any non-residential building, unless otherwise provided for in the Phase IV: Gradually Resume guidelines promulgated by the Office of the Mayor (\\\"Gradually Resume Guidelines\\\")”    Additionally, but separately, “Individuals must, at all times and as much as reasonably possible, maintain Social Distancing from any other person who does not live with them.”    https://web.archive.org/web/20201116163255/https://www.chicago.gov/content/dam/city/sites/covid/health-orders/CDPH%20Order%202020-9%20-%205th%20Amended%20FINAL%209.30.20_AAsigned.pdf\""
  },
  {
    "objectID": "labs/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy\nIn Illinois, the -4’s seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of facial_coverings to construct our face_mask variable, we’re probably over stating the extent the extensiveness of policies in effect.\nSo we should be cautious in how we interpret our collapsed variable, face_mask. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy."
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-central-tendency",
    "href": "labs/01-lab-comments.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n[ {x}=_{i=1}^n x_i ]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\n\nmean(covid_us$new_cases_pc, na.rm=T)\n\n[1] 28.11486\n\n\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nFormally, you’ll often see this written in terms of Expected Values: Something like\n[ E[Y|X=x] ]\nOr to make it more concrete:\n[ E[ | ] ]\nIn code, we could accomplish this manually, using the index operator:\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n[1] 10.26168\n\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommend”\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"Recommended\"], na.rm=T)\n\n[1] 16.61408\n\n\nBy using group_by() with summarise() we can accomplish this more quickly:\n\ncovid_us%&gt;%\n  group_by(face_masks)%&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  )\n\n# A tibble: 6 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2\n6 &lt;NA&gt;                           11.8\n\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n[ M_x = X_i : ^{x_i} f_x(X)dx=^f_x(X)dx=1/2 ]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\nmedian(covid_us$new_cases_pc, na.rm=T)\n\n[1] 10.52355\n\n\nInteresting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the new_cases_pc has a “long tail” or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.\n\n\nMedians are less influenced by outliers than means\n\n\n\nhist(covid_us$new_cases_pc, breaks = 100)\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\nsort(table(covid_us$face_masks))\n\n\n    Required all times              No policy            Recommended \n                  1032                   3893                   8879 \nRequired shared places      Some requirements \n                 15088                  24786 \n\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:"
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-dispersion",
    "href": "labs/01-lab-comments.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\nrange(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916 1531.8669\n\nmin(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916\n\nmax(covid_us$new_cases_pc,na.rm = T)\n\n[1] 1531.867\n\n\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\ncovid_us %&gt;%\n  filter(\n    new_cases_pc &lt; -188 |\n    new_cases_pc &gt; 1500\n  )%&gt;%\n  select(state, date,new_cases_pc)\n\n# A tibble: 5 × 3\n# Groups:   state [5]\n  state        date       new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;            &lt;dbl&gt;\n1 Florida      2021-06-04        -189.\n2 Rhode Island 2022-01-04        1532.\n3 Tennessee    2023-01-01        -267.\n4 Nebraska     2022-10-28        -276.\n5 Kentucky     2022-10-11        -198.\n\n\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n[ p_x = X_i : ^{x_i} f_x(X)dx= p; ^f_x(X)dx=1-p ]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\n\nquantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)\n\n     25%      75% \n 0.00000 32.14152 \n\n\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n[ ^2_x=_{i=1}n(x_i-{x})2 ]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\nUse the var() function to calculate the variance of the new_cases_pc variable.\n\nvar(covid_us$new_cases_pc,na.rm=T)\n\n[1] 3402.718\n\n# Calculate by hand\n\nsum(\n  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, \n  na.rm=T\n  )/(sum(!is.na(covid_us$new_cases_pc))-1)\n\n[1] 3402.718\n\n\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n[ _x== ]\nStandard deviations are easier to interpet because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\nAgain, let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable\n\nsd(covid_us$new_cases_pc,na.rm=T)\n\n[1] 58.33282"
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-association",
    "href": "labs/01-lab-comments.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n[ cov(x,y)=_{i=1}^n(x_i-{x})(y_i-{y}) ]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\nvar(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)\n\n[1] -19.96569\n\n\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n[ _{x,y} = ]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\n\ncor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = \"complete.obs\")\n\n[1] -0.01369243\n\n\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\ncovid_us %&gt;%\n  filter(year &gt; 2020)%&gt;%\n  ungroup() %&gt;%\n  group_by(year,month)%&gt;%\n  summarise(\n    mn_per_vax = mean(percent_vaccinated, na.rm=T),\n    cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n  )\n\nError in `summarise()`:\nℹ In argument: `cor = cor(new_cases_pc, percent_vaccinated, use =\n  \"complete.obs\")`.\nℹ In group 28: `year = 2023` and `month = 4`.\nCaused by error in `cor()`:\n! no complete element pairs"
  },
  {
    "objectID": "labs/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "href": "labs/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?\nProbably not that much. Different Face mask policies are implemented at different times in the pandemic. For example, by 2021, almost all states have some requirements. Comparing the average for new cases in states with no policy to states with full requirements, is comparing the state of world in early 2020 to the state of the world in late 2020 to mid 2021. But lots of things differ between these periods. Other policies are also going into effect, new variants are emerging.\nIn short, those simple conditional means across the full data don’t really provide an apples to apples comparison.\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,n,fill=face_masks))+\n  geom_bar(stat=\"identity\")\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    new_cases = sum(new_cases)\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,new_cases))+\n  geom_smooth()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 328 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "labs/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\nSo when we compare states in September 2020, we see that rates of new cases tend to be much higher in states with only recommend face mask policies compared to states with at least some requirements.\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  filter(year_month == \"2020-09\")%&gt;%\n  group_by(face_masks) %&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc,na.rm=T)\n  )\n\n# A tibble: 4 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Recommended                    43.9\n2 Some requirements              13.5\n3 Required shared places         13.0\n4 Required all times             10.1\n\n\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(year_month,face_masks) %&gt;%\n  summarise(\n    n = length(unique(state)),\n    new_cases_pc = round(mean(new_cases_pc,na.rm=T)),\n    total_cases = round(mean(confirmed,na.rm=T))\n  ) -&gt; cases_by_month_and_policy\n\n`summarise()` has grouped output by 'year_month'. You can override using the\n`.groups` argument.\n\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\nDT::datatable(cases_by_month_and_policy,\n              filter = \"top\")\n\n\n\n\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\ncases_by_month_and_policy %&gt;%\n  ggplot(aes(\n    x= year_month,\n    y = new_cases_pc,\n    col=face_masks))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/01-lab-comments.html#footnotes",
    "href": "labs/01-lab-comments.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/02-lab.html",
    "href": "labs/02-lab.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to reproduce this figure:\n\n\n\n\n\n\n\n\n\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides (week 1 here, week 2 here) and last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nLearn how to take a complex problem (“How do I produce a figure that shows the average rate of new cases per month for states with a particular type of face mask policy”) which you may think you have no idea how to do and break this challenge down into concrete tasks which you do know how do (“Well first, I’ll need to load some packages to work with and visualize data. Then, I’ll need to get the data. And then…”)\n\nThink and write programmatically\n\nIn this .Rmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll produce a collective outline of what we need to do before getting to the actual coding\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nAgain you shouldn’t have to write much code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\nBut shouldn’t we be writing our own code?! Yes. Sure. Eventually.\nThe tutorials give you practice writing single commands, and by the end of the class you should be able write this code like this for to accomplish similar tasks\nBut even then, you will not be writing code from memory. I still have to Google functions, and often search my old code to find a clever solution to task.\nEveryone starts learning to code by copying and pasting other people’s code.\nThis will help minimize (but not eliminate) syntactic errors, while over time we get better writing code from scratch and fixing errors as the develop.\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nFor fun, let’s say that the first group that successfully recreates this figure gets to choose one of the following non-monetary prizes:\n\nI’ll tell them a joke\nOne AMA I will answer truthfully\nOne question to be asked on the weekly class survey\n0.00001% extra credit added to their final grade for the course.\n\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!"
  },
  {
    "objectID": "labs/02-lab.html#step-2.1",
    "href": "labs/02-lab.html#step-2.1",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "3.1 Step 2.1",
    "text": "3.1 Step 2.1\nDescribe briefly (in a sentence or two or a couple of bullet points) what this section does\n\n# Write the code for Step 2.1 here"
  },
  {
    "objectID": "labs/02-lab.html#step-2.2",
    "href": "labs/02-lab.html#step-2.2",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "3.2 Step 2.2",
    "text": "3.2 Step 2.2\nDescribe briefly (in a sentence or two or a couple of bullet points) what this section does\n\n# Write the code for Step 2.2 here"
  },
  {
    "objectID": "labs/comments/03-lab-comments.html",
    "href": "labs/comments/03-lab-comments.html",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "",
    "text": "Today we will explore the logic and design of Broockman and Kalla’s 2016 study, “Durably reducing transphobia: A field experiment on door-to-door canvassing”, from the recruitment of subjects for the study to the delivery of their interventions. Then we will explore whether the intervention had any effect on respondents’ feelings toward transgender individuals.\nTo accomplish this we will:\n\nSummarize the study (5 Minutes)\nSet up our work space (2-3 Minutes)\nLoad a portion of the replication data (1-2 Minutes)\nGet a high level overview of the data (5 minutes)\nDescribe the distribution of covariates in the full dataset (5 minutes)\nExamine the difference in covariates between those who did and did not complete the survey (10 minutes)\nExamine the difference in covariates between those assigned to each treatment condition in the study. (10 minutes)\nEstimate the average treatment effect of the intervention (10 minutes)\nPlot the results and comment on the study (10 minutes)\nTake the weekly survey (3-5 minutes)\n\nOne of these 9 tasks (excluding the weekly) will be randomly selected as the graded question for the lab.\n\nset.seed(20220217)\ngraded_question &lt;- sample(1:9,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 2 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nError in `tibble::tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 5: Column `Group 7`.\nℹ Only values of size one are recycled.\n\n\nError in eval(expr, envir, enclos): object 'groups_df' not found"
  },
  {
    "objectID": "labs/comments/03-lab-comments.html#footnotes",
    "href": "labs/comments/03-lab-comments.html#footnotes",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find the full set of replication files here↩︎\nThe actual study contains a number of measures about transgender attitudes and policies which are scaled together to produce a single measure of subjects latent tolerance. For simplicity, we’ll focus on this single survey item.↩︎\nRecall that only some people who completed the baseline and were assigned to receive the treatment actually answered the door when canvassers came knocking.↩︎"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html",
    "href": "labs/comments/06-lab-comments.html",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "",
    "text": "Today we will explore the critiques and alternative explanations for the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nRecall the core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\n\nA skeptic of this claim might argue that relationship between electoral and epidemelogical outcomes is spurious. There are lots of ways that Red States differ from Blue States — demographics, economics, geography, culture, and so on – and it is these differences that explain the phenomena of Red Covid. If we were to control for these omitted variables the relationship between a state’s partisan leanings and Covid-19 would go away.\nIn this lab, we will see how we can explore these claims using multiple regression to control for competing explanations.\nTo accomplish this we will:\n\nGet set up to work (10 minutes)\n\nThen we will estimate and interpret a series of regression models:\n\nA baseline Red Covid model using simple bivariate regression using the Republican vote share of states to predict the 14-day average of per capita Covid-19 deaths on September 23, 2021 (10 Minutes)\nA multiple regression model controlling for Republican vote share the median age (15 minutes)\nA model controlling for Republican vote share, the median age and median income (15 minutes)\nA model controlling for Republican vote share, the median age median income and vaccination rates (15 minutes)\nA model using Republican vote share, the median age median income to predict vaccination rates (15 minutes)\n\nFinally, we’ll take the weekly survey which will serve as a mid semester check in.\nOne of these 6 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3092022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 6 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#load-packages",
    "href": "labs/comments/06-lab-comments.html#load-packages",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\nThere’s one new package, htmltools which we’ll use to display regression tables while we work.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#load-the-data",
    "href": "labs/comments/06-lab-comments.html#load-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the data that we created in class on Tuesday which provides a snapshot of the state of Covid-19 on September 23, 2021 in the U.S.\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nAfter running this code, the data frame covid_lab should appear in your environment pane in R Studio"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#describe-the-data",
    "href": "labs/comments/06-lab-comments.html#describe-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nIn the code chunk below, please write some code get an high level overview of the data:\n\n# High level overview\n# Number of observations and variables\ndim(covid_lab)\n\n[1] 50 14\n\n# Names of variables\nnames(covid_lab)\n\n [1] \"state\"                  \"state_po\"               \"date\"                  \n [4] \"new_deaths_pc_14da\"     \"percent_vaccinated\"     \"winner\"                \n [7] \"rep_voteshare\"          \"med_age\"                \"med_income\"            \n[10] \"population\"             \"rep_voteshare_std\"      \"med_age_std\"           \n[13] \"med_income_std\"         \"percent_vaccinated_std\"\n\n# Glimpse of data\nglimpse(covid_lab)\n\nError in glimpse(covid_lab): could not find function \"glimpse\"\n\n# Summary of data\nsummary(covid_lab)\n\n    state             state_po              date            new_deaths_pc_14da\n Length:50          Length:50          Min.   :2021-09-23   Min.   :0.1342    \n Class :character   Class :character   1st Qu.:2021-09-23   1st Qu.:0.2630    \n Mode  :character   Mode  :character   Median :2021-09-23   Median :0.4089    \n                                       Mean   :2021-09-23   Mean   :0.5753    \n                                       3rd Qu.:2021-09-23   3rd Qu.:0.8733    \n                                       Max.   :2021-09-23   Max.   :1.8151    \n percent_vaccinated   winner   rep_voteshare      med_age        med_income   \n Min.   :43.55      Trump:25   Min.   :30.38   Min.   :30.80   Min.   :45081  \n 1st Qu.:49.38      Biden:25   1st Qu.:41.45   1st Qu.:37.12   1st Qu.:55511  \n Median :54.49                 Median :49.59   Median :38.40   Median :60981  \n Mean   :55.83                 Mean   :50.03   Mean   :38.48   Mean   :62631  \n 3rd Qu.:61.76                 3rd Qu.:58.00   3rd Qu.:39.50   3rd Qu.:70601  \n Max.   :72.20                 Max.   :69.50   Max.   :44.70   Max.   :84805  \n   population       rep_voteshare_std   med_age_std       med_income_std   \n Min.   :  578759   Min.   :-1.89751   Min.   :-3.25163   Min.   :-1.7060  \n 1st Qu.: 1827712   1st Qu.:-0.82886   1st Qu.:-0.57300   1st Qu.:-0.6922  \n Median : 4558234   Median :-0.04312   Median :-0.03303   Median :-0.1604  \n Mean   : 6560454   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 7530849   3rd Qu.: 0.76915   3rd Qu.: 0.43282   3rd Qu.: 0.7747  \n Max.   :39512223   Max.   : 1.87973   Max.   : 2.63502   Max.   : 2.1554  \n percent_vaccinated_std\n Min.   :-1.5345       \n 1st Qu.:-0.8061       \n Median :-0.1680       \n Mean   : 0.0000       \n 3rd Qu.: 0.7405       \n Max.   : 2.0449       \n\n# Variables I want to calculate sd for\nthe_vars &lt;- c(\"new_deaths_pc_14da\",\"rep_voteshare\",\"med_age\",\"med_income\",\"percent_vaccinated\")\n\n# Calculate standard deviations\ncovid_lab %&gt;%\n  select(all_of(the_vars))%&gt;%\n  summarise_all(sd)\n\nError in covid_lab %&gt;% select(all_of(the_vars)) %&gt;% summarise_all(sd): could not find function \"%&gt;%\"\n\n\nPlease use this HLO to answer the following questions:\n\nHow many observations are there: 50\nWhat is an observation (i.e. what is the unit of analysis): A U.S. State (on September 23, 2021)\nWhat is the primary outcome variable for today: The 14-day average of new Covid-19 deaths\nWhat are the four main predictors we’ll be using: We’ll be predicting Covid-19 deaths with measures of Republican Vote Share (rep_voteshare), Median Age (med_age), Median Income (med_income), and Percent Vaccintated (percent_vaccinated).\nWill we be using the the raw values of these predictors or their standardized values? We’ll be using the standardized version of these variables which all have the suffix _std\nWhat are the standard deviations of our outcome and predictor variables:\n\nCovid-19 deaths: 0.40 deaths\nRepublican vote share: 10.4 percentage points\nMedian age: 2.36 years\nMedian income: $ 10,288\nVaccination Rate: 7.98 percentage points"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#fit-the-model",
    "href": "labs/comments/06-lab-comments.html#fit-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.1 Fit the model",
    "text": "2.1 Fit the model\n\nm1 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std, covid_lab)"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#summarize-the-results",
    "href": "labs/comments/06-lab-comments.html#summarize-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.2 Summarize the results",
    "text": "2.2 Summarize the results\nNow we apply the summary() function to our model m1\n\nsummary(m1)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ rep_voteshare_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66967 -0.21572 -0.03715  0.11169  1.00580 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.57530    0.04846  11.872 6.88e-16 ***\nrep_voteshare_std  0.22571    0.04895   4.611 2.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3427 on 48 degrees of freedom\nMultiple R-squared:  0.307, Adjusted R-squared:  0.2925 \nF-statistic: 21.26 on 1 and 48 DF,  p-value: 2.99e-05\n\n\nWe see that m1 returns two coefficients, which define a line of best fit predicting Covid-19 deaths with the Republican vote share of the 2020 Presidential election:\n\n\\(\\beta_0\\) corresponds to the intercept. This is model’s prediction for a state where Trump got 0 percent of the vote. This is typically not something we care about.\n\\(\\beta_1\\) corresponds to the slope. Because we used a standardized measure of vote share, we would say that a 1-standard deviation (about 10 percentage points) increase in Republican vote share is associated with a 0.23 increase the average number of new Covid-19 deaths. Given that this per-capita measure has a standard deviation of 0.4, this is a fairly sizable association.\nFinally, note that last column of summary(m1) Pr(&gt;|t|) both the coefficients for the intercept \\((\\beta_0)\\) and rep_voteshare_std (\\((\\beta_1)\\)) are statistically significant (ie have an * next to them)."
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#display-the-model-as-a-regression-table",
    "href": "labs/comments/06-lab-comments.html#display-the-model-as-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.3 Display the model as a regression table",
    "text": "2.3 Display the model as a regression table\nNext we’ll format the results of summary(m1) into a regression table using the htmlreg() function.\nRegression tables are a the standard way of concisely presenting the results of regression models.\n\nEach named row corresponds to the coefficients form the model\nIf there is an asterisks next to a coefficient, that coefficient is statistically significant with a p value below a certain threshold.\nThe numbers in parentheses below each coefficient correspond to the standard error of the coefficient (more on that later)2\nThe bottom of the table contains summary statistics of of our model, which we’ll ignore for today.\n\nThe code after htmlreg(m1) allows you to see what output of the table will look like in the html document while you’re working in the Rmd file.\n\n\nError in htmlreg(list(m1)) %&gt;% HTML() %&gt;% browsable(): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#visualize-the-model",
    "href": "labs/comments/06-lab-comments.html#visualize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.4 Visualize the model",
    "text": "2.4 Visualize the model\nNow let’s visualize the results of our m1 with a scatter plot.\nIn the code below, please write a comment explaining what each section of code is doing\n\n# 1. Tell ggplot what data to use\ncovid_lab %&gt;%\n# 2. Set the aesthetic mappings of our figure\n  ggplot(aes(x = rep_voteshare_std,\n             y = new_deaths_pc_14da,\n             label = state_po))+\n# 3. Draw points with x values corresponding to Rep vote share and y values corresponding to Covid deaths. \n  geom_point(\n# 4. Make the points smaller in size\n    size = .5\n    )+\n# 5. Add labels using `label=state_po` aesthetic \n  geom_text_repel(\n# 6. Make the label size smaller    \n    size = 2)+\n# 7. Plot the regression model\n  geom_smooth(method = \"lm\")+\n# 8. Change the axis labels\n  labs(\n    x = \"Republican Vote Share (std)\",\n    y = \"New Covid-19 Deaths\\n(14-day ave)\"\n  )\n\nError in covid_lab %&gt;% ggplot(aes(x = rep_voteshare_std, y = new_deaths_pc_14da, : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#interpret-the-results",
    "href": "labs/comments/06-lab-comments.html#interpret-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.5 Interpret the results",
    "text": "2.5 Interpret the results\nIn a sentence our two, summarize the results of your analysis in this section\nOur model in this section provides results consistent with the phenomena of Red Covid. State’s with higher Republican vote shares tended to have higher per capita rates of death from Covid-19 on September 23, 2022."
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#fit-the-model-1",
    "href": "labs/comments/06-lab-comments.html#fit-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.1 Fit the model",
    "text": "3.1 Fit the model\nNow let’s test our skeptics’ claims by fitting a model m2 that controls for Age (med_age_std).\n\nRemember the first argument in lm() is formula of the form outcome variable ~ predictor1 + predictor2 + ...\n\n\nm2 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std + med_age_std, covid_lab)"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#summarize-the-model",
    "href": "labs/comments/06-lab-comments.html#summarize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.2 Summarize the model",
    "text": "3.2 Summarize the model\nNow let’s print out a statistical summary of m2\n\nsummary(m2)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ rep_voteshare_std + med_age_std, \n    data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6101 -0.2269 -0.0551  0.1124  0.9425 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.57530    0.04825  11.924 8.14e-16 ***\nrep_voteshare_std  0.24497    0.05135   4.771 1.82e-05 ***\nmed_age_std        0.06121    0.05135   1.192    0.239    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3412 on 47 degrees of freedom\nMultiple R-squared:  0.3273,    Adjusted R-squared:  0.2987 \nF-statistic: 11.43 on 2 and 47 DF,  p-value: 8.988e-05"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#display-the-model-as-a-regression-table-1",
    "href": "labs/comments/06-lab-comments.html#display-the-model-as-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.3 Display the model as a regression table",
    "text": "3.3 Display the model as a regression table\nNext, let’s create a regression table that displays m1 in the first column and m2 in the second column.\n\nTo do this, change list(m1) from the code above to list(m1, m2)\n\n\n\nError in htmlreg(list(m1, m2)) %&gt;% HTML() %&gt;% browsable(): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#interpret-the-results-1",
    "href": "labs/comments/06-lab-comments.html#interpret-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.4 Interpret the results",
    "text": "3.4 Interpret the results\nIn a few sentences, explain whether the results from m2 support the skeptics criticisms or not?\n\nThey do not. Controlling for differences in the median age of state’s population does little to change the relationship between partisanship and Covid-19 that we saw in m1. If anything the the relationship is slightly stronger, while the coefficient on age is substantively small and statistically non-significant.\n\nPart of what’s at play here, is that relationship between age and Covid-19 outcomes at the state level is pretty weak, perhaps reflecting the early focus on vaccinating the eldery.\nSimilarly, the idea that Red States tend to be older doesn’t appear to be empirically true, perhaps reflecting differences between the general population and voting population.\n\n# Fit models\nm2_death_age &lt;- lm(new_deaths_pc_14da ~ med_age_std, covid_lab)\nm2_rep_age &lt;- lm(rep_voteshare_std ~ med_age_std, covid_lab)\n\n# Summarize models\nsummary(m2_death_age)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ med_age_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4298 -0.3080 -0.1646  0.2981  1.2433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.57530    0.05817   9.891 3.63e-13 ***\nmed_age_std -0.01587    0.05876  -0.270    0.788    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4113 on 48 degrees of freedom\nMultiple R-squared:  0.001517,  Adjusted R-squared:  -0.01928 \nF-statistic: 0.07293 on 1 and 48 DF,  p-value: 0.7883\n\nsummary(m2_rep_age)\n\n\nCall:\nlm(formula = rep_voteshare_std ~ med_age_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.78067 -0.71029  0.01972  0.60954  2.33183 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  1.565e-16  1.356e-01   0.000   1.0000  \nmed_age_std -3.146e-01  1.370e-01  -2.297   0.0261 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.959 on 48 degrees of freedom\nMultiple R-squared:  0.099, Adjusted R-squared:  0.08023 \nF-statistic: 5.274 on 1 and 48 DF,  p-value: 0.02605\n\n# Display as formatted regression table\nhtmlreg(list(m2_rep_age,\n             m2_death_age),\n        custom.model.names = c(\"Deaths\", \"Rep Vote\"),\n        custom.coef.names = c(\"(Intercept)\",\"Median Age (std)\"),\n        custom.header = list(\"DV\"=1:2)  )%&gt;% HTML() %&gt;% browsable()\n\nError in htmlreg(list(m2_rep_age, m2_death_age), custom.model.names = c(\"Deaths\", : could not find function \"%&gt;%\"\n\n# Visualize the models\ncovid_lab %&gt;%\n  ggplot(aes(x = med_age_std,\n             y = rep_voteshare_std,\n             label = state_po))+\n  geom_point(size = .5)+\n  geom_text_repel(size = 2)+\n  geom_smooth(method = \"lm\")\n\nError in covid_lab %&gt;% ggplot(aes(x = med_age_std, y = rep_voteshare_std, : could not find function \"%&gt;%\"\n\ncovid_lab %&gt;%\n  ggplot(aes(x = med_age_std,\n             y = new_deaths_pc_14da,\n             label = state_po))+\n  geom_point(size = .5)+\n  geom_text_repel(size = 2)+\n  geom_smooth(method = \"lm\")\n\nError in covid_lab %&gt;% ggplot(aes(x = med_age_std, y = new_deaths_pc_14da, : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#fit-the-model-2",
    "href": "labs/comments/06-lab-comments.html#fit-the-model-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.1 Fit the Model",
    "text": "4.1 Fit the Model\nPlease fit a model called m3 implied by the skeptic’s revised claims\n\nm3 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#summarize-the-model-1",
    "href": "labs/comments/06-lab-comments.html#summarize-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.2 Summarize the model",
    "text": "4.2 Summarize the model\nSummarize the model m3 using summary()\n\nsummary(m3)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51238 -0.16846 -0.06056  0.15048  0.89939 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.57530    0.04478  12.848  &lt; 2e-16 ***\nrep_voteshare_std  0.09686    0.06951   1.393  0.17021    \nmed_age_std        0.00379    0.05153   0.074  0.94169    \nmed_income_std    -0.19341    0.06609  -2.927  0.00531 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3166 on 46 degrees of freedom\nMultiple R-squared:  0.4329,    Adjusted R-squared:  0.3959 \nF-statistic:  11.7 on 3 and 46 DF,  p-value: 8.018e-06"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#display-the-models-in-a-regression-table",
    "href": "labs/comments/06-lab-comments.html#display-the-models-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.3 Display the models in a regression table",
    "text": "4.3 Display the models in a regression table\nAnd then display the results of models m1, m2, and m3.\n\nhtmlreg(list(m1,m2,m3)) %&gt;% HTML() %&gt;% browsable()\n\nError in htmlreg(list(m1, m2, m3)) %&gt;% HTML() %&gt;% browsable(): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#interpret-the-skeptics-claims",
    "href": "labs/comments/06-lab-comments.html#interpret-the-skeptics-claims",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.4 Interpret the skeptic’s claims",
    "text": "4.4 Interpret the skeptic’s claims\nIn a few sentences, explain whether the results from m3 support the skeptics criticisms or not?\nControlling for median age and income, the coefficient on Republican sote share decreases in size by more than half and is no longer statistically significant. The coefficient on median income is statistically significant and substantively suggests that states with higher median incomes tended to have fewer Covid-19 deaths on September 23, 2021."
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#fit-the-model-3",
    "href": "labs/comments/06-lab-comments.html#fit-the-model-3",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.1 Fit the model",
    "text": "5.1 Fit the model\nYou know the drill.\n\nm4 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std + med_age_std + med_income_std + percent_vaccinated_std, covid_lab)"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#summarize-the-results-1",
    "href": "labs/comments/06-lab-comments.html#summarize-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.2 Summarize the results",
    "text": "5.2 Summarize the results\nAgain, let’s get a quick summary of our results\n\nsummary(m4)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ rep_voteshare_std + med_age_std + \n    med_income_std + percent_vaccinated_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52486 -0.16880 -0.03045  0.13917  0.94190 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.57530    0.04175  13.781  &lt; 2e-16 ***\nrep_voteshare_std      -0.06533    0.08671  -0.753  0.45515    \nmed_age_std             0.07318    0.05400   1.355  0.18213    \nmed_income_std         -0.11191    0.06807  -1.644  0.10716    \npercent_vaccinated_std -0.27582    0.09798  -2.815  0.00721 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2952 on 45 degrees of freedom\nMultiple R-squared:  0.5178,    Adjusted R-squared:  0.475 \nF-statistic: 12.08 on 4 and 45 DF,  p-value: 9.425e-07"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#display-the-models-in-a-regression-table-1",
    "href": "labs/comments/06-lab-comments.html#display-the-models-in-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.3 Display the models in a regression table",
    "text": "5.3 Display the models in a regression table\nAnd add m4 to list of models in our regression table\n\n\nError in htmlreg(list(m1, m2, m3, m4)) %&gt;% HTML() %&gt;% browsable(): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#interpet-the-results",
    "href": "labs/comments/06-lab-comments.html#interpet-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.4 Interpet the results",
    "text": "5.4 Interpet the results\nBriefly interpret the results of m4\nControlling for vaccination rates, none of the other variables in m4 are statistically significant predictors of Covid-19 deaths."
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#fit-the-model-4",
    "href": "labs/comments/06-lab-comments.html#fit-the-model-4",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.1 Fit the model",
    "text": "6.1 Fit the model\nNow let’s fit the model. For ease of interpretation, let’s use the unstandardized measure of vaccination rates, percent_vaccinated as our outcome variable.\n\nm5 &lt;- lm(percent_vaccinated ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#summarize-the-results-2",
    "href": "labs/comments/06-lab-comments.html#summarize-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.2 Summarize the results",
    "text": "6.2 Summarize the results\nAnd summarize the results\n\nsummary(m5)\n\n\nCall:\nlm(formula = percent_vaccinated ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3665 -2.6483  0.1951  1.6016  7.0928 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        55.8304     0.5029 111.025  &lt; 2e-16 ***\nrep_voteshare_std  -4.7065     0.7806  -6.029 2.62e-07 ***\nmed_age_std         2.0136     0.5787   3.479  0.00111 ** \nmed_income_std      2.3652     0.7422   3.187  0.00258 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.556 on 46 degrees of freedom\nMultiple R-squared:  0.8147,    Adjusted R-squared:  0.8027 \nF-statistic: 67.43 on 3 and 46 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#display-the-results-in-a-regression-table",
    "href": "labs/comments/06-lab-comments.html#display-the-results-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.3 Display the results in a regression table",
    "text": "6.3 Display the results in a regression table\nDisplay them in a regression table\n\n\nError in htmlreg(list(m5)) %&gt;% HTML() %&gt;% browsable(): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#interpret-the-results-2",
    "href": "labs/comments/06-lab-comments.html#interpret-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.4 Interpret the results",
    "text": "6.4 Interpret the results\nSummarize the results of m5 and offer some broader discussion of what we’ve learned today\nIn m5 all three predictors have strong statistically significant relationships with vaccination rates in the expected direction. States where more people voted for Trump in 2020 tend to have lower rates of vaccination. States with an older population, and a richer population tend to have higher rates of vaccination.\nThinking back to the previous models we’ve estimated, we might argue that the effects these predictors have on Covid-19 death rates is mediated through their relationship with vaccination rates.\nMore broadly, what does this analysis mean for arguments about Red Covid. I guess, I’d say it’s complicated. Vaccines are clearly effective at reducing Covid-19 deaths. In both aggregate and invidual level data, Republicans appear to be less willing to get vaccinated. But lots of other factors influence vaccination rates and public health more broadly.\nRegression is a tool for trying to explore these competing explanations, but without strong theory and clever design, it’s unlikely to resolve debates. There’s almost always a skeptic waiting to say “Yes, but have you controlled for …” We can try to address there concerns by controlling for more and more variables. But a better strategy is often to say, I don’t need to control for X because the logic of my design already accounts for X.\nStill it’s hard to think what that kind of design would be for something like for debates about Red Covid. Maybe we need different (individual data) or maybe we’d want to reframe the question or draw out further testable implications of the claim. If your group is looking for a final project, there are number of directions you could take this kind of analysis:\n\nLooking at different periods overtime\nLooking at smaller units of aggregation (counties)\nControlling for alternative factors\nLeveraging variation in the availability of vaccines over time and place."
  },
  {
    "objectID": "labs/comments/06-lab-comments.html#footnotes",
    "href": "labs/comments/06-lab-comments.html#footnotes",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn short, these * correspond to \\(p-values\\) below different thresholds. One * typically means \\(p &lt; 0.05\\). A p-value is a conditional probability that arises from a hypothesis test summarizing the likelihood of observing a particular test statistic (here a regression coefficient, or more specifically, a t-statistic which is the regression coefficient divided by its standard error) given a paritcular hypothesis (typically, but not allows a null hypothesis that the true coefficient is 0). In sum, a p-value assess the likelihood of seeing what we did, if in fact, there was no relationship. If that likelihood is small (p&lt;0.05), we reject the claim of no relationship. We remain uncertain about the true value of the coefficient, but we are pretty confident it’s not 0.↩︎\nA standard error is another one of those things that in the cart we’re putting before horse today. Briefly, it is an estimate of the standard deviation of the sampling distribution of a coefficient and describes how much our coefficient might vary had we had a different sample…↩︎"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html",
    "href": "labs/comments/09-lab-comments.html",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "",
    "text": "Today, we will return to exploring Russians’ support the war in Ukraine using a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project.\nThe survey was conducted by phone using a random sample of mobile phone numbers to produce a sample of respondents representative of the population in terms of age, sex, and geography. It was in the field from February 28 to March 2.\nFirst, we will explore how support for the war varies with the demographic predictors age, sex and education. We will compare the results of modeling this relationship using Ordinary Least Squares regression and Logisitic Regression\nWe’ll talk more about the technical aspects of logistic regression next week. For today we’ll simply compare the results from these two approaches.\nNext, we will gain insight into how our estimates from these models might vary using the statistical process of bootstrapping. Specifically, we will simulate the idea of repeated sampling that is the foundation of frequentist interpretations of probability, by sampling from our sample with replacement.\nWe’ll walk through the mechanics of simulation together. Then you’ll quantify the uncertainty described by these bootstrapped sampling distributions.\nFinally, we’ll see what other questions we might ask of these data and practice various skills we’ve developed through out the course.\nPlan to spend the following amount of time on each section\n\nGet set up to work (5 minutes)\nModel the relationship between demographic predictors and war support using OLS and Logistic regression (20 minutes)\nAssess the uncertainty around your estimated coefficients (15 minutes)\nQuantify the uncertainty described by your sampling distributions (10 minutes)\nExplore other relationships in the data. (30 minutes)\n\nThe graded question for today is:\n\nset.seed(472022)\ngraded_question &lt;- sample(1:5,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 5 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file produced by knitting the lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nYou can find your assigned groups in previous labs"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#load-packages",
    "href": "labs/comments/09-lab-comments.html#load-packages",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  \"flair\", # Comments only\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\", \"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"boot\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\n\nThere are three packages in particular that we’ll need to maker sure are installed and loaded\n\nmodelr\npurrr\nbroom\n\nIf ipak didn’t return TRUE for each of these, please uncomment and run:\n\n# install.packages(\"modelr\")\n# install.packages(\"purrr\")\n# install.packages(\"broom\")"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#load-the-data",
    "href": "labs/comments/09-lab-comments.html#load-the-data",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the recoded data for the lab\nOur primary outcome of interest (dependent variable) for today is a binary measure of support for war:\n\nsupport_war01 “Please tell me, do you support or do not support Russia’s military actions on the territory of Ukraine?” (1=yes, 0 = no)\n\nOur key predictors (independent variables) are the following demographic variables:\n\nage “How old are you?”\nsex “Gender of respondent” (As assessed by the interviewer)\neducation_n “What is your highest level of education (confirmed by a diploma, certificate)?” (1 = Primary school, 2 = “High School”, 3 = “Vocational School” 4 = “College”, 5 = Graduate Degree)1\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\ndf_drww %&gt;%\n  mutate(\n    person_id = 1:n()\n  ) -&gt; df_drww\n\nError in df_drww %&gt;% mutate(person_id = 1:n()): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#fit-the-models",
    "href": "labs/comments/09-lab-comments.html#fit-the-models",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.1 Fit the models",
    "text": "2.1 Fit the models\nPlease estimate the following models:\n\nAn OLS regression model called m1 using lm()\nA Logistic regression model called m2 using glm() with family=binomial\n\n\n# OLS\nm1 &lt;- lm(support_war01 ~ age + sex + education_n, df_drww)\n\n# Logisitic \nm2 &lt;- glm(support_war01 ~ age + sex + education_n, df_drww,\n          family = binomial)"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#display-the-results-in-a-regression-table",
    "href": "labs/comments/09-lab-comments.html#display-the-results-in-a-regression-table",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.2 Display the results in a regression table",
    "text": "2.2 Display the results in a regression table\nNext, please display the results of your regressions in a table using htmlreg()\n# Regression Table\nhtmlreg(list(m1,m2))\nError in htmlreg(list(m1, m2)): could not find function \"htmlreg\""
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#produce-predicted-values-from-the-model",
    "href": "labs/comments/09-lab-comments.html#produce-predicted-values-from-the-model",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.3 Produce predicted values from the model",
    "text": "2.3 Produce predicted values from the model\nThe coefficients from a logistic regression aren’t easy to directly interpret.\nInstead, we will produce predicted values for each model\nTo do this, we will need to create a prediction dataframe called pred_df Every variable in your model, needs to be represented in your prediction data frame.\n\nUse expand_grid() to create a data frame where\n\nage varies from 18 to 99\nsex is held constant at “Female”\neducation_n is held constant at it’s mean\n\n\n\n## Create prediction data frame\npred_df &lt;- expand_grid(\n  age = 18 : 99,\n  sex = \"Female\",\n  education_n = mean(df_drww$education_n, na.rm = T)\n)\n\nError in expand_grid(age = 18:99, sex = \"Female\", education_n = mean(df_drww$education_n, : could not find function \"expand_grid\"\n\n\nThen you use the predict() function to produce predicted values from each model.\nSave the output of predict() for m1 to a new column in pred_df called pred_ols.\nFor m2 you need to tell are to transform the predictions from m2 back into the units of the response (outcome) variable, by setting the argument type = \"response\". Save the output of predict() for m1 to a new column in pred_df called pred_logit.\n\n# #Predicted values for m1\npred_df$pred_ols &lt;- predict(m1,\n                            newdata = pred_df)\n\nError in eval(expr, envir, enclos): object 'pred_df' not found\n\n# Predicted values for m2\n# Remember to add type = \"response\"\npred_df$pred_logit &lt;- predict(m2,\n                            newdata = pred_df,\n                            type = \"response\")\n\nError in eval(expr, envir, enclos): object 'pred_df' not found"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#plot-the-predicted-values-and-interpet-the-results",
    "href": "labs/comments/09-lab-comments.html#plot-the-predicted-values-and-interpet-the-results",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.4 Plot the predicted values and interpet the results",
    "text": "2.4 Plot the predicted values and interpet the results\nNow we can compare the predictions of OLS and Logistic regression by plotting the predicted values of support for the war from each model.\nTo produce this plot you’ll need to\n\nspecify data (you want to use the values from pred_df)\nmap values from your data aesthetics in ggplot\n\nput age on the x axis and and pred_ols on the y-axis\n\nspecify the geometries to plot\n\nadd two geom_line() to the plot\nleave the first one empty (e.g. geom_line())\nfor the second, specify a new aes of y=pred_logit\n\n\n\n# data\npred_df%&gt;%\n  # aesthetics\n  ggplot(aes(age, pred_ols, col = \"OLS\"))+\n  # geometries\n  geom_line()+\n  geom_line(aes(y = pred_logit, col = \"Logistic\"))+\n  geom_jitter(data=df_drww, aes(age, support_war01),\n              col = \"black\",\n              height = .05,\n              size = .5,\n              alpha = .5)+\n  labs(\n    col = \"Model\",\n    x = \"Age\",\n    y = \"Predicted Values\"\n  )\n\nError in pred_df %&gt;% ggplot(aes(age, pred_ols, col = \"OLS\")): could not find function \"%&gt;%\"\n\n\nHow do the predictions of the two models compare\nSo the predictions from OLS produce impossible values (levels of support above 100 percent) at for very old respondents, while the predictions from logistic regression are constrained to be between 0 and 1.\nIf we think that logistic regression provides a more credible way of modeling support for the war, then our OLS regression appears to overstate the level of support among young and old, while possibly understating the level of support among the middle age. The differences aren’t huge – a few percentage points – but for a binary outcome we will often prefer to model it with logistic regression.\nAlso note that marginal effect for age in the logistic regression is not constant. An increase in age from 25 to 26 is associated with a larger increase in support, than an increase in age from 75 to 76."
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#take-1000-bootstrap-samples-from-df_drww",
    "href": "labs/comments/09-lab-comments.html#take-1000-bootstrap-samples-from-df_drww",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.1 Take 1,000 bootstrap samples from df_drww",
    "text": "3.1 Take 1,000 bootstrap samples from df_drww\nBelow we create 1,000 boostrapped samples\n\n# Make sure these packages are loaded\nlibrary(modelr)\nlibrary(purrr)\nlibrary(broom)\n# Set random seed for reproducability\n\nset.seed(1234)\n\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n\nLet’s take a moment to understand what boot is and why we’re sampling with replacement.\nThe object boot contains 1,000 bootstrapped samples from df_drww.\nIf we look at the first bootstrap we see:\n\nboot$strap[[1]]\n\n&lt;resample [1,807 x 42]&gt; 1308, 1018, 1125, 1004, 623, 905, 645, 934, 400, 900, ...\n\n\nThe numbers 1308, 1018, 1125, 1004, 623, 905, ... correspond to rows in df_drww. So person 1308 is the first observation in this boot strap sample, then person 1018 and so on.\nBecause we are sampling with replacement observations from df_drww can appear multiple times. In our first bootstrap sample:\n\n666 observations appeared once\n342 appeared twice\n105 appeared three times\n27 appeared four times\n5 appeared five times.\n2 appeared six times\n\n\ntable(table(boot$strap[[1]]$idx))\n\n\n  1   2   3   4   5   6 \n666 342 104  27   5   2 \n\n\nWhy would we want to sample with replacement?\nWell, what we’d really love are 1,000 separate random surveys all drawn from the same population in the same way.\nSince that’s not feasible, we instead use the one sample we do have to learn things like how much our estimate might vary in repeated sampling. Efron (1979) called this procedure “bootstrapping” after the idiom “to pull oneself up by one’s own bootstraps”\nWe do this by sampling from our sample with replacement.\nWhen we sample with replacement, we are sampling from our sample, as our sample was sampled from the population.\nWith replacement means that some observations will appear multiple times in our bootstrapped sample (while others will not be included at all).\nWhen an observation appears multiple times in a bootstrap sample, conceptually, we’re using that original observation to represent the other people like that observation in the population who – had we taken a different sample – might have been included in our data.\nNote each bootstrap sample is a different random sample with replacement. In our second bootstrap sample, one observation (person 1496) appeared five times.\n\ntable(table(boot$strap[[2]]$idx))\n\n\n  1   2   3   4   5   6 \n661 342 105  31   1   3 \n\n# Person 406\nsum(boot$strap[[2]]$idx == 1496)\n\n[1] 5\n\n# Person 1 is not in boostrap 2\nsum(boot$strap[[2]]$idx == 1)\n\n[1] 0"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "href": "labs/comments/09-lab-comments.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples",
    "text": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples\nNow let’s estimate our model for each bootstrapped sample, using the map function.\n\nIn the code below, for every sample in boot, map estimates the model lm(support_war01 ~ age + sex + education_n) plugging in the bootstrap sample into the data=..\n\n\n# bootstrap simulations\nbs_ols &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + sex + education_n, data =.))\n\nThe end result is a large list with 1,000 separate linear regression models estimated on each bootstrapped sample.\nThe coefficients from each bootstrap vary from one simulation\n\n# First bootstrap\nbs_ols[[1]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.305019     0.009746     0.081039    -0.024142  \n\n\nto the next:\n\n# Second boostrap\nbs_ols[[2]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.245000     0.009142     0.095631    -0.009285  \n\n\nBecause they’re estimated off of different bootstrapped samples.\nWe will visualize and quantify that variation to describe the uncertainty associated with our estimates.\nBut first, we need to transform our large list of linear models, into a more tidy data frame that’s easier to manipulate."
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#tidy-the-results-of-our-bootstrapping",
    "href": "labs/comments/09-lab-comments.html#tidy-the-results-of-our-bootstrapping",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.3 Tidy the results of our bootstrapping",
    "text": "3.3 Tidy the results of our bootstrapping\nIn the code below we transform this large list of models into a tidy data frame of coefficients.\n\n# Tidy bootstrapp sims\nbs_ols_df &lt;- map_df(bs_ols, tidy, .id = \"id\")\n\nIn the resulting data frame the id variable identifies the bootstrap simulation (1 to 1,000), the term variable indentifies the specific coefficient from the model estimated for that simulation.\n\nhead(bs_ols_df)\n\n# A tibble: 6 × 6\n  id    term        estimate std.error statistic  p.value\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 1     (Intercept)  0.305    0.0522        5.84 6.29e- 9\n2 1     age          0.00975  0.000702     13.9  3.31e-41\n3 1     sexMale      0.0810   0.0222        3.65 2.73e- 4\n4 1     education_n -0.0241   0.0107       -2.26 2.39e- 2\n5 2     (Intercept)  0.245    0.0533        4.60 4.61e- 6\n6 2     age          0.00914  0.000731     12.5  3.36e-34"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "href": "labs/comments/09-lab-comments.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age",
    "text": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age\nFinally, let’s get a sense of how our coefficients could vary.\nSpecifically, let’s compare the the observed coefficient from m1 for age, to the bootstrapped sampling distribution of coefficients in bs_ols_df\n\nFirst, we’ll create a basic plot called p_ols_age that shows the distribution of the coefficients for age from our simulation\n\n\np_ols_age &lt;- bs_ols_df %&gt;%\n  filter(term == \"age\") %&gt;%\n  ggplot(aes(estimate))+\n    geom_density()\n\nError in ggplot(., aes(estimate)): could not find function \"ggplot\"\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\nNext we’ll add some additional geometries and labels to our figure\n\nFirst we’ll put a rug to show the individual coefficients\n\n\np_ols_age +\n  geom_rug() -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\n\nThen we’ll add a vertical line where our observed coefficient on age\n\n\np_ols_age +\n  geom_vline(xintercept =  coef(m1)[2],\n             linetype = 2) -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\n\nFinally, let’s add some nice labels\n\n\np_ols_age +\n  theme_bw()+\n  labs(\n    x = \"Age\",\n    y = \"\",\n    title = \"Bootstrapped Sampling Distribution of Age Coefficient\"\n  ) -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\nCongratulations you’ve just produced and visualized your first bootstrapped sampling distribution!\nConceptually, this distribution describes *how much we would expect the coefficient on age in model to vary** from sample to sample.\nJust from eyeballing the figure above, it looks like the observed the relationship between age and support for war or 0.009 could be about as high as 0.011, and as low as 0.007.\nOf course, as the budding quantitative social scientists that we are, we can do better than just eyeballing the data."
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#footnotes",
    "href": "labs/comments/09-lab-comments.html#footnotes",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think, google translate was a bit unclear. But higher numbers equal more education.↩︎"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html",
    "href": "labs/comments/01-lab-comments.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#uncomment-and-run-the-following-code",
    "href": "labs/comments/01-lab-comments.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#uncomment-and-run-the-code-below",
    "href": "labs/comments/01-lab-comments.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\ncovid_us %&gt;%\n  filter(facial_coverings == -4) %&gt;%\n  select(date, state) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    n = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n  )%&gt;%\n  arrange(earliest_date)\n\n# A tibble: 4 × 4\n  state              n earliest_date latest_date\n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;        &lt;date&gt;     \n1 Illinois         156 2020-10-01    2021-05-15 \n2 Massachusetts     35 2020-10-02    2020-11-05 \n3 South Carolina    61 2020-10-13    2020-12-12 \n4 Maryland         158 2020-11-06    2021-04-12"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "href": "labs/comments/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explain in words your best understanding of what each line of code is doing:",
    "text": "6.2 Please explain in words your best understanding of what each line of code is doing:\n\ncovid_us %&gt;% tells R that every line of code after will use covid_us dataframe\nfilter(facial_coverings == -4) %&gt;% tells R to filter out only the rows where the facial coverings variable equals -4\nselect(date, state) %&gt;% tells R to select the columns named date and state\ngroup_by(state) %&gt;% tells R that subsequent commands should be done separately for each unique value of state\nsummarize( tells R we want to summarize the output of susequent commands\nn = n(), tells R to count the number of observations (state-dates) for each state that had a value of -4 on the facial_coverings variable\nearliest_date = min(date), tells R to report the earliest date that each state had a value of -4\nlatest_date = max(date), tells R to report the last date that each state had a value of -4\n)%&gt;% tells R we’re finished with the summarize() function\narrange(earliest_date) arranges the data in asscending order from earliest to latest start date\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\nSubstantively, what does the previous chunk of code tell us?\n\nSo there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. Illinois was the first state where this code appears, and it appears present in 156 observations while Montana was the last adopting a policy code -4 on March 25, 2021\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#please-run-the-following-code",
    "href": "labs/comments/01-lab-comments.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  )%&gt;%\n  filter(RegionName == \"Illinois\", \n         date &gt; \"2020-08-01\", \n         date &lt; \"2021-01-01\",\n         !is.na(H6_Notes)) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\n# A tibble: 8 × 4\n  date       `H6_Facial Coverings` H6_Flag H6_Notes                             \n  &lt;date&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                \n1 2020-08-21                     2       1 \"In Executive Order 2020-52, Executi…\n2 2020-08-26                     2       1 \"Effective from 26 August 2020, the …\n3 2020-09-18                     2       1 \"On 18 September, in Executive Order…\n4 2020-10-01                     4       0 \"Originally coded a 3T, but looking …\n5 2020-10-16                     4       0 \"In Executive Order (EO) 2020-59, Go…\n6 2020-11-13                     4       0 \"Noting that Executive Order 2020-71…\n7 2020-11-20                     4       0 \"Executive Order 2020-73 requires pe…\n8 2020-12-01                     3       1 \"Chicago seems to have changed its g…"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/comments/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;% Tells R to use the Oxford policy data\nmutate(date = ymd(Date))%&gt;% Creates a date variable of class date from the original Date variable (which was class numeric)\nfilter(RegionName == \"Illinois\", subsets the data to just Illinois\ndate &gt; \"2020-08-01\", filters out dates before August 1, 2020\ndate &lt; \"2021-01-01\", filters out observations with dates after January 1,2021\n!is.na(H6_Notes)) %&gt;% filters out observations without notes (which appear in the data when policy changes)\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks Selects just the date and notes variables and saves them to an object called il_facemasks\nil_facemasks prints the obejct in the console\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\n[1] \"On 18 September, in Executive Order 2020-55, the Governor reissued most executive orders, extending a majority of the provisions through 17 October 2020. This includes mask requirements.      https://web.archive.org/web/20200922144918/https://www2.illinois.gov/Pages/Executive-Orders/ExecutiveOrder2020-55.aspx\"\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\n# il_facemasks$H6_Notes[???]"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/comments/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy\nIn Illinois, the -4’s seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of facial_coverings to construct our face_mask variable, we’re probably over stating the extent the extensiveness of policies in effect.\nSo we should be cautious in how we interpret our collapsed variable, face_mask. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-central-tendency",
    "href": "labs/comments/01-lab-comments.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n[ {x}=_{i=1}^n x_i ]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\n\nmean(covid_us$new_cases_pc, na.rm=T)\n\n[1] 28.11486\n\n\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nFormally, you’ll often see this written in terms of Expected Values: Something like\n[ E[Y|X=x] ]\nOr to make it more concrete:\n[ E[ | ] ]\nIn code, we could accomplish this manually, using the index operator:\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n[1] 10.26168\n\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommend”\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"Recommended\"], na.rm=T)\n\n[1] 16.61408\n\n\nBy using group_by() with summarise() we can accomplish this more quickly:\n\ncovid_us%&gt;%\n  group_by(face_masks)%&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  )\n\n# A tibble: 6 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2\n6 &lt;NA&gt;                           11.8\n\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n[ M_x = X_i : ^{x_i} f_x(X)dx=^f_x(X)dx=1/2 ]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\nmedian(covid_us$new_cases_pc, na.rm=T)\n\n[1] 10.52355\n\n\nInteresting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the new_cases_pc has a “long tail” or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.\n\n\nMedians are less influenced by outliers than means\n\n\n\nhist(covid_us$new_cases_pc, breaks = 100)\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\nsort(table(covid_us$face_masks))\n\n\n    Required all times              No policy            Recommended \n                  1032                   3893                   8879 \nRequired shared places      Some requirements \n                 15088                  24786 \n\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:\n\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Visualisation_mode_median_mean.svg/240px-Visualisation_mode_median_mean.svg.png\")"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-dispersion",
    "href": "labs/comments/01-lab-comments.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\nrange(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916 1531.8669\n\nmin(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916\n\nmax(covid_us$new_cases_pc,na.rm = T)\n\n[1] 1531.867\n\n\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\ncovid_us %&gt;%\n  filter(\n    new_cases_pc &lt; -188 |\n    new_cases_pc &gt; 1500\n  )%&gt;%\n  select(state, date,new_cases_pc)\n\n# A tibble: 5 × 3\n# Groups:   state [5]\n  state        date       new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;            &lt;dbl&gt;\n1 Florida      2021-06-04        -189.\n2 Rhode Island 2022-01-04        1532.\n3 Tennessee    2023-01-01        -267.\n4 Nebraska     2022-10-28        -276.\n5 Kentucky     2022-10-11        -198.\n\n\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n[ p_x = X_i : ^{x_i} f_x(X)dx= p; ^f_x(X)dx=1-p ]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\n\nquantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)\n\n     25%      75% \n 0.00000 32.14152 \n\n\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n[ ^2_x=_{i=1}n(x_i-{x})2 ]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\nUse the var() function to calculate the variance of the new_cases_pc variable.\n\nvar(covid_us$new_cases_pc,na.rm=T)\n\n[1] 3402.718\n\n# Calculate by hand\n\nsum(\n  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, \n  na.rm=T\n  )/(sum(!is.na(covid_us$new_cases_pc))-1)\n\n[1] 3402.718\n\n\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n[ _x== ]\nStandard deviations are easier to interpet because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\nAgain, let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable\n\nsd(covid_us$new_cases_pc,na.rm=T)\n\n[1] 58.33282"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-association",
    "href": "labs/comments/01-lab-comments.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n[ cov(x,y)=_{i=1}^n(x_i-{x})(y_i-{y}) ]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\nvar(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)\n\n[1] -19.96569\n\n\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n[ _{x,y} = ]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\n\ncor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = \"complete.obs\")\n\n[1] -0.01369243\n\n\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\ncovid_us %&gt;%\n  filter(year &gt; 2020)%&gt;%\n  ungroup() %&gt;%\n  group_by(year,month)%&gt;%\n  summarise(\n    mn_per_vax = mean(percent_vaccinated, na.rm=T),\n    cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n  )\n\nError in `summarise()`:\nℹ In argument: `cor = cor(new_cases_pc, percent_vaccinated, use =\n  \"complete.obs\")`.\nℹ In group 28: `year = 2023` and `month = 4`.\nCaused by error in `cor()`:\n! no complete element pairs"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "href": "labs/comments/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?\nProbably not that much. Different Face mask policies are implemented at different times in the pandemic. For example, by 2021, almost all states have some requirements. Comparing the average for new cases in states with no policy to states with full requirements, is comparing the state of world in early 2020 to the state of the world in late 2020 to mid 2021. But lots of things differ between these periods. Other policies are also going into effect, new variants are emerging.\nIn short, those simple conditional means across the full data don’t really provide an apples to apples comparison.\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,n,fill=face_masks))+\n  geom_bar(stat=\"identity\")\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    new_cases = sum(new_cases)\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,new_cases))+\n  geom_smooth()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 322 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/comments/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\nSo when we compare states in September 2020, we see that rates of new cases tend to be much higher in states with only recommend face mask policies compared to states with at least some requirements.\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  filter(year_month == \"2020-09\")%&gt;%\n  group_by(face_masks) %&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc,na.rm=T)\n  )\n\n# A tibble: 4 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Recommended                    43.9\n2 Some requirements              13.5\n3 Required shared places         13.0\n4 Required all times             10.1\n\n\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(year_month,face_masks) %&gt;%\n  summarise(\n    n = length(unique(state)),\n    new_cases_pc = round(mean(new_cases_pc,na.rm=T)),\n    total_cases = round(mean(confirmed,na.rm=T))\n  ) -&gt; cases_by_month_and_policy\n\n`summarise()` has grouped output by 'year_month'. You can override using the\n`.groups` argument.\n\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\nDT::datatable(cases_by_month_and_policy,\n              filter = \"top\")\n\n\n\n\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\ncases_by_month_and_policy %&gt;%\n  ggplot(aes(\n    x= year_month,\n    y = new_cases_pc,\n    col=face_masks))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#footnotes",
    "href": "labs/comments/01-lab-comments.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html",
    "href": "labs/comments/10-lab-comments.html",
    "title": "Lab 10 The final lab!!",
    "section": "",
    "text": "In our final lab, you will apply concepts and skills from this course to explore data from the 2020 American National Election Study. Specifically you will\n\nIdentify an outcome of interest (5-10 minutes)\nIdentify key predictors and covariates (5-10 minutes)\nRecode your data (20 minutes)\nDescribe your data (20 minutes)\nDescribe your question, expectations, and models (10 minutes)\nEstimate, present, and interpret your models (20 minutes, Graded Question)\n\nIdeally, each group will pursue a question that interests them. I will also complete these tasks live, so, if you’re not feeling confident, you can follow along with me and submit the code I demo in class as your lab for a grade of 85.."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#summary-statistics",
    "href": "labs/comments/10-lab-comments.html#summary-statistics",
    "title": "Lab 10 The final lab!!",
    "section": "4.1 Summary statistics:",
    "text": "4.1 Summary statistics:\nProducing a table of summary statistics requires a little foresight.\nEssentially you want to make a data frame where each row is a (numeric) variable, and each column is a statistic (minimum, 25th percentile, median, mean, 75th percentile, max, Number of missing).\nTo do this, I would:\n\ncreate a object called the_vars which contains the names (in quotation marks) of the variables you want to summarize.\nSelect these variables from your data set. using df%&gt;%select(all_of(the_vars))\nUse %&gt;%pivot_longer() specifying cols=select(all_of(the_vars)), and names_to equals \"Variable\" and values_to = \"value\" to transform this wide dataset into a long dataset\nThen use %&gt;%group_by(Variable)%&gt;% and summarise() to calculate the statistics for each variable of interest (e.g. %&gt;%summarise(Mean = mean(value, na.rm=T))))\nSave the output to an object called something like sum_df\nIn a new chunk use knitr::kable(sum_df) %&gt;% kableExtra::kable_styling() to format your table. Set echo=F in the code chunk head\n\n\n# Summarise data\n\nthe_vars &lt;- c(\"ft_police\",\n              \"is_white\",\n              \"been_arrested\",\n              \"income\")\n\ndf%&gt;%\n  select(all_of(the_vars)) %&gt;%\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\",\n    values_to = \"value\"\n  )%&gt;%group_by(Variable)%&gt;%\n  summarise(\n    Min = min(value, na.rm=T),\n    p25 = quantile(value, probs = .25,na.rm=T),\n    Median = median(value, na.rm=T),\n    Mean = mean(value, na.rm=T),\n    p75 = quantile(value, probs = .75,na.rm=T),\n    Max = max(value, na.rm=T)\n\n            ) %&gt;% \n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  ) %&gt;% \n  arrange(Variable) -&gt; sum_df\n\nError in UseMethod(\"select\"): no applicable method for 'select' applied to an object of class \"function\"\n\n\n# Display results\nknitr::kable(sum_df, digits = 2) %&gt;% kableExtra::kable_styling()\nError in eval(expr, envir, enclos): object 'sum_df' not found"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#descriptive-figures",
    "href": "labs/comments/10-lab-comments.html#descriptive-figures",
    "title": "Lab 10 The final lab!!",
    "section": "4.2 Descriptive Figures",
    "text": "4.2 Descriptive Figures\nTo create a figure, you’ll need to specificy the following\n\ndata (e.g. df %&gt;%)\naesthetic mappings, ggplot(aes(x = predictor, y = outcome))\ngeometries\n\nUnivariate: geom_density(), geom_boxplot() geom_histogram()\nBivariate: geom_point() (for a scatterplot), geom_line() for a trend.\n\n\nOnce you have a minimal working example, play around with other grammars of graphics:\n\nlabs() for custom labels\ntheme_XXX for custom themes\nfacet_wrap(~group) to produce the same plot facetted by some categorical grouping variable\n\nWhen you’re happy with your figure, save it as object in R (e.g. fig1 &lt;- df %&gt;% ggplot(aes(predictor, outcome))+geom_point()). Put that object in its own chunk to display it in your document.\nDon’t let the perfect be the enemy of the good.\n\n# Descriptive figures"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#descrptive-interpretation",
    "href": "labs/comments/10-lab-comments.html#descrptive-interpretation",
    "title": "Lab 10 The final lab!!",
    "section": "4.3 Descrptive Interpretation:",
    "text": "4.3 Descrptive Interpretation:\nPlease provide an overview of the data (source, number of observations, unit of analysis).\nDescribe a typical observation, making reference to the statistics in your summary table.\nOffer a substantive interpretation of your descriptive figure(s). What do they tell us about the distribution of a key variable, or the relationship between two variables."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#fit-the-models",
    "href": "labs/comments/10-lab-comments.html#fit-the-models",
    "title": "Lab 10 The final lab!!",
    "section": "6.1 Fit the models",
    "text": "6.1 Fit the models\n\n# Model 1: Bivariate Model\n\n# Model 2: Multiple Regression"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#display-the-models-in-a-regression-table",
    "href": "labs/comments/10-lab-comments.html#display-the-models-in-a-regression-table",
    "title": "Lab 10 The final lab!!",
    "section": "6.2 Display the models in a regression table",
    "text": "6.2 Display the models in a regression table\n\n# Regression table"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#interpet-your-models",
    "href": "labs/comments/10-lab-comments.html#interpet-your-models",
    "title": "Lab 10 The final lab!!",
    "section": "6.3 Interpet your models",
    "text": "6.3 Interpet your models\nPlease write a 1 paragraph summary interpreting your results in terms of both their statistical and substantive significance. Assume your audience is smart, but has never taken POLS 1600. Explain to them what a regression model is, what a standard error, p-value, and/or confidence interval is. How should they interepret the substantive findings of your model. How should they assess the statistical uncertainty around these results?\nPerhaps you might reade create a plot of predicted values from a model to help facilitate the substantive interpretation of your results. If so, here’s a code chunk for you:\n\n# Additional code chunk to facilitate interpretation of models"
  },
  {
    "objectID": "labs/comments/04-lab-comments.html",
    "href": "labs/comments/04-lab-comments.html",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "set.seed(20231005)\n  the.questions&lt;-1:12\n  graded&lt;-sample(the.questions,1)\n  graded\n\n[1] 3\nQuestion 3 is the graded question for this assignment"
  },
  {
    "objectID": "labs/comments/04-lab-comments.html#goals",
    "href": "labs/comments/04-lab-comments.html#goals",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "0.1 Goals",
    "text": "0.1 Goals\nConceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html",
    "href": "labs/comments/05-lab-comments.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3032022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\n\nGrading Questin 9: Basically, if you made any changes to fig_m5 100 percent. If you simply recreated fig_m5 80 percent. If you didn’t create figure fig_m5 0 percent. Sorry! But don’t fret, remember your 3 lowest lab scores are dropped from your lab grade.\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nError in `tibble::tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 5: Column `Group 7`.\nℹ Only values of size one are recycled.\n\n\nError in eval(expr, envir, enclos): object 'groups_df' not found"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#load-covid-19-data",
    "href": "labs/comments/05-lab-comments.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid data\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  level = 2,\n  verbose = F\n)"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#load-election-data",
    "href": "labs/comments/05-lab-comments.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\n# This joyously stopped working last night...\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\nError in get_dataframe_by_name(\"1976-2020-president.tab\", \"doi:10.7910/DVN/42MVDX\"): could not find function \"get_dataframe_by_name\"\n\n# Backup\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#recode-the-covid-19-data",
    "href": "labs/comments/05-lab-comments.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide, starting here and ending here\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\nError in covid %&gt;% filter(!administrative_area_level_2 %in% territories) %&gt;% : could not find function \"%&gt;%\"\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\nError in covid_us %&gt;% dplyr::group_by(state) %&gt;% mutate(new_cases = confirmed - : could not find function \"%&gt;%\"\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(face_masks = case_when(facial_coverings == : could not find function \"%&gt;%\"\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(year = year(date), month = month(date), year_month = paste(year, : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/comments/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, change the new_deaths_pc_7da to new_deaths_pc_14da and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7da = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14da = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\nError in covid_us %&gt;% dplyr::group_by(state) %&gt;% mutate(new_deaths = deaths - : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/comments/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df\n\nError in pres_df %&gt;% mutate(year_election = year, state = str_to_title(state), : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-all-the-observations",
    "href": "labs/comments/05-lab-comments.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7da)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\nError in covid_us %&gt;% group_by(winner) %&gt;% summarise(new_deaths = mean(new_deaths, : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/comments/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\nError in covid_us %&gt;% filter(date &lt; \"2021-04-19\") %&gt;% group_by(winner) %&gt;% : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/comments/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\nError in covid_us %&gt;% filter(date &gt; \"2021-04-19\") %&gt;% group_by(winner) %&gt;% : could not find function \"%&gt;%\"\n\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that states which Biden won had about 27 new deaths compared to 23.8 new deaths in states which Trump one.\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that rates tend to be higher in Red States (0.415 deaths per 100k) than Blue States (0.349 deaths per 100k). This difference reflects the fact that Biden tended to win more populous states than trump, so simply looking at the average number of new deaths is bit misleading. Comparing 7-day averages per 100,000 residents adjusts for differences in population between Red and Blue States.\nWhen we limit our analysis, to just observations before April 19, 2021, the difference in the 7-day average rate of new Covid-19 deaths per 100,000 residents is relatively small (0.02 more deaths per 100,000 residents in Red States)\nWhen we look at observations after the vaccine became widely available the difference is more than 6 times as big (0.125 more deaths per 100,000 residents in Red States)"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#recreating-the-nyt-figures",
    "href": "labs/comments/05-lab-comments.html#recreating-the-nyt-figures",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "9.1 Recreating the NYT Figures",
    "text": "9.1 Recreating the NYT Figures\nThis turned out to be more annoying than I thought, but if you really wanted to recreate the figures from the articles, this was as close as I could get:\n\n# Vector containing labeled states\nthe_labs &lt;- c(\"WV\",\"WY\",\"MS\",\"KY\",\"TX\",\"FL\",\"GA\",\"IL\",\"NY\",\"VT\",\"MD\",\"CA\")\n\ncovid_us %&gt;%\n  # Only include labels for states in the the_labs\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  # Subset data\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics, flipping vax to % unvaxxed\n  ggplot(aes(x = rep_voteshare,\n             y = (100-percent_vaccinated),\n             label = nyt_labs\n             ))+\n  # points coloreded by vote share\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # color gradient\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # add simple regression line\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  # add labels\n  geom_text_repel()+\n  # futz with limits\n  ylim(15,60)+\n  # add grid lines by hand\n  geom_hline(yintercept = 60, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 40, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 20, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=20, yend = 60), col = \"lightgrey\", size = .25)+\n  # Add arrows\n  geom_segment(aes(x = 34, xend = 32, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add biden text\n  annotate(\"text\",x = 34.5, y=18.5 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=17.1 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.7, y=17.1 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  # Add trump arrow\n  geom_segment(aes(x = 70, xend = 72, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add trump text\n  annotate(\"text\",x = 69.5, y=18.5 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.1, y=17.1 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=17.1 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  # Label y-axis\n  annotate(\"text\",x = 30, y=20 ,label = \"20%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=40 ,label = \"40%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=60 ,label = \"60% of residents not fully vaccinated\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  # get rid of default theme\n  theme_void()\n\nError in covid_us %&gt;% mutate(nyt_labs = ifelse(state_po %in% the_labs, : could not find function \"%&gt;%\"\n\n\n\n# Same as above, but now modeling deaths with rep vote share\n\ncovid_us %&gt;%\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14da,\n             label = nyt_labs\n             ))+\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  geom_text_repel()+\n  # theme_void()+\n  ylim(-.2,2.2)+\n  geom_hline(yintercept = 0, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = .5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1.5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 2, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=0, yend = 2), col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 34, xend = 32, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.1 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.2 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.82, y=-.2 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  geom_segment(aes(x = 70, xend = 72, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.1 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.09, y=-.2 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.2 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  annotate(\"text\",x = 30, y=0.5 ,label = \"0.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1 ,label = \"1\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1.5 ,label = \"1.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=2 ,label = \"2 deaths per 100,000 residents\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  theme_void()\n\nError in covid_us %&gt;% mutate(nyt_labs = ifelse(state_po %in% the_labs, : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#footnotes",
    "href": "labs/comments/05-lab-comments.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html",
    "href": "labs/comments/02-lab-comments.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to reproduce this figure:\n\n\n\n\n\n\n\n\n\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides (week 1 here, week 2 here) and last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nLearn how to take a complex problem (“How do I produce a figure that shows the average rate of new cases per month for states with a particular type of face mask policy”) which you may think you have no idea how to do and break this challenge down into concrete tasks which you do know how do (“Well first, I’ll need to load some packages to work with and visualize data. Then, I’ll need to get the data. And then…”)\n\nThink and write programmatically\n\nIn this .Rmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll produce a collective outline of what we need to do before getting to the actual coding\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nAgain you shouldn’t have to write much code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\nBut shouldn’t we be writing our own code?! Yes. Sure. Eventually.\nThe tutorials give you practice writing single commands, and by the end of the class you should be able write this code like this for to accomplish similar tasks\nBut even then, you will not be writing code from memory. I still have to Google functions, and often search my old code to find a clever solution to task.\nEveryone starts learning to code by copying and pasting other people’s code.\nThis will help minimize (but not eliminate) syntactic errors, while over time we get better writing code from scratch and fixing errors as the develop.\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nFor fun, let’s say that the first group that successfully recreates this figure gets to choose one of the following non-monetary prizes:\n\nI’ll tell them a joke\nOne AMA I will answer truthfully\nOne question to be asked on the weekly class survey\n0.00001% extra credit added to their final grade for the course.\n\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "href": "labs/comments/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.1 Create an object listing all the packages I will use today",
    "text": "2.1 Create an object listing all the packages I will use today\nThis code creates a object called the_packages which contains a vector of character strings corresponding to the names of the packages I want to use today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "href": "labs/comments/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.2 Define a function to install and load packages",
    "text": "2.2 Define a function to install and load packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "href": "labs/comments/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.3 Use the ipak function to load the necessary packages",
    "text": "2.3 Use the ipak function to load the necessary packages\nNow I run the ipak() giving it the object the_packages as an input. It sorts through the packages, checks to see if they’re installed, if not installs them, and then loads all of the packages so I can use them.\n\nipak(the_packages)\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#filter-out-u.s.-territories",
    "href": "labs/comments/02-lab-comments.html#filter-out-u.s.-territories",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.1 Filter out U.S. Territories",
    "text": "5.1 Filter out U.S. Territories\nFor simplicity, (and practice filtering observations), I’ve asked us to remove observations from U.S. territories.\nThe code below\n\nCreates an object called us_territories.\nUse this object to filter out observations that are US territories\nCreates a new data frame that is just observations from the 50 U.S. states. and D.C.\nChecks that this recoding seems to have worked\n\n\n# U.S. Territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out U.S. Territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\nError in covid %&gt;% filter(!administrative_area_level_2 %in% territories): could not find function \"%&gt;%\"\n\n# Check to make sure covid_us contains only 50 states and D.C.\ndim(covid)\n\n[1] 80156    47\n\ndim(covid_us)\n\nError in eval(expr, envir, enclos): object 'covid_us' not found\n\nlength(unique(covid$administrative_area_level_2)) \n\n[1] 56\n\nlength(unique(covid_us$administrative_area_level_2)) == 51\n\nError in eval(expr, envir, enclos): object 'covid_us' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-a-state-variable",
    "href": "labs/comments/02-lab-comments.html#create-a-state-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.2 Create a state variable",
    "text": "5.2 Create a state variable\nThis is purely for convenience, because typing administrative_area_level_2 is annoying. The code copies the values of this variable into a new variable called state using the mutate() function.\nMutate returns the original data frame plus the new column. We have to save this output for our our changes to persist (i.e. we have to assign the output of mutate() back into covid_us)\nIn last week’s lab, I just piped the output to the next command, did some more recoding with mutate, and then finally saved the output back into covid_us. In this lab, I’ll save the output after each step.\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(state = administrative_area_level_2, ): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "href": "labs/comments/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.3 Group by the state variable to calculate new Covid-19 cases",
    "text": "5.3 Group by the state variable to calculate new Covid-19 cases\nNow I use this shorter variable state to calculate the number of new cases (new_cases) in a given state on a given date, and rescale this variable so that it’s expressed in terms of new cases per 100,000 residents.\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000\n    ) -&gt; covid_us\n\nError in covid_us %&gt;% dplyr::group_by(state) %&gt;% mutate(new_cases = confirmed - : could not find function \"%&gt;%\"\n\n\nThe slides from Tuesday, helped demonstrate what this code was doing, and why we wanted to group by state.\nHere’s an example for a subset of the data from April 1, 2020 to April 7, 2020\nWe see that the lag() function simply moves the observation of a variable “up” one row so that we can take the difference between the total number of cases in a state on one date and the total number of cases on the date before, to calculate the number of new cases\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  group_by(state) %&gt;%\n  select(state, date, confirmed)%&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\nError in covid_us %&gt;% filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;% : could not find function \"%&gt;%\"\n\n\nIf we hadn’t grouped by state, then when we lagged the confirmed variable, R thinks the number of confirmed cases in California before April 1, 2020, is 986 which is actually the number of cases in Minnesota on April 7, 2020\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  ungroup() %&gt;%\n  select(state, date, confirmed)%&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\nError in covid_us %&gt;% filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;% : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#recode-the-facial_coverings-variable",
    "href": "labs/comments/02-lab-comments.html#recode-the-facial_coverings-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.4 Recode the facial_coverings variable",
    "text": "5.4 Recode the facial_coverings variable\nNext we use the case_when() function inside the mutate() function to create a variable called face_masks based on the values of the facial_coverings variable in the data.\ncase_when() when uses R’s ability to make logical comparisons. When the variable facial_coverings equals 0, R will input the character string \"No policy\" into the face_masks variable.\nWhen the absolute value of facial_coverings equals 1 (i.e.facial_coverings equals 1 or -1 ), R will input the character string \"Recommended\" into the face_masks variable. And so on.\nWe use the abs() function to take the absolute value of the facial_coverings variable because codebook for these data implied:\n\nIn short: positive integers identify policies applied to the entire administrative area. Negative integers are used to identify policies that represent a best guess of the policy in force, but may not represent the real status of the given area. The negative sign is used solely to distinguish the two cases, it should not be treated as a real negative value.\n\nWe know from last weeks lab, that negative values in the U.S. typically seem to be cases where a city had a more stringent policy than the state (e.g. Chicago adopts more stringent face mask policies than Illinois).\nFinally, we put a %&gt;% after the output of case_when() and pass it’s output to the factor() function.\nThe . acts as sort of placeholder, factor() expects some input here (like a variable from a data frame), . tells R to use the output of case_when().\nThe levels = then transforms the character data produced by case_when() into a factor with an implicit ordering of levels (i.e. “No policy” &lt; “Recommended”&lt; “Some requirements” &lt;“Required shared places” &lt;“Required all times”) which turns out to be useful trick for organizing how data are plotted and visualized.\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(face_masks = case_when(facial_coverings == : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "href": "labs/comments/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.5 Create a variable capturing the year and month of the observation",
    "text": "5.5 Create a variable capturing the year and month of the observation\nFinally we create some variables that extract components of an observation’s date:\n\nyear = year(date) returns just the year from a variable of class Date\nmonth = month(date) returns just the month from a variable of class Date\nyear_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\") pastes these to variables together.\nstr_pad(month, width = 2, pad=0) adds a leading 0 to any month with only 1 digit, to ensure that all the months have 2 characters.\n\nThe code from your lab also calculates the percent of a states population that is vaccinated, which isn’t strictly needed for today.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(year = year(date), month = month(date), year_month = paste(year, : could not find function \"%&gt;%\"\n\n\nCreating separte year and month variables aren’t strictly necessary,\nWe could have written something like:\n\ncovid_us %&gt;%\n  mutate(\n    year_month = paste(year(date), str_pad(month(date), width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nBut that year_month line was already feeling kind of clunky, and maybe we’ll want the year and month variables later."
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#adding-meaningful-labels-and-title",
    "href": "labs/comments/02-lab-comments.html#adding-meaningful-labels-and-title",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.1 Adding meaningful labels and title",
    "text": "8.1 Adding meaningful labels and title\nBecause we saved the output of our ggplot to an object called fig1 we can add additional commands to this object using the + without having to rewrite all the code.\nFirst let’s add better labels to the graph.\n\nNote that even though we flipped the coordinates, the aes aesthetic mappings stay the same. So to change the label of the figures y-axis to “Date” we change the label of x = \"Date\"\nggplot automatically generates a legend for aesthetic mappings like color We can add a line break using the the special character \\n in our code\n\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\n Policy\"\n  )\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nNote the code above didn’t update fig1\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nWe have to save the output (if we like it) for our changes to persist.\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\nPolicy\"\n  ) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#changing-the-theme-of-the-plot",
    "href": "labs/comments/02-lab-comments.html#changing-the-theme-of-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.2 Changing the theme of the plot",
    "text": "8.2 Changing the theme of the plot\nHere’s an example of some different themes\n\n# Black and white\nfig1 +\n  theme_bw()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n# Minimal\nfig1 +\n  theme_minimal()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n# Classic\nfig1 +\n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nThis is pretty personal, and depends of the figure itself. I like a white background and some guide lines:\n\nfig1 +\n  theme_bw() -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "href": "labs/comments/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.3 Make the size of the dots reflect the number of states with this policy",
    "text": "8.3 Make the size of the dots reflect the number of states with this policy\nIn the cases_by_month_and_policy we have a column called n which is the number of states which had a given policy in a given month.\nWe can add an aesthetic to our plot that varies the size of the points by the number of states.\n\nfig1 +\n  aes(size = n) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nWe call this type of plot a bubble plot{target=“_blank”\nI have mixed feelings about multiple legends. We can remove the legend for size using the scale_size() function. I had to Google how to do this for the millionth time.\n\nfig1 +\n  scale_size(guide = \"none\") -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#facet-the-plot",
    "href": "labs/comments/02-lab-comments.html#facet-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.4 Facet the plot",
    "text": "8.4 Facet the plot\nVarying the size of the dots by the number of states conveys more information. But makes the chart a little harder to read. Dots overlap.\nThe facet_wrap command will produce separate bubble plots for each level of the “facetting” variable, in this case `face_masks\n\nfig1 +\n  facet_wrap(~face_masks) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nNow I think also want a second legend for the number of states\n\nfig1 +\n  scale_size(guide = \"legend\")+\n  labs(\n    size = \"# of States\\nwith Policy\"\n  )-&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nThis seems pretty good if our goal was to show in general terms\n\nIt shows the average number new cases for states with a given face mask policy over time.\nIt shows how the mix of types of face mask policies states have adopted has changed over time\n\nIf our goal was to make comparisons across face mask policies over a given time period, I’m might still prefer something closer to our original graph."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html",
    "href": "labs/comments/08-lab-comments.html",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Today we’ll work through exercise 6.6.2 A Probability Model for Betting Market Election Prediction from QSS (pp. 309-310). We will use the daily data from the Intrade betting market to derive probabilities from degenerate gamblers about the likelihood that Obama would win the 2008 presidential election. We will use these probabilities to simulate possible elections and the summarize outcomes of these simulations graphically.\nPlan to spend the following amount of time\n\nGet set up to work (5 minutes)\nCalculate Obama’s expected electoral vote share on November 3, 2008 (the day before the election) (10 minutes)\nSimulate a 1000 elections for November 3, 2008 using the betting market prices a measure of the probability that Obama wins or loses a state (10 minutes)\nDisplay the results of your simulation with a histogram. (5 minutes)\nTransform these probabilities to reduce the likelihood that Obama wins states like Alabama and increase the likelihood that Obama wins states like California (5 minutes)\nSimulate another 1000 elections using these transformed probabilities. Compare the results to your initial simulation. (10)\nCalculate Obama’s expected total number of votes for each day in the 120 days before the 2008 election (15 minutes)\nSimulate 100 elections for each of the 120 days before the election, plot the results of your simulation. (20 minutes)\nTake the class survey for Donuts.\n\n\n\nConceptually, the main goal of this lab is to give you some practice working with probabilities in an application to the real world (predicting elections)\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)\n\n\n\n\n\n\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#goals",
    "href": "labs/comments/08-lab-comments.html#goals",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Conceptually, the main goal of this lab is to give you some practice working with probabilities in an application to the real world (predicting elections)\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#workflow",
    "href": "labs/comments/08-lab-comments.html#workflow",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "As with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#load-packages",
    "href": "labs/comments/08-lab-comments.html#load-packages",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst we’ll load the pacakges we need for today:\n\nlibrary(tidyverse)\nlibrary(qss)\n\nNow we’ll load two data sets from the qss package: pres08 and intrade08\n\npres08 contains the 2008 US presidential election outcomes by state.\nintrade08 contains from Intrade, an online prediction market, in days leading up to the 2008 United States Presidential Election.\n\n\n# Results from 2008 election\ndata(\"pres08\")\n\n# Daily betting market data\ndata(\"intrade08\")"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#provide-a-high-level-overview-each-data-set",
    "href": "labs/comments/08-lab-comments.html#provide-a-high-level-overview-each-data-set",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.2 Provide a high level overview each data set",
    "text": "1.2 Provide a high level overview each data set\nIn the code chunk below, please write some code to provide a high level overview of each data set.\nWe will primarily work with the EV variable from pres08 which contains the electoral votes for each state, and the PriceD variable from intrade08 from which we will construct a probability that Obama wins that state.\n\n# HLO\n# pres08\nglimpse(pres08)\n\nRows: 51\nColumns: 5\n$ state.name &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"…\n$ state      &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",…\n$ Obama      &lt;int&gt; 39, 38, 45, 39, 61, 54, 61, 92, 62, 51, 47, 72, 36, 62, 50,…\n$ McCain     &lt;int&gt; 60, 59, 54, 59, 37, 45, 38, 7, 37, 48, 52, 27, 62, 37, 49, …\n$ EV         &lt;int&gt; 9, 3, 10, 6, 55, 9, 7, 3, 3, 27, 15, 4, 4, 21, 11, 7, 6, 8,…\n\nsummary(pres08)\n\n  state.name           state               Obama           McCain     \n Length:51          Length:51          Min.   :33.00   Min.   : 7.00  \n Class :character   Class :character   1st Qu.:43.00   1st Qu.:40.00  \n Mode  :character   Mode  :character   Median :51.00   Median :47.00  \n                                       Mean   :51.37   Mean   :47.06  \n                                       3rd Qu.:57.50   3rd Qu.:56.00  \n                                       Max.   :92.00   Max.   :66.00  \n       EV       \n Min.   : 3.00  \n 1st Qu.: 4.50  \n Median : 8.00  \n Mean   :10.55  \n 3rd Qu.:11.50  \n Max.   :55.00  \n\n# intrade08\nglimpse(intrade08)\n\nRows: 36,891\nColumns: 10\n$ .row      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ day       &lt;date&gt; 2006-11-12, 2006-11-12, 2006-11-12, 2006-11-12, 2006-11-12,…\n$ statename &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"C…\n$ MarketD   &lt;chr&gt; \"Democratic Party Nominee to win Alabama's Electoral College…\n$ PriceD    &lt;dbl&gt; 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 33.3, 40.0, …\n$ VolumeD   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MarketR   &lt;chr&gt; \"Republican Party Nominee to win Alabama's Electoral College…\n$ PriceR    &lt;dbl&gt; 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 33.3, 40.0, …\n$ VolumeR   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state     &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"GA\", …\n\nsummary(intrade08)\n\n      .row            day              statename           MarketD         \n Min.   :    1   Min.   :2006-11-12   Length:36891       Length:36891      \n 1st Qu.: 9224   1st Qu.:2007-05-12   Class :character   Class :character  \n Median :18446   Median :2007-11-09   Mode  :character   Mode  :character  \n Mean   :18446   Mean   :2007-11-09                                        \n 3rd Qu.:27668   3rd Qu.:2008-05-08                                        \n Max.   :36891   Max.   :2008-11-19                                        \n     PriceD         VolumeD          MarketR              PriceR     \n Min.   : 0.00   Min.   :    0.0   Length:36891       Min.   : 0.00  \n 1st Qu.:12.50   1st Qu.:    0.0   Class :character   1st Qu.:15.20  \n Median :43.00   Median :    0.0   Mode  :character   Median :51.00  \n Mean   :46.78   Mean   :    6.4                      Mean   :51.27  \n 3rd Qu.:82.50   3rd Qu.:    0.0                      3rd Qu.:85.00  \n Max.   :99.90   Max.   :17353.0                      Max.   :99.50  \n    VolumeR             state          \n Min.   :    0.000   Length:36891      \n 1st Qu.:    0.000   Class :character  \n Median :    0.000   Mode  :character  \n Mean   :    5.583                     \n 3rd Qu.:    0.000                     \n Max.   :14885.000                     \n\nsum(table(intrade08$state)&gt;723)\n\n[1] 3\n\nwhich(table(intrade08$state)&gt;723)\n\nIN MO NC \n16 25 28 \n\n\nBriefly describe each data set\n\nThe pres08 data set contains the 51 observations corresponding the 50 states plus the District of Columbia. Two variables describe the state name (state.name) and postal abbreviation (state). The Obama and McCain variables correspond to the vote share of each candidate in the each state in 2008 and the EV variable corresponds the Electoral College votes for each state.\nThe intrade08 data contain 36,891 observations. Each observation corresponds to a “state-day” and in total there are about just under two years of observations (Most states have 723 observations, Indiana, Missouri, and North Carolina have more than 723 observations, for some reason). That data contain information on the price of “bets” that the Democratic or Republican candidates will win that state in the election (PriceD and PriceR), as well was descriptions of the volume of trading (VolumeD and VolumeR) and the specific market."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#re-arrange-the-intrade",
    "href": "labs/comments/08-lab-comments.html#re-arrange-the-intrade",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.3 Re-Arrange the Intrade",
    "text": "1.3 Re-Arrange the Intrade\nIf you look closely at the data, you’d see that both are arranged alphabetically by state name, but in the pres08 data, the District of Columbia is named “D.C.”, while in the intrade08 data it is called “District of Columbia”.\n\npres08$state.name[8]\n\n[1] \"D.C.\"\n\nintrade08$statename[9]\n\n[1] \"District of Columbia\"\n\n\nAs a result, D.C. comes before Delaware in pres08 but after Delaware in intrade08. It will be useful below, for the states to be in the same order in both data sets.\n\n# DC and DE are reversed\npres08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\"\n\nintrade08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" \"DC\"\n\n# Same abbrevs\nsum(pres08$state %in% intrade08$state)\n\n[1] 51\n\n# Different namings of DC\nsum(pres08$state.name %in% intrade08$statename)\n\n[1] 50\n\n\nPlease use the arrange() function to re-arrange both data sets using the state variable which is the postal abbreviation code for each state and is the same in both data sets.\nFor the pres08 data arrange() by state and intrade08 by day and then by state\n\nRemember to save the output of arrange() back into each respective data frame\n\n\n# arrange pres08\npres08 &lt;- pres08 %&gt;%\n  arrange(state)\n\n# arrange intrade08\nintrade08 &lt;- intrade08 %&gt;%\n  arrange(day,state)\n\nIf your code was correct, DC should now come before DE in intrade08\n\n# pres08$state[1:9]\n# intrade08$state[1:9]\n\nAnd the following code should return TRUE\n\n# all.equal(pres08$state[1:51], intrade08$state[intrade08$day==\"2008-11-03\"])"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "href": "labs/comments/08-lab-comments.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.4 Convert the PriceD variable in intrade08 to a probability",
    "text": "1.4 Convert the PriceD variable in intrade08 to a probability\nThe intrade08 data contain a variable called PriceD which we will treat as the probability that Obama will win the presidential election.\nRecall that probabilities must be between 0 and 1. Please create a variable called prob_obama_win in intrade08 by dividing PriceD by 100.\n\n# Create variable prob_obama_win from PriceD in intrade08\nintrade08 %&gt;%\n  mutate(\n    prob_obama_win = PriceD/100\n  ) -&gt; intrade08"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#create-a-subset-of-intrade08-called-df_nov3",
    "href": "labs/comments/08-lab-comments.html#create-a-subset-of-intrade08-called-df_nov3",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.5 Create a subset of intrade08, called df_nov3",
    "text": "1.5 Create a subset of intrade08, called df_nov3\nNext we’ll create a subset of the data, called df_nov3 that contains just the data from the day before the election (that is Monday, November 3, 2008)\n\nHint: try filter()\n\n\ndf_nov3 &lt;- intrade08 %&gt;%\n  filter(day == \"2008-11-03\")"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#footnotes",
    "href": "labs/comments/08-lab-comments.html#footnotes",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we’re assuming these probabilities are independent. Actual forecasting models like 538 or the NYT generally assume the probabilities of winning some states correlate with the probability of winning other neighboring states.↩︎"
  },
  {
    "objectID": "labs/comments/07-lab-comments.html",
    "href": "labs/comments/07-lab-comments.html",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "",
    "text": "Today, we’ll explore Russians’ support the war in Ukraine using a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project.\nThe survey was conducted by phone using a random sample of mobile phone numbers to produce a sample of respondents representative of the population in terms of age, sex, and geography. It was in the field from February 28 to March 2.\nWe will look at how support for the war varies with the demographic predictors age, sex and education. We will see how multiple regression can be used to describe more complex relationships. From our baseline model, we will ask:\n\nDoes the relationship between age and support for the war vary among male and female respondents (Interaction model)\nDoes the relationship between age and support vary at different levels of age (Polynomial regression model)\nDoes the relationship between age and support vary at different levels of age and does the nature of this variation differ among male and female respondents (Polynomial regression model with an interaction term)\n\nTo accomplish this, we will do the following:\n\nGet set up to work and describe our data (10 minutes)\nGet practice interpreting long chunks of codes with lots of %&gt;%s (10 minutes)\nEstimate four models of increasing complexity (15 minutes)\nPresent these models in a regression table and interpret the results (10 minutes)\nEvaluate the relative performance of these models in terms of their variance explained \\(R^2\\)’s (15 minutes)\nProduce predicted values to help us interpret and compare our baseline model to a model where the “effect” of an increase in age changes with age (10 minutes)\nProduce predicted values to help us interpret and compare models where the “effect” of age is allowed to vary with respondent’s sex (10 minutes)\n\nFinally, if there’s time, we will:\n\nExplore additional questions of our choosing in the data\n\nOne of questions 1-7 will be randomly selected as the graded question for the lab.\n\nset.seed(3172022)\ngraded_question &lt;- sample(1:7,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 5 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file produced by knitting the lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nYou can find your assigned groups in previous labs"
  },
  {
    "objectID": "labs/comments/07-lab-comments.html#load-packages",
    "href": "labs/comments/07-lab-comments.html#load-packages",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\"\n  , # Comments only\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "labs/comments/07-lab-comments.html#load-the-data",
    "href": "labs/comments/07-lab-comments.html#load-the-data",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the recoded data for the lab.\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))"
  },
  {
    "objectID": "labs/comments/07-lab-comments.html#describe-the-data",
    "href": "labs/comments/07-lab-comments.html#describe-the-data",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nAs always, it’s important to get a high level overview of data when we first load it into R.\nBelow we take a look at the first few values of all the data. You’ll see that df_drww includes both the Russian data and recoded revisions of the data (which are typically appended with _n for numeric data or _f for factor data).\n\nglimpse(df_drww)\n\nError in glimpse(df_drww): could not find function \"glimpse\"\n\n\nIn the first part of this lab, we’ll work with the following variables\n\nsupport_war01 “Please tell me, do you support or do not support Russia’s military actions on the territory of Ukraine?” (1=yes, 0 = no)\nage “How old are you?”\nsex “Gender of respondent” (As assessed by the interviewer)\neducation_n “What is your highest level of education (confirmed by a diploma, certificate)?” (1 = Primary school, 2 = “High School”, 3 = “Vocational School” 4 = “College”, 5 = Graduate Degree)3\n\nIn the code chunk below, I create a data frame of summary statistics:\n\nthe_vars &lt;- c(\"support_war01\",\"age\", \"is_female\", \"education_n\")\n\ndf_drww %&gt;%\n  mutate(\n    is_female = ifelse(sex == \"Female\",1, 0)\n  ) %&gt;%\n  select(all_of(the_vars)) %&gt;%\n  pivot_longer(\n    cols = all_of(the_vars ),\n    names_to = \"Variable\",\n    values_to = \"Value\"\n  ) %&gt;%\n  group_by(Variable) %&gt;%\n  summarise(\n    `N obs` = n(),\n    Missing = sum(is.na(Value)),\n    Min = min(Value, na.rm = T),\n    `25th perc` = quantile(Value, .25, na.rm=T),\n    Mean = mean(Value, na.rm=T),\n    Median = median(Value, na.rm = T),\n    `75th perc` = quantile(Value, .75, na.rm=T),\n    Max = max(Value, na.rm = T)\n  ) %&gt;%\n  mutate(\n    Variable = case_when(\n      Variable == \"age\" ~ \"Age\",\n      Variable == \"education_n\" ~ \"Education\",\n      Variable == \"is_female\" ~ \"Female\",\n      Variable == \"support_war01\" ~ \"Support for War\",\n      \n    ),\n    Variable = factor(Variable, levels = c(\"Support for War\",\"Age\",\"Female\",\"Education\"))\n    ) %&gt;%\n  arrange(Variable) -&gt;  summary_table\n\nsummary_table\n\n# A tibble: 4 × 9\n  Variable     `N obs` Missing   Min `25th perc`   Mean Median `75th perc`   Max\n  &lt;fct&gt;          &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Support for…    1807     334     0           0  0.720      1           1     1\n2 Age             1807       0    18          34 46.6       45          60    99\n3 Female          1807       0     0           0  0.470      0           1     1\n4 Education       1807      13     1           3  3.17       3           4     5\n\n\nWhich we can then format into a table of summary statistics:\n\nkable(summary_table,\n      digits = 2) %&gt;% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  kableExtra::pack_rows(\n    group_label = \"Outcome\",\n    start_row = 1,\n    end_row = 1\n  )%&gt;%\n  kableExtra::pack_rows(\n    group_label = \"Predictors\",\n    start_row = 2,\n    end_row = 4\n  )\n\n\n\n\nVariable\nN obs\nMissing\nMin\n25th perc\nMean\nMedian\n75th perc\nMax\n\n\n\n\nOutcome\n\n\nSupport for War\n1807\n334\n0\n0\n0.72\n1\n1\n1\n\n\nPredictors\n\n\nAge\n1807\n0\n18\n34\n46.65\n45\n60\n99\n\n\nFemale\n1807\n0\n0\n0\n0.47\n0\n1\n1\n\n\nEducation\n1807\n13\n1\n3\n3.17\n3\n4\n5\n\n\n\n\n\n\n\nPlease use this table to describe a typical respondent to the survey YOUR DESCRIPTION HERE\nThe median respondent in this sample is about 45 years old and has complete some form of vocational school. About 47 percent of the respondents are female, and 72 percent of the respondents support Russia’s actions in Ukraine.\nAlso note that 334 respondents (over 18 percent) declined to give an answer on the support for war question. If all of these respondents supported the war, total support would rise to 77 percent, while if all of these respondents opposed the war, support would drop to 59 percent. I suspect that that 72 percent support may overstate the true level of support, if those who are opposed to war are less likely to answer the question or more likely to misrepresent their views.\n\nmean(is.na(df_drww$support_war01))\n\n[1] 0.1848367\n\n# What if these missing respondents all supported or opposed the war?\n\ndf_drww %&gt;%\n  mutate(\n    support_war01_NAsupport = ifelse(is.na(support_war01),1,support_war01),\n    support_war01_NAoppose = ifelse(is.na(support_war01),0,support_war01)\n  ) %&gt;%\n  select(starts_with(\"support_war01\")) %&gt;%\n  summarise_all(mean, na.rm=T)\n\n  support_war01 support_war01_NAsupport support_war01_NAoppose\n1     0.7202987               0.7719978               0.587161"
  },
  {
    "objectID": "labs/comments/07-lab-comments.html#describe-your-expectations-for-these-models",
    "href": "labs/comments/07-lab-comments.html#describe-your-expectations-for-these-models",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "3.1 Describe your expectations for these models",
    "text": "3.1 Describe your expectations for these models\nBefore you estimate these models, please answer the following:\nIn the baseline model, m1 what do you expect the sign of the coefficient for each predictor to be:\n\nAge (Positive/Negative)\nSex (Positive/Negative)4\nEducation (Positive/Negative)\n\nIn the interaction model, m2\n\nDo you think the relationship between age and support will vary by sex (Yes/No)\nIf you said yes, do you think the coefficient on the interaction between age and sex will be positive or negative (Positive/Negative)\n\nIn the polynomial model, m3\n\nIf the coefficient on age is positive and the coefficient on age^2 is positive, then as age increases, the increase in the predicted level of support will be (increasing/decreasing)\nIf the coefficient on age is positive and the coefficient on age^2 is negative, then as age increases, the increase in the predicted level of support will be (increasing/decreasing)\n\nIn m4\n\nIf the coefficients on the interaction terms between the sex variable and the age variables (age and age^2) is statistically significant, this implies that the relationship between age and support for the war is (similar/different) for male and female respondents.\n\nIn the baseline model, m1 what do you expect the sign of the coefficient for each predictor to be:\n\nAge I’d expect the coefficient to be Positive reflecting my weak prior belief that young people tend to be less supportive of war than old people (maybe because they’re more likely to be the ones fighting and dying in the war.)\nSex The coefficient on the sex variable describes the difference between male and female respondents (controlling for age and education). Because sex is a character string, R converts it into a binary indicator, sexMale, which takes a value of 1 when respodents are Male and 0 otherwise (when respondents are Female). R chose Female as the reference category because the letter F comes before M alphabetically. The coefficient on sexMale tells us how Male respondents differ from Female respondents. Again I don’t have strong prior beliefs, but my hunch is that men tend to be more supportive of War than women for some complex set of reasons that reflect mixture of Nature vs Nurture type explanations.\nEducation I’d expect the coefficient on education to be negative, because education, in the U.S. context tends to be correlated with more liberal or progressive policy views which tend to be more Doveish on matters of foreign policy.\n\nHonestly though, you could probably make the case for opposite sign on each of these coefficients.\n\nMaybe age is associated with less support, because older people lived through Russia’s wars in Afghanistan\nMaybe Men are more likely to be opposed to war because they are more likely to fight in it.\nMaybe the educated in Russia are more likely to support the war because they are more likely to be part of the ruling elite/consume more media (and state media)/less likely to have to fight, etc.\n\nThe real point of this question is to give you practice thinking about how to test the empirical implications of your theoretical expectations with regression.\nIn the interaction model, m2\n\nDo you think the relationship between age and support will vary by sex? No. But then I’ve already seen the data.\nIf you said yes, do you think the coefficient on the interaction between age and sex will be positive or negative? This question required you to know what the term R would choose to represent sex. In m2 the coefficient on age describes the relationship between age and support for female respondents. The coeficient on age:sexMale describes how this slope/relationship changes for men.\n\nIf the relationship for age is **positive for female respondents, and the coefficient on the interaction term is negative, this implies that support for the war increases more slowly with age for male respondents compared to female respondents.\nSimilarly, if the coefficient on the interaction term was positive, this implies that support for war increases more rapidly with age for men compared to women.\nIn the polynomial model, m3\n\nIf the coefficient on age is positive and the coefficient on age^2 is positive, then as age increases, the increase in the predicted level of support will be increasing (Think of \\(\\cup\\)-shaped parabola )\nIf the coefficient on age is positive and the coefficient on age^2 is negative, then as age increases, the increase in the predicted level of support will be decreasing (Think of \\(\\cap\\)-shaped parabola )\n\nIn m4\n\nIf the coefficients on the interaction terms between the sex variable and the age variables (age and age^2) is statistically significant, this implies that the relationship between age and support for the war is different for male and female respondents.\n\nIn general, interaction terms allow relationships to vary accross groups or values of predictors.\nIf both m2 and m3 had yielded statistically signficiant results, maybe m4 would have been justified, but honestly I had you fit m4 primarily for pedagogical reasons so you could see that even very complciated models can be understood using predicted values."
  },
  {
    "objectID": "labs/comments/07-lab-comments.html#estimate-the-regression-models",
    "href": "labs/comments/07-lab-comments.html#estimate-the-regression-models",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "3.2 Estimate the regression models",
    "text": "3.2 Estimate the regression models\nUncomment the code below, and replace the ??? with the appropriate terms to fit the following models.\n\n# # Baseline Model\n# m1 &lt;- lm(support_war01 ~ age + sex + education_n, df_drww)\n# \n# # Interaction model: Allow coefficient for age to vary with sex\n# m2 &lt;- lm(support_war01 ~ age*??? + education_n, df_drww)\n# \n# # Polynomial model: Allow coefficient for age to vary by age\n# m3 &lt;- lm(support_war01 ~ age + I(???^2) + sex + education_n, df_drww)\n# \n# # Separate Polynomial: Allow coefficient for age to vary by age separately  by sex\n# m4 &lt;- lm(support_war01 ~ (age + I(???))*??? + education_n, df_drww)\n\n\n# Baseline Model\nm1 &lt;- lm(support_war01 ~ age + sex + education_n, df_drww)\n\n# Interaction model: Allow coefficient for age to vary with sex\nm2 &lt;- lm(support_war01 ~ age*sex + education_n, df_drww)\n\n# Polynomial model: Allow coefficient for age to vary by age\nm3 &lt;- lm(support_war01 ~ age + I(age^2) + sex + education_n, df_drww)\n\n# Separate Polynomial: Allow coefficient for age to vary by age separately  by sex\nm4 &lt;- lm(support_war01 ~ (age + I(age^2))*sex + education_n, df_drww)"
  },
  {
    "objectID": "labs/comments/07-lab-comments.html#footnotes",
    "href": "labs/comments/07-lab-comments.html#footnotes",
    "title": "Comments for Lab 07 - Exploring Russians’ Attitudes About the War in Ukraine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMathematically, recall that the slope/first derivative of the line \\(y = f(x) = 2x\\) is constant \\((f'(x) = 2)\\). If we increase x by 1, we expect y to increase by 2, while the derivative of a parabola \\(y = f(z) = z^2\\) varies with \\(z\\) \\((f'(z) = 2x)\\). Going from z= 2 to z= 3 is associated with a greater increase in y, then going from z=1 to z=2. Our model, however, is still linear in parameters \\(\\beta\\). That is, it is still a linear regression. If our model included some parameter \\(\\theta^2\\), then it would be a non-linear regression.↩︎\nIn a machine learning framework, we trying to find an optimal tradeoff between reducing bias in our predictors by including more predictors and minimizing variance in predictions by not overfitting the data.↩︎\nI think, google translate was a bit unclear. But higher numbers equal more education.↩︎\nThis is tricky, you need to know what the reference (excluded) category will be.↩︎\nBasically, I’m asking whether the coefficient on I(age^2) is statistically significant. If it is, then the change in predicted support for the war among say 20-year-olds compared to 30-year-olds, would be different than change between 30- to 40-year-olds. Interpreting polynomials terms and interaction models is much easier if, as we do later, we simply obtain and plot the predicted values from this model↩︎\nAs in the previous question, basically you need to look at the table and see if the coefficients on the interaction terms are statistically significant. It’s a little more complicated than this, but if they are significant, this is evidence of differences across Sex.↩︎\nBasically, I’m asking whether the coefficient on I(age^2) is statistically significant. If it is, then the change in predicted support for the war among say 20-year-olds compared to 30-year-olds, would be different than change between 30- to 40-year-olds. Interpreting polynomials terms and interaction models is much easier if, as we do later, we simply obtain and plot the predicted values from this model↩︎\nAs in the previous question, basically you need to look at the table and see if the coefficients on the interaction terms are statistically significant. It’s a little more complicated than this, but if they are significant, this is evidence of differences across Sex.↩︎"
  },
  {
    "objectID": "labs/03-lab.html",
    "href": "labs/03-lab.html",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "",
    "text": "Today we will explore the logic and design of Broockman and Kalla’s 2016 study, “Durably reducing transphobia: A field experiment on door-to-door canvassing”, from the recruitment of subjects for the study to the delivery of their interventions. Then we will explore whether the intervention had any effect on respondents’ feelings toward transgender individuals.\nTo accomplish this we will:\n\nSummarize the study (5 Minutes)\nSet up our work space (2-3 Minutes)\nLoad a portion of the replication data (1-2 Minutes)\nGet a high level overview of the data (5 minutes)\nDescribe the distribution of covariates in the full dataset (5 minutes)\nExamine the difference in covariates between those who did and did not complete the survey (10 minutes)\nExamine the difference in covariates between those assigned to each treatment condition in the study. (10 minutes)\nEstimate the average treatment effect of the intervention (10 minutes)\nPlot the results and comment on the study (10 minutes)\nTake the weekly survey (3-5 minutes)\n\nOne of these 9 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nError in `tibble::tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 5: Column `Group 7`.\nℹ Only values of size one are recycled.\n\n\nError in eval(expr, envir, enclos): object 'groups_df' not found"
  },
  {
    "objectID": "labs/03-lab.html#footnotes",
    "href": "labs/03-lab.html#footnotes",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find the full set of replication files here↩︎\nThe actual study contains a number of measures about transgender attitudes and policies which are scaled together to produce a single measure of subjects latent tolerance. For simplicity, we’ll focus on this single survey item.↩︎\nRecall that only some people who completed the baseline and were assigned to receive the treatment actually answered the door when canvassers came knocking.↩︎"
  },
  {
    "objectID": "labs/01-lab.html",
    "href": "labs/01-lab.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/01-lab.html#uncomment-and-run-the-following-code",
    "href": "labs/01-lab.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\n# covid_us %&gt;%\n#   mutate(\n#     year = year(date),\n#     month = month(date),\n#     year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n#     percent_vaccinated = people_fully_vaccinated/population*100  \n#     ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/01-lab.html#uncomment-and-run-the-code-below",
    "href": "labs/01-lab.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\n# covid_us %&gt;%\n#   filter(facial_coverings == -4) %&gt;%\n#   select(date, state) %&gt;%\n#   group_by(state) %&gt;%\n#   summarize(\n#     n = n(),\n#     earliest_date = min(date),\n#     latest_date = max(date),\n#   )%&gt;%\n#   arrange(earliest_date)"
  },
  {
    "objectID": "labs/01-lab.html#please-explainwhat-each-line-of-code-is-doing",
    "href": "labs/01-lab.html#please-explainwhat-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explainwhat each line of code is doing:",
    "text": "6.2 Please explainwhat each line of code is doing:\n\ncovid_us %&gt;% Write your explanation here\nfilter(facial_coverings == -4) %&gt;%\nselect(date, state) %&gt;%\ngroup_by(state) %&gt;%\nsummarize(\nn = n(),\nearliest_date = min(date),\nlatest_date = max(date),\n)%&gt;%\narrange(earliest_date)\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\n\n6.2.1 Substantively, what does the previous chunk of code tell us?\n\n\n\n\n\n\nNote\n\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/01-lab.html#please-run-the-following-code",
    "href": "labs/01-lab.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  ) %&gt;%\n  filter(\n    RegionName == \"Illinois\",\n    date &gt; \"2020-08-01\", \n    date &lt; \"2021-01-01\",\n    !is.na(H6_Notes)\n    ) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\n\nError in oxford_us %&gt;% mutate(date = ymd(Date)) %&gt;% filter(RegionName == : could not find function \"%&gt;%\"\n\nil_facemasks\n\nError in eval(expr, envir, enclos): object 'il_facemasks' not found"
  },
  {
    "objectID": "labs/01-lab.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/01-lab.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;%\nmutate(date = ymd(Date))%&gt;%\nfilter(RegionName == \"Illinois\",\ndate &gt; \"2020-08-01\",\ndate &lt; \"2021-01-01\",\n!is.na(H6_Notes)) %&gt;%\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\nError in eval(expr, envir, enclos): object 'il_facemasks' not found\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\n# il_facemasks$H6_Notes[???]"
  },
  {
    "objectID": "labs/01-lab.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/01-lab.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-central-tendency",
    "href": "labs/01-lab.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nLater in the course we’ll talk about how we can use something like a mean to estimate an Expected Value: Something like\n\\[ E[Y|X=x] \\]\nOr to make it more concrete:\n\\[ E[\\text{New Cases} | \\text{Policy = \"recommended\"}] \\]\nIn code, we could accomplish this manually, using the index operator:\n\n# mean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommended”\nBy using group_by() with summarise() we can accomplish this more quickly:\n\n# covid_us%&gt;%\n#   group_by(face_masks)%&gt;%\n#   summarise(\n#     new_cases_pc = mean(new_cases_pc, na.rm=T)\n#   )\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n\\[\nM_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx=\\int_{x_i}^\\infty f_x(X)dx=1/2\n\\]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\n\n\n\n\n\nNote\n\n\n\n\nMedians are less influenced by outliers than means\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\n# sort(table(covid_us$face_masks))\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-dispersion",
    "href": "labs/01-lab.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n\\[\np_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx= p; \\int_{x_i}^\\infty f_x(X)dx=1-p\n\\]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n\\[\n\\sigma^2_x=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2\n\\]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\n\n8.2.3.1 Use the var() function to calculate the variance of the new_cases_pc variable.\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n\\[\n\\sigma_x=\\sqrt{\\sigma^2_x}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nStandard deviations are easier to interpret because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\n\n8.2.4.1 let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-association",
    "href": "labs/01-lab.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n\\[\ncov(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n\\[\n\\rho_{x,y} = \\frac{cov(x_y)}{\\sigma_x,\\sigma_y}\n\\]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\n# covid_us %&gt;%\n#   filter(year &gt; 2020)%&gt;%\n#   ungroup() %&gt;%\n#   group_by(year,month)%&gt;%\n#   summarise(\n#     mn_per_vax = mean(percent_vaccinated, na.rm=T),\n#     cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n#   )"
  },
  {
    "objectID": "labs/01-lab.html#what-do-these-averages-really-tell-us",
    "href": "labs/01-lab.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?"
  },
  {
    "objectID": "labs/01-lab.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/01-lab.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\n# DT::datatable(cases_by_month_and_policy,\n#               filter = \"top\")\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\n# cases_by_month_and_policy %&gt;%\n#   ggplot(aes(\n#     x= year_month,\n#     y = new_cases_pc,\n#     col=face_masks))+\n#   geom_point()+coord_flip()\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/01-lab.html#footnotes",
    "href": "labs/01-lab.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/09-lab.html",
    "href": "labs/09-lab.html",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "",
    "text": "Today, we will return to exploring Russians’ support the war in Ukraine using a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project.\nThe survey was conducted by phone using a random sample of mobile phone numbers to produce a sample of respondents representative of the population in terms of age, sex, and geography. It was in the field from February 28 to March 2.\nFirst, we will explore how support for the war varies with the demographic predictors age, sex and education. We will compare the results of modeling this relationship using Ordinary Least Squares regression and Logisitic Regression\nWe’ll talk more about the technical aspects of logistic regression next week. For today we’ll simply compare the results from these two approaches.\nNext, we will gain insight into how our estimates from these models might vary using the statistical process of bootstrapping. Specifically, we will simulate the idea of repeated sampling that is the foundation of frequentist interpretations of probability, by sampling from our sample with replacement.\nWe’ll walk through the mechanics of simulation together. Then you’ll quantify the uncertainty described by these bootstrapped sampling distributions.\nFinally, we’ll see what other questions we might ask of these data and practice various skills we’ve developed through out the course.\nPlan to spend the following amount of time on each section\n\nGet set up to work (5 minutes)\nModel the relationship between demographic predictors and war support using OLS and Logistic regression (20 minutes)\nAssess the uncertainty around your estimated coefficients (15 minutes)\nQuantify the uncertainy described by your sampling distributions (10 minutes)\nExplore other relationships in the data. (30 minutes)\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file produced by knitting the lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nYou can find your assigned groups in previous labs"
  },
  {
    "objectID": "labs/09-lab.html#load-packages",
    "href": "labs/09-lab.html#load-packages",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\", \"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"boot\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled        modelr \n         TRUE          TRUE          TRUE          TRUE          TRUE \n        purrr         ggmap       ggrepel      ggridges      ggthemes \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       ggpubr        GGally        scales       dagitty         ggdag \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggforce       COVID19          maps       mapdata           qss \n         TRUE          TRUE          TRUE          TRUE          TRUE \n   tidycensus     dataverse DeclareDesign          boot \n         TRUE          TRUE          TRUE          TRUE \n\n\nThere are three packages in particular that we’ll need to maker sure are installed and loaded\n\nmodelr\npurrr\nbroom\n\nIf ipak didn’t return TRUE for each of these, please uncomment and run:\n\n# install.packages(\"modelr\")\n# install.packages(\"purrr\")\n# install.packages(\"broom\")"
  },
  {
    "objectID": "labs/09-lab.html#load-the-data",
    "href": "labs/09-lab.html#load-the-data",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the recoded data for the lab\nOur primary outcome of interest (dependent variable) for today is a binary measure of support for war:\n\nsupport_war01 “Please tell me, do you support or do not support Russia’s military actions on the territory of Ukraine?” (1=yes, 0 = no)\n\nOur key predictors (independent variables) are the following demographic variables:\n\nage “How old are you?”\nsex “Gender of respondent” (As assessed by the interviewer)\neducation_n “What is your highest level of education (confirmed by a diploma, certificate)?” (1 = Primary school, 2 = “High School”, 3 = “Vocational School” 4 = “College”, 5 = Graduate Degree)1\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))"
  },
  {
    "objectID": "labs/09-lab.html#fit-the-models",
    "href": "labs/09-lab.html#fit-the-models",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.1 Fit the models",
    "text": "2.1 Fit the models\nPlease estimate the following models:\n\nAn OLS regression model called m1 using lm()\nA Logistic regression model called m2 using glm() with family=binomial\n\n\n# OLS\n# m1 &lt;- ???\n\n# Logisitic \n# m2 &lt;- ???"
  },
  {
    "objectID": "labs/09-lab.html#display-the-results-in-a-regression-table",
    "href": "labs/09-lab.html#display-the-results-in-a-regression-table",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.2 Display the results in a regression table",
    "text": "2.2 Display the results in a regression table\nNext, please display the results of your regressions in a table using htmlreg()\n# Regression Table"
  },
  {
    "objectID": "labs/09-lab.html#produce-predicted-values-from-the-model",
    "href": "labs/09-lab.html#produce-predicted-values-from-the-model",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.3 Produce predicted values from the model",
    "text": "2.3 Produce predicted values from the model\nThe coefficients from a logistic regression aren’t easy to directly interpret.\nInstead, we will produce predicted values for each model\nTo do this, we will need to create a prediction dataframe called pred_df Every variable in your model, needs to be represented in your prediction data frame.\n\nUse expand_grid() to create a data frame where\n\nage varies from 18 to 99\nsex is held constant at “Female”\neducation_n is held constant at its mean\n\n\n\n## Create prediction data frame\n# pred_df &lt;- expand_grid(\n#   age = ??? : ???,\n#   ??? = ???,\n#   ???\n# )\n\nThen you use the predict() function to produce predicted values from each model.\nSave the output of predict() for m1 to a new column in pred_df called pred_ols.\nFor m2 you need to tell are to transform the predictions from m2 back into the units of the response (outcome) variable, by setting the argument type = \"response\". Save the output of predict() for m1 to a new column in pred_df called pred_logit.\n\n# #Predicted values for m1\n# pred_df$pred_ols &lt;- predict(???,\n#                             newdata = ???)\n# #Predicted values for m2\n# #Remember to add type = \"response\"\n# pred_df$pred_logit &lt;- ???"
  },
  {
    "objectID": "labs/09-lab.html#plot-the-predicted-values-and-interpet-the-results",
    "href": "labs/09-lab.html#plot-the-predicted-values-and-interpet-the-results",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.4 Plot the predicted values and interpet the results",
    "text": "2.4 Plot the predicted values and interpet the results\nNow we can compare the predictions of OLS and Logistic regression by plotting the predicted values of support for the war from each model.\nTo produce this plot you’ll need to\n\nspecify data (you want to use the values from pred_df)\nmap values from your data aesthetics in ggplot\n\nput age on the x axis and and pred_ols on the y-axis\n\nspecify the geometries to plot\n\nadd two geom_line() to the plot\nleave the first one empty (e.g. geom_line())\nfor the second, specify a new aes of y=pred_logit\n\n\n\n# data %&gt;%\n\n# aesthetics with ggplot()\n\n# geometries geom_line()\n\nHow do the predictions of the two models compare\nSo the predictions from OLS produce impossible values (levels of support above 100 percent) at for very old respondents, while the predictions from logistic regression are constrained to be between 0 and 1.\nIf we think that logistic regression provides a more credible way of modeling support for the war, then our OLS regression appears to overstate the level of support among young and old, while possibly understating the level of support among the middle age. The differences aren’t huge – a few percentage points – but for a binary outcome we will often prefer to model it with logistic regression.\nAlso note that marginal effect for age in the logistic regression is not constant. An increase in age from 25 to 26 is associated with a larger increase in support, than an increase in age from 75 to 76."
  },
  {
    "objectID": "labs/09-lab.html#take-1000-bootstrap-samples-from-df_drww",
    "href": "labs/09-lab.html#take-1000-bootstrap-samples-from-df_drww",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.1 Take 1,000 bootstrap samples from df_drww",
    "text": "3.1 Take 1,000 bootstrap samples from df_drww\nBelow we create 1,000 boostrapped samples\n\n# Make sure these packages are loaded\nlibrary(modelr)\nlibrary(purrr)\nlibrary(broom)\n# Set random seed for reproducability\n\nset.seed(1234)\n\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n\nLet’s take a moment to understand what boot is and why we’re sampling with replacement.\nThe object boot contains 1,000 bootstrapped samples from df_drww.\nIf we look at the first bootstrap we see:\n\nboot$strap[[1]]\n\n&lt;resample [1,807 x 42]&gt; 1308, 1018, 1125, 1004, 623, 905, 645, 934, 400, 900, ...\n\n\nThe numbers 1308, 1018, 1125, 1004, 623, 905, ... correspond to rows in df_drww. So person 1308 is the first observation in this boot strap sample, then person 1018 and so on.\nBecause we are sampling with replacement observations from df_drww can appear multiple times. In our first bootstrap sample:\n\n666 observations appeared once\n342 appeared twice\n105 appeared three times\n27 appeared four times\n5 appeared five times.\n2 appeared six times\n\n\ntable(table(boot$strap[[1]]$idx))\n\n\n  1   2   3   4   5   6 \n666 342 104  27   5   2 \n\n\nWhy would we want to sample with replacement?\nWell, what we’d really love are 1,000 separate random surveys all drawn from the same population in the same way.\nSince that’s not feasible, we instead use the one sample we do have to learn things like how much our estimate might vary in repeated sampling. Efron (1979) called this procedure “bootstrapping” after the idiom “to pull oneself up by one’s own bootstraps”\nWe do this by sampling from our sample with replacement.\nWhen we sample with replacement, we are sampling from our sample, as our sample was sampled from the population.\nWith replacement means that some observations will appear multiple times in our bootstrapped sample (while others will not be included at all).\nWhen an observation appears multiple times in a bootstrap sample, conceptually, we’re using that original observation to represent the other people like that observation in the population who – had we taken a different sample – might have been included in our data.\nNote each bootstrap sample is a different random sample with replacement. In our second bootstrap sample, one observation (person 1496) appeared five times.\n\ntable(table(boot$strap[[2]]$idx))\n\n\n  1   2   3   4   5   6 \n661 342 105  31   1   3 \n\n# Person 406\nsum(boot$strap[[2]]$idx == 1496)\n\n[1] 5\n\n# Person 1 is not in boostrap 2\nsum(boot$strap[[2]]$idx == 1)\n\n[1] 0"
  },
  {
    "objectID": "labs/09-lab.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "href": "labs/09-lab.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples",
    "text": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples\nNow let’s estimate our model for each bootstrapped sample, using the map function.\n\nIn the code below, for every sample in boot, map estimates the model lm(support_war01 ~ age + sex + education_n) plugging in the bootstrap sample into the data=..\n\n\n# bootstrap simulations\nbs_ols &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + sex + education_n, data =.))\n\nThe end result is a large list with 1,000 separate linear regression models estimated on each bootstrapped sample.\nThe coefficients from each bootstrap vary from one simulation\n\n# First bootstrap\nbs_ols[[1]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.305019     0.009746     0.081039    -0.024142  \n\n\nto the next:\n\n# Second boostrap\nbs_ols[[2]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.245000     0.009142     0.095631    -0.009285  \n\n\nBecause they’re estimated off of different bootstrapped samples.\nWe will visualize and quantify that variation to describe the uncertainty associated with our estimates.\nBut first, we need to transform our large list of linear models, into a more tidy data frame that’s easier to manipulate."
  },
  {
    "objectID": "labs/09-lab.html#tidy-the-results-of-our-bootstrapping",
    "href": "labs/09-lab.html#tidy-the-results-of-our-bootstrapping",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.3 Tidy the results of our bootstrapping",
    "text": "3.3 Tidy the results of our bootstrapping\nIn the code below we transform this large list of models into a tidy data frame of coefficients.\n\n# Tidy bootstrapp sims\nbs_ols_df &lt;- map_df(bs_ols, tidy, .id = \"id\")\n\nIn the resulting data frame the id variable identifies the bootstrap simulation (1 to 1,000), the term variable indentifies the specific coefficient from the model estimated for that simulation.\n\nhead(bs_ols_df)\n\n# A tibble: 6 × 6\n  id    term        estimate std.error statistic  p.value\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 1     (Intercept)  0.305    0.0522        5.84 6.29e- 9\n2 1     age          0.00975  0.000702     13.9  3.31e-41\n3 1     sexMale      0.0810   0.0222        3.65 2.73e- 4\n4 1     education_n -0.0241   0.0107       -2.26 2.39e- 2\n5 2     (Intercept)  0.245    0.0533        4.60 4.61e- 6\n6 2     age          0.00914  0.000731     12.5  3.36e-34"
  },
  {
    "objectID": "labs/09-lab.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "href": "labs/09-lab.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age",
    "text": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age\nFinally, let’s get a sense of how our coefficients could vary.\nSpecifically, let’s compare the the observed coefficient from m1 for age, to the bootstrapped sampling distribution of coefficients in bs_ols_df\n\nFirst, we’ll create a basic plot called p_ols_age that shows the distribution of the coefficients for age from our simulation\n\n\np_ols_age &lt;- bs_ols_df %&gt;%\n  filter(term == \"age\") %&gt;%\n  ggplot(aes(estimate))+\n    geom_density()\n\np_ols_age\n\n\n\n\n\n\n\n\nNext we’ll add some additional geometries and labels to our figure\n\nFirst we’ll put a rug to show the individual coefficients\n\n\np_ols_age +\n  geom_rug() -&gt; p_ols_age\n\np_ols_age\n\n\n\n\n\n\n\n\n\nThen we’ll add a vertical line where our observed coefficient on age\n\n\np_ols_age +\n  geom_vline(xintercept =  coef(m1)[2],\n             linetype = 2) -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'm1' not found\n\np_ols_age\n\n\n\n\n\n\n\n\n\nFinally, let’s add some nice labels\n\n\np_ols_age +\n  theme_bw()+\n  labs(\n    x = \"Age\",\n    y = \"\",\n    title = \"Bootstrapped Sampling Distribution of Age Coefficient\"\n  ) -&gt; p_ols_age\n\np_ols_age\n\n\n\n\n\n\n\n\nCongratulations you’ve just produced and visualized your first bootstrapped sampling distribution!\nConceptually, this distribution describes *how much we would expect the coefficient on age in model to vary** from sample to sample.\nJust from eyeballing the figure above, it looks like the observed the relationship between age and support for war or 0.009 could be about as high as 0.011, and as low as 0.007.\nOf course, as the budding quantitative social scientists that we are, we can do better than just eyeballing the data."
  },
  {
    "objectID": "labs/09-lab.html#footnotes",
    "href": "labs/09-lab.html#footnotes",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think, google translate was a bit unclear. But higher numbers equal more education.↩︎"
  },
  {
    "objectID": "labs/05-lab.html",
    "href": "labs/05-lab.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/05-lab.html#load-covid-19-data",
    "href": "labs/05-lab.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid-19 data"
  },
  {
    "objectID": "labs/05-lab.html#load-election-data",
    "href": "labs/05-lab.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\n# Load election data\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/05-lab.html#recode-the-covid-19-data",
    "href": "labs/05-lab.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide, starting here and ending here\n\n# Create a vector containing of US territories\n\n\n# Filter out Territories and create state variable\n\n\n# Calculate new cases, new cases per capita, and 7-day average\n\n\n\n# Recode facemask policy (Not strictly necessary so feel free to skip)\n\n\n# Create year-month and percent vaccinated variables"
  },
  {
    "objectID": "labs/05-lab.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/05-lab.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, copy the code for new_deaths_pc_7da change the new_deaths_pc_7da to new_deaths_pc_14da and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\n# Create the following variables:\n# new_deaths\n# new_deaths_pc\n# new_deaths_pc_7da\n# new_deaths_pc_14da"
  },
  {
    "objectID": "labs/05-lab.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/05-lab.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data"
  },
  {
    "objectID": "labs/05-lab.html#for-all-the-observations",
    "href": "labs/05-lab.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7da)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7da"
  },
  {
    "objectID": "labs/05-lab.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/05-lab.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7da before April 19, 2021"
  },
  {
    "objectID": "labs/05-lab.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/05-lab.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7da after April 19, 2021\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that …\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that …\nWhen we limit our analysis, to just observations before April 19, 2021 …\nWhen we look at observations after the vaccine became widely available …"
  },
  {
    "objectID": "labs/05-lab.html#footnotes",
    "href": "labs/05-lab.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/06-lab.html",
    "href": "labs/06-lab.html",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "",
    "text": "Today we will explore the critiques and alternative explanations for the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nRecall the core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\n\nA skeptic of this claim might argue that relationship between electoral and epidemelogical outcomes is spurious, saying somthing like:\n\nThere are lots of ways that Red States differ from Blue States — demographics, economics, geography, culture, and so on – and it is these differences that explain the phenomena of Red Covid. If we were to control for these omitted variables the relationship between a state’s partisan leanings and Covid-19 would go away.\n\nIn this lab, we will see how we can explore these claims using multiple regression to control for competing explanations.\nTo accomplish this we will:\n\nGet set up to work (10 minutes)\n\nThen we will estimate and interpret a series of regression models:\n\nA baseline Red Covid model using simple bivariate regression using the Republican vote share of states to predict the 14-day average of per capita Covid-19 deaths on September 23, 2021 (10 Minutes)\nA multiple regression model controlling for Republican vote share the median age (15 minutes)\nA model controlling for Republican vote share, the median age and median income (15 minutes)\nA model controlling for Republican vote share, the median age median income and vaccination rates (15 minutes)\nA model using Republican vote share, the median age median income to predict vaccination rates (15 minutes)\n\nFinally, we’ll take the weekly survey which will serve as a mid semester check in.\nOne of these 6 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3092022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 6 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/06-lab.html#load-packages",
    "href": "labs/06-lab.html#load-packages",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\nThere’s one new package, htmltools which we’ll use to display regression tables while we work.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "labs/06-lab.html#load-the-data",
    "href": "labs/06-lab.html#load-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the data that we created in class on Tuesday which provides a snapshot of the state of Covid-19 on September 23, 2021 in the U.S.\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nAfter running this code, the data frame covid_lab should appear in your environment pane in R Studio"
  },
  {
    "objectID": "labs/06-lab.html#describe-the-data",
    "href": "labs/06-lab.html#describe-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nIn the code chunk below, please write some code get an high level overview of the data:\n\n# High level overview\n# Number of observations and variables\n\n\n# Names of variables\n\n\n# Glimpse of data\n\n\n# Summary of data\n\n\n# Calculate standard deviations\n\nPlease use this HLO to answer the following questions:\n\nHow many observations are there:\nWhat is an observation (i.e. what is the unit of analysis):\nWhat is the primary outcome variable for today:\nWhat are the four main predictors we’ll be using:\nWill we be using the the raw values of these predictors or their standardized values?\nWhat is the standard deviation of our outcome and predictor variables:\n\nCovid-19 deaths:\nRepublican vote share:\nMedian age:\nMedian income:\nVaccination Rate:"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model",
    "href": "labs/06-lab.html#fit-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.1 Fit the model",
    "text": "2.1 Fit the model\n\nm1 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results",
    "href": "labs/06-lab.html#summarize-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.2 Summarize the results",
    "text": "2.2 Summarize the results\nNow we apply the summary() function to our model m1\n\nsummary(m1)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ rep_voteshare_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66967 -0.21572 -0.03715  0.11169  1.00580 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.57530    0.04846  11.872 6.88e-16 ***\nrep_voteshare_std  0.22571    0.04895   4.611 2.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3427 on 48 degrees of freedom\nMultiple R-squared:  0.307, Adjusted R-squared:  0.2925 \nF-statistic: 21.26 on 1 and 48 DF,  p-value: 2.99e-05\n\n\nWe see that m1 returns two coefficients, which define a line of best fit predicting Covid-19 deaths with the Republican vote share of the 2020 Presidential election:\n\n\\(\\beta_0\\) corresponds to the intercept. This is model’s prediction for a state where Trump got 0 percent of the vote. This is typically not something we care about.\n\\(\\beta_1\\) corresponds to the slope. Because we used a standardized measure of vote share, we would say that a 1-standard deviation (about 10 percentage points) increase in Republican vote share is associated with a 0.23 increase the average number of new Covid-19 deaths. Given that this per-capita measure has a standard deviation of 0.4, this is a fairly sizable association.\nFinally, note that last column of summary(m1) Pr(&gt;|t|) both the coefficients for the intercept \\((\\beta_0)\\) and rep_voteshare_std (\\((\\beta_1)\\)) are statistically significant (ie have an * next to them)."
  },
  {
    "objectID": "labs/06-lab.html#display-the-model-as-a-regression-table",
    "href": "labs/06-lab.html#display-the-model-as-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.3 Display the model as a regression table",
    "text": "2.3 Display the model as a regression table\nNext we’ll format the results of summary(m1) into a regression table using the htmlreg() function.\nRegression tables are a the standard way of concisely presenting the results of regression models.\n\nEach named row corresponds to the coefficients form the model\nIf there is an asterisks next to a coefficient, that coefficient is statistically significant with a p value below a certain threshold.\nThe numbers in parentheses below each coefficient correspond to the standard error of the coefficient (more on that later)2\nThe bottom of the table contains summary statistics of of our model, which we’ll ignore for today.\n\nThe code after htmlreg(m1) allows you to see what output of the table will look like in the html document while you’re working in the Rmd file.\n\n\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n0.58***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n50\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab.html#visualize-the-model",
    "href": "labs/06-lab.html#visualize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.4 Visualize the model",
    "text": "2.4 Visualize the model\nNow let’s visualize the results of our m1 with a scatter plot.\nIn the code below, please write a comment explaining what each section of code is doing\n\nReplace the #1. Comment for. with a brief comment explaining what the code below the comment does.\n\n\n# 1. Comment for the line of code below\ncovid_lab %&gt;%\n# 2. Comment for ggplot()\n  ggplot(aes(x = rep_voteshare_std,\n             y = new_deaths_pc_14da,\n             label = state_po))+\n# 3. Comment for geom_point() \n  geom_point(\n# 4. Comment for size =\n    size = .5\n    )+\n# 5. Comment for geom_text_repel()\n  geom_text_repel(\n# 6. Comment for size =   \n    size = 2)+\n# 7. Comment for geom_smooth\n  geom_smooth(method = \"lm\")+\n# 8. Comment for labs\n  labs(\n    x = \"Republican Vote Share (std)\",\n    y = \"New Covid-19 Deaths\\n(14-day ave)\"\n  )\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results",
    "href": "labs/06-lab.html#interpret-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.5 Interpret the results",
    "text": "2.5 Interpret the results\nIn a sentence our two, summarize the results of your analysis in this section\nYou words here!"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-1",
    "href": "labs/06-lab.html#fit-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.1 Fit the model",
    "text": "3.1 Fit the model\nNow let’s test our skeptics’ claims by fitting a model m2 that controls for Age (med_age_std).\n\nRemember the first argument in lm() is formula of the form outcome variable ~ predictor1 + predictor2 + ...\n\n\n# m2"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-model",
    "href": "labs/06-lab.html#summarize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.2 Summarize the model",
    "text": "3.2 Summarize the model\nNow let’s print out a statistical summary of m2\n\n# summary of m2"
  },
  {
    "objectID": "labs/06-lab.html#display-the-model-as-a-regression-table-1",
    "href": "labs/06-lab.html#display-the-model-as-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.3 Display the model as a regression table",
    "text": "3.3 Display the model as a regression table\nNext, let’s create a regression table that displays m1 in the first column and m2 in the second column.\n\nTo do this, change list(m1) from the code above to list(m1, m2)"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results-1",
    "href": "labs/06-lab.html#interpret-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.4 Interpret the results",
    "text": "3.4 Interpret the results\nIn a few sentences, explain whether the results from m2 support the skeptics criticisms or not?"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-2",
    "href": "labs/06-lab.html#fit-the-model-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.1 Fit the Model",
    "text": "4.1 Fit the Model\nPlease fit a model called m3 implied by the skeptic’s revised claims\n\n# m3"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-model-1",
    "href": "labs/06-lab.html#summarize-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.2 Summarize the model",
    "text": "4.2 Summarize the model\nSummarize the model m3 using summary()\n\n# summary m3"
  },
  {
    "objectID": "labs/06-lab.html#display-the-models-in-a-regression-table",
    "href": "labs/06-lab.html#display-the-models-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.3 Display the models in a regression table",
    "text": "4.3 Display the models in a regression table\nAnd then display the results of models m1, m2, and m3.\n\n# regression table of m1, m2, m3"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-skeptics-claims",
    "href": "labs/06-lab.html#interpret-the-skeptics-claims",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.4 Interpret the skeptic’s claims",
    "text": "4.4 Interpret the skeptic’s claims\nIn a few sentences, explain whether the results from m3 support the skeptics criticisms or not?\nControlling for median age and income, the coefficient on Republican sote share decreases in size by more than half and is no longer statistically significant. The coefficient on median income is statistically significant and substantively suggests that states with higher median incomes tended to have fewer Covid-19 deaths on September 23, 2021."
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-3",
    "href": "labs/06-lab.html#fit-the-model-3",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.1 Fit the model",
    "text": "5.1 Fit the model\nYou know the drill.\n\n# m4"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results-1",
    "href": "labs/06-lab.html#summarize-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.2 Summarize the results",
    "text": "5.2 Summarize the results\nAgain, let’s get a quick summary of our results\n\n# summary of m4"
  },
  {
    "objectID": "labs/06-lab.html#display-the-models-in-a-regression-table-1",
    "href": "labs/06-lab.html#display-the-models-in-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.3 Display the models in a regression table",
    "text": "5.3 Display the models in a regression table\nAnd add m4 to list of models in our regression table"
  },
  {
    "objectID": "labs/06-lab.html#interpet-the-results",
    "href": "labs/06-lab.html#interpet-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.4 Interpet the results",
    "text": "5.4 Interpet the results\nBriefly interpret the results of m4"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-4",
    "href": "labs/06-lab.html#fit-the-model-4",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.1 Fit the model",
    "text": "6.1 Fit the model\nNow let’s fit the model. For ease of interpretation, let’s use the unstandardized measure of vaccination rates, percent_vaccinated as our outcome variable.\n\n# m5"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results-2",
    "href": "labs/06-lab.html#summarize-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.2 Summarize the results",
    "text": "6.2 Summarize the results\nAnd summarize the results\n\n# summary of m5"
  },
  {
    "objectID": "labs/06-lab.html#display-the-results-in-a-regression-table",
    "href": "labs/06-lab.html#display-the-results-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.3 Display the results in a regression table",
    "text": "6.3 Display the results in a regression table\nDisplay them in a regression table"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results-2",
    "href": "labs/06-lab.html#interpret-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.4 Interpret the results",
    "text": "6.4 Interpret the results\nSummarize the results of m5 and offer some broader discussion of what we’ve learned today"
  },
  {
    "objectID": "labs/06-lab.html#footnotes",
    "href": "labs/06-lab.html#footnotes",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn short, these * correspond to \\(p-values\\) below different thresholds. One * typically means \\(p &lt; 0.05\\). A p-value is a conditional probability that arises from a hypothesis test summarizing the likelihood of observing a particular test statistic (here a regression coefficient, or more specifically, a t-statistic which is the regression coefficient divided by its standard error) given a paritcular hypothesis (typically, but not allows a null hypothesis that the true coefficient is 0). In sum, a p-value assess the likelihood of seeing what we did, if in fact, there was no relationship. If that likelihood is small (p&lt;0.05), we reject the claim of no relationship. We remain uncertain about the true value of the coefficient, but we are pretty confident it’s not 0.↩︎\nA standard error is another one of those things that in the cart we’re putting before horse today. Briefly, it is an estimate of the standard deviation of the sampling distribution of a coefficient and describes how much our coefficient might vary had we had a different sample…↩︎"
  },
  {
    "objectID": "assignments/a1.html",
    "href": "assignments/a1.html",
    "title": "POLS 1600: Research Topics",
    "section": "",
    "text": "Asking a good research question is one of the most important skills you will develop in your academic careers. It’s also one of the hardest.\nWe often think we’re asking one question, when in fact the study we conduct really addresses a related but distinct question. When a priest asked Willie Sutton why he robbed banks, he replied the “Well, that’s where the money is”. The priest’s question was about why rob at all, while Sutton answered the different question “Given one robs, why rob banks?” Similarly, Medieval philosophers might ask why objects stay in motion, while Newton suggests what really need is not an explanation of motion itself but of changes in motion.\nThe object of our question shapes the form of our explanation.\nIn this assignment, I would like your group to craft three potential research questions that we might explore in our research project for this class. Each question, should be a single sentence, with a few sentences answering the following questions (More details below):\n\nWhy do we care about the answer to this research question?\nWhat’s would a hypothetical “ideal experiment” to answer this question look like?\nWhat would a study with observational data look like?\nA published study that relates to this question\nHow feasible would it be to do a study like this for the course\n\nYou may use this Rmd file as a template (click here to download) or create your own file. Please submit your responses to Canvas.\nYou might start by writing down several questions of different forms about the same topic:\n\nWhy do people vote?\nWhy do people not vote?\nWhy do the rich vote at higher rates than the poor?\nWhen might people who don’t vote, be motivated to vote?\nWhat is the effect of encouraging someone to vote via a phone call?\nAre phone calls more or less effective than in-person contact for get-out the vote efforts?\n\nEach of these questions addresses a general topic that political scientists seem to think is important. Each carries some suppositions and assumptions that in turn influence the type of explanation we might find convincing. Why do people vote feels a bit broad to me. People probably vote for many reasons. How can we hope to adjudicate between all the possible reasons for voting? (Further are these the same reasons for not voting or do we need another set of explanations altogether?)\nWhether phone calls are more or less effective than in-person contacts for GOTV efforts seems more tractable, but also perhaps to narrow. Do we really care? If we’re confident we can identify an effect or difference in one study, are we sure we’ll see similar effects in a different study conducted under different circumstances?\nIn crafting your research questions, you want to strike a balance between things we actually care about (why do people vote) and things we can actually assess (what’s effect of a particularly type of encouragement to vote). A few thoughts on this process:\n\n“Why” questions tend to be more compelling than “What” or “How” or “Do” questions, I think in part because “why” questions often imply a theory and suggest a counterfactual (why this and not that), while other ways of asking questions feel more descriptive. For example, why do the rich vote at higher rates than the poor. Well, one explanation may be that their relative social and economic status means they are more likely to be targets of mobilization efforts by campaigns (among many things). So a natural follow up to this larger question might be, what’s the effect of providing similar mobilization efforts to the poor. Would they vote at similar rates to the rich? If so, then we’ve learned something about how mobilization explains class differences in participation.\nThinking about questions in terms of puzzles is another useful trick. Why do parties exist when politicians’ ideological preferences can explain the vast majority of their legislative behavior? Note this type of question contains a lot of presuppositions (how do we measure ideological preferences? Do they really explain legislative behavior? Is that what we care about?), but as point of departure for a study these type arguments can be useful\nTry to be simple and clear. Don’t worry about asking the perfect question right away. Your questions can and should evolve over time, and I suspect some of you will write a paper that has nothing to do with the questions you posed here.\n\nFor each question, please discuss the following:\n\nWhy do we care? Why we should care about the answer to this question. A strong justification is often that existing theories yield conflicting predictions and so your study will offer some insight into how to adjudicate betweeen these theories. A less strong justification is that no one has ever studied this before. Even if this is true (and it’s often not) it may be true for good reason. No need for formal citations, but if there are specific theories or claims your addressing feel free to name names.\nThe ideal experiment Please describe an “ideal” experiment that you could run that would give you some purchase on your question. Note the key feature of an experiment, is that you the researcher are able to manipulate (through random assignment) some facet of the world. Assume money, resources, physics, and even ethics are not an object. If you could randomly assign anything, what would you manipulate. At what level of analysis would your manipulation occur (i.e. are your units of analysis individuals or countries or something else). How would you measure your outcome, again assuming you were all power and all-seeing. If that manipulation isn’t feasible, what does that say about the ability to make a causal claim about your question?\nThe observational study Finally, considering some of the potential limitations that might prevent you from implementing your ideal experiment (it’s hard to randomly assign democratic government), what is one way you might address your research question with observational data. Would your study use cross-sectional or longitudinal data. What are some of the concerns (selection on observables) that arise in this setting. Is there a natural experiment or some sort of discontinuity you might leverage to approximate this experimental ideal.\n\nAgain, each paragraph should be brief and to the point. No need to specify a full research design–just give me the broad strokes. You’re writing for each question should not exceed a page.\nAfter you’ve thought through how you might go about answering your question, please find\n\nA published study that relates to this question. It need not be exactly your question as posed, but it should be in a similar area. Include a full citation, and link to the study. Then in a paragraph sentences try to summarize:\n\n\nThe study’s research question\nEmprical design\nCore findings.\n\nFinally on a scale of 1 (least feasible) to 10 (most feasible), please evaluate how likely you think it is you could write an empirical paper on this question for this course.\nDon’t worry about getting everything right. Your final projects can, will, and probably should change. The point of this exercise is to get some practice thinking about questions that interest you in the language of causal inference and potential outcomes.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care",
    "href": "assignments/a1.html#why-do-we-care",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment",
    "href": "assignments/a1.html#the-ideal-experiment",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study",
    "href": "assignments/a1.html#the-observational-study",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study",
    "href": "assignments/a1.html#a-published-study",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10",
    "href": "assignments/a1.html#feasibility-x10",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care-1",
    "href": "assignments/a1.html#why-do-we-care-1",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment-1",
    "href": "assignments/a1.html#the-ideal-experiment-1",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study-1",
    "href": "assignments/a1.html#the-observational-study-1",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study-1",
    "href": "assignments/a1.html#a-published-study-1",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10-1",
    "href": "assignments/a1.html#feasibility-x10-1",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care-2",
    "href": "assignments/a1.html#why-do-we-care-2",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment-2",
    "href": "assignments/a1.html#the-ideal-experiment-2",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study-2",
    "href": "assignments/a1.html#the-observational-study-2",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study-2",
    "href": "assignments/a1.html#a-published-study-2",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10-2",
    "href": "assignments/a1.html#feasibility-x10-2",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a4.html",
    "href": "assignments/a4.html",
    "title": "A4: Project Drafts",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "You have three types of assignments in this course",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#labs",
    "href": "assignments/index.html#labs",
    "title": "Assignments",
    "section": "Labs",
    "text": "Labs\n\nEach Thursday we will work in groups to complete an in-class lab\nThe labs are designed to reinforce and extend concepts from lecture using real world data.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#tutorials",
    "href": "assignments/index.html#tutorials",
    "title": "Assignments",
    "section": "Tutorials",
    "text": "Tutorials\nCoding tutorials to reinforce concepts from lecture and textbook.\nAccessed by running in the console of R Studio\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nComplete the tutorial. Save output as “LASTNAME_TutorialNumber.html”\nUpload output to Canvas by Friday by 11:59 pm",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#final-project",
    "href": "assignments/index.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\nFinally, throughout the semester you will be have periodic assignments to ensure that you’re making progress on your final projects.\n\nWeek 4:  Research Topics\nWeek 6:  Potential Data\nWeek 8:  Explortatory Analysis\nWeek 11: Draft\nWeek 12: Presentations\nWeek 13: Final Paper",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "resources/00-software-setup.html#tldr",
    "href": "resources/00-software-setup.html#tldr",
    "title": "Getting started with R",
    "section": "TLDR",
    "text": "TLDR\nIf you’re pressed for time here’s the short version of what you should do before next class.\n\nUpdate your operating system\nDownload R\nDownload R Studio\nOpen R Studio on your computer\nCreate an Quarto Markdown Document (.qmd) in R Studio\n\n\nFile &gt; New File &gt; Quarto Document\nInstall any requested packages\n\n\nRender this .Qmd into an html file.\n\n\nClick the render arrow with the needle OR:\nUse the hot keys: Mac: cmd + shift + k PC: crtl + shift + k\n\n\nInstall some additional packages for the course\n\nSpecifically copy and paste the following into the console in R Studio and hit enter:\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nYou’ll need to have the package rmarkdown installed for\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\nTo work, so make sure you’ve copied and pasted the code above into your console\n\nIf you have a question or somethings not working, don’t hesitate to ask. Please email me at paul_testa@brown.edu or come to my office at 111 Thayer St Rm 339."
  },
  {
    "objectID": "resources/00-software-setup.html#install-the-devtools-and-remotes-packages",
    "href": "resources/00-software-setup.html#install-the-devtools-and-remotes-packages",
    "title": "Getting started with R",
    "section": "7.1 Install the devtools and remotes packages",
    "text": "7.1 Install the devtools and remotes packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers.\nTo get the most out of R, you’ll need to install add-on packages, which are user-written to extend/expand your R capabilities.\nPackages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE).\nAlternatively, they may be available via the software sharing platform GitHub.\n\nTo download these packages, you first need to install the devtools and remotes packages.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n\nPlace your cursor in the console (lower left panel), and copy and paste each line of code above. After you’ve pasted a line, hit Enter/Return and R will execute (run) that line of code. So type:\n\ninstall.packages(“devtools”)\n\nHit enter.\nThen type\n\ninstall.packages(“remotes”)\n\nAnd hit enter again.\nEach time, R will likely spit out some cryptic red text as it installs the packages.\nWhen it’s done, R will you should see a line with a single &gt; in the console.\nYou should be able to see the newly installed packages by scrolling through or searching the Packages pane on the bottom left"
  },
  {
    "objectID": "resources/00-software-setup.html#install-packages-for-course",
    "href": "resources/00-software-setup.html#install-packages-for-course",
    "title": "Getting started with R",
    "section": "7.2 Install Packages for Course",
    "text": "7.2 Install Packages for Course\nNow we’ll use the intall_github() function from the remotes package, to install some packages we’ll use for this course.\nAgain, copy and paste each line of code into your console, and hit Enter/Return to run that code.\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nWe’ll go over this during our next meeting so don’t worry if this doesn’t work\nYou will likely be asked to update some existing packages\nType 1 in the console and hit enter\n\nIn particular, we’ll be using a version of Matthew Blackwell’s qsslearnr as problem sets for this course.\nYou can see the available problem sets by running the following code in your console:\n\nlearnr::run_tutorial(package = \"qsslearnr\")\n\nAnd start a tutorial by running:\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nTo try and explain in words what this code is doing:\n\nlearnr::run_tutorial( Says use the run_tutorial() from the learnr package\n\"00-intro\" tells run_tutorial() to run the \"00-intro\" tutorial\npackage = \"qsslearnr\" tells run_tutorial() to look for this tutorial in the qsslearnr package.\n\nIf you run this code, you should see the following tutorial show up in the upper right panel:"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-adding-a-tutorial-panel",
    "href": "resources/00-software-setup.html#optional-adding-a-tutorial-panel",
    "title": "Getting started with R",
    "section": "7.3 Optional: Adding a Tutorial Panel",
    "text": "7.3 Optional: Adding a Tutorial Panel\nYou can also add a “Tutorial” panel to R Studio.\n\nClick on “Tools &gt; Global Options”\n\nAlternatively you can use the hotkey combination cmd + , on a Mac cntrl + , … No Shortcut for PC :(\n\n\n\n\nSelect the Pain Layout tab. In the upper right of the four pains, check the box next to Tutorial\nYou may need to close and re-open R Studio. When you do, in the upper right tab you should see:"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-adjusting-quarto-r-markdown-display-options",
    "href": "resources/00-software-setup.html#optional-adjusting-quarto-r-markdown-display-options",
    "title": "Getting started with R",
    "section": "7.4 Optional: Adjusting Quarto / R Markdown Display Options",
    "text": "7.4 Optional: Adjusting Quarto / R Markdown Display Options\n\n2+2\n\n[1] 4\n\n\nThis is just personal perference, but while we’re changin some global options, I’d recommend\n\nClick on `RMarkdown\nSet “Show In Document Outline” to `All Sections and Chunks\nUncheck the box that says “Show Output in line for all R Markdown Documents”"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-dont-save-r-history",
    "href": "resources/00-software-setup.html#optional-dont-save-r-history",
    "title": "Getting started with R",
    "section": "7.5 Optional: Don’t Save R History",
    "text": "7.5 Optional: Don’t Save R History\nFinally, in the R General tab, I’d strongly recommend unchecking the box that says “Always save R History”\n\n** Be sure to click OK** when you’re done updating these settings."
  },
  {
    "objectID": "slides/08-slides.html#general-plan",
    "href": "slides/08-slides.html#general-plan",
    "title": "Week 08:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nLab Preview\nLecture\n\nProbability Theory\nConditional Probability\nRandom variables and probability distributions\nExpected values and variances"
  },
  {
    "objectID": "slides/08-slides.html#get-set-up-to-work",
    "href": "slides/08-slides.html#get-set-up-to-work",
    "title": "Week 08:",
    "section": "Get set up to work",
    "text": "Get set up to work"
  },
  {
    "objectID": "slides/08-slides.html#new-packages",
    "href": "slides/08-slides.html#new-packages",
    "title": "Week 08:",
    "section": "New packages",
    "text": "New packages\nAnd tools for doing Permutations and Combinations\n\ninstall.packages(\"gtools\")"
  },
  {
    "objectID": "slides/08-slides.html#packages-for-today",
    "href": "slides/08-slides.html#packages-for-today",
    "title": "Week 08:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\", \"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\", \n  \"gtools\" #&lt;&lt;\n)"
  },
  {
    "objectID": "slides/08-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/08-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 08:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/08-slides.html#load-packages-for-today",
    "href": "slides/08-slides.html#load-packages-for-today",
    "title": "Week 08:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign           zoo        gtools \n         TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/08-slides.html#load-data-for-today",
    "href": "slides/08-slides.html#load-data-for-today",
    "title": "Week 08:",
    "section": "Load Data for today",
    "text": "Load Data for today"
  },
  {
    "objectID": "slides/08-slides.html#what-well-cover-today",
    "href": "slides/08-slides.html#what-well-cover-today",
    "title": "Week 08:",
    "section": "What we’ll cover today",
    "text": "What we’ll cover today\n\nProbability\n\nProbability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur\nProbability follows three simple rules (from which numerous theorems follow)\n\nConditional Probability\n\nConditional probability allow us to describe how our beliefs about one event change after observing another event(s)\nWe can update beliefs using Bayes Rule\n\nRandom Variables and Probability Distributions\n\nRandom variables map events in the world onto numbers\nProbability distributions describe the likelihood that random variables take certain values\nThe expected value of a random variable is a probability weighted average that tells us the most likely value a variable will take"
  },
  {
    "objectID": "slides/08-slides.html#probability-1",
    "href": "slides/08-slides.html#probability-1",
    "title": "Week 08:",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event happening.\nStatistics uses probability to quantify uncertainty about estimates and hypotheses.\nTo do this, we will need to understand:\nDefinitions (experiment, sample space, events,)\nThree rules of probability (Kolmogorov axioms)\nTwo interpretations interpreting probabilities (Frequentist and Bayesian)\n\nSome rules for counting"
  },
  {
    "objectID": "slides/08-slides.html#experiments-sample-spaces-sets-and-events",
    "href": "slides/08-slides.html#experiments-sample-spaces-sets-and-events",
    "title": "Week 08:",
    "section": "Experiments, sample spaces, sets, and events",
    "text": "Experiments, sample spaces, sets, and events\n\nIn probability theory, an experiment describes a repeatable process where the outcome is uncertain\n\nProcesses where the outcomes are uncertain are called non-deterministic or stochastic\n\nThe sample space of an experiment is the set \\((\\Omega\\) “omega”, or \\(S\\)) of all the possible outcomes of an experiment\nSets can be:\n\nempty \\(( A: \\{\\emptyset\\}\\)\na single event \\(( Coin: \\{\\text{Heads}\\}\\)\nmultiple events \\(( Odd\\, \\#s: \\{\\text{1,3,5}\\}\\)\ninfinite \\((\\mathbb{R}: \\text{ The set of real numbers}\\{ -\\infty \\dots +\\infty\\}\\))\n\nAn event, \\((E\\) or \\(A)\\) is a subset of outcomes in the sample space\n\nThe sample space for a coin flip is \\(\\Omega = \\{\\text{Heads, Tails}\\}\\)\nThe event Heads is a subset of \\(\\Omega\\)\n\n\nbackground-image:url(“https://www.playmonster.com/wp-content/uploads/2019/09/1000_set_pkgcontents-1.png”) background-size:contain"
  },
  {
    "objectID": "slides/08-slides.html#operations-on-sets",
    "href": "slides/08-slides.html#operations-on-sets",
    "title": "Week 08:",
    "section": "Operations on Sets",
    "text": "Operations on Sets\n\nEmpty Set: \\(\\emptyset\\) a set with no elements\nSubset:\n\nLet \\(D\\) be the set outcomes for a 6-side die: \\(D=\\{1,2,3,4,5,6\\}\\)\n\\(Primes=\\{2,3,5\\}\\)\n\\(Primes \\subset D \\iff \\forall X \\in Primes, X \\in D\\)\n\nUnions\n\n\\(A \\cup B = \\{X:X \\in A \\lor X \\in B \\}\\)\nEither \\(A\\), \\(B\\) or both \\(A and B\\) occur\n\nIntersections\n\n\\(A \\cap B = \\{X:X \\in A \\land X \\in B \\}\\)\nBoth \\(A\\) and \\(B\\) occur\n\nComplements\n\n\\(A'=A^\\complement = \\{X:X\\notin A\\}\\)\n\\(A'=A^\\complement\\) means \\(A\\) does not occur\n\\(\\emptyset^\\complement=S\\) and \\(S^\\complement=\\emptyset\\)\n\n\nbackground-image:url(“https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png”) background-size:contain\nSource"
  },
  {
    "objectID": "slides/08-slides.html#three-rules-of-probability",
    "href": "slides/08-slides.html#three-rules-of-probability",
    "title": "Week 08:",
    "section": "Three Rules of Probability",
    "text": "Three Rules of Probability\n\nProbability is defined by three rules or assumptions called the Kolmogorov Axioms\n\n\nThe probability of any event \\(A\\) is nonnegative\n\n\\[Pr(A) \\geq 0 \\]\n\nThe probability that one of the outcomes in the same space occurs is 1\n\n\\[Pr(\\Omega) = 1 \\]\n\nIf events \\(A\\) and \\(B\\) are mutually exclusive, then:\n\n\\[Pr(A \\text{ or } B) = Pr(A) + Pr(B)\\] ## The Addition Rule\nFor any given events, \\(A\\) and \\(B\\), the addition rule says we can find the probability of either \\(A\\) or \\(B\\) occurring:\n\\[Pr(A \\cup B) = Pr(A \\text{ or } B) = Pr(A) + Pr(B) - \\underbrace{Pr(A \\text{ and } B)}_{\\text{aka } Pr(A \\cap B)}\\] In words: The probability of either A or B occurring is the probability that A occurs plus the probability that B occurs - minus the probability that both occur (so that we’re not double counting…)\nbackground-image:url(“https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png”) background-size:contain\nSource"
  },
  {
    "objectID": "slides/08-slides.html#the-law-of-total-probability-part-1",
    "href": "slides/08-slides.html#the-law-of-total-probability-part-1",
    "title": "Week 08:",
    "section": "The Law of Total Probability (Part 1)",
    "text": "The Law of Total Probability (Part 1)\nFor any event two events, \\(A\\) and \\(B\\), the probability of \\(A\\) \\((Pr(A)\\) can be decomposed into the sum of the probabilities of two mutually exclusive events:\n\\[Pr(A) = Pr(A \\text{ and } B) + Pr(A \\text{ and } B^{\\complement})\\]\nbackground-image:url(“https://www.onlinemathlearning.com/image-files/set-operations-venn-diagrams.png”) background-size:contain\nSource"
  },
  {
    "objectID": "slides/08-slides.html#two-interpretations-of-probablity",
    "href": "slides/08-slides.html#two-interpretations-of-probablity",
    "title": "Week 08:",
    "section": "Two interpretations of probablity",
    "text": "Two interpretations of probablity\n\nProbabilities are defined by these three axioms\nThe are two broad ways of interpreting what probabilities mean:\n\nFrequentist\nBayesian"
  },
  {
    "objectID": "slides/08-slides.html#frequentist-interpretations-of-probability",
    "href": "slides/08-slides.html#frequentist-interpretations-of-probability",
    "title": "Week 08:",
    "section": "Frequentist interpretations of probability",
    "text": "Frequentist interpretations of probability\n\nProbability describes how likely it is that some event happens.\n\nFlip a fair coin, the probability of heads is Pr(Heads) = 0.5\n\n\n–\n\nFrequentist: view this probability as the limit of the relative frequency of an event over repeated trials.\n\n\\[Pr(E) = \\lim_{n \\to \\infty} \\frac{n_{E}}{n} \\approx \\frac{ \\text{# of Times E happened}}{\\text{Total # of Trials}}\\]\n\nThinking about probability as a relative frequency, requires us to know how to the number of times an event occurred.\n\nbackground-image:url(“https://www.kindpng.com/picc/m/99-991302_transparent-sesame-street-count-clipart-count-sesame-street.png”) background-size:contain"
  },
  {
    "objectID": "slides/08-slides.html#how-many-elements-in-a-set",
    "href": "slides/08-slides.html#how-many-elements-in-a-set",
    "title": "Week 08:",
    "section": "How many elements in a set?",
    "text": "How many elements in a set?\nA set can be:\n\nCountably Finite:\n\nRoll a die, there six possible outcomes \\(\\{1,2,3,4,5,6\\}\\)\n\n\n–\n\nCountably Infinite:\n\nNumber of rolls until a six appears \\({1,2,3,\\dots}\\)\n\n\n–\n\nUncountably Infinite:\n\nAll the real numbers between 0.25 and 0.75"
  },
  {
    "objectID": "slides/08-slides.html#the-fundamental-counting-principle",
    "href": "slides/08-slides.html#the-fundamental-counting-principle",
    "title": "Week 08:",
    "section": "The Fundamental Counting Principle",
    "text": "The Fundamental Counting Principle\nThe Fundamental Counting Principle says that if there \\(x\\) ways to do one thing and \\(z\\) ways to do another then, then are their are \\(x \\times z\\) total ways of doing both tasks.\nMore generally, if there are\n\nIf there are \\(j\\) tasks or decision stages\nAnd \\(k\\) choices at each decision stage \\(j\\) such that \\(n_{j,k_j} = k\\)\nThe total number of possible outcomes is the product of the number number of choices \\(k\\) and each stage, \\(j\\) \\(\\prod_{j=1}^{j} n_{j,k_j} = n_{1,k_1}*n_{2,k_2}*\\dots*n_{j,k_j}\\)\n\nThat is, just multiply the number of choices at each stage"
  },
  {
    "objectID": "slides/08-slides.html#how-many-different-outfits-could-you-have-made-me-wear",
    "href": "slides/08-slides.html#how-many-different-outfits-could-you-have-made-me-wear",
    "title": "Week 08:",
    "section": "How many different outfits could you have made me wear?",
    "text": "How many different outfits could you have made me wear?\n\nPalette: \\(\\{\\text{Fall, Winter, Spring, Summer}\\}\\)\nJacket: \\(\\{\\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\\}\\)\nTop: \\(\\{\\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\\}\\)\nPant: \\(\\{\\text{Slacks, Khakis, Jeans, Shorts}\\}\\)\nShoe: \\(\\{\\text{Dress Shoe, Dress boot, Jordans, Dunks, Basketball, Crocs}\\}\\)\nTie: \\(\\{\\text{Repp, Pattern, Knit, Bow, No tie}\\}\\)\nPattern: \\(\\{\\text{Simple, Stripes, Checks, Graphic}\\}\\)\nHow many decision stages?\nHow many choices at each stage?\nHow many possible outfits?"
  },
  {
    "objectID": "slides/08-slides.html#how-many-different-suggested-outfits-could-you-have-made-me-wear",
    "href": "slides/08-slides.html#how-many-different-suggested-outfits-could-you-have-made-me-wear",
    "title": "Week 08:",
    "section": "How many different suggested outfits could you have made me wear?",
    "text": "How many different suggested outfits could you have made me wear?\n\nHow many decision stages? \\(j=7\\)\nHow many choices at each stage?\n\nPalette: \\(\\{\\text{Fall, Winter, Spring, Summer}\\} = n_{1,4} = 4\\)\nJacket: \\(\\{\\text{Tuxedo, Tweed,Blazer, Sportcoat, No coat}\\} = n_{2,5} = 5\\)\nTop: \\(\\{\\text{Dress Shirt, Polo, Tee Shirt, Sports Jersey}\\} = n_{3,4} = 4\\)\nPant: \\(\\{\\text{Slacks, Khakis, Jeans, Shorts}\\} = n_{4,4} = 4\\)\nShoe: \\(\\{\\text{Dress Shoe, Boots, Jordans, Dunks, Basketball, Crocs}\\} = n_{5,6} = 6\\)\nTie: \\(\\{\\text{Repp, Pattern, Knit, Bowtie, No tie}\\} = n_{6,7} = 5\\)\nPattern: \\(\\{\\text{Simple, Stripes, Checks, Graphic}\\} = n_{7,4} = 4\\)\n\nHow many possible suggestions? \\(\\prod_{i=1}^{j} n_{jk} = 4 \\times 5 \\times 4 \\times 4 \\times 6 \\times 5 \\times 4 = 38,400 \\text{ outfits}\\)\n\nAnd yet you chose this:\n\n\nNaive professor in class survey: “Help me with my fit”Students: “Ok, bet. Tuxedo, crocs, and a jersey” https://t.co/NPmBZhsfen pic.twitter.com/cWlytcZ12h\n\n— Paul Testa ((ProfPaulTesta?)) March 17, 2023"
  },
  {
    "objectID": "slides/08-slides.html#perumations-and-combinations",
    "href": "slides/08-slides.html#perumations-and-combinations",
    "title": "Week 08:",
    "section": "Perumations and Combinations",
    "text": "Perumations and Combinations\nWhen we’re counting how many “events” we can construct from a set of elements, our answer depends on whether we:\n\nCan distinguish the order of elements\nAllow for elements to be repeated\n\nSituations where we can distinguish the order of elements being selected are described by Permutations\nSituations where we cannot distinguish the order of elements being selected are described by Combinations\nIn both cases, the total number of permutations or combinations, varies, depending on whether we allow repeats or not."
  },
  {
    "objectID": "slides/08-slides.html#factorials",
    "href": "slides/08-slides.html#factorials",
    "title": "Week 08:",
    "section": "Factorials",
    "text": "Factorials\nThe symbol ! is the factorial function. 5! is read as “five factorial” and is a shorthand way of writing:\n\\[5! = 5\\times 4\\times 3\\times 2 \\times 1\\]\nCombined with the fundamental counting principle, factorials let us mathematically represent situations where\n\norder does or does not matter\nrepeats are or are not allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder Matters\nRepetition Allowed\nFormula\nExample = {abc}\n\n\n\n\nPermutation\nYes\nYes\n  \\({}_{n}P_k =  n^k\\)\nP(3,3) = Pick three letters in order with replacement = \\(3^3 = 27\\) permutations\n\n\nPerumation\nYes\nNo\n  \\({}_{n}P_k = \\frac{n!}{(n-k)!}\\)\nP(3,3) = Pick three letters in order without replacement = \\(\\frac{3!}{(3-3)!} = \\frac{3*2*1}{1} = 6\\) permutations\n\n\nCombination\nNo\nNo\n  \\({}_{n}C_k = \\binom{n}{k} = \\frac{{}_{n}P_k}{k!} \\frac{n!}{k!(n-k)!}\\)\nC(3,3) = Choose three letters without replacement = \\(\\frac{3!}{3!(3-3)!} = \\frac{3*2*1}{3*2*1} = 1\\) combination\n\n\nCombination\nNo\nYes\n  \\({}_{n}C_k = \\binom{n + k - 1}{k} =\\frac{(n + k -1)!}{k!(n-1)!}\\)\nC(3,3) = Choose three letters with replacement = \\(\\frac{3+3 -1!}{3!(3-1)!} = \\frac{5*4*3*2*1}{(3*2*1)(2*1)} = 10\\) combinations"
  },
  {
    "objectID": "slides/08-slides.html#permutations-and-combinations",
    "href": "slides/08-slides.html#permutations-and-combinations",
    "title": "Week 08:",
    "section": "Permutations and Combinations",
    "text": "Permutations and Combinations\nCounting turns out to be vary useful for a number of questions/problems in social science.\nFor example, in a randomized experiment, how we randomize treatment assignments can have big implications for how we conduct statistical inference.\n\nIn short, we will “analyze as we randomize”\nCompare what we observed from one possible way of assigning treatment, to what we could have observed under different assignments, if some claim (hypothesis) were true\nSimple random assignment (equal probability, flipping a coin) yields a lot of possible assignments\nComplete random assignment, fixes the # treated (e.g. N/2), ruling cases where everyone or no-one gets the treatment\nSimilarly, paired random assignment randomizes within pairs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandomization\nTotal # of Treatment Assignments\nN=4\nN=8\nN=16\n\n\n\n\nCoin Flip\n  \\(N^2\\)\n16\n256\n65,536\n\n\nCompletely Randomized\n  \\(\\binom{N}{N/2}\\)\n6\n70\n12870\n\n\nPair-Randomized\n  \\(2^{N/2}\\)\n4\n16\n256"
  },
  {
    "objectID": "slides/08-slides.html#frequentist-interpretations-of-probability-1",
    "href": "slides/08-slides.html#frequentist-interpretations-of-probability-1",
    "title": "Week 08:",
    "section": "Frequentist interpretations of probability",
    "text": "Frequentist interpretations of probability\n\nProbabilities from a Frequentist perspective are defined by fixed and unknown parameters\nThe goal of statistics for a frequentist is to learn about these parameters from data.\nFrequentist statistics often ask questions like “What is the probability of observing some data \\(Y\\), given a hypothesis about the true value of parameter(s), \\(\\theta\\), that generated it."
  },
  {
    "objectID": "slides/08-slides.html#frequentist-interpretations-of-probability-2",
    "href": "slides/08-slides.html#frequentist-interpretations-of-probability-2",
    "title": "Week 08:",
    "section": "Frequentist interpretations of probability",
    "text": "Frequentist interpretations of probability\nFor example, suppose we wanted to test whether a coin is “fair” \\((p = Pr(Heads) = .5; q = Pr(Tails) = 1-p = .5).\\) We could:\n\nFlip a fair coin 10 times. Our estimate of the \\(Pr(H)\\) is the number of heads divided by 10. It could be 0.5, but also 0 or 1, or some number in between.\nFlip a coin 100 times and our estimate will be closer to the true \\(paramter\\).\nFlip a coin an \\(\\infty\\) amount of times and the relative frequency will converge to the true parameter/ \\((Pr(H) = \\lim_{n \\to \\infty} \\frac{n_{H}}{n} = p = 0.5 \\text{ for a fair coin})\\)"
  },
  {
    "objectID": "slides/08-slides.html#bayesian-interpretations-of-probability",
    "href": "slides/08-slides.html#bayesian-interpretations-of-probability",
    "title": "Week 08:",
    "section": "Bayesian interpretations of probability",
    "text": "Bayesian interpretations of probability\n\nFrequentist interpretations make sense for describing processes that we could easily repeat (e.g. Coin flips, Surveys, Experiments)\n\n–\n\nBut feel more convoluted when trying to describe events like “the probability of that Biden wins reelection.”\n\n–\n\nBayesian interpretations of probability view probabilities as subjective beliefs.\n\n–\n\nThe task for a Bayesian statistics is to update these prior beliefs () based on a model of the likelihood of observing some data to form new beliefs after observing the data (called posterior beliefs).\nBayesians do this using Bayes Rule, which says:\n\n\\[\\text{posterior} \\propto \\text{likelihood} \\times \\text{prior}\\] More formally:\n\\[\\underbrace{Pr(\\theta|Y)}_{\\text{Posterior}} \\propto \\underbrace{Pr(Y|\\theta)}_{\\text{Likelihood}}) \\times \\underbrace{Pr(\\theta)}_{\\text{Prior}}\\]\nbackground-image:url(“https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png”) background-size:contain"
  },
  {
    "objectID": "slides/08-slides.html#bayesian-vs-frequentists",
    "href": "slides/08-slides.html#bayesian-vs-frequentists",
    "title": "Week 08:",
    "section": "Bayesian vs Frequentists",
    "text": "Bayesian vs Frequentists\nOur two main tools for doing statistical inference in this course\n\nHypothesis Testing\nInterval Estimation\n\nFollow largely from frequentist interpretations of probability\n–\nThe differences between Bayesian and Frequentist frameworks, are both philosophical and technical in nature\n\nIs probability a relative frequency or subjective belief? How do we form and use prior beliefs\nBayesian statistics relies heavily on algorhithms for Markov Chain Monte-Carlo simulations made possible by advances in computing.\n\n–\nFor most of the questions in this course, these two frameworks will yield similar (even identical) conclusions.\n\nSometimes it’s helpful to think like a Bayesian, others, like a frequentist\nAll models are wrong, some models are useful."
  },
  {
    "objectID": "slides/08-slides.html#summary-probability",
    "href": "slides/08-slides.html#summary-probability",
    "title": "Week 08:",
    "section": "Summary: Probability",
    "text": "Summary: Probability\n\nProbability is a measure of uncertainty telling us how likely an event (or events) is (are) to occur\nProbabilities are:\n\nNon-negative\nUnitary\nAdditive\n\nTwo different interpretations of probability:\n\nFrequentists: Probability is a long run relative frequency\nBayesians: Probability reflect subjective beliefs which we update upon observing data\n\nIt helps to know how to count. When we count it matters if we do so with or without replacement and with our without regard to order"
  },
  {
    "objectID": "slides/08-slides.html#qss-example-p.-256-race-and-gender-in-the-fl-voter-file",
    "href": "slides/08-slides.html#qss-example-p.-256-race-and-gender-in-the-fl-voter-file",
    "title": "Week 08:",
    "section": "QSS Example (p. 256): Race and Gender in the FL Voter File",
    "text": "QSS Example (p. 256): Race and Gender in the FL Voter File\nFLVoters contains a random sample of 10,000 voters\n\nlibrary(qss)\ndata(\"FLVoters\")\ndim(FLVoters)\n\n[1] 10000     6\n\nFLVoters &lt;- na.omit(FLVoters)\ndim(FLVoters)\n\n[1] 9113    6\n\nhead(FLVoters)\n\n     surname county VTD age gender  race\n1     PIEDRA    115  66  58      f white\n2      LYNCH    115  13  51      m white\n4    LATHROP    115  80  54      m white\n5     HUMMEL    115   8  77      f white\n6 CHRISTISON    115  55  49      m white\n7      HOMAN    115  84  77      f white"
  },
  {
    "objectID": "slides/08-slides.html#marginal-probabilities-gender",
    "href": "slides/08-slides.html#marginal-probabilities-gender",
    "title": "Week 08:",
    "section": "Marginal Probabilities: Gender",
    "text": "Marginal Probabilities: Gender\n\n# Pr(Gender)\ntable(FLVoters$gender)\n\n\n   f    m \n4883 4230 \n\nmargin_gender &lt;- prop.table(table(FLVoters$gender))\nmargin_gender\n\n\n        f         m \n0.5358279 0.4641721"
  },
  {
    "objectID": "slides/08-slides.html#marginal-probabilities-race",
    "href": "slides/08-slides.html#marginal-probabilities-race",
    "title": "Week 08:",
    "section": "Marginal Probabilities: Race",
    "text": "Marginal Probabilities: Race\n\n# Pr(Race)\ntable(FLVoters$race)\n\n\n   asian    black hispanic   native    other    white \n     175     1194     1192       29      310     6213 \n\nmargin_race &lt;- prop.table(table(FLVoters$race))\nmargin_race\n\n\n      asian       black    hispanic      native       other       white \n0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291"
  },
  {
    "objectID": "slides/08-slides.html#cross-tab-of-race-and-gender",
    "href": "slides/08-slides.html#cross-tab-of-race-and-gender",
    "title": "Week 08:",
    "section": "Cross Tab of race and gender",
    "text": "Cross Tab of race and gender\n\ntable(FLVoters$gender,FLVoters$race, useNA = \"ifany\")\n\n   \n    asian black hispanic native other white\n  f    83   678      666     17   158  3281\n  m    92   516      526     12   152  2932"
  },
  {
    "objectID": "slides/08-slides.html#joint-probability-of-race-and-gender",
    "href": "slides/08-slides.html#joint-probability-of-race-and-gender",
    "title": "Week 08:",
    "section": "Joint probability of race and gender",
    "text": "Joint probability of race and gender\n\njoint_p &lt;- prop.table(table(FLVoters$gender,FLVoters$race))\njoint_p\n\n   \n          asian       black    hispanic      native       other       white\n  f 0.009107868 0.074399210 0.073082410 0.001865467 0.017337869 0.360035115\n  m 0.010095468 0.056622408 0.057719741 0.001316800 0.016679469 0.321738176\n\n\nMarginal probabilities from joint probabilities:\n\nprob_tab&lt;-addmargins(table(FLVoters$gender,FLVoters$race))\nprob_tab\n\n     \n      asian black hispanic native other white  Sum\n  f      83   678      666     17   158  3281 4883\n  m      92   516      526     12   152  2932 4230\n  Sum   175  1194     1192     29   310  6213 9113\n\n\nMarginal probabilities from joint probabilities: Gender\n\n# Marginal probability of Gender\nprob_tab[,\"Sum\"]/prob_tab[\"Sum\",\"Sum\"]\n\n        f         m       Sum \n0.5358279 0.4641721 1.0000000 \n\nprop.table(table(FLVoters$gender))\n\n\n        f         m \n0.5358279 0.4641721 \n\n\nMarginal probabilities from joint probabilities: Race\n\n# Marginal probability of Race\nprob_tab[\"Sum\",]/prob_tab[\"Sum\",\"Sum\"]\n\n      asian       black    hispanic      native       other       white \n0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 \n        Sum \n1.000000000 \n\nprop.table(table(FLVoters$race))\n\n\n      asian       black    hispanic      native       other       white \n0.019203336 0.131021617 0.130802151 0.003182267 0.034017338 0.681773291 \n\n\nTotal Probability: Pr(Black)\n\\[Pr(Black)=Pr(Black \\cap female) + Pr(Black \\cap male)\\]\n\nprop.table(table(FLVoters$gender,FLVoters$race))[\"f\",\"black\"] +\n  prop.table(table(FLVoters$gender,FLVoters$race))[\"m\",\"black\"]\n\n[1] 0.1310216\n\nprop.table(table(FLVoters$race))[\"black\"]\n\n    black \n0.1310216 \n\n\nConditional Probability: Pr(Black|Female)\n\\[Pr(Black|Female) = \\frac{Pr(Black \\text{ and } Female)}{Pr(Female)} \\approx \\frac{0.074}{0.536} \\approx 0.139\\]"
  },
  {
    "objectID": "slides/08-slides.html#independence",
    "href": "slides/08-slides.html#independence",
    "title": "Week 08:",
    "section": "Independence",
    "text": "Independence\nEvents \\(A\\) and \\(B\\) are independent if\n\\[Pr(A|B) = Pr(A) \\text{ and } Pr(B|A) = Pr(B)\\]\nConceputally, If \\(A\\) and \\(B\\) are independent knowing whether \\(B\\) occurred, tells us nothing about \\(A\\), and so the conditional probability of \\(A\\) given \\(B\\), \\(Pr(A|B)\\) is equal to the unconditional, or marginal probability, \\(Pr(A)\\)\nFormally, two events are statistically independent if and only if the joint probability is equal to product of the marginal probabilities\n\\[Pr(A\\text{ and }B) = Pr(A)Pr(B)\\]\nRace and gender are approximately independent in these data\n.pull-left[\n\nplot(x = c(margin_race*margin_gender[\"f\"]),\n     y = joint_p[\"f\",],\n     xlim = c(0,.4),\n     ylim = c(0,.4),\n     xlab = \"Pr(race)*Pr(female)\",\n     ylab = \"Pr(race and gender)\"\n             )\nabline(0,1)\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "slides/08-slides.html#conditional-independence",
    "href": "slides/08-slides.html#conditional-independence",
    "title": "Week 08:",
    "section": "Conditional Independence",
    "text": "Conditional Independence\nWe can extend the concept of independence to situations with more than two events:\nIf events \\(A\\), \\(B\\), and \\(C\\) are jointly independent then:\n\\[Pr(A \\cap B \\cap C) = Pr(A)Pr(B)Pr(C)\\]\nJoint independence implies pairwise independence and conditional independence:\n\\[Pr(A \\cap B | C) = Pr(A|C)Pr(B|C)\\] But not the reverse."
  },
  {
    "objectID": "slides/08-slides.html#bayes-rule",
    "href": "slides/08-slides.html#bayes-rule",
    "title": "Week 08:",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nBayes rule is theorem for how we should update our beliefs about \\(A\\) given that \\(B\\) occurred:\n\\[Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}\\] Where\n\n\\(Pr(A)\\) is called the prior probability of A (our initial belief)\n\\(Pr(A|B)\\) is called the posterior probability of A given B (our updated belief after observing B)\n\nWhat’s the probability you have Covid-19 given a positive test\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}\\]\n              | Has Covid | Does Not Have Covid |\n||-|| | Test Positive | True Positive | False Positive | | Test Negative | False Negative | True Negative |\nWhat’s the probability you have Covid-19 given a positive test\nLet’s assume:\n\n1 out 100 people have Covid-19\nOur test correctly identifies true positives 95 percent of the time (sensitivity = True Positive Rate)\nOur test correctly identifies true negatives 95 percent of the time (specifity = True Negative Rate)\n\nWhat’s the probability you have Covid-19 given a positive test\nIn a sample of 100,000 people then:\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}\\]\n              | Has Covid | Does Not Have Covid |\n||-|| | Test Positive | 950 | 4950 | | Test Negative | 50 | 94050 |\n\n\\(Pr(+|Covid) = 950/(1000) \\approx 0.95\\)\n\n\\(Pr(Covid) = 1000/100000 \\approx 0.01\\)\n\n\\(Pr(+) = Pr(+|Covid) + Pr(+|Covid)= .95*.01 + .05*.99 \\approx 0.059\\)\n\nWhat’s the probability you have Covid-19 given a positive test\nConverting this table into marginal probabilities\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}\\]\n              | Has Covid | Does Not Have Covid | Prob(Test)\n||-||| | Test Positive | 0.0095 | 0.0495 |0.059 | Test Negative | 0.0005 | 0.9505 | 0.941 | Prob(Covid) | 0.01 | 0.99 | 1\n\\[Pr(Covid|Test +) = \\frac{Pr(+|Covid)Pr(Covid)}{Pr(Test +)}\\] \\[Pr(Covid|Test +) = \\frac{0.95 \\times 0.01}{0.059} \\approx 0.16\\] ## What if we took a second test?\nWe could use posterior belief as our prior:\n\\[Pr(Covid|2nd +) = \\frac{0.95 \\times 0.16}{0.16\\times0.95 + (1-0.16)\\times 0.95 } \\approx 0.783\\] Now we’re much more confident that we have Covid-19"
  },
  {
    "objectID": "slides/08-slides.html#random-variables",
    "href": "slides/08-slides.html#random-variables",
    "title": "Week 08:",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables assign numeric values to each event in an experiment.\n\nMutually exclusive and exhaustive, together cover the entire sample space.\n\nDiscrete random variables take on finite, or countably infinite distinct values.\nContinuous variables can take on an uncountably infinite number of values."
  },
  {
    "objectID": "slides/08-slides.html#example-toss-two-coins",
    "href": "slides/08-slides.html#example-toss-two-coins",
    "title": "Week 08:",
    "section": "Example: Toss Two Coins",
    "text": "Example: Toss Two Coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)"
  },
  {
    "objectID": "slides/08-slides.html#probability-distributions",
    "href": "slides/08-slides.html#probability-distributions",
    "title": "Week 08:",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\nThe can be represented in terms of:\n\nProbability Mass/Density Functions\n\nDiscrete variables have probability mass functions (PMF)\nContinuous variables have probability density functions (PDF)\n\nCumulative Density Functions\n\nDiscrete: Summation of discrete probabilities\nContinuous: Integration over a range of values"
  },
  {
    "objectID": "slides/08-slides.html#discrete-distributions",
    "href": "slides/08-slides.html#discrete-distributions",
    "title": "Week 08:",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nProbability Mass Function (pmf): \\(f(x)=p(X=x)\\)\nAssigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\nCumulative Distribution Function (cdf) \\(F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)\\)\n\nSum of the probability mass for events less than or equal to \\(x_j\\)"
  },
  {
    "objectID": "slides/08-slides.html#example-toss-two-coins-1",
    "href": "slides/08-slides.html#example-toss-two-coins-1",
    "title": "Week 08:",
    "section": "Example: Toss Two coins",
    "text": "Example: Toss Two coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)\n\n\\(f(X=0)=p(X=0)=1/4\\)\n\\(f(X=1)=p(X=1)=1/2\\)\n\\(F(X\\leq 1) = p(X \\leq 1)= 3/4\\)\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X&lt;=2) = 1/6 + 1/6 = 1/3"
  },
  {
    "objectID": "slides/08-slides.html#continuous-distributions",
    "href": "slides/08-slides.html#continuous-distributions",
    "title": "Week 08:",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nProbability Density Functions (PDF): \\(f(x)\\)\n\nAssigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\nBut… since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n\nCumulative Distribution Function (CDF) \\(F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx\\)\n\nInstead of summing up to a specific value (discrete) we integrate over all possible values up to \\(x\\)\nProbability of having a value less than x"
  },
  {
    "objectID": "slides/08-slides.html#integrals",
    "href": "slides/08-slides.html#integrals",
    "title": "Week 08:",
    "section": "Integrals",
    "text": "Integrals\nFirst, a brief aside on integral calculus:\nWhat’s the area of the rectangle? \\(base\\times height\\)"
  },
  {
    "objectID": "slides/08-slides.html#integrals-1",
    "href": "slides/08-slides.html#integrals-1",
    "title": "Week 08:",
    "section": "Integrals",
    "text": "Integrals\nHow would we find the area under a curve?"
  },
  {
    "objectID": "slides/08-slides.html#integrals-2",
    "href": "slides/08-slides.html#integrals-2",
    "title": "Week 08:",
    "section": "Integrals",
    "text": "Integrals\nWell suppose we added up the areas of a bunch of rectangles roughly whose height’s approximated the height of the curve?\n\nCan we do any better?"
  },
  {
    "objectID": "slides/08-slides.html#integrals-3",
    "href": "slides/08-slides.html#integrals-3",
    "title": "Week 08:",
    "section": "Integrals",
    "text": "Integrals\nLet’s make the rectangles smaller\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get’s even better:"
  },
  {
    "objectID": "slides/08-slides.html#link-between-pdf-and-cdf",
    "href": "slides/08-slides.html#link-between-pdf-and-cdf",
    "title": "Week 08:",
    "section": "Link between PDF and CDF",
    "text": "Link between PDF and CDF\nIf \\[F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx \\]\nThen by the fundamental theorem of calculus\n\\[\\frac{d}{dx}F(x)=f(x)\\]\nIn words\n\nthe PDF \\((f(x))\\)is the derivative (rate of change) of the CDF \\((F(X))\\)\nthe CDF describes the area under the curve defined by f(x) up to x"
  },
  {
    "objectID": "slides/08-slides.html#properties-of-the-cdf",
    "href": "slides/08-slides.html#properties-of-the-cdf",
    "title": "Week 08:",
    "section": "Properties of the CDF",
    "text": "Properties of the CDF\n\n\\(0\\leq F(x) \\leq 1\\)\n\\(F\\) is non-decreasing and right continuous\n\\(\\lim_{x\\to-\\infty}F(x)=0\\)\n\\(\\lim_{x\\to\\infty}F(x)=1\\)\nFor all \\(a,b \\in \\mathbb{R}\\) s.t. \\(a&lt;b\\)\n\n\\[p(a &lt; X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx \\]"
  },
  {
    "objectID": "slides/08-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "href": "slides/08-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "title": "Week 08:",
    "section": "Recall the PMF and CDF of a die",
    "text": "Recall the PMF and CDF of a die"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-probability",
    "href": "slides/08-slides.html#whats-the-probability",
    "title": "Week 08:",
    "section": "What’s the probability",
    "text": "What’s the probability\n\n\\(p(X=1)...p(X=6) = 1/6\\)\n\\(p( 2 &lt; X \\leq 5) = F(5)-F(2)=5/6-2/6=3/6=1/2\\)"
  },
  {
    "objectID": "slides/08-slides.html#bernoulli-random-variables",
    "href": "slides/08-slides.html#bernoulli-random-variables",
    "title": "Week 08:",
    "section": "Bernoulli Random Variables",
    "text": "Bernoulli Random Variables\nLet’s start with our old friend the coin flip\nA coin flip is an example of a Bernoulli random variable defined by 1 parameter \\(p\\), the probability of success. It has a pmf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF of\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x&lt;1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x&lt;1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array}\n    \\right.\\]\nNote that in our coin flip example \\(p=0.5\\) but it need not. Just imagine a weighted coin like the Patriots use at Foxborough"
  },
  {
    "objectID": "slides/08-slides.html#uniform-distribution",
    "href": "slides/08-slides.html#uniform-distribution",
    "title": "Week 08:",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nOur fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range \\([a,b]\\), the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                \\frac{1}{b-a}& \\mathrm{if\\ } a \\leq x\\leq b \\\\\n                0 & \\text{otherwise} \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF:\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                        0 & x &lt;a \\\\\n                \\frac{x-a}{b-a}& \\mathrm{if\\ } a \\leq x &lt; b \\\\\n                1 & x \\geq b \\\\\n        \\end{array}\n    \\right.\\]\nWe won’t run into uniform distributions all that often except in examples like rolling a fair sided die, but often they’re used in Bayesian analysis as a form of uninformative prior."
  },
  {
    "objectID": "slides/08-slides.html#binomial-distributions",
    "href": "slides/08-slides.html#binomial-distributions",
    "title": "Week 08:",
    "section": "Binomial Distributions",
    "text": "Binomial Distributions\nThe binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.\nThe key parameters are the number of trials \\(n\\) and the probability of success for each trial \\(p\\) and the pdf of a binomial distribution is:\n\\[f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n\\] So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads\n\nThe binomial distribution will come in handy when trying to model binary outcomes."
  },
  {
    "objectID": "slides/08-slides.html#poisson-distributions",
    "href": "slides/08-slides.html#poisson-distributions",
    "title": "Week 08:",
    "section": "Poisson Distributions",
    "text": "Poisson Distributions\nWhat would happen if you let the \\(n\\) in a binomial distribution go to infinity and \\(p\\) go to 0 so that \\(np\\) stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter \\(\\lambda\\) which represents rate at which events occur.\n\\[f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\nWe use these distributions to try and predict to predict the probability of a given number of events occurring in a fixed interval of time. Things like how many acts of political participation would a voter engage in over a year."
  },
  {
    "objectID": "slides/08-slides.html#geometric-distributions",
    "href": "slides/08-slides.html#geometric-distributions",
    "title": "Week 08:",
    "section": "Geometric Distributions",
    "text": "Geometric Distributions\nWhat if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:\n\\[f(x)=p(1-p) ^{x}\\]\nGeometric and related distributions are useful for describing the time until an event occurs"
  },
  {
    "objectID": "slides/08-slides.html#exponential-distributions",
    "href": "slides/08-slides.html#exponential-distributions",
    "title": "Week 08:",
    "section": "Exponential Distributions",
    "text": "Exponential Distributions\nTaking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a \\(\\lambda = \\frac{1}{\\beta}\\) rate parameter\n\\[f(x)=\\frac{1}{\\beta}\\exp\\left[-x/\\beta\\right]\\]\nCioffa-Revilla (1984) uses an exponential distribution to model the stability of Italian governments."
  },
  {
    "objectID": "slides/08-slides.html#normal-distribution",
    "href": "slides/08-slides.html#normal-distribution",
    "title": "Week 08:",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nFinally, there’s the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter \\(\\mu\\) that determines the center of a distribution and a scale parameter \\(\\sigma^2\\) that determines the spread of a distribution\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]\\]\nStandard normal: \\(X \\sim N(\\mu =0,\\sigma^2=1)\\)\n\n\n\n\n\n\n\n\n\n\nAs we’ll see normal distributions tend to arise when ever you’re summing variables.\nThat is sum together a bunch of values from almost any distribution and the distribution of their sums tends to follow a normal distribution.\nSince lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)\n\nConsider a binomial distribution with N=100 and p=.5.\nThe pmf of this variable (black lollipops) follows a distribution that’s closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.\nA relationship explained more generally by the Central Limit Theorem, which we’ll cover next week.\n\n\n\n\n\n\n\n\n\nWhat’s the \\(p(X \\leq 0)\\) for a normal distirbution with mean 0 and sd 1\nSince the normal distribution is so common, it’s useful to get practice working with it’s pdf and cdf.\nConsider the following question: If X is normally distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\), what’s the probability that X is less than 0 \\(p(X\\leq0)=?\\) We could solve:\n\\[\\int_{-\\infty}^{0}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.5\\]\nBut R’s pnorm() function will quickly tell us\n\n\\(p(X\\leq0)=\\) 0.5\n\nAnd we can visualize this as follows:\n\n\n\n\n\n\n\n\n\nConsider some other questions?\n\n\\(p(X=0)=0\\)\n\nThe probability that a continuous variable is exactly some value is always 0.\n\n\\(p(X&lt;0)=0.5\\)\n\\(p(-1&lt; X&lt; 1)\\)\n\\(p(-2&lt; X&lt; 2)\\)\n\np(-1 &lt; X &lt; 1)\n\n\\(p(-1&lt; X&lt; 1)=pr(X&lt;1)-pr(X&lt;-1)\\)\n\n\\[\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.841-0.158=0.682\\]\n\n\n\n\n\n\n\n\n\np(-2 &lt; X &lt; 2)\n\n\\(p(-2&lt; X\\leq 2)=\\) 0.9544997\n\n\n\n\n\n\n\n\n\n\nWe’ll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0."
  },
  {
    "objectID": "slides/08-slides.html#expected-value",
    "href": "slides/08-slides.html#expected-value",
    "title": "Week 08:",
    "section": "Expected Value",
    "text": "Expected Value\nA (probability) weighted average of the possible outcomes of a random variable, often labeled \\(\\mu\\)\nDiscrete:\n\\[\\mu_X=E(X)=\\sum xp(x)\\]\nContinuous\n\\[\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx\\]"
  },
  {
    "objectID": "slides/08-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "href": "slides/08-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "title": "Week 08:",
    "section": "What’s the expected value of a 1 roll of fair die?",
    "text": "What’s the expected value of a 1 roll of fair die?\n\\[\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-slides.html#properties-of-expected-values",
    "href": "slides/08-slides.html#properties-of-expected-values",
    "title": "Week 08:",
    "section": "Properties of Expected Values",
    "text": "Properties of Expected Values\n\n\\(E(c)=c\\)\n\\(E(a+bX)=a+bE[X]\\)\n\\(E[E[X]]=X\\)\n\\(E[E[Y|X]]=E[Y]\\)\n\\(E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx\\)\n\\(E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)\\)\n\\(E[XY]=E[X]E[Y]\\) if \\(X\\) and \\(Y\\) are independent"
  },
  {
    "objectID": "slides/08-slides.html#how-many-times-would-you-have-to-roll-a-fair-die-to-get-all-six-sides",
    "href": "slides/08-slides.html#how-many-times-would-you-have-to-roll-a-fair-die-to-get-all-six-sides",
    "title": "Week 08:",
    "section": "How many times would you have to roll a fair die to get all six sides?",
    "text": "How many times would you have to roll a fair die to get all six sides?\nWe can think of this as the sum of the expected values for a series of geometric distributions with varying probabilities of success. The expected value of a geometric variable is:\n\\[\\begin{align*}E(X)&=\\sum_{k=1}^{\\infty}kp(1-p)^{k-1} \\\\\n&=p\\sum_{k=1}^{\\infty}k(1-p)^{k-1} \\\\\n&=p\\left(-\\frac{d}{dp}\\sum_{k=1}^{\\infty}(1-p)^k\\right) \\text{(Chain rule)} \\\\\n&=p\\left(-\\frac{d}{dp}\\frac{1-p}{p}\\right) \\text{(Geometric Series)} \\\\\n&=p\\left(\\frac{d}{dp}\\left(1-\\frac{1}{p}\\right)\\right)=p\\left(\\frac{1}{p^2}\\right)=\\frac1p\\end{align*}\\]\nFor this question, we need to calculate the probability of success, p, after getting a side we need.\nThe probability of getting a side you need on your first role is 1. The probability of getting a side you need on the second role, is 5/6 and so the expected number of roles is 6/5, and so the expected number of rolls to get all six is:\n\nev &lt;- c()\nfor(i in 6:1){\n  ev[i] &lt;- 6/i\n  \n}\n# Expected rolls for each 1 through 6th side\nrev(ev)\n\n[1] 1.0 1.2 1.5 2.0 3.0 6.0\n\n# Total \nsum(ev)\n\n[1] 14.7"
  },
  {
    "objectID": "slides/08-slides.html#variance",
    "href": "slides/08-slides.html#variance",
    "title": "Week 08:",
    "section": "Variance",
    "text": "Variance\nIf \\(X\\) has a finite mean \\(E[X]=\\mu\\), the \\(E[(X-\\mu)^2]\\) is finite and called the variance of \\(X\\) which we write as \\(\\sigma^2\\) or \\(Var[X]\\).\nNote:\n\\[\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}\\]\n\n“The variance of X is equal to the expected value of X-squared, minus the square of X’s expected value.”\n\\(\\sigma^2=E[X^2]-E[X]^2\\) is a useful identity in proofs and derivations"
  },
  {
    "objectID": "slides/08-slides.html#variance-and-standard-deviations",
    "href": "slides/08-slides.html#variance-and-standard-deviations",
    "title": "Week 08:",
    "section": "Variance and Standard Deviations",
    "text": "Variance and Standard Deviations\nWe often think of variances \\(Var[X]\\) as describing the spread of a distribution\n\\[\\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2\\]\nA standard deviation is just the square root of the variance\n\\[\\sigma=\\sqrt{Var[X]}\\]"
  },
  {
    "objectID": "slides/08-slides.html#covariance",
    "href": "slides/08-slides.html#covariance",
    "title": "Week 08:",
    "section": "Covariance",
    "text": "Covariance\nCovariance measures the degree to which two random variables vary together.\n\n\\(Cov[X,Y] \\to +\\) An increase in \\(X\\) tends to be larger than its mean when \\(Y\\) is larger than its mean\n\n\\[Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\\]"
  },
  {
    "objectID": "slides/08-slides.html#properties-of-variance-and-covariance",
    "href": "slides/08-slides.html#properties-of-variance-and-covariance",
    "title": "Week 08:",
    "section": "Properties of Variance and Covariance",
    "text": "Properties of Variance and Covariance\n\n\\(Cov[X,Y]=E[XY]-E[X]E[Y]\\)\n\\(Var[X]=E[X^2]-(E[X])^2\\)\n\\(Var[X|Y]=E[X^2|Y]-(E[X|Y])^2\\)\n\\(Cov[X,Y]=Cov[X,E[Y|X]]\\)\n\\(Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]\\)\n\\(Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]\\)\n\nWYNK: Honestly, for this class, you won’t need to know these properties. They’ll show up in proofs and theorems and become important when you’re trying to evaluate properties of an estimator (isn’t unbiased, is it “efficient”, or consistent does it have minimum variance?) but that’s for another day/course."
  },
  {
    "objectID": "slides/08-slides.html#correlation",
    "href": "slides/08-slides.html#correlation",
    "title": "Week 08:",
    "section": "Correlation",
    "text": "Correlation\n\nThe correlation between \\(X\\) and \\(Y\\) is simply the covariance of \\(X\\) and \\(Y\\) divided by the standard deviation of each.\n\n\\[\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}\\]\n\nNormalize covariance to a scale that runs between [-1,1]"
  },
  {
    "objectID": "slides/08-slides.html#question-if-two-variables-have-zero-covariance-are-they-independent",
    "href": "slides/08-slides.html#question-if-two-variables-have-zero-covariance-are-they-independent",
    "title": "Week 08:",
    "section": "Question: If two variables have zero covariance, are they independent?",
    "text": "Question: If two variables have zero covariance, are they independent?\nNo\nA trivial example\n\n\\(p(X=x)=1/3\\) for \\(x = -1,0,1\\) and let \\(Y=X^2\\)\nY clearly depends on X even though their covariance is 0\n\n\np=1/3\nx=-1:1\ny=x^2\ncor(x,y)\n\n[1] 0"
  },
  {
    "objectID": "slides/08-slides.html#summary-random-variables-and-probability-distributions",
    "href": "slides/08-slides.html#summary-random-variables-and-probability-distributions",
    "title": "Week 08:",
    "section": "Summary: Random Variables and Probability Distributions",
    "text": "Summary: Random Variables and Probability Distributions\n\nRandom variables assign numeric values to each event in an experiment.\nProbability distributions assign probabilities to the values that a Random variable can take.\n\nDiscrete distributions are despribed by their pmf and cdf\nContinuous distributions by their pdf and cdf\n\nProbability distributions let us describe the data generating process and encode information about the world into our models\n\nThere are lots of distributions\nDon’t worry about memorizing formulas\nDo develop intuitions about the nature of your data generating process (Is my outcome continuous or disrecte, binary or count, etc.)\n\nTwo key features of probability distributions are their:\n\nExpected values\nVariances\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/04-slides-old.html#general-plan",
    "href": "slides/04-slides-old.html#general-plan",
    "title": "Week 04:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\n\nPackages\nData\n\nReview\n\nStatistical programming\nDescriptive statistics\nData visualization\nCausal inference\n\nExperimental vs Observational Studies\nThree Approaches to Covariate Adjustment\n\nSubclassification\nMatching\nRegression\n\n\nTuesday:\n\nThree Designs for Causal Inference in Observational Studies\n\nDifference in Differences\nRegression Discontinuity\nInstrumental Variables"
  },
  {
    "objectID": "slides/04-slides-old.html#what-youll-learn",
    "href": "slides/04-slides-old.html#what-youll-learn",
    "title": "Week 04:",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nCausal inference in observational and experimental studies is about counterfactual comparisons\nIn observational studies, to make causal claims we generally make some assumption of conditional independence:\n\n\\[\nY_i(1),Y_i(0) \\perp D_i |X_i\n\\]\n\nThe credibility of this assumption depends less on the data, and more on how the data were generated.\n\nSelection on Observables is rarely a credible assumption\n\nObservational designs that produce credible causal inference, leverage aspects of the world that create natural experiments\nYou should be able to describe the logic and assumptions of common designs in social science\n\nDifference-in-Differences: Parallel Trends\nRegression Discontiniuity: Continuity at the cutoff\nInstrumental Variables: Instruments need to be Relevant and Exogenous\n\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/04-slides-old.html#new-packages",
    "href": "slides/04-slides-old.html#new-packages",
    "title": "Week 04:",
    "section": "New packages",
    "text": "New packages\nLet’s try installing these packages manually:\n\ninstall.packages(\"dataverse\")\n\n\ninstall.packages(\"tidycensus\")\n\n\ninstall.packages(\"easystats\", repos = \"https://easystats.r-universe.dev\")\n\n\ninstall.packages(\"DeclareDesign\")\n\n\nSome further guidance available here\nParticular, for next week, please follow the instructions to install a Census API key in R"
  },
  {
    "objectID": "slides/04-slides-old.html#packages-for-today",
    "href": "slides/04-slides-old.html#packages-for-today",
    "title": "Week 04:",
    "section": "Packages for today",
    "text": "Packages for today"
  },
  {
    "objectID": "slides/04-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/04-slides-old.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 04:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/04-slides-old.html#load-packages-for-today",
    "href": "slides/04-slides-old.html#load-packages-for-today",
    "title": "Week 04:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT     tidyverse     lubridate       forcats \n         TRUE          TRUE          TRUE          TRUE          TRUE \n        haven      labelled         ggmap       ggrepel      ggridges \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggthemes        ggpubr        GGally        scales       dagitty \n         TRUE          TRUE          TRUE          TRUE          TRUE \n        ggdag       ggforce       COVID19          maps       mapdata \n         TRUE          TRUE          TRUE          TRUE          TRUE \n          qss    tidycensus     dataverse DeclareDesign     easystats \n         TRUE          TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today"
  },
  {
    "objectID": "slides/04-slides-old.html#load-data-for-today",
    "href": "slides/04-slides-old.html#load-data-for-today",
    "title": "Week 04:",
    "section": "Load Data for today",
    "text": "Load Data for today\nWe’ll load the data we need in some of the examples below as well:\n\n## Lab data\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))\n\n## RDD data\nlibrary(qss)\ndata(MPs)"
  },
  {
    "objectID": "slides/04-slides-old.html#review-what-have-we-covered",
    "href": "slides/04-slides-old.html#review-what-have-we-covered",
    "title": "Week 04:",
    "section": "Review: What have we covered",
    "text": "Review: What have we covered\n\nStatistical Programming\n\nSlides 02 on Data Wrangling\nCheat sheets\nQSS: Chapter 1.3, Chapter 2.2\n\n\n–\n\nDescriptive statistics\n\nSlides 01, Lab 01\nQSS: Chapter 2.6, Chapter 3,6, Exercises 1.5,\n\n\n–\n\nData visualization as tool for descriptive inference\n\nSlides 02, Lab 02\nggplot2 cheat sheet\nQSS: Chapter 1.3, Chapter 2.2, Chapter 3.3, 3.6\n\n\n–\n\nCausal inference in experimental designs\n\nSlides 03, Lab 03\nQSS: Chapter 2.3, 2.4\n\n\nclass:inverse, middle, center # 🔍 ## Statistical Programming"
  },
  {
    "objectID": "slides/04-slides-old.html#mapping-concepts-to-code",
    "href": "slides/04-slides-old.html#mapping-concepts-to-code",
    "title": "Week 04:",
    "section": "Mapping Concepts to Code",
    "text": "Mapping Concepts to Code\n\n\n\nYou're learning how to map conceptual tasks to commands in R\n\n\nSkill\nCommon Commands\n\n\n\n\nSetup R\nlibrary(), ipak()\n\n\nLoad data\nread_csv(), load()\n\n\nGet HLO of data\ndf$x, glimpse(), table(), summary()\n\n\nTransform data\n&lt;-, mutate(), ifelse(), case_when()\n\n\nReshape data\npivot_longer(), left_join()\n\n\nSummarize data numerically\nmean(), median(), summarise(), group_by()\n\n\nSummarize data graphically\nggplot(), aes(), geom_"
  },
  {
    "objectID": "slides/04-slides-old.html#mapping-concepts-to-code-1",
    "href": "slides/04-slides-old.html#mapping-concepts-to-code-1",
    "title": "Week 04:",
    "section": "Mapping Concepts to Code",
    "text": "Mapping Concepts to Code\n\nTakes time and practice\nDon’t be afraid to FAAFO\nDon’t worry about memorizing everything.\nStatistical programming is necessary to actually do empirical research\nLearning to code will help us understand statistical concepts.\nLearning to think programmatically and algorithmically will help us tackle complex problems"
  },
  {
    "objectID": "slides/04-slides-old.html#data-wrangling-from-the-lab",
    "href": "slides/04-slides-old.html#data-wrangling-from-the-lab",
    "title": "Week 04:",
    "section": "Data Wrangling from the Lab:",
    "text": "Data Wrangling from the Lab:\n\ndf %&gt;%\n  # Calculate difference between Survey and Non-Survey Takers\n  group_by(treatment_assigned) %&gt;%\n  filter(!is.na(treatment_assigned)) %&gt;%\n  # Calculate means for variables that start with \"therm_trans\"\n  summarise(across(starts_with(\"therm_trans\"), \\(x) mean(x, na.rm = TRUE)))%&gt;%\n  # Transpose (flip) data\n  pivot_longer(\n    cols = starts_with(\"therm_trans\"),\n    names_to = \"Covariate\",\n    values_to = \"Means\"\n  ) %&gt;%\n  # Gather means into separate columns by treatment status\n  pivot_wider(\n    names_from = treatment_assigned,\n    values_from = Means\n  )%&gt;%\n  # Calculate ATEs, add labels\n  mutate(\n    ATE = `Trans-Equality` - Recycling,\n    Days = c(0,3,21,42,90)\n    )-&gt; ate_df\n\n\ndf %&gt;%\n  # Calculate difference between Survey and Non-Survey Takers\n  group_by(treatment_assigned) %&gt;%\n  filter(!is.na(treatment_assigned)) %&gt;%\n  # Calculate means for variables that start with \"therm_trans\"\n  summarise(across(starts_with(\"therm_trans\"), \\(x) mean(x, na.rm = TRUE))) #&lt;&lt;\n\n\n\n# A tibble: 2 × 6\n  treatment_assigned therm_trans_t0 therm_trans_t1 therm_trans_t2 therm_trans_t3\n  &lt;chr&gt;                       &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 Recycling                    52.9           54.1           54.6           52.5\n2 Trans-Equality               53.6           60.8           59.1           58.7\n# ℹ 1 more variable: therm_trans_t4 &lt;dbl&gt;\n\n\n\ndf %&gt;%\n  # Calculate difference between Survey and Non-Survey Takers\n  group_by(treatment_assigned) %&gt;%\n  filter(!is.na(treatment_assigned)) %&gt;%\n  # Calculate means for variables that start with \"therm_trans\"\n  summarise(across(starts_with(\"therm_trans\"), \\(x) mean(x, na.rm = TRUE))) %&gt;% \n  # Transpose (flip) data\n  pivot_longer(  #&lt;&lt;\n    cols = starts_with(\"therm_trans\"), #&lt;&lt;\n    names_to = \"Covariate\", #&lt;&lt;\n    values_to = \"Means\" #&lt;&lt;\n  ) #&lt;&lt;\n\n\n\n# A tibble: 10 × 3\n   treatment_assigned Covariate      Means\n   &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;\n 1 Recycling          therm_trans_t0  52.9\n 2 Recycling          therm_trans_t1  54.1\n 3 Recycling          therm_trans_t2  54.6\n 4 Recycling          therm_trans_t3  52.5\n 5 Recycling          therm_trans_t4  53.4\n 6 Trans-Equality     therm_trans_t0  53.6\n 7 Trans-Equality     therm_trans_t1  60.8\n 8 Trans-Equality     therm_trans_t2  59.1\n 9 Trans-Equality     therm_trans_t3  58.7\n10 Trans-Equality     therm_trans_t4  58.7\n\n\n\ndf %&gt;%\n  group_by(treatment_assigned) %&gt;%\n  filter(!is.na(treatment_assigned)) %&gt;%\n  summarise(across(starts_with(\"therm_trans\"), \\(x) mean(x, na.rm = TRUE)))%&gt;%\n  pivot_longer(\n    cols = starts_with(\"therm_trans\"),\n    names_to = \"Covariate\",\n    values_to = \"Means\"\n  ) %&gt;%\n  # Gather means into separate columns by treatment status\n  pivot_wider(  #&lt;&lt;\n    names_from = treatment_assigned,  #&lt;&lt;\n    values_from = Means  #&lt;&lt;\n  )  #&lt;&lt;\n\n\n\n# A tibble: 5 × 3\n  Covariate      Recycling `Trans-Equality`\n  &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt;\n1 therm_trans_t0      52.9             53.6\n2 therm_trans_t1      54.1             60.8\n3 therm_trans_t2      54.6             59.1\n4 therm_trans_t3      52.5             58.7\n5 therm_trans_t4      53.4             58.7\n\n\n\ndf %&gt;%\n  group_by(treatment_assigned) %&gt;%\n  filter(!is.na(treatment_assigned)) %&gt;%\n  summarise(across(starts_with(\"therm_trans\"), \\(x) mean(x, na.rm = TRUE)))%&gt;%\n  pivot_longer(\n    cols = starts_with(\"therm_trans\"),\n    names_to = \"Covariate\",\n    values_to = \"Means\"\n  ) %&gt;%\n  pivot_wider(\n    names_from = treatment_assigned,\n    values_from = Means\n  )%&gt;%\n  # Calculate ATEs, add labels, save output\n  mutate(  #&lt;&lt;\n    ATE = `Trans-Equality` - Recycling, #&lt;&lt;\n    Days = c(0,3,21,42,90) #&lt;&lt;\n    )-&gt; ate_df #&lt;&lt;\nate_df #&lt;&lt;\n\n\n\n# A tibble: 5 × 5\n  Covariate      Recycling `Trans-Equality`   ATE  Days\n  &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 therm_trans_t0      52.9             53.6 0.664     0\n2 therm_trans_t1      54.1             60.8 6.76      3\n3 therm_trans_t2      54.6             59.1 4.49     21\n4 therm_trans_t3      52.5             58.7 6.23     42\n5 therm_trans_t4      53.4             58.7 5.30     90\n\n\nclass: inverse, middle, center # 🔍 ## Descriptive statistics"
  },
  {
    "objectID": "slides/04-slides-old.html#descriptive-statistics",
    "href": "slides/04-slides-old.html#descriptive-statistics",
    "title": "Week 04:",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nDescriptive statistics help us describe what’s typical of our data\n\n\n\n\n- What’s a typical value in our data - Mean - Median - Mode\n\n\n\n\n- How much do our data vary? - Variance - Standard deviation\n\n\n\n\nAs one variable changes how does another change?\n\nCovariance\nCorrelation\n\n\n\n\n\n- Descriptive statistics are: - Diagnostic - Generative\n\n\n## Descriptive statistics: Levels of understanding\n\n\n- Conceptual\n\n\n- Practical\n\n\n- Definitional\n\n\n- Theoretical\n\n\n## Descriptive statistics: Levels of understanding\n\n\n- Conceptual\n\n\n- Practical\n\n\n- Definitional\n\n\n- Theoretical\n\n\n## Mean: Conceptual Understanding\n\n\nA mean is:\n\n\n- A common and important measure of central tendency (what’s typical)\n\n\n- It’s the arithmetic average you learned in school\n\n\n- We can think of it as the balancing point of a distribution\n\n\n- A conditional mean is the average of one variable \\(X\\), when some other variable, \\(Z\\) takes a value \\(z\\)\n\n\n- Think about the average height in our class (unconditional mean) vs the average height among men and women (conditional means)\n\n\n## Mean as a balancing point\n\n\n\n\n\nSource\n\n\n## Mean: Practical\n\n\nThere are lots of ways to calculate means in R\n\n\n- The simplest is to use the mean() function\n\n\n- If our data have missing values, we need to to tell R to remove them\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} mean(df$x, na.rm=T) :::\n\n\n## Conditional Means: Practical\n\n\n- To calculate a conditional mean we could us a logical index [df$z == 1]\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} mean(df$x[df$z == 1], na.rm=T) :::\n\n\n\n\nIf we wanted to a calculate a lot of conditional means we could use the mean() in combination with group_by() and summarise()\n\n\ndf %&gt;% \n  group_by(z)%&gt;%\n  summarise(\n    x = mean(x, na.rm=T)\n  )"
  },
  {
    "objectID": "slides/04-slides-old.html#mean-definitional",
    "href": "slides/04-slides-old.html#mean-definitional",
    "title": "Week 04:",
    "section": "Mean: Definitional",
    "text": "Mean: Definitional\nFormally, we define the arithmetic mean of \\(x\\) as \\(\\bar{x}\\):\n\\[\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}\n\\]\nIn words, this formula says, to calculate the average of x, we sum up all the values of \\(x_i\\) from observation \\(i=1\\) to \\(i=n\\) and then divide by the total number of observations \\(n\\)"
  },
  {
    "objectID": "slides/04-slides-old.html#mean-definitional-1",
    "href": "slides/04-slides-old.html#mean-definitional-1",
    "title": "Week 04:",
    "section": "Mean: Definitional",
    "text": "Mean: Definitional\n\nIn this class, I don’t put a lot of weight on memorizing definitions (that’s what Google’s for).\nBut being comfortable with “the math” is important and useful\nDefinitional knowledge is a prerequisite for understanding more theoretical claims."
  },
  {
    "objectID": "slides/04-slides-old.html#mean-theoretical",
    "href": "slides/04-slides-old.html#mean-theoretical",
    "title": "Week 04:",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nSuppose I asked you to show that the sum of deviations from a mean equals 0?\n\\[\n\\text{Claim:} \\sum_{i=1}^n (x_i -\\bar{x}) = 0\n\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#mean-theoretical-1",
    "href": "slides/04-slides-old.html#mean-theoretical-1",
    "title": "Week 04:",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nKnowing the definition of an arithmetic mean, we could write:\n\\[\\begin{aligned}\n\\sum_{i=1}^n (x_i -\\bar{x}) &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n\\bar{x} & \\text{Distribute Summation}\\\\\n              &= \\sum_{i=1}^n x_i - n\\bar{x} & \\text{Summing a constant, } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - n\\times \\left ( \\frac{1}{n} \\sum_{i=1}^n{x_i}\\right ) & \\text{Definition of } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n{x_i} & n \\times \\frac{1}{n}=1\\\\\n              &= 0             \n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#mean-theoretical-2",
    "href": "slides/04-slides-old.html#mean-theoretical-2",
    "title": "Week 04:",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nWhy do we care?\n\nShowing the deviations sum to 0 is another way of saying the mean is a “balancing point.”\nThis turns out to be a useful property of means that will reappear throughout the course\nIf I asked you to make a prediction, \\(\\hat{x}\\) of a random person’s height in this class, the mean would have the lowest “mean squared error” (MSE \\(=\\frac{1}{n}\\sum (x_i - \\hat{x_i})^2)\\)"
  },
  {
    "objectID": "slides/04-slides-old.html#mean-theoretical-3",
    "href": "slides/04-slides-old.html#mean-theoretical-3",
    "title": "Week 04:",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nOccasionally, you’ll read or here me say say things like:\n\nThe sample mean is an unbiased estimator of the population mean\n\nIn a statistics class, we would take time to prove this."
  },
  {
    "objectID": "slides/04-slides-old.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean",
    "href": "slides/04-slides-old.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean",
    "title": "Week 04:",
    "section": "The sample mean is an unbiased estimator of the population mean",
    "text": "The sample mean is an unbiased estimator of the population mean\nClaim:\nLet \\(x_1, x_2, \\dots x_n\\) from a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\nThen:\n\\[\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n x_i\\right )\n\\]\nis an unbiased estimator of \\(\\mu\\)\n\\[\nE[\\bar{x}] = \\mu\n\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean-1",
    "href": "slides/04-slides-old.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean-1",
    "title": "Week 04:",
    "section": "The sample mean is an unbiased estimator of the population mean",
    "text": "The sample mean is an unbiased estimator of the population mean\nProof:\n\\[\\begin{aligned}\nE\\left [\\bar{x} \\right] &= E\\left [\\frac{1}{n}\\left (\\sum_{i=1}^n x_i \\right) \\right] & \\text{Definition of } \\bar{x} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^nE\\left [ x_i \\right]  & \\text{Linearity of Expectations} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu  & E[x_i] = \\mu \\\\\n&= \\frac{n}{n}  \\mu  & \\sum_{i=1}^n \\mu = n\\mu \\\\\n&= \\mu  & \\blacksquare \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#levels-of-understanding",
    "href": "slides/04-slides-old.html#levels-of-understanding",
    "title": "Week 04:",
    "section": "Levels of understanding",
    "text": "Levels of understanding\nIn this course, we tend to emphasize the\n\nConceptual\nPractical\n\nOver\n\nDefinitional\nTheoretical\n\nIn an intro statistics class, the ordering might be reversed.\nTrade offs:\n\nPro: We actually get to work with data and do empirical research much sooner\nCons: We substitute intuitive understandings for more rigorous proofs\n\nclass:inverse, middle, center # 🔍 ## Data visualization"
  },
  {
    "objectID": "slides/04-slides-old.html#data-visualization-as-a-tool-for-descriptive-inference",
    "href": "slides/04-slides-old.html#data-visualization-as-a-tool-for-descriptive-inference",
    "title": "Week 04:",
    "section": "Data visualization as a tool for descriptive inference",
    "text": "Data visualization as a tool for descriptive inference\nA statistical graphic is a mapping of data variables to aes thetic attributes of geom etric objects.\nAt a minimum, a graphic contains three core components:\n\ndata: the dataset containing the variables of interest.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars.\n\nIsmay and Kim (2022)"
  },
  {
    "objectID": "slides/04-slides-old.html#data-visualization-from-the-lab",
    "href": "slides/04-slides-old.html#data-visualization-from-the-lab",
    "title": "Week 04:",
    "section": "Data Visualization from the Lab",
    "text": "Data Visualization from the Lab\n\nate_df %&gt;%\n  ggplot(aes(Days, ATE))+\n  geom_point()+\n  geom_hline(yintercept = 0,linetype=2)+\n  theme_bw()+\n  labs(x=\"Days Since Treatment\",\n       y = \"ATE on Feeling Thermometers\\n toward Transgender People\")"
  },
  {
    "objectID": "slides/04-slides-old.html#ates-with-confidence-intervals",
    "href": "slides/04-slides-old.html#ates-with-confidence-intervals",
    "title": "Week 04:",
    "section": "ATEs with Confidence Intervals",
    "text": "ATEs with Confidence Intervals\n\ndf %&gt;%\n  # Get the select the columns whose names start with vf_\n  select(starts_with(\"therm_trans\"))%&gt;%\n  # Extract the column names\n  names()%&gt;%\n  # Create the formulas to calculate the diff in means for\n  # Each variable\n  purrr::map(~ formula(paste0(.,\" ~ treatment_group\"))) %&gt;%\n  # Caclulate the difference in means for each variable\n  purrr::map(~difference_in_means(., data = df)) %&gt;%\n  # Put the output into a tidy data frame.\n  map_df(tidy)%&gt;%\n  mutate(\n    ATE = estimate,\n    Days = c(0,3,21,42,90)\n  ) -&gt; ate_df\nate_df[,-1]\n\n    estimate std.error  statistic    p.value  conf.low conf.high       df\n1 -0.6101865  2.470775 -0.2469616 0.80504095 -5.464758  4.244385 492.0410\n2  6.4618301  2.690313  2.4018875 0.01675624  1.173250 11.750410 408.6734\n3  4.0591652  2.718401  1.4932177 0.13618853 -1.285385  9.403716 390.1030\n4  6.3459985  2.633331  2.4098747 0.01641918  1.168734 11.523263 390.8996\n5  5.2990725  2.782481  1.9044419 0.05762449 -0.172306 10.770451 371.6676\n         outcome        ATE Days\n1 therm_trans_t0 -0.6101865    0\n2 therm_trans_t1  6.4618301    3\n3 therm_trans_t2  4.0591652   21\n4 therm_trans_t3  6.3459985   42\n5 therm_trans_t4  5.2990725   90\n\n\n\nate_df %&gt;%\n  ggplot(aes(Days, ATE))+\n  geom_point()+\n  geom_linerange(aes(ymin=conf.low, ymax=conf.high))+ #&lt;&lt;\n  geom_hline(yintercept = 0,linetype=2)+\n  theme_bw()+\n  labs(x=\"Days Since Treatment\",\n       y = \"ATE on Feeling Thermometers\\n toward Transgender People\")"
  },
  {
    "objectID": "slides/04-slides-old.html#data-visualization-is-an-iterative-process",
    "href": "slides/04-slides-old.html#data-visualization-is-an-iterative-process",
    "title": "Week 04:",
    "section": "Data visualization is an iterative process",
    "text": "Data visualization is an iterative process\n\nData visualization is an iterative process\nGood data viz requires lots of data transformations\nStart with a minimum working example and build from there\nDon’t let the perfect be the enemy of the good enough.\n\nclass:inverse, middle, center # 🔍 ## Causal inference"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-inference-is-about-counterfactual-comparisons",
    "href": "slides/04-slides-old.html#causal-inference-is-about-counterfactual-comparisons",
    "title": "Week 04:",
    "section": "Causal inference is about counterfactual comparisons",
    "text": "Causal inference is about counterfactual comparisons\n\nCausal inference is about counterfactual comparisons\n\nWhat would have happened if some aspect of the world either had or had not been present"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-inference-is-about-counterfactual-comparisons-1",
    "href": "slides/04-slides-old.html#causal-inference-is-about-counterfactual-comparisons-1",
    "title": "Week 04:",
    "section": "Causal inference is about counterfactual comparisons",
    "text": "Causal inference is about counterfactual comparisons\nTwo ways to represent causal claims\n–\n\nDAGs helped illustrate types of bias\n\nConfounder bias: Failing to control for a common cause (aka Selection Bias, Omitted Variable Bias)\nCollider bias: Controlling for a common consequence\n\n\n–\n\nPotential Outcomes Notation helped illustrate the Fundamental Problem of Causal Inference"
  },
  {
    "objectID": "slides/04-slides-old.html#fundamental-problem-of-causal-inference",
    "href": "slides/04-slides-old.html#fundamental-problem-of-causal-inference",
    "title": "Week 04:",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\n\nFor an individual, we only observe one of potential many potential outcomes.\n\nDid taking an aspirin make your headache go away?\nNeed to compare you with aspirin and without, but only see one version of the world.\n\nIndividual Causal Effects are unidentified"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification",
    "href": "slides/04-slides-old.html#causal-identification",
    "title": "Week 04:",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCausal Identification: What do we need to assume for your causal claim to be credible\nRandomization offers a solution to the fundamental problem by ensuring that treatment is independent of potential outcomes \\((Y(1), Y(0))\\), observed \\((X)\\), and unobserved covariates \\((U)\\).\n\n\\[\nY(1), Y(0),X,U, \\perp D\n\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#randomization-creates-credible-counterfactual-comparisons",
    "href": "slides/04-slides-old.html#randomization-creates-credible-counterfactual-comparisons",
    "title": "Week 04:",
    "section": "Randomization creates credible counterfactual comparisons",
    "text": "Randomization creates credible counterfactual comparisons\nIf treatment has been randomly assigned, then:\n\nThe only thing that differs between treatment and control is that one group got the treatment, and another did not.\nWe can estimate the Average Treatment Effect (ATE) using the difference of sample means\n\n\\[\\begin{aligned}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#whats-the-counterfactual",
    "href": "slides/04-slides-old.html#whats-the-counterfactual",
    "title": "Week 04:",
    "section": "What’s the counterfactual?",
    "text": "What’s the counterfactual?\n\nBroadly speaking, I don’t like computers. I don’t like bending over the small desks, staring at a screen, etc. The class is fine: it’s something I have to do, all things worth doing are difficult, and I’m learning to think in new ways. That said, I tend to doubt that computer programing is conducive to human flourishing. Would the world be a better place if everyone learned how to code? I tend to think no. Would the world be a better place if everyone read Shakespeare? Probably. This isn’t really a practical concern, but rather a question about, y’know, what we’re doing here.\n\n\nMore concretely, I found the lab hard to complete, but I’ll look over the notes and review. Part of me thinks I’d learn better if I started the labs before class. I’d ask better questions, be less confused, etc.\n\n\nIn sum: the class is fine. It’s good to learn to think in new ways and good to be challenged, even if I have some broader questions about coding as a human activity.”"
  },
  {
    "objectID": "slides/04-slides-old.html#whats-the-counterfactual-1",
    "href": "slides/04-slides-old.html#whats-the-counterfactual-1",
    "title": "Week 04:",
    "section": "What’s the counterfactual?",
    "text": "What’s the counterfactual?\n\nBroadly speaking, I don’t like computers. I don’t like bending over the small desks, staring at a screen, etc. The class is fine: it’s something I have to do, all things worth doing are difficult, and I’m learning to think in new ways. That said, I tend to doubt that computer programing is conducive to human flourishing. Would the world be a better place if everyone learned how to code? I tend to think no. Would the world be a better place if everyone read Shakespeare? Probably. This isn’t really a practical concern, but rather a question about, y’know, what we’re doing here.\n\n\nMore concretely, I found the lab hard to complete, but I’ll look over the notes and review. Part of me thinks I’d learn better if I started the labs before class. I’d ask better questions, be less confused, etc.\n\n\nIn sum: the class is fine. It’s good to learn to think in new ways and good to be challenged, even if I have some broader questions about coding as a human activity.”"
  },
  {
    "objectID": "slides/04-slides-old.html#would-the-world-be-a-better-place-if-everyone-learned-how-to-code",
    "href": "slides/04-slides-old.html#would-the-world-be-a-better-place-if-everyone-learned-how-to-code",
    "title": "Week 04:",
    "section": "Would the world be a better place if everyone learned how to code?",
    "text": "Would the world be a better place if everyone learned how to code?\n–\n\nHow should we define/measure human flourishing?\n\n–\n\nWhat are some counterfactuals we might make?\n\nEveryone learns to code:\nYou learning to code:\nLearning to code vs reading Shakespeare\n\n\n–\n\nWhy might these comparisons be misleading?\n\nCS concentrators to POLS concentrators\nPOLS 1600 takers to POLS 0500\n\n\n–\n\nHow could we assess the “effects” of POLS 1600?\n\nclass:,bottom, center background-image:url(“https://media.giphy.com/media/dUaHl1MDcaGLGtVBbI/giphy-downsized-large.gif”) background-size:contain"
  },
  {
    "objectID": "slides/04-slides-old.html#learning-to-code",
    "href": "slides/04-slides-old.html#learning-to-code",
    "title": "Week 04:",
    "section": "Learning to code",
    "text": "Learning to code\nclass: inverse, bottom, center background-image:url(“https://media.giphy.com/media/h5pRkbOXAH66zlZVML/giphy.gif”) background-size:contain ## Thinking Programatically\nclass: inverse, center, middle # 💡 # Casual Inference in Experimental and Observational Designs"
  },
  {
    "objectID": "slides/04-slides-old.html#experimental-and-observational-designs",
    "href": "slides/04-slides-old.html#experimental-and-observational-designs",
    "title": "Week 04:",
    "section": "Experimental and Observational Designs",
    "text": "Experimental and Observational Designs\n\nExperimental designs are studies in which a causal variable of interest, the treatement, is manipulated by the researcher to examine its causal effects on some outcome of interest\n\nRandomized Controlled Trial (RCTs) each unit is randomly assigned to a treatment(s) or control group\n\nObservational designs are studies in which a causal variable of interest is assigned by someone other than the researcher (nature, governments, people)"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-1",
    "href": "slides/04-slides-old.html#causal-identification-1",
    "title": "Week 04:",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCasual Identification refers to “the assumptions needed for statistical estimates to be given a causal interpretation” Keele (2015)\n\n–\n\nWhat’s Your Casual Identification Strategy: What are the assumptions that make your research design credible?\n\n–\n\nIdentification &gt; Estimation"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-with-experimental-designs",
    "href": "slides/04-slides-old.html#causal-identification-with-experimental-designs",
    "title": "Week 04:",
    "section": "Causal Identification with Experimental Designs",
    "text": "Causal Identification with Experimental Designs\nCausal identification for an experiment, requires relatively few assumptions:\n\nIndependence (Satisfied by Randomization)\n\n\\(Y(1), Y(0),X,U, \\perp D\\)\n\nSUTVA Stable Unit Treatment Value Assumption (Depends on features of the design)\n\nNo interference between units \\(Y_i(d_1, d_2, \\dots, d_N) = Y_i(d_i)\\)\nNo hidden values of the treatment/Variation in the treatment"
  },
  {
    "objectID": "slides/04-slides-old.html#internal-vs-external-validity",
    "href": "slides/04-slides-old.html#internal-vs-external-validity",
    "title": "Week 04:",
    "section": "Internal vs External Validity",
    "text": "Internal vs External Validity\n\nInternal validity the extent to which causal assumptions are satisfied in a study\nExternal validity the extent to which conclusions can be generalized beyond a particular study"
  },
  {
    "objectID": "slides/04-slides-old.html#internal-vs-external-validity-1",
    "href": "slides/04-slides-old.html#internal-vs-external-validity-1",
    "title": "Week 04:",
    "section": "Internal vs External Validity",
    "text": "Internal vs External Validity\nExperimental designs are said to have high internal validity, but may lack external validity\n\nThe college sophomore problem\nThe weirdest people in the world\nThe Hawthorne Effect refers to the phenomenon where study subjects behave differently because they know they are being observed by researchers. (QSS Ch 2 p. 52)"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-in-observational-designs",
    "href": "slides/04-slides-old.html#causal-identification-in-observational-designs",
    "title": "Week 04:",
    "section": "Causal Identification in Observational Designs",
    "text": "Causal Identification in Observational Designs\n\nIn an observational study the researcher does not control the treatment assignment\nNo guarantee that treatment (D=1) and control groups (D=0) are comparable (That is that we’re comparing like with like)\nInstead, we have justify our claims by theory and assumption rather than direct manipulation through random assignment."
  },
  {
    "objectID": "slides/04-slides-old.html#conditional-independence-ignorability",
    "href": "slides/04-slides-old.html#conditional-independence-ignorability",
    "title": "Week 04:",
    "section": "Conditional Independence (ignorability)",
    "text": "Conditional Independence (ignorability)\nIf treatment is not randomly assigned then in general:\n\\[\nY_i(1),Y_i(0) \\text{ is not} \\perp D_i\n\\]\nHowever, in some situations, it may be plausible to claim that conditional on some variable(s) \\(X\\), the distribution of potential outcomes \\(Y\\) is the same (independent) across levels of treatment \\(D\\) (conditional ignorability)\n\\[\nY_i(1),Y_i(0) \\perp D_i |X_i\n\\]\n\nConditional on a some covariate(s) \\(X_i\\) our treatment is as-if randomized"
  },
  {
    "objectID": "slides/04-slides-old.html#as-if-randomized",
    "href": "slides/04-slides-old.html#as-if-randomized",
    "title": "Week 04:",
    "section": "As-if randomized",
    "text": "As-if randomized"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-in-observational-designs-1",
    "href": "slides/04-slides-old.html#causal-identification-in-observational-designs-1",
    "title": "Week 04:",
    "section": "Causal Identification in Observational Designs",
    "text": "Causal Identification in Observational Designs\n\nThe claim that treatment is as-if randomized requires further justification in the theory and design of your study\n\n\\[\nY_i(1),Y_i(0) \\perp D_i |X_i\n\\]\n\nWhile we can’t “prove” this assumption, we typically can test some observable implications of this claim,\n\nFor example, in lab 03 we tested for covariate balance by comparing the average values of pre-treatment covariates from the voter file."
  },
  {
    "objectID": "slides/04-slides-old.html#the-experimental-ideal",
    "href": "slides/04-slides-old.html#the-experimental-ideal",
    "title": "Week 04:",
    "section": "The Experimental Ideal",
    "text": "The Experimental Ideal\n\n“The planner of an observational study should always ask himself: How would the study be conducted if it were possible to do it by controlled experimentation” (Cochran 1965)\n\n\nWhat would you have to randomly assign to answer your question as posed?\nIs it feasible to imagine changing just the treatment and nothing else?\n\nIf yes, maybe we should do an experiment\nIf not, maybe we should rethink our question and ask what about the design of observational study allows us to make the credible comparisons that an experiment would generate?"
  },
  {
    "objectID": "slides/04-slides-old.html#observational-designs",
    "href": "slides/04-slides-old.html#observational-designs",
    "title": "Week 04:",
    "section": "Observational Designs",
    "text": "Observational Designs\nObservational designs all involve covariate adjustment\nWe will discuss three approaches to covariate adjustment\n\nSubclassification\nMatching\nRegression\n\nThen we will consider three types of observational desings that can produce credible causal claims:\n\nDifference in Difference\nRegression Discontinuities\nInstrumental Variables\n\nclass: inverse, center, middle background-image:url(“https://m.media-amazon.com/images/I/71lsG3fCaOL._SL1200_.jpg”) background-size:cover # Break ## Take the survey\nclass: inverse, center, middle"
  },
  {
    "objectID": "slides/04-slides-old.html#covariate-adjustment",
    "href": "slides/04-slides-old.html#covariate-adjustment",
    "title": "Week 04:",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment"
  },
  {
    "objectID": "slides/04-slides-old.html#covariate-adjustment-1",
    "href": "slides/04-slides-old.html#covariate-adjustment-1",
    "title": "Week 04:",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment\nCovariate adjustment refers a broad class of procedures that try to make a comparison more credible or meaningful by adjusting for some other potentially confounding factor."
  },
  {
    "objectID": "slides/04-slides-old.html#covariate-adjustment-2",
    "href": "slides/04-slides-old.html#covariate-adjustment-2",
    "title": "Week 04:",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment\nWhen you hear people talk about\n\nControlling for age\nConditional on income\nHolding age and income constant\nCeteris paribus (All else equal)\n\nThey are typically talking about some sort of covariate adjustment."
  },
  {
    "objectID": "slides/04-slides-old.html#three-approaches-to-for-covariate-adjustment",
    "href": "slides/04-slides-old.html#three-approaches-to-for-covariate-adjustment",
    "title": "Week 04:",
    "section": "Three approaches to for covariate adjustment",
    "text": "Three approaches to for covariate adjustment\n\nSubclassification\nMatching\nRegression"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-through-subclassification",
    "href": "slides/04-slides-old.html#causal-identification-through-subclassification",
    "title": "Week 04:",
    "section": "Causal Identification through Subclassification",
    "text": "Causal Identification through Subclassification\nMotivation: Treatment, \\(D\\) is not randomly assigned\n\\[\nY_i(1),Y_i(0) \\text{ is not} \\perp D_i\n\\]\nIdentifying assumptions\n\\[\\begin{aligned}\nY_i(1),Y_i(0) \\perp D_i |X_i && \\text{Selection on Observables}\\\\\n0 &lt; Pr(D_i = 1|X_i) &lt; 1  && \\text{Common Support}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-through-subclassification-1",
    "href": "slides/04-slides-old.html#causal-identification-through-subclassification-1",
    "title": "Week 04:",
    "section": "Causal Identification through Subclassification",
    "text": "Causal Identification through Subclassification\nIf these assumptions hold, we can estimate the average treatment effect:\n\\[\\begin{aligned}\nATE = E[Y_i(1)- Y_i(0)|X_i] &= E[Y_i(1)- Y_i(0)|X_i, D_i=1]\\\\\n& = E[Y_i |X_i, D_i=1] - E[Y_i |X_i, D_i=0]\n\\end{aligned}\\]\nThe average treatment effect is identified by the observed difference of means between treatment and control conditional on the values of \\(X\\)"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-through-subclassification-2",
    "href": "slides/04-slides-old.html#causal-identification-through-subclassification-2",
    "title": "Week 04:",
    "section": "Causal Identification through Subclassification",
    "text": "Causal Identification through Subclassification\n\nEconomists call \\(Y_i(1),Y_i(0) \\perp D_i |X_i\\) an assumption of Selection on Observables\n\nControlling for what we can observe \\(X\\), \\(D\\) is conditionally independent of Potential Outcomes\nViolated if there were some other factor, \\(U\\) that influenced both \\(D\\) and \\(Y\\)\n\n\\(0 &lt; Pr(D = 1|X) &lt; 1\\) is called an assumption of Common Support\n\nThere is a non-zero probability of receiving the treatment for all values of X\nViolated if only one subgroup had access to the treatment (Vaccine by age group comparisons)"
  },
  {
    "objectID": "slides/04-slides-old.html#example-of-subclassification",
    "href": "slides/04-slides-old.html#example-of-subclassification",
    "title": "Week 04:",
    "section": "Example of Subclassification",
    "text": "Example of Subclassification\n\nWe used subclassification when we compared the unconditional rates of new Covid-19 cases by face mask policy to the conditional rates new cases by policy regime in each month of our data.\n\nOverall rates are misleading.\nLots of things differ between January 2020 and January 2022\nSubclassification by month provides a “fairer” comparison\nBut is it “causal”\n\n\n\n\n\n- Even controlling for “month” there are other omitted variables: - Other policies in place? - Socio-economic differences between states - Others?\n\n\n\n\n- Trying to subclassify (stratify) comparisons on more than one or two variables gets hard - The Curse of Dimensionality\n\n\n## The Curse of Dimensionality\n\n\n- As we try to control for more factors, the number of observations per dimension declines rapidly - Men vs Women - Men, ages 20-30 vs Men ages 30-40 - Men, ages 20-30 with college degrees and blue eyes vs Men ages 20-30 with college degrees and green eyes\n\n\n- Subclassification with more than a few variables, will often produce a lack of common support: - Not enough observations to make credible counterfactual comparisons\n\n\n## Matching\n\n\n- Matching refers to a broad set of procedures that essentially try to generalize subclassification to - address to curse of dimensionality - achieve balance on a range of observable covariates between treated and control groups\n\n\n- Many different types of matching procedures:\n\n\n- Exact matching: Find exact matches between treatment and control observations for all covariates \\(X\\) - Coarsened exact matching: Find approximate matches within ranges of values for \\(X\\) - Distance-metric matching: Calculate a distance metric between observations based on their values of \\(X\\), and match treated and control to minimize that distance - Propensity score matching: Calculate the propsenity to receive treatment using \\(X\\) to predict \\(D\\) and treated and control based on their propensity scores\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\n\nSource\n\n\n## Causal Identification with Matching (ICYI)\n\n\nMatching again requires an assumption of selection on observables\n\n\n\\[\\begin{aligned}\nY_i(1),Y_i(0) \\perp D_i |X_i && \\text{Selection on Observables}\\\\\n0 &lt; Pr(D_i = 1|X_i) &lt; 1  && \\text{Common Support}\n\\end{aligned}\\]\n\n\nMatching procedures like propensity score matching, allow us to match treated and control observations based on a propensity score, a predicted value of receiving the treatment, \\(D\\) based on observed variables, \\(X\\).\n\n\n\\[\np(X_i) = Pr(D=1|X_i) = \\pi_i\n\\]\n\n\nAllowing us to estimate an ATE conditional on \\(\\pi_i\\)\n\n\n\\[\\begin{aligned}\nATE &= E[Y_i(1)- Y_i(0)|p(X_i) = \\pi_i] \\\\\n&= E[Y_i(1)- Y_i(0)|p(X_i) = \\pi_i, D_i=1]\\\\\n& = E[Y_i |p(X_i) = \\pi_i, D_i=1] - E[Y_i |p(X_i) = \\pi_i, D_i=0]\n\\end{aligned}\\]\n\n\n## What to Know about Matching\n\n\n- The mechanics of matching are beyond the scope of this course\n\n\n- Just think of it as a generalization of subclassification when we want to condition on multiple variables\n\n\n- “Solves” the curse of dimensionality, creating Treatment-Control comparisons between groups that are similar on observed covariates\n\n\n- But no guarantee that matching produces balance on unobserved covariates.\n\n\n## Regression\n\n\n- We will spend the next two weeks talking in detail about regression, in general and linear regression in particular.\n\n\n- Today we’ll introduce some basic notation and simple examples\n\n\n- Conceptually, think of regression as - a tool to make predictions - by fitting lines to data\n\n\n- Theoretically, we will build towards an understanding of linear regression as a “linear estimate of the conditional expectation function \\((CEF = E[Y|X])\\)\n\n\n## Regression\n\n\nBroadly regression is a statistical procedural to help us model relationships that consists of:\n\n\n- \\(Y\\) an outcome variable or thing we’re trying to explain - AKA: The Dependent Variable, The Response Variable, The Lefthand side of the model\n\n\n- \\(X\\) a set of predictor variables or things we think explain variation in our outcome - AKA: The independent variable, covariates, the right hand side of the model.\n\n\n- \\(\\beta\\) a set of unknown parameters that describe the relationship between our outcome \\(Y\\) and our predictors \\(X\\)\n\n\n- \\(\\epsilon\\) the error term representing variation in \\(Y\\) not explained by our model.\n\n\n## Linear Regression\n\n\nNext week, we will start to consider simple (bivariate) linear regressions of the form:\n\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\n\n\n- We call this a bivariate regression, because there are only two variables.\n\n\n- We call this a linear regression, because \\(y_i = \\beta_0 + \\beta_1 x_i\\) is the equation for a line, where: - \\(\\beta_0\\) corresponds to the \\(y\\) intercept, or the model’s prediction when \\(x = 0\\). - \\(\\beta_1\\) corresponds to the slope, or how \\(y\\) is predicted to change as \\(x\\) changes.\n\n\n## Linear Regression\n\n\n- If you find this notation confusing, try plugging in substantive concepts for what \\(y\\) and \\(x\\) represent - Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.\n\n\nThe generic linear model\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon\\]\n\n\nReflects:\n\n\n\\[\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\]\n\n\n## Estimating a Linear Regression\n\n\n- We estimate linear regressions in R using the lm() function. - lm() requires two arguments: - a formula of the general form y ~ x read as “Y modeled by X” or below “Transgender Feeling Thermometer’s modeled by Age - a data telling R\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} load(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\")) :::\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} m1 &lt;- lm(therm_trans_t0 ~ vf_age, data = df) m1\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n```\n\n\nCall: lm(formula = therm_trans_t0 ~ vf_age, data = df)\n\n\nCoefficients: (Intercept) vf_age 62.8196 -0.2031 ```\n\n\n::: :::\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\n\n## Multiple Regression\n\n\nIn two weeks, we will generalize this to include multiple predictors\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2  x_{2i} + \\epsilon\\]\n\n\n- \\(\\beta_0\\) in this model still corresponds to the y intercept the model’s prediction when everything else is 0 - \\(\\beta_1\\) now describes the relationship between \\(x\\)\n\n\n\\[\\text{FT}_{Trans} = \\beta_0 + \\beta_1 \\text{Age}_{1i} + \\beta_2  \\text{Party ID}_{2i} + \\epsilon\\]\n\n\n## Multiple Regression with Matrix Notation\n\n\nWith more than two predictors, we will write equations using Matrix notation\n\n\n\\[y_i = X\\beta + \\epsilon\\]\n\n\n.pull-left[ Where X is a matrix of variables:\n\n\n\\[X=\n\\begin{bmatrix}\n1  & X_{1,1}&\\dots & X_{1,k}\\\\\n1  & X_{2,1}&\\dots & X_{2,k}\\\\\n\\vdots&\\vdots&  & \\vdots \\\\\n1  & X_{n,1}&\\dots & X_{n,k}\\\\\n\\end{bmatrix}\\] ] .pull-right[ And \\(\\beta\\) is a vector of coefficients\n\n\n\\[\\beta=\n\\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1\\\\\n\\vdots \\\\\n\\beta_k\\\\\n\\end{bmatrix}\\] ]\n\n\n## Causal Identification with Regression\n\n\nLike subclassification and matching, causal identification with Linear Regression requires us to believe the assumption of Selection on Observables\n\n\n\\[\nY_i(1),Y_i(0) \\perp D_i |X_i\n\\]\n\n\nAnd additionally asks us to make assumptions about the “functional form”:\n\n\n\\[\nY = \\beta_0 + \\tau D + X\\beta + \\epsilon\n\\]\n\n\n## Causal Identification with Regression\n\n\n- To interpret a linear regression’s estimate of \\(\\tau\\) as the causal effect of \\(D\\) on \\(Y\\), we need to believe we’ve got the right model\n\n\n- The only factors that predict both \\(Y\\) and \\(D\\) are those factors \\(X\\) which we control for in our model.\n\n\n- The relationship between \\(Y\\) and these predictors is linear\n\n\n\n\nWe almost never have the right model.\n\nDo we think age is the only factor that influences attitudes toward Trangendered folk? Probably not.\n\n\n–\n\nRegression is still a robust and useful tool\n\nWhy do we see negative relationship between age and transgender attitudes?\n\n\n–\n\nBut for causal inference, there’s nothing inherent about estimating a regression, or controlling for a lot of variables that makes for particularly credible causal claims."
  },
  {
    "objectID": "slides/04-slides-old.html#methods-for-covariate-adjustment",
    "href": "slides/04-slides-old.html#methods-for-covariate-adjustment",
    "title": "Week 04:",
    "section": "Methods for covariate adjustment",
    "text": "Methods for covariate adjustment\n\nSubclassification\n\n👍: Easy to implement and interpret\n👎: Curse of dimensionality, Selection on observables\n\nMatching\n\n👍: Balance on multiple covariates, Mirrors logic of experimental design\n👎: Selection on observables, Only provides balance on observed variables, Lot’s of technical details…\n\nRegression\n\n👍: Easy to implement, control for many factors (good and bad)\n👎: Selection on observables, easy to fit “bad” models\n\n\nclass: inverse, center, middle"
  },
  {
    "objectID": "slides/04-slides-old.html#credible-cauasal-inference-in-observational-studies",
    "href": "slides/04-slides-old.html#credible-cauasal-inference-in-observational-studies",
    "title": "Week 04:",
    "section": "Credible Cauasal Inference in Observational Studies",
    "text": "Credible Cauasal Inference in Observational Studies\nSubclassification, matching, and regression all require an assumption of selection on observables:\n\\[\nY_i(1),Y_i(0) \\perp D_i |X_i\n\\]\nBut how do we know if we’ve got the right model or we’ve controlled for the right variables?\n–\nTypically, we don’t"
  },
  {
    "objectID": "slides/04-slides-old.html#credible-cauasal-inference-in-observational-studies-1",
    "href": "slides/04-slides-old.html#credible-cauasal-inference-in-observational-studies-1",
    "title": "Week 04:",
    "section": "Credible Cauasal Inference in Observational Studies",
    "text": "Credible Cauasal Inference in Observational Studies\nInstead, social scientists look for situations where the credibility of\n\\[\nY_i(1),Y_i(0) \\perp D_i |X_i\n\\]\ndepends less on how much data you have and much more on how your data were generated.\nclass: middle\n.pull-left[ &gt; Empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact. … [T]he primary engine driving improvement has been a focus on the quality of empirical research designs. (p. 4)\n]\n.pull-right[\n\n\n\n\n\n\n\n\n\nSource\n]\nclass: middle\n.pull-left[ &gt; Design-based studies are distinguished by their prima facie credibility and by the attention investigators devote to making both an institutional and a data-driven case for causality (p. 5)\n]\n.pull-right[\n\n\n\n\n\n\n\n\n\nSource ]\nclass: middle\n.pull-left[ &gt; The econometric methods that feature most prominently in quasi-experimental studies are instrumental variables, regression discontinuity methods,and differences-in-differences-style policy analysis. … The best of today’s design-based studies make a strong institutional case, backed up with empirical evidence, for the variation thought to generate a useful natural experiment.(p. 12)\n]\n.pull-right[\n\n\n\n\n\n\n\n\n\nSource ]"
  },
  {
    "objectID": "slides/04-slides-old.html#three-designs-for-causal-inference-in-observational-studies-1",
    "href": "slides/04-slides-old.html#three-designs-for-causal-inference-in-observational-studies-1",
    "title": "Week 04:",
    "section": "Three Designs for Causal Inference in Observational Studies",
    "text": "Three Designs for Causal Inference in Observational Studies\n\nDifference in Differences\nRegression Discontinuity\nInstrumental Variables"
  },
  {
    "objectID": "slides/04-slides-old.html#three-designs-for-causal-inference-in-observational-studies-2",
    "href": "slides/04-slides-old.html#three-designs-for-causal-inference-in-observational-studies-2",
    "title": "Week 04:",
    "section": "Three Designs for Causal Inference in Observational Studies",
    "text": "Three Designs for Causal Inference in Observational Studies\n\nDifference in Differences (Today)\nRegression Discontinuity\nInstrumental Variables"
  },
  {
    "objectID": "slides/04-slides-old.html#three-designs-for-causal-inference-in-observational-studies-3",
    "href": "slides/04-slides-old.html#three-designs-for-causal-inference-in-observational-studies-3",
    "title": "Week 04:",
    "section": "Three Designs for Causal Inference in Observational Studies",
    "text": "Three Designs for Causal Inference in Observational Studies\n\nDifference in Differences\nRegression Discontinuity (Next Week)\nInstrumental Variables (Next Week)\n\nclass: inverse, center, middle # 💡 Difference in Differences\nclass: inverse, center, middle background-image:url(https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc) background-size: cover ## London in the Time of Cholera"
  },
  {
    "objectID": "slides/04-slides-old.html#motivating-example-what-causes-cholera",
    "href": "slides/04-slides-old.html#motivating-example-what-causes-cholera",
    "title": "Week 04:",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/04-slides-old.html#notation",
    "href": "slides/04-slides-old.html#notation",
    "title": "Week 04:",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-effects",
    "href": "slides/04-slides-old.html#causal-effects",
    "title": "Week 04:",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/04-slides-old.html#average-treatment-on-treated",
    "href": "slides/04-slides-old.html#average-treatment-on-treated",
    "title": "Week 04:",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/04-slides-old.html#average-treatment-on-treated-1",
    "href": "slides/04-slides-old.html#average-treatment-on-treated-1",
    "title": "Week 04:",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n              | Pre-Period (T=0) | Post-Period (T=1) |\n||–|-| | Treated \\(D_{i}=1\\) | \\(E[Y_{0i}(0)\\vert D_i = 1]\\) | \\(E[Y_{1i}(1)\\vert D_i = 1]\\) | | Control \\(D_i=0\\) | \\(E[Y_{0i}(0)\\vert D_i = 0]\\) | \\(E[Y_{0i}(1)\\vert D_i = 0]\\) |"
  },
  {
    "objectID": "slides/04-slides-old.html#data",
    "href": "slides/04-slides-old.html#data",
    "title": "Week 04:",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0\n135\n147"
  },
  {
    "objectID": "slides/04-slides-old.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/04-slides-old.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "Week 04:",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/04-slides-old.html#before-vs-after-comparisons",
    "href": "slides/04-slides-old.html#before-vs-after-comparisons",
    "title": "Week 04:",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854"
  },
  {
    "objectID": "slides/04-slides-old.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/04-slides-old.html#treatment-control-comparisons-in-the-post-period.",
    "title": "Week 04:",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V"
  },
  {
    "objectID": "slides/04-slides-old.html#difference-in-differences",
    "href": "slides/04-slides-old.html#difference-in-differences",
    "title": "Week 04:",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period\n\nWhich is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/04-slides-old.html#difference-in-differences-1",
    "href": "slides/04-slides-old.html#difference-in-differences-1",
    "title": "Week 04:",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/04-slides-old.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "Week 04:",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V\n\n\nWhere:\n\n\\(E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\)\n\\(E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}\\} = 0]\\)\n\\(E[Y_{1i}(1) − Y_{0i}(1)|D_{i} = 1]\\)"
  },
  {
    "objectID": "slides/04-slides-old.html#summary",
    "href": "slides/04-slides-old.html#summary",
    "title": "Week 04:",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\n\nTaking the pre-post difference removes any fixed differences between the units\nThen taking the difference between treated and control differences removes any common differences over time\n\nThe key identifying assumption of a DiD design is the “assumption of parallel trends”\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to test"
  },
  {
    "objectID": "slides/04-slides-old.html#extensions-and-limitations",
    "href": "slides/04-slides-old.html#extensions-and-limitations",
    "title": "Week 04:",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nDiD easy to estimate with linear regression\nGeneralizes to multiple periods and treatment interventions\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\n\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/04-slides-old.html#applications",
    "href": "slides/04-slides-old.html#applications",
    "title": "Week 04:",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?\n\nclass: inverse, center, middle # 💡 Regression Discontinuity Design"
  },
  {
    "objectID": "slides/04-slides-old.html#motivating-example",
    "href": "slides/04-slides-old.html#motivating-example",
    "title": "Week 04:",
    "section": "Motivating Example",
    "text": "Motivating Example\n.pull-left[ - Do Members of Parliament in the UK get richer from holding office (QSS Chapter 4.3.4)] .pull-right[\n\nEggers and Hainmueller (2009) ]"
  },
  {
    "objectID": "slides/04-slides-old.html#logic-of-the-regression-discontinuity-design-rdd",
    "href": "slides/04-slides-old.html#logic-of-the-regression-discontinuity-design-rdd",
    "title": "Week 04:",
    "section": "Logic of the Regression Discontinuity Design (RDD)",
    "text": "Logic of the Regression Discontinuity Design (RDD)\n\nWhat’s the effect of holding elected office in the UK on personal wealth?\nPeople who win elections differ in many ways from people who lose elections.\nLogic of an RDD:\n\nJust look at the wealth of individuals who either narrowly won or lost elections.\nCandidates close to 50 percent cutoff (discontinuity) should be more comparable (better counterfactuals)"
  },
  {
    "objectID": "slides/04-slides-old.html#data-from-eggers-and-hainmueller-2009",
    "href": "slides/04-slides-old.html#data-from-eggers-and-hainmueller-2009",
    "title": "Week 04:",
    "section": "Data from Eggers and Hainmueller (2009)",
    "text": "Data from Eggers and Hainmueller (2009)\n\nlibrary(qss)\ndata(MPs)\nglimpse(MPs)\n\nRows: 427\nColumns: 10\n$ surname    &lt;chr&gt; \"Llewellyn\", \"Morris\", \"Walker\", \"Walker\", \"Waring\", \"Brown…\n$ firstname  &lt;chr&gt; \"David\", \"Claud\", \"George\", \"Harold\", \"John\", \"Ronald\", \"Le…\n$ party      &lt;chr&gt; \"tory\", \"labour\", \"tory\", \"labour\", \"tory\", \"labour\", \"tory…\n$ ln.gross   &lt;dbl&gt; 12.13591, 12.44809, 12.42845, 11.91845, 13.52022, 12.46052,…\n$ ln.net     &lt;dbl&gt; 12.135906, 12.448091, 10.349009, 12.395034, 13.520219, 9.63…\n$ yob        &lt;int&gt; 1916, 1920, 1914, 1927, 1923, 1921, 1907, 1912, 1905, 1920,…\n$ yod        &lt;int&gt; 1992, 2000, 1999, 2003, 1989, 2002, 1987, 1984, 1998, 2004,…\n$ margin.pre &lt;dbl&gt; NA, NA, -0.057168204, -0.072508894, -0.269689620, 0.3409586…\n$ region     &lt;chr&gt; \"Wales\", \"South West England\", \"North East England\", \"Yorks…\n$ margin     &lt;dbl&gt; 0.05690404, -0.04973833, -0.04158868, 0.02329524, -0.230005…"
  },
  {
    "objectID": "slides/04-slides-old.html#variables",
    "href": "slides/04-slides-old.html#variables",
    "title": "Week 04:",
    "section": "Variables",
    "text": "Variables\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsurname\nsurname of the candidate\n\n\nfirstname\nfirst name of the candidate\n\n\nparty\nparty of the candidate (labour or tory)\n\n\nln.gross\nlog gross wealth at the time of death\n\n\nln.net\nlog net wealth at the time of death\n\n\nyob\nyear of birth of the candidate\n\n\nyod\nyear of death of the candidate\n\n\nmargin.pre\nmargin of the candidate’s party in the previous election electoral\n\n\nregion\nregion\n\n\nmargin\nmargin of victory (vote share)\n\n\n\n\n\n\n\n\nMPs %&gt;%\n  ggplot(aes(margin, ln.net))+\n  geom_point(shape=1)+\n  facet_grid(~party)+\n  geom_smooth(data =MPs %&gt;%\n                filter(margin &lt;0),\n              method = \"lm\")+\n  geom_smooth(data =MPs %&gt;%\n                filter(margin &gt;0),\n              method = \"lm\")+\n  theme_bw() -&gt; fig_rdd\nfig_rdd"
  },
  {
    "objectID": "slides/04-slides-old.html#rdd-notation",
    "href": "slides/04-slides-old.html#rdd-notation",
    "title": "Week 04:",
    "section": "RDD Notation",
    "text": "RDD Notation\n\n\\(X\\) is a forcing variable\nTreatment \\(D\\) is a determined by \\(X\\)\n\n\\[\nD_i = 1\\{X_i &gt; c\\}\n\\]\n\n\\(X\\) is the margin variable in the example data, and \\(D=1\\) if margin is greater than 0 (i.e. the candidate won the election)\nInterested in the differences in the outcome at the threshold\n\n\\[\\lim_{x \\downarrow  c} E[Y_i|X=x] - \\lim_{x \\uparrow  c} E[Y_i|X=x]\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-with-an-rdd",
    "href": "slides/04-slides-old.html#causal-identification-with-an-rdd",
    "title": "Week 04:",
    "section": "Causal Identification with an RDD",
    "text": "Causal Identification with an RDD\nIf we assume \\(E[Y_i(0)|X=x]\\) and \\(E[Y_i(1)|X=x]\\) are continuous in x, then we can estimate a (local) ATE at the threshold:\n\\[\\begin{align}\nATE_{RDD} &= E[Y(1)-Y(0)|X_i=c] \\\\\n&=  E[Y(1)|X_i=c] -  E[Y(0)|X_i=c]\\\\\n&= \\lim_{x \\downarrow  c} E[Y_i|X=x] - \\lim_{x \\uparrow  c} E[Y_i|X=x] \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#continuity-assumption",
    "href": "slides/04-slides-old.html#continuity-assumption",
    "title": "Week 04:",
    "section": "Continuity Assumption",
    "text": "Continuity Assumption\n\nCunningham (2022)"
  },
  {
    "objectID": "slides/04-slides-old.html#causal-identification-with-an-rdd-1",
    "href": "slides/04-slides-old.html#causal-identification-with-an-rdd-1",
    "title": "Week 04:",
    "section": "Causal Identification with an RDD",
    "text": "Causal Identification with an RDD\n\nThe continuity assumption is a formal way of saying that observations close to the threshold are good counterfactuals for each other\nWe can’t prove this assumption\nBut if it holds, we should observe\n\nno sorting around the cutoff (no self selection)\nsimilar distributions of covariates around the cutoff (balance tests)\nno effect of treatment on things measured pre-treatment (placebo tests)\n\n\nclass: inverse, center, middle # 💡 ## Instrumental Variables"
  },
  {
    "objectID": "slides/04-slides-old.html#instrumental-variables",
    "href": "slides/04-slides-old.html#instrumental-variables",
    "title": "Week 04:",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables\nInstrumental variables are an economists favorite tool for dealing with omitted variable bias\n\nWe have some non random treatment whose effects we’d like to assess\nWe’re worried that these effects are confounded by some unobserved, omitted variable, that influences both the treatment and the outcome\nWe find an instrumental variable that satisfies the following:\n\nRandomization\nExcludability\nFirst-stage relatioship\nMonotonicity\n\nAllowing us estimate a Local Average Treatment Effect (LATE) using the only the variation in our treatment is exogenous (uncorrelated with ommited variables)\n\nclass: center ## IV Assumption: Randomization\n.left-column[ - No path from \\(U\\) to \\(Z\\)] .right-column[\n\n\n\n\n\n\n\n\n\nSource\n]\nclass: center ## IV Assumption: Excludability\n.left-column[ - No path from \\(Z\\) to \\(Y\\)]\n.right-column[\n\n\n\n\n\n\n\n\n\nSource\n]\nclass: center ## IV Assumption: First Stage\n.left-column[ - Path from \\(Z\\) to \\(D\\)] .right-column[\n\n\n\n\n\n\n\n\n\nSource\n]\nclass: center ## IV Assumption: Monotonicity\n.left-column[\n\n\\(D_i(Z=1)\\geq D_i(Z=0)\\)\n“No Defiers”\n\n]\n.right-column[\n\n\n\n\n\n\n\n\n\nSource ]\nclass: center ## Compliance\nWith a binary treatment, \\(D\\) and binary instrument \\(Z\\) there are four types of compliance\n\n\n\n\n\nType\n$D_i(Z=1)$\n$D_i(Z=0)$\n\n\n\n\nAlways Takers\n1\n1\n\n\nNever Takers\n0\n0\n\n\nCompliers\n1\n0\n\n\nDefiers\n0\n1\n\n\n\n\n\n\n\n\nAssuming Monotonicity means there are “No Defiers”"
  },
  {
    "objectID": "slides/04-slides-old.html#estimating-the-local-average-treatment-effect",
    "href": "slides/04-slides-old.html#estimating-the-local-average-treatment-effect",
    "title": "Week 04:",
    "section": "Estimating the Local Average Treatment Effect",
    "text": "Estimating the Local Average Treatment Effect\nIf we believe our assumptions of:\n\nRandomization\nExcludability\nFirst-stage relationship\nMonotonicity\n\nThen we can estimate Local Average Treatment Effect (LATE) sometimes called the Complier Average Treatment Effect CATE)"
  },
  {
    "objectID": "slides/04-slides-old.html#estimating-the-local-average-treatment-effect-1",
    "href": "slides/04-slides-old.html#estimating-the-local-average-treatment-effect-1",
    "title": "Week 04:",
    "section": "Estimating the Local Average Treatment Effect",
    "text": "Estimating the Local Average Treatment Effect\nIt can be shown that the LATE:\n\\[LATE = \\frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1]-E[D|Z=0]}= \\frac{ATE_{Z\\to Y}}{ATE_{Z\\to D}}\\]\nWhere:\n\n\\(ATE_{Z\\to Y}\\) is the known as the “Intent to Treat” effect (ITT) (i.e. the effect of being assigned to treatment)\n\\(ATE_{Z\\to D}\\) is the “effect” of being assigned to treatment on actually receiving treatment"
  },
  {
    "objectID": "slides/04-slides-old.html#example-earnings-and-military-service",
    "href": "slides/04-slides-old.html#example-earnings-and-military-service",
    "title": "Week 04:",
    "section": "Example: Earnings and Military Service",
    "text": "Example: Earnings and Military Service\nAdapted from Edward Rubin\nExample: If we want to estimate the effect of veteran status on earnings, \\[\\begin{align}\n  \\text{Earnings}_i = \\beta_0 + \\beta_1 \\text{Veteran}_i + u_i \\tag{1}\n\\end{align}\\]\n–\nWe would love to calculate \\(\\color{#e64173}{\\text{Earnings}_{1i}} - \\color{#6A5ACD}{\\text{Earnings}_{0i}}\\), but we can’t.\n–\nAnd OLS will likely be biased for \\((1)\\) due to selection/omitted-variable bias."
  },
  {
    "objectID": "slides/04-slides-old.html#introductory-example",
    "href": "slides/04-slides-old.html#introductory-example",
    "title": "Week 04:",
    "section": "Introductory example",
    "text": "Introductory example\nImagine that we can split veteran status into an exogenous (as-if random, unbiased) part and an endogenous (non-random, biased) part…\n–\n\\[\\begin{align}\n  \\text{Earnings}_i\n  &= \\beta_0 + \\beta_1 \\text{Veteran}_i + u_i \\tag{1} \\\\\n  &= \\beta_0 + \\beta_1 \\left(\\text{Veteran}_i^{\\text{Exog.}} + \\text{Veteran}_i^{\\text{Endog.}}\\right) + u_i \\\\\n  &= \\beta_0 + \\beta_1 \\text{Veteran}_i^{\\text{Exog.}} + \\underbrace{\\beta_1 \\text{Veteran}_i^{\\text{Endog.}} + u_i}_{w_i} \\\\\n  &= \\beta_0 + \\beta_1 \\text{Veteran}_i^{\\text{Exog.}} + w_i\n\\end{align}\\]\n–\nWe could use this exogenous variation in veteran status to consistently estimate \\(\\beta_1\\).\n–\nQ: What would exogenous variation in veteran status mean?"
  },
  {
    "objectID": "slides/04-slides-old.html#introductory-example-1",
    "href": "slides/04-slides-old.html#introductory-example-1",
    "title": "Week 04:",
    "section": "Introductory example",
    "text": "Introductory example\nQ: What would exogenous variation in veteran status mean?\n–\nA.sub[1]: Choices to enlist in the military that are essentially random—or at least uncorrelated with omitted variables and the disturbance.\n–\nA.sub[2]: .No selection bias: \\[\\begin{align}\n  \\color{#e64173}{\\mathop{E}\\left(\\text{Earnings}_{0i}\\mid\\text{Veteran}_i = 1\\right)} - \\color{#6A5ACD}{\\mathop{E}\\left( \\text{Earnings}_{0i} \\mid \\text{Veteran}_i = 0 \\right)} = 0\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-slides-old.html#instruments",
    "href": "slides/04-slides-old.html#instruments",
    "title": "Week 04:",
    "section": "Instruments",
    "text": "Instruments"
  },
  {
    "objectID": "slides/04-slides-old.html#q-how-do-we-isolate-this-exogenous-variation-in-our-explanatory-variable",
    "href": "slides/04-slides-old.html#q-how-do-we-isolate-this-exogenous-variation-in-our-explanatory-variable",
    "title": "Week 04:",
    "section": "Q: How do we isolate this exogenous variation in our explanatory variable?",
    "text": "Q: How do we isolate this exogenous variation in our explanatory variable?\nA: Find an instrument (an instrumental variable).\n–"
  },
  {
    "objectID": "slides/04-slides-old.html#q-whats-an-instrument",
    "href": "slides/04-slides-old.html#q-whats-an-instrument",
    "title": "Week 04:",
    "section": "Q: What’s an instrument?",
    "text": "Q: What’s an instrument?\nA: An instrument is a variable that is\n\ncorrelated with the explanatory variable of interest (relevant),\nuncorrelated with the error term (exogenous)."
  },
  {
    "objectID": "slides/04-slides-old.html#instruments-1",
    "href": "slides/04-slides-old.html#instruments-1",
    "title": "Week 04:",
    "section": "Instruments",
    "text": "Instruments\nSo if we want an instrument \\(z_i\\) for endogenous veteran status in\n\\[\\begin{align}\n  \\text{Earnings}_i = \\beta_0 + \\beta_1 \\text{Veteran}_i + u_i\n\\end{align}\\]\n\nRelevant: \\(\\mathop{\\text{Cov}} \\left( \\text{Veteran}_i,\\, z_i \\right) \\neq 0\\)\nExogenous: \\(\\mathop{\\text{Cov}} \\left( z_i,\\, u_i \\right) = 0\\)"
  },
  {
    "objectID": "slides/04-slides-old.html#instruments-relevance",
    "href": "slides/04-slides-old.html#instruments-relevance",
    "title": "Week 04:",
    "section": "Instruments: Relevance",
    "text": "Instruments: Relevance\nRelevance: We need the instrument to cause a change in (correlate with) our endogenous explanatory variable.\nWe can actually test this requirement using regression and a t test.\n–\nExample: For the veteran status, consider three potential instruments:\n.pull-left[ 1. Social security number\n\nPhysical fitness\nVietnam War draft ]\n\n–\n.pull-right[ - Probably not relevant uncorrelated with military service\n\nPotentially relevant service may correlate with fitness\nRelevant being drafted led to service ]"
  },
  {
    "objectID": "slides/04-slides-old.html#instruments-exogeneity",
    "href": "slides/04-slides-old.html#instruments-exogeneity",
    "title": "Week 04:",
    "section": "Instruments: Exogeneity",
    "text": "Instruments: Exogeneity\n.hi[Exogeneity:] The instrument to be independent of omitted factors that affect our outcome variable—as good as randomly assigned.\n\\(z_i\\) must be uncorrelated with our disturbance \\(u_i\\). .hi[Not testable.]\n–\nExample: For the .pink[veteran status], consider three potential instruments:\n.pull-left[ 1. Social security number\n\nPhysical fitness\nVietnam War draft ]\n\n–\n.pull-right[ - Exogenous SSN essentially random\n\nNot Exogenous fitness correlated with many things\nExogenous draft via lottery ]"
  },
  {
    "objectID": "slides/04-slides-old.html#relevant-and-exogenous",
    "href": "slides/04-slides-old.html#relevant-and-exogenous",
    "title": "Week 04:",
    "section": "Relevant and Exogenous",
    "text": "Relevant and Exogenous"
  },
  {
    "objectID": "slides/04-slides-old.html#relevant-not-exogenous",
    "href": "slides/04-slides-old.html#relevant-not-exogenous",
    "title": "Week 04:",
    "section": "Relevant, Not Exogenous",
    "text": "Relevant, Not Exogenous"
  },
  {
    "objectID": "slides/04-slides-old.html#not-relevant-and-not-exogenous",
    "href": "slides/04-slides-old.html#not-relevant-and-not-exogenous",
    "title": "Week 04:",
    "section": "Not Relevant and Not Exogenous",
    "text": "Not Relevant and Not Exogenous"
  },
  {
    "objectID": "slides/04-slides-old.html#relevant-not-exogenous-1",
    "href": "slides/04-slides-old.html#relevant-not-exogenous-1",
    "title": "Week 04:",
    "section": "Relevant, Not Exogenous",
    "text": "Relevant, Not Exogenous"
  },
  {
    "objectID": "slides/04-slides-old.html#iv-applications",
    "href": "slides/04-slides-old.html#iv-applications",
    "title": "Week 04:",
    "section": "IV Applications",
    "text": "IV Applications\n\n(AndrewHeiss?)"
  },
  {
    "objectID": "slides/04-slides-old.html#iv-summary",
    "href": "slides/04-slides-old.html#iv-summary",
    "title": "Week 04:",
    "section": "IV Summary",
    "text": "IV Summary\nInstrumental variables require a number of assumptions to yield credible causal claims:\n\nRandomization\nExcludability\nFirst-stage relationship\nMonotonicity\n\nEstimation and inference of IVs is beyond the scope of this course.\n\nSee Edward Rubin’s excellent slides\nAnd Matt Blackwells notes\nUnderstanding the identifying assumptions of IV can help you critique a study (even if the you don’t fully understand the math)\n\nclass: inverse, middle, center # 💡 ## Summary"
  },
  {
    "objectID": "slides/04-slides-old.html#what-you-need-to-know",
    "href": "slides/04-slides-old.html#what-you-need-to-know",
    "title": "Week 04:",
    "section": "What you need to know",
    "text": "What you need to know\n\nCausal inference in observational and experimental studies is about counterfactual comparisons\nIn observational studies, to make causal claims we generally make some assumption of conditional independence:\n\n\\[\nY_i(1),Y_i(0), \\perp D_i |X_i\n\\]\n\nThe credibility of this assumption depends less on the data, and more on how the data were generated.\nSelection on Observables is rarely a credible assumption\nObservational designs that produce credible causal inference, leverage aspects of the world that create natural experiments\nYou should be able to describe the logic and assumptions of common designs in social science\n\nDifference-in-Differences: Parallel Trends\nRegression Discontiniuity: Continuity at the cutoff\nInstrumental Variables: Instruments need to be Relevant and Exogenous\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/00-slides.html#overview",
    "href": "slides/00-slides.html#overview",
    "title": "Welcome to POLS 1600",
    "section": "Overview",
    "text": "Overview\n\n\nGoals and Expectations\n\n\n\n\nCourse Structure\n\n\n\n\nCourse Policies\n\n\n\n\nA Few Fundamental Truths"
  },
  {
    "objectID": "slides/00-slides.html#what-you-will-learn",
    "href": "slides/00-slides.html#what-you-will-learn",
    "title": "Welcome to POLS 1600",
    "section": "What you will learn",
    "text": "What you will learn\nYou will learn\n\n\nhow to think like a social scientist\n\n\n\n\nhow to use data to make descriptive, predictive, and causal claims\n\n\n\n\nhow to quantify uncertainty about these claims\n\n\n\n\nhow to present, interpret, and critique these claims"
  },
  {
    "objectID": "slides/00-slides.html#reasons-to-take-this-class",
    "href": "slides/00-slides.html#reasons-to-take-this-class",
    "title": "Welcome to POLS 1600",
    "section": "Reasons to take this class",
    "text": "Reasons to take this class\n\nYou want to change the world"
  },
  {
    "objectID": "slides/00-slides.html#why-is-this-study-important",
    "href": "slides/00-slides.html#why-is-this-study-important",
    "title": "Welcome to POLS 1600",
    "section": "Why is this study important?",
    "text": "Why is this study important?\n\n\nFindings provide evidence of benefits of social spending/universal basic income"
  },
  {
    "objectID": "slides/00-slides.html#why-should-we-believe-these-results",
    "href": "slides/00-slides.html#why-should-we-believe-these-results",
    "title": "Welcome to POLS 1600",
    "section": "Why should we believe these results",
    "text": "Why should we believe these results\n\n\nBecause it’s in the Times?\nBecause the authors are professors at good schools?\nBecause of how the study was done!\n\nRandom assignment provides a reasoned basis for inference\nCreates informative counter-factual comparisons\nPre-registered hypotheses ensure that we’re not cherry-picking results"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\n\nHow strong are the effects?\n\nIs a fifth of a standard deviation a lot?\n\nWhy do we care about brain waves?\nWhat’s the mechanism?\nHow confident are we that these results couldn’t have happened just by chance"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-1",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-1",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-2",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-2",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-3",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-3",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/00-slides.html#reasons-to-take-this-class-1",
    "href": "slides/00-slides.html#reasons-to-take-this-class-1",
    "title": "Welcome to POLS 1600",
    "section": "Reasons to take this class",
    "text": "Reasons to take this class\n\n\nYou want to change the world\n\nData, design, and analysis are incredlibly powerful tools\nYou want to understand their strengths and limits\n\nYou want to be a better consumer of data and knowledge\nYou want to be a better consumer of data and knowledge\nYou want to get a job / go to grad school\nYou have to\nYou’re just in it for the memes"
  },
  {
    "objectID": "slides/00-slides.html#great-expectations",
    "href": "slides/00-slides.html#great-expectations",
    "title": "Welcome to POLS 1600",
    "section": "Great expectations",
    "text": "Great expectations\n\nI expect that you will come to class ready to engage with:\n\nsocial science\ndata\nprogramming\nmath"
  },
  {
    "objectID": "slides/00-slides.html#requirements",
    "href": "slides/00-slides.html#requirements",
    "title": "Welcome to POLS 1600",
    "section": "Requirements",
    "text": "Requirements\nI assume that you will\n\n\nDo the readings\n\n\n\n\nBring your computers 1\n\n\n\n\nWork through classwork\n\n\n\n\nAsk questions\n\n\nIf you only have a desktop/or tablet let me know and we’ll figure out a solution."
  },
  {
    "objectID": "slides/00-slides.html#course-structure",
    "href": "slides/00-slides.html#course-structure",
    "title": "Welcome to POLS 1600",
    "section": "Course structure",
    "text": "Course structure"
  },
  {
    "objectID": "slides/00-slides.html#class",
    "href": "slides/00-slides.html#class",
    "title": "Welcome to POLS 1600",
    "section": "Class",
    "text": "Class\n\nTuesday: Lecture/Demonstration\nThursday: Lab/Exploration"
  },
  {
    "objectID": "slides/00-slides.html#class-websites",
    "href": "slides/00-slides.html#class-websites",
    "title": "Welcome to POLS 1600",
    "section": "Class websites",
    "text": "Class websites\n\nSlides, labsm and additional resources available here: pols1600.paultesta.org\nAssignments uploaded here: Canvas"
  },
  {
    "objectID": "slides/00-slides.html#software-and-computing",
    "href": "slides/00-slides.html#software-and-computing",
    "title": "Welcome to POLS 1600",
    "section": "Software and computing",
    "text": "Software and computing\n\nStatistics done using R\n\nOpen source (free) statistical language\n\nThrough R Studio\n\nAn integrated development environment for R\n\nResults written up using R Markdown\n\nLanguage for combing R code with html Markdown"
  },
  {
    "objectID": "slides/00-slides.html#r",
    "href": "slides/00-slides.html#r",
    "title": "Welcome to POLS 1600",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "slides/00-slides.html#r-studio",
    "href": "slides/00-slides.html#r-studio",
    "title": "Welcome to POLS 1600",
    "section": "R Studio",
    "text": "R Studio"
  },
  {
    "objectID": "slides/00-slides.html#quarto",
    "href": "slides/00-slides.html#quarto",
    "title": "Welcome to POLS 1600",
    "section": "Quarto",
    "text": "Quarto\n\n\n\nProject options in YAML\nCode in triple backtick chunks:\n\nChunk options set with “#|” (hashpipe)\n\n\n```{r}\n#| label = \"simulate_data\"\nx &lt;- rnorm(100)\ny &lt;- 2*x + rnorm(100)\n```\n\nWrite up in Markdown\nOutput rendered as an html file"
  },
  {
    "objectID": "slides/00-slides.html#getting-set-up-for-the-course",
    "href": "slides/00-slides.html#getting-set-up-for-the-course",
    "title": "Welcome to POLS 1600",
    "section": "Getting set up for the course:",
    "text": "Getting set up for the course:\nHere’s a link to a guide to get you setup for the course.\nTake a crack at it after class, over the weekend.\nEmail me with any issues (there are always issues), and drop by my office hours on Tuesday so we can trouble shoot."
  },
  {
    "objectID": "slides/00-slides.html#textbook",
    "href": "slides/00-slides.html#textbook",
    "title": "Welcome to POLS 1600",
    "section": "Textbook",
    "text": "Textbook\n\nhttps://press.princeton.edu/books/paperback/9780691222288/quantitative-social-science"
  },
  {
    "objectID": "slides/00-slides.html#how-to-read-imai",
    "href": "slides/00-slides.html#how-to-read-imai",
    "title": "Welcome to POLS 1600",
    "section": "How to Read Imai",
    "text": "How to Read Imai\n\nActive reading\nCopy and run the code in the text. To do so, do the following:\n\n\nif (!require(\"devtools\")){\n  install.packages(\"devtools\")\n  }\nlibrary(\"devtools\")\ninstall_github(\"kosukeimai/qss-package\",  \n               build_vignettes  =  TRUE)"
  },
  {
    "objectID": "slides/00-slides.html#how-to-read-imai-1",
    "href": "slides/00-slides.html#how-to-read-imai-1",
    "title": "Welcome to POLS 1600",
    "section": "How to Read Imai",
    "text": "How to Read Imai\nOnce you’ve rune the following\n\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\nAnywhere the text loads data:\n\nafghan &lt;- read_csv(\"afgahn.csv\")\n\nYou can do\n\nlibrary(\"qss\")\ndata(\"afghan\")\nsummary(afghan$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.00   22.00   30.00   32.39   40.00   80.00"
  },
  {
    "objectID": "slides/00-slides.html#additional-readings",
    "href": "slides/00-slides.html#additional-readings",
    "title": "Welcome to POLS 1600",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nOccasionally, I will provide additional readings, available on both Canvas and pols1600.paultesta.org"
  },
  {
    "objectID": "slides/00-slides.html#assignments-1",
    "href": "slides/00-slides.html#assignments-1",
    "title": "Welcome to POLS 1600",
    "section": "Assignments",
    "text": "Assignments\nYou have three types of assignments in this course\n\nLabs\nTutorials\nFinal Project"
  },
  {
    "objectID": "slides/00-slides.html#labs",
    "href": "slides/00-slides.html#labs",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs\n\nEach Thursday we will work in groups to complete an in-class lab\nThe labs are designed to reinforce and extend concepts from lecture using real world data."
  },
  {
    "objectID": "slides/00-slides.html#labs-1",
    "href": "slides/00-slides.html#labs-1",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "slides/00-slides.html#labs-2",
    "href": "slides/00-slides.html#labs-2",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "slides/00-slides.html#labs-3",
    "href": "slides/00-slides.html#labs-3",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs\n\nWeeks 1 and 2 we’ll work collectively\nWeeks 3 on, you’ll be assigned to small groups\nEach week:\n\nLog on to the Canvas, download the lab .qmd file\nOpen R Studio\nRender the qmd file to get ready to work\nComplete the lab\nUpload the rendered html file to Canvas by the end of class\n\nOne question randomly graded\n\n100% if correct\n85% if incorrect, but you tried\n0% if you did not try/absent for the lab\n\nComments/Answers posted immediately after class"
  },
  {
    "objectID": "slides/00-slides.html#problem-setstutorials",
    "href": "slides/00-slides.html#problem-setstutorials",
    "title": "Welcome to POLS 1600",
    "section": "Problem Sets/Tutorials",
    "text": "Problem Sets/Tutorials\n\nCoding tutorials to reinforce concepts from lecture and textbook.\nAccessed by running\n\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\n\nComplete the tutorial. Save output as “LASTNAME_TutorialNumber.pdf”\nUpload output to Canvas by Friday by 11:59 pm\nGrades:\n\n100% any upload\n0% no upload"
  },
  {
    "objectID": "slides/00-slides.html#final-project",
    "href": "slides/00-slides.html#final-project",
    "title": "Welcome to POLS 1600",
    "section": "Final Project",
    "text": "Final Project"
  },
  {
    "objectID": "slides/00-slides.html#your-first-assignment",
    "href": "slides/00-slides.html#your-first-assignment",
    "title": "Welcome to POLS 1600",
    "section": "Your First Assignment:",
    "text": "Your First Assignment:\n\nDownload and install R and R Studio\n\nEmail me if you have troubles\nTroubleshoot by Zoom or in-person (111 Thayer Room 339)\n\nWork through 00-software_setup before next class."
  },
  {
    "objectID": "slides/00-slides.html#portals-of-discovery",
    "href": "slides/00-slides.html#portals-of-discovery",
    "title": "Welcome to POLS 1600",
    "section": "Portals of Discovery",
    "text": "Portals of Discovery"
  },
  {
    "objectID": "slides/00-slides.html#errors",
    "href": "slides/00-slides.html#errors",
    "title": "Welcome to POLS 1600",
    "section": "Errors",
    "text": "Errors\n\nish happens\nSeeing red is a good thing\nWe learn by making errors"
  },
  {
    "objectID": "slides/00-slides.html#final-reports",
    "href": "slides/00-slides.html#final-reports",
    "title": "Welcome to POLS 1600",
    "section": "Final Reports",
    "text": "Final Reports\n\nCan be on any topic you like\nMore info to come\nDue dates:\n\nWeek 2 Groups assigned\nWeek 3 Research Topics\nWeek 6 Data Proposal\nWeek 8 Data Explorations\nWeek 11 Drafts\nWeek 12 Presentations\nWeek 13 Final Paper"
  },
  {
    "objectID": "slides/00-slides.html#grading",
    "href": "slides/00-slides.html#grading",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-1",
    "href": "slides/00-slides.html#grading-1",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-2",
    "href": "slides/00-slides.html#grading-2",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-3",
    "href": "slides/00-slides.html#grading-3",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-4",
    "href": "slides/00-slides.html#grading-4",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading\n\n5% Attendance\n10% Class involvement and participation\n10% Tutorials\n30% Labs\n20% Assignments for final paper\n20% Final paper"
  },
  {
    "objectID": "slides/00-slides.html#course-policies",
    "href": "slides/00-slides.html#course-policies",
    "title": "Welcome to POLS 1600",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nCommunity standards\nIncomplete/late work"
  },
  {
    "objectID": "slides/00-slides.html#two-fundamental-truths",
    "href": "slides/00-slides.html#two-fundamental-truths",
    "title": "Welcome to POLS 1600",
    "section": "Two Fundamental Truths",
    "text": "Two Fundamental Truths"
  },
  {
    "objectID": "slides/00-slides.html#testas-first-fundamental-truth",
    "href": "slides/00-slides.html#testas-first-fundamental-truth",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s first fundamental truth",
    "text": "Testa’s first fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#testas-first-fundamental-truth-1",
    "href": "slides/00-slides.html#testas-first-fundamental-truth-1",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s first fundamental truth",
    "text": "Testa’s first fundamental truth\nWhy would I profess my utter ignorance on the first day of class?\nFour possible reasons…"
  },
  {
    "objectID": "slides/00-slides.html#expectation-management",
    "href": "slides/00-slides.html#expectation-management",
    "title": "Welcome to POLS 1600",
    "section": "1. Expectation Management",
    "text": "1. Expectation Management"
  },
  {
    "objectID": "slides/00-slides.html#pedagogical-tomfoolery",
    "href": "slides/00-slides.html#pedagogical-tomfoolery",
    "title": "Welcome to POLS 1600",
    "section": "2. Pedagogical Tomfoolery",
    "text": "2. Pedagogical Tomfoolery"
  },
  {
    "objectID": "slides/00-slides.html#positionality",
    "href": "slides/00-slides.html#positionality",
    "title": "Welcome to POLS 1600",
    "section": "3. Positionality",
    "text": "3. Positionality"
  },
  {
    "objectID": "slides/00-slides.html#epistemology",
    "href": "slides/00-slides.html#epistemology",
    "title": "Welcome to POLS 1600",
    "section": "4. Epistemology",
    "text": "4. Epistemology"
  },
  {
    "objectID": "slides/00-slides.html#testas-second-fundamental-truth",
    "href": "slides/00-slides.html#testas-second-fundamental-truth",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s second fundamental truth",
    "text": "Testa’s second fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#testas-second-fundamental-truth-1",
    "href": "slides/00-slides.html#testas-second-fundamental-truth-1",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s second fundamental truth",
    "text": "Testa’s second fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#two-kinds-of-people-in-this-world",
    "href": "slides/00-slides.html#two-kinds-of-people-in-this-world",
    "title": "Welcome to POLS 1600",
    "section": "Two kinds of people in this world",
    "text": "Two kinds of people in this world"
  },
  {
    "objectID": "slides/00-slides.html#what-is-it-that-we-say-we-do-here",
    "href": "slides/00-slides.html#what-is-it-that-we-say-we-do-here",
    "title": "Welcome to POLS 1600",
    "section": "What is it that we say we do here",
    "text": "What is it that we say we do here"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do",
    "href": "slides/00-slides.html#what-does-quantitative-research-do",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions"
  },
  {
    "objectID": "slides/00-slides.html#descriptions",
    "href": "slides/00-slides.html#descriptions",
    "title": "Welcome to POLS 1600",
    "section": "Descriptions",
    "text": "Descriptions"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-1",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-1",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations"
  },
  {
    "objectID": "slides/00-slides.html#explanations",
    "href": "slides/00-slides.html#explanations",
    "title": "Welcome to POLS 1600",
    "section": "Explanations",
    "text": "Explanations"
  },
  {
    "objectID": "slides/00-slides.html#explanations-1",
    "href": "slides/00-slides.html#explanations-1",
    "title": "Welcome to POLS 1600",
    "section": "Explanations",
    "text": "Explanations"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-2",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-2",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations\nPredictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty",
    "href": "slides/00-slides.html#predictions-and-uncertainty",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty-1",
    "href": "slides/00-slides.html#predictions-and-uncertainty-1",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty-2",
    "href": "slides/00-slides.html#predictions-and-uncertainty-2",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-3",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-3",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations\nPredictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#two-kinds-of-people-in-this-world-1",
    "href": "slides/00-slides.html#two-kinds-of-people-in-this-world-1",
    "title": "Welcome to POLS 1600",
    "section": "Two kinds of people in this world",
    "text": "Two kinds of people in this world"
  },
  {
    "objectID": "slides/00-slides.html#introductions-1",
    "href": "slides/00-slides.html#introductions-1",
    "title": "Welcome to POLS 1600",
    "section": "Introductions",
    "text": "Introductions"
  },
  {
    "objectID": "slides/00-slides.html#my-research",
    "href": "slides/00-slides.html#my-research",
    "title": "Welcome to POLS 1600",
    "section": "My research",
    "text": "My research\n\nI study American Poltical Behavior with focus on poltics of race and criminal justice\nHow do we break cycles of inequality when those most affected by injustice are the least likely to participate and those unaffected are the least likely to care?\nHow can we use methodological tools to better answer these questions?"
  },
  {
    "objectID": "slides/00-slides.html#but-enough-about-me",
    "href": "slides/00-slides.html#but-enough-about-me",
    "title": "Welcome to POLS 1600",
    "section": "But enough about me",
    "text": "But enough about me"
  },
  {
    "objectID": "slides/00-slides.html#class-survey",
    "href": "slides/00-slides.html#class-survey",
    "title": "Welcome to POLS 1600",
    "section": "Class survey",
    "text": "Class survey\nPlease click here to take a brief survey that will help me structure the class going forward."
  },
  {
    "objectID": "slides/00-slides.html#next-week",
    "href": "slides/00-slides.html#next-week",
    "title": "Welcome to POLS 1600",
    "section": "Next Week:",
    "text": "Next Week:\n\nComplete the class survey\nDownload and Install R and R studio\nRead Chapters 1 (Friday) and start Chapter 3 in QSS\nTuesday: Lecture: Describing Data in R\nThursday: Lab: Exploring COVID-19 data in the US\nFriday: Submit Tutorials: “00-intro” & “01-measurement”\n\nOnly time you’ll have two tutorials due (Ok to submit late)\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/07-slides.html#general-plan",
    "href": "slides/07-slides.html#general-plan",
    "title": "Week 07:",
    "section": "General Plan",
    "text": "General Plan\n\nGroup Assignment 2: Data\nSetup\n\nPackages\nData\n\nReview\n\nMultiple Regression\nRegression Discontinuity Designs\n\nInterpreting regression models\n\nWhat does it mean to control for “variables”\nProducing predicted values from our models\nEvaluating model performance\n\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/07-slides.html#new-packages",
    "href": "slides/07-slides.html#new-packages",
    "title": "Week 07:",
    "section": "New packages",
    "text": "New packages\nNo new packages this week"
  },
  {
    "objectID": "slides/07-slides.html#packages-for-today",
    "href": "slides/07-slides.html#packages-for-today",
    "title": "Week 07:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/07-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/07-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 07:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/07-slides.html#load-packages-for-today",
    "href": "slides/07-slides.html#load-packages-for-today",
    "title": "Week 07:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today"
  },
  {
    "objectID": "slides/07-slides.html#data-red-covid",
    "href": "slides/07-slides.html#data-red-covid",
    "title": "Week 07:",
    "section": "Data: Red Covid",
    "text": "Data: Red Covid\nToday we’ll work with the data from last week’s lab\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))"
  },
  {
    "objectID": "slides/07-slides.html#data-russian-opinion-on-the-war-in-ukraine",
    "href": "slides/07-slides.html#data-russian-opinion-on-the-war-in-ukraine",
    "title": "Week 07:",
    "section": "Data: Russian Opinion on the War In Ukraine",
    "text": "Data: Russian Opinion on the War In Ukraine\n\nWe’ll also load the data for this week’s lab, which examines a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project\nThe code chunk below sources a script I wrote called drww_english_recode.R to download the raw data and recode Cyrilic into English\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))"
  },
  {
    "objectID": "slides/07-slides.html#hlo-of-data",
    "href": "slides/07-slides.html#hlo-of-data",
    "title": "Week 07:",
    "section": "HLO of data",
    "text": "HLO of data\n\nglimpse(df_drww)\n\nRows: 1,807\nColumns: 42\n$ sex                          &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Male…\n$ age                          &lt;dbl&gt; 99, 78, 73, 73, 69, 69, 59, 54, 49, 48, 4…\n$ support_war                  &lt;fct&gt; \"(НЕ ЗАЧИТЫВАТЬ) Затрудняюсь ответить\", \"…\n$ trust_gov                    &lt;fct&gt; (НЕ ЗАЧИТЫВАТЬ) Затрудняюсь ответить, Дов…\n$ employ_working               &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,…\n$ employ_student               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_retired               &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_maternity_leave       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_homemaker             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ employ_unemployed            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,…\n$ employ_other_employment      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ other_employ_open_response   &lt;chr&gt; \"                                        …\n$ education                    &lt;fct&gt; (НЕ ЗАЧИТЫВАТЬ) Затрудняюсь ответить, Выс…\n$ social_classmates            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_in_contact_with       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_facebook              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_instagram             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_twitter               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_telegram              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_whatsapp              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_viber                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ social_tiktok                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,…\n$ social_other_social_networks &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ none_social                  &lt;fct&gt; НЕ ВЫБРАН, ВЫБРАН, НЕ ВЫБРАН, ВЫБРАН, НЕ …\n$ dk_social                    &lt;fct&gt; ВЫБРАН, НЕ ВЫБРАН, НЕ ВЫБРАН, НЕ ВЫБРАН, …\n$ other_social_open_response   &lt;chr&gt; \"                                        …\n$ other_social_group           &lt;fct&gt; NA, NA, Ютуб, NA, Ютуб, NA, NA, NA, NA, N…\n$ geo_urban_rural              &lt;fct&gt; NA, NA, \"город, поселок городского типа\",…\n$ geo_district                 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ weight                       &lt;dbl&gt; 0.000, 0.000, 1.208, 0.000, 1.231, 1.208,…\n$ support_war_f                &lt;fct&gt; NA, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes…\n$ support_war01                &lt;dbl&gt; NA, 1, 1, 1, 1, 1, 0, 1, 1, NA, 1, 1, 1, …\n$ trust_gov_f                  &lt;fct&gt; NA, Partly, Largely, Fully, Partly, Fully…\n$ trust_gov_n                  &lt;dbl&gt; NA, 1, 2, 3, 1, 3, 0, NA, 2, NA, NA, 0, 3…\n$ education_f                  &lt;fct&gt; NA, College or some college, Vocational, …\n$ education_n                  &lt;dbl&gt; NA, 4, 3, 4, 1, 3, 4, NA, 4, NA, 3, 3, 3,…\n$ geo_urban_rural_f            &lt;fct&gt; NA, NA, Urban, NA, Rural, Urban, Rural, N…\n$ geo_district_f               &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ social_youtube               &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ social_yandex                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ total_social_media_use       &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 9, 0, 1, 0, 1, 0,…\n$ no_social_media              &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n\n\n\n\n\n\n\n\nclass:inverse, center, middle # 📢 ## Feedback\nStill time to take weekly survey from last week, which will have a real, measurable impact on class on Thursday:\nhttps://brown.co1.qualtrics.com/jfe/form/SV_6mUeyI7tqVURHQW"
  },
  {
    "objectID": "slides/07-slides.html#what-do-you-need-to-know",
    "href": "slides/07-slides.html#what-do-you-need-to-know",
    "title": "Week 07:",
    "section": "What do you need to know?",
    "text": "What do you need to know?\n–\n\nIt depends. What do you want to do?\n\n–\n\nStudents who want to do their own research (in a good way)\n\nSkills to work with data (Data Wrangling)\nTools to think about questions and theory (Causal Inference)\nTools to describe relationships and test claims (Estimation with Linear Models)\nTools to describe uncertainty in these relationships (Statistical inference)"
  },
  {
    "objectID": "slides/07-slides.html#do-you-need-to-know-the-math",
    "href": "slides/07-slides.html#do-you-need-to-know-the-math",
    "title": "Week 07:",
    "section": "Do you need to know the math?",
    "text": "Do you need to know the math?\n\n\n\n- Yes!\n\n\n- Math is a precise and concise way to describe what we’re doing - A whole range of studies can be described by the same design (experiment, Diff-in-Diff, regression, etc) represented with the same formula - If you want to prove some property of some formula (e.g. show that under the Gauss-Markov assumptions linear regression is BLUE), you will do this with Math.\n\n\n## Do you need to know the math?\n\n\n- No!\n\n\n- Computers love math. Humans not so much. - We can understand and demonstrate a lot of methods and theories through programming - It’s easier to simulate the Central Limit Theorem than it is to prove it. - But the reasons the simulations “work” is because of the underlying math\n\n\n## Do you need to know the math?\n\n\n- Maybe!\n\n\n- I assume very little background math - I show you some mathematical concepts because: - For some people it’s helpful - You’ll encounter these symbols and formulas elsewhere - Even if you don’t understand/remember the chain rule you can hopefully understand at a conceptual level what’s going on when we minimize the SSR - If you want to go to grad school, you’ll have to do the math - If you want to do research, you’ll have understand the implications of the math\n\n\n## Do you need to know how to program?\n\n\n- Yes.\n\n\n- It’s unavoidable, if you want to do applied, empirical work\n\n\n- It lets us avoid having to walk through formal proofs\n\n\n- It’s incredibly marketable.\n\n\nclass:inverse, middle, center # 🔍 ## Review: Regression\n\n\n## What you need to know about Regression: Conceptual - Simple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n\n\\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\]\n\n\n- Multiple regression generalizes this approach to include multiple predictors\n\n\n\\[y_i = \\beta_0 + \\beta_1x_1 +  \\beta_2 x_2 + \\dots + \\beta_j x_j + \\epsilon_i \\] \\[y_i = X\\beta + \\epsilon_i \\] - The coefficients in regression models tell us how the outcome \\(y\\) is expected to change as some predictor, \\(x\\) changes.\n\n\n## What you need to know about Regression: Practical\n\n\n- We estimate regressions using the lm() function\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} m1 &lt;- lm(y ~ x, data = df) :::\n\n\n- We control for additional variables by adding them to the formula\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} m2 &lt;- lm(y ~ x + z, data = df) :::\n\n\n- We can interact variables using the * operator. This creates a new variable that is the product of the two\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} m3 &lt;- lm(y ~ x*z, data = df) # Same as m3 &lt;- lm(y ~ x + z + x:z, data = df) :::\n\n\n## What you need to know about Regression: Practical\n\n\n- We can get statistical summaries of regression models using the summary() function\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} summary(m1) :::\n\n\n- We display this information in a regression table: - Each column is a model - Each named row is a coefficient - The values in parantheses are standard errors - Coefficients with *’s are statistically significant\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} texreg::htmlreg(list(m1, m2, m3)) :::\n\n\n## What is good but not strictly necessary to know about Regression\n\n\n- Technical/Definitional - Linear regression chooses coefficients to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta\n} \\text{ arg min}_{\\hat{\\beta\n}} \\sum (y_i-(X\\hat{\\beta}))^2\\]\n\n\n- Theoretical - Linear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\)\n\n\nclass: inverse, center, middle # 💡 Regression Discontinuity Design\n\n\n## Motivating Example\n\n\n.pull-left[ - Do Members of Parliament in the UK get richer from holding office (QSS Chapter 4.3.4)] .pull-right[\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\n\nEggers and Hainmueller (2009) ]\n\n\n## Logic of the Regression Discontinuity Design (RDD)\n\n\n- What’s the effect of holding elected office in the UK on personal wealth?\n\n\n- People who win elections differ in many ways from people who lose elections.\n\n\n- Logic of an RDD:\n\n\n- Just look at the wealth of individuals who either narrowly won or lost elections.\n\n\n- Candidates close to 50 percent cutoff (discontinuity) should be more comparable (better counterfactuals)\n\n\n## Data from Eggers and Hainmueller (2009)\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} library(qss) data(MPs) glimpse(MPs)\n\n\n::: {.cell-output .cell-output-stdout}\n\n\nRows: 427 Columns: 10 $ surname    &lt;chr&gt; \"Llewellyn\", \"Morris\", \"Walker\", \"Walker\", \"Waring\", \"Brown… $ firstname  &lt;chr&gt; \"David\", \"Claud\", \"George\", \"Harold\", \"John\", \"Ronald\", \"Le… $ party      &lt;chr&gt; \"tory\", \"labour\", \"tory\", \"labour\", \"tory\", \"labour\", \"tory… $ ln.gross   &lt;dbl&gt; 12.13591, 12.44809, 12.42845, 11.91845, 13.52022, 12.46052,… $ ln.net     &lt;dbl&gt; 12.135906, 12.448091, 10.349009, 12.395034, 13.520219, 9.63… $ yob        &lt;int&gt; 1916, 1920, 1914, 1927, 1923, 1921, 1907, 1912, 1905, 1920,… $ yod        &lt;int&gt; 1992, 2000, 1999, 2003, 1989, 2002, 1987, 1984, 1998, 2004,… $ margin.pre &lt;dbl&gt; NA, NA, -0.057168204, -0.072508894, -0.269689620, 0.3409586… $ region     &lt;chr&gt; \"Wales\", \"South West England\", \"North East England\", \"Yorks… $ margin     &lt;dbl&gt; 0.05690404, -0.04973833, -0.04158868, 0.02329524, -0.230005…\n\n\n::: :::\n\n\n## Variables\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}\n\n\n`````{=html}\n\n\n`````\n\n\n::: :::\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} MPs %&gt;% ggplot(aes(margin, ln.net))+ geom_point(shape=1)+ facet_grid(~party)+ geom_smooth(data =MPs %&gt;% filter(margin &lt;0), method = \"lm\")+ geom_smooth(data =MPs %&gt;% filter(margin &gt;0), method = \"lm\")+ theme_bw() -&gt; fig_rdd fig_rdd :::\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\n\n## RDD Notation\n\n\n- \\(X\\) is a forcing variable - Treatment \\(D\\) is a determined by \\(X\\)\n\n\n\\[\nD_i = 1\\{X_i &gt; c\\}\n\\]\n\n\n- \\(X\\) is the margin variable in the example data, and \\(D=1\\) if margin is greater than 0 (i.e. the candidate won the election)\n\n\n- Interested in the differences in the outcome at the threshold\n\n\n\\[\\lim_{x \\downarrow  c} E[Y_i|X=x] - \\lim_{x \\uparrow  c} E[Y_i|X=x]\\]\n\n\n## Causal Identification with an RDD\n\n\nIf we assume \\(E[Y_i(0)|X=x]\\) and \\(E[Y_i(1)|X=x]\\) are continuous in x, then we can estimate a (local) ATE at the threshold:\n\n\n\\[\\begin{align}\nATE_{RDD} &= E[Y(1)-Y(0)|X_i=c] \\\\\n&=  E[Y(1)|X_i=c] -  E[Y(0)|X_i=c]\\\\\n&= \\lim_{x \\downarrow  c} E[Y_i|X=x] - \\lim_{x \\uparrow  c} E[Y_i|X=x] \\\\\n\\end{align}\\]\n\n\n## Continuity Assumption\n\n\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\n\nCunningham (2022)\n\n\n## Causal Identification with an RDD\n\n\n- The continuity assumption is a formal way of saying that observations close to the threshold are good counterfactuals for each other\n\n\n- We can’t prove this assumption\n\n\n- But if it holds, we should observe\n\n\n- no sorting around the cutoff (no self selection)\n\n\n- similar distributions of covariates around the cutoff (balance tests)\n\n\n- no effect of treatment on things measured pre-treatment (placebo tests)\n\n\n## Summary: Regression Discontinuity Designs\n\n\n- Regression discontinuity designs (RDD) leverage natural discontinuities in the probability of receiving treatment to estimate *local average treatment effects\n\n\n- Common discontinuities include: elections, test score cutoffs, time/events\n\n\n- A local average treatment effect means the effect applies only to a subset of observations, here those observations at or close to the cutoff.\n\n\n- The identifying assumption of an RDD is continuity at the cutoff - This implies that observations just below the cutoff for receiving treatment provide reasonable counterfactuals to those just above the cutoff - We can’t prove this, but we can test it’s empirical implications (i.e. covariate balance near the cutoff)\n\n\nclass: inverse, center, middle # 💡 # What does it mean to “control for x”\n\n\n## Models partion variance\n\n\n- Regression models “partition variance”\n\n\n- They separate the variation in the outcome (the thing we’re trying to explain), into variation explained by the predictors in our model and the remaining variation not explained by these predictors\n\n\n## Models partion variance\n\n\n\\[\\begin{aligned}\n\\textrm{Total Variance} &= \\textrm{Variance Explained by Model} + \\textrm{Unexplained Variance} \\\\\n\\textrm{Observed} &= \\textrm{Predicted Value} + \\textrm{Error}\\\\\n\\textrm{Y} &=  E[Y|X] + \\epsilon\\\\\n\\textrm{Y} &=  X\\hat{\\beta} + \\hat{\\epsilon}\\\\\n\\textrm{Y} &= \\hat{Y} + \\hat{\\epsilon}\n\\end{aligned}\\]\n\n\n## Coefficients describe the unique variance in Y explained by X (and only X)\n\n\nWhen we fit a multiple regression model, the coefficients in that model describe the variation in the outcome explained by that predictor, and only that predictor.\n\n\nLet’s fit three models from last week’s lab\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} # load(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\")) m1 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std, covid_lab) m2 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std + med_age_std, covid_lab) m3 &lt;- lm(new_deaths_pc_14da ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab) :::\n\n\n## Why do coefficients change when we control for variables?\n\n\n::: {.cell layout-align=“center”}\n\n\n{.r .cell-code} htmlreg(list(m1, m2, m3)) %&gt;% HTML() %&gt;% browsable()\n\n\n::: {.cell-output-display}\n\n\n```{=html}\n\n\n```\n\n\n::: :::\n\n\n## Residualized Regression\n\n\n\n\nResidualized regression is way of understanding what it means to control for variables in a regression.\n\n–\n\nResiduals are the part of the outcome variable, not explained by the predictors in a model\n\n\\[y = \\overbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2  + \\dots \\beta_j x_j}^{\\text{Predictors}} + \\underbrace{\\epsilon}_{\\text{Residuals}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression",
    "href": "slides/07-slides.html#residualized-regression",
    "title": "Week 07:",
    "section": "Residualized Regression",
    "text": "Residualized Regression\n\nResiduals are uncorrelated with (orthogonal to) predictors \\(X\\), and predicted values \\(X\\beta\\)"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression-1",
    "href": "slides/07-slides.html#residualized-regression-1",
    "title": "Week 07:",
    "section": "Residualized Regression",
    "text": "Residualized Regression\n\nResiduals are uncorrelated with predictors \\(X\\), and predicted values \\(X\\beta\\)\nWe can verify this for m2 below. We’d find the same for m3\n\n\ncor(resid(m2),covid_lab$rep_voteshare_std) \n\n[1] -3.051486e-17\n\ncor(resid(m2),covid_lab$med_age_std)\n\n[1] 5.297719e-17\n\ncor(resid(m2),fitted(m2))\n\n[1] -6.728116e-17\n\n\nResidualized Regression\nWe can think of coefficients in a multiple regression as describing the variation in the outcome explained by that predictor, and only that predictor.\nResidualized Regression\nSo for a model like m2:\n\ncoef(m2)\n\n      (Intercept) rep_voteshare_std       med_age_std \n       0.57530417        0.24497151        0.06121058 \n\n\nWe can recover the coefficient on rep_voteshare_std by:\n\nRegressing new_deaths_pc_14da on med_age_std\n\n\nThe residuals from this regression represent the variation in Covid-19 deaths not explained by the median age of a states’ populations\n\n\nRegressing rep_voteshare_std on med_age_std\n\n\nThe residuals from this regression represent the variation in Republican Vote Share not explained by age\n\n\nRegressing the residuals from 1. (Deaths not explained by age) on the residuals from 2. (Vote share not explained by age)\n\n\nThe coefficient from this simple residualized regression will be exactly the same as the coefficient for rep_voteshare_std from m2"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression-4",
    "href": "slides/07-slides.html#residualized-regression-4",
    "title": "Week 07:",
    "section": "Residualized Regression",
    "text": "Residualized Regression\n\n# 1. Regressing `new_deaths_pc_14da` on `med_age_std`\nm2_death_by_age &lt;- lm(new_deaths_pc_14da ~ med_age_std, covid_lab)\n# Save residuals\ncovid_lab$res_death_no_age &lt;- resid(m2_death_by_age)\n\n# 2. Regressing `rep_voteshare_std` on `med_age_std` \nm2_repvs_by_age &lt;- lm(rep_voteshare_std ~ med_age_std, covid_lab)\n# Save residuals\ncovid_lab$res_repvs_no_age &lt;- resid(m2_repvs_by_age)\n\n# 3. Residualized regression of deaths on Rep Vote Share\nm2_res &lt;- lm(res_death_no_age ~ res_repvs_no_age, covid_lab)\n\nbackground-image:url(“https://i.imgflip.com/68by8w.jpg”) backgroun-size:contain"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression-5",
    "href": "slides/07-slides.html#residualized-regression-5",
    "title": "Week 07:",
    "section": "Residualized Regression",
    "text": "Residualized Regression\n\n# Mutliple regression\ncoef(m2)[2]\n\nrep_voteshare_std \n        0.2449715 \n\n# Residualized regression\ncoef(m2_res)[2]\n\nres_repvs_no_age \n       0.2449715"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression-6",
    "href": "slides/07-slides.html#residualized-regression-6",
    "title": "Week 07:",
    "section": "Residualized Regression",
    "text": "Residualized Regression\nThe same principle holds when controlling for multiple factors like age and income\n\nRegress the outcome on the other controls\nRegress the predictor of interest on the other controls\nRegress the residuals from 1. and on the residuals from 2 to get the multiple regression coefficient\n\n\n# 1. Regressing `new_deaths_pc_14da` on `med_age_std`\nm3_death_by_age_income &lt;- lm(new_deaths_pc_14da ~ med_age_std + med_income_std, covid_lab)\n# Save residuals\ncovid_lab$res_death_no_age_income &lt;- resid(m3_death_by_age_income)\n\n# 2. Regressing `rep_voteshare_std` on `med_age_std` \nm3_repvs_by_age_income &lt;- lm(rep_voteshare_std ~ med_age_std + med_income_std, covid_lab)\n# Save residuals\ncovid_lab$res_repvs_no_age_income &lt;- resid(m3_repvs_by_age_income)\n\n# 3. Residualized regression of deaths on Rep Vote Share\nm3_res &lt;- lm(res_death_no_age_income ~ res_repvs_no_age_income, covid_lab)\n\n# multiple regression coefficient\ncoef(m3)[2]\n\nrep_voteshare_std \n       0.09685722 \n\n# Same as  residualized regression coefficient\ncoef(m3_res)[2]\n\nres_repvs_no_age_income \n             0.09685722"
  },
  {
    "objectID": "slides/07-slides.html#why-did-the-coefficient-on-rep-vote-share-change-in-m3-but-not-m2",
    "href": "slides/07-slides.html#why-did-the-coefficient-on-rep-vote-share-change-in-m3-but-not-m2",
    "title": "Week 07:",
    "section": "Why did the coefficient on Rep Vote Share change in m3 but not m2?",
    "text": "Why did the coefficient on Rep Vote Share change in m3 but not m2?\n\n\n\nStatistical models\n\n\n \nDV: Death\nDV: Vote Share\nDV: Res. Deaths\n\n\n \nBaseline\nMutliple\nDeaths\nVote Share\nDeaths\n\n\n\n\n(Intercept)\n0.58***\n0.58***\n0.58***\n0.00\n0.00\n\n\n \n(0.05)\n(0.05)\n(0.06)\n(0.14)\n(0.05)\n\n\nrep_voteshare_std\n0.23***\n0.24***\n \n \n \n\n\n \n(0.05)\n(0.05)\n \n \n \n\n\nmed_age_std\n \n0.06\n-0.02\n-0.31*\n \n\n\n \n \n(0.05)\n(0.06)\n(0.14)\n \n\n\nres_repvs_no_age\n \n \n \n \n0.24***\n\n\n \n \n \n \n \n(0.05)\n\n\nR2\n0.31\n0.33\n0.00\n0.10\n0.33\n\n\nAdj. R2\n0.29\n0.30\n-0.02\n0.08\n0.31\n\n\nNum. obs.\n50\n50\n50\n50\n50\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical models\n\n\n \nDV: Death\nDV: Vote Share\nDV: Res. Death\n\n\n \nBaseline\nMutliple\nDeaths\nVote Share\nDeaths\n\n\n\n\n(Intercept)\n0.58***\n0.58***\n0.58***\n0.00\n-0.00\n\n\n \n(0.05)\n(0.04)\n(0.05)\n(0.09)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.10\n \n \n \n\n\n \n(0.05)\n(0.07)\n \n \n \n\n\nmed_age_std\n \n0.00\n-0.03\n-0.35***\n \n\n\n \n \n(0.05)\n(0.05)\n(0.10)\n \n\n\nmed_income_std\n \n-0.19**\n-0.26***\n-0.69***\n \n\n\n \n \n(0.07)\n(0.05)\n(0.10)\n \n\n\nres_repvs_no_age_income\n \n \n \n \n0.10\n\n\n \n \n \n \n \n(0.07)\n\n\nR2\n0.31\n0.43\n0.41\n0.58\n0.04\n\n\nAdj. R2\n0.29\n0.40\n0.38\n0.56\n0.02\n\n\nNum. obs.\n50\n50\n50\n50\n50\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#summary-what-does-it-mean-to-control-for-x",
    "href": "slides/07-slides.html#summary-what-does-it-mean-to-control-for-x",
    "title": "Week 07:",
    "section": "Summary: What does it mean to control for “x”?",
    "text": "Summary: What does it mean to control for “x”?\n\nModels partition variance in a outcome into that which can be explained by the model’s predictions, and the remaining unexplained variation\n\n–\n\nCoefficients in a linear regression describe the “marginal effect” of a change in a predictor on the outcome, after accounting for (holding constat/controlling) the other predictors in the model.\n\n–\n\nResidualized regression provides a way of illustrating how multiple regression isolates the variance explained by specific predictors (and only those predictors)\n\nclass: inverse, center, middle # 💡 # Producing predicted values from regression models"
  },
  {
    "objectID": "slides/07-slides.html#predicted-values",
    "href": "slides/07-slides.html#predicted-values",
    "title": "Week 07:",
    "section": "Predicted Values",
    "text": "Predicted Values\n\nThe coefficients in a regression model define a formula which produces a predcited value of the outcome \\(y\\) when the predictors \\(X\\) take particular values.\nTo produce predicted values, we simply need to plug in values for each \\(x\\), multiply them by their corresponding coefficents, \\(\\beta_x\\) and ad them together."
  },
  {
    "objectID": "slides/07-slides.html#predicted-values-1",
    "href": "slides/07-slides.html#predicted-values-1",
    "title": "Week 07:",
    "section": "Predicted Values",
    "text": "Predicted Values\n\nIn a simple, bivariate regression, the predicted values are the points are the points along the line defined by the intercept \\(\\beta_0\\) and the “slope” \\(\\beta_1\\)\nWhen we add predictors, we are adding dimensions to our model. In a model, with two predictors, the predicted values are descibed by a plane\n\n\nSource"
  },
  {
    "objectID": "slides/07-slides.html#predicted-values-2",
    "href": "slides/07-slides.html#predicted-values-2",
    "title": "Week 07:",
    "section": "Predicted Values",
    "text": "Predicted Values\n\nIn a simple, bivariate regression, the predicted values are the points are the points along the line defined by the intercept \\(\\beta_0\\) and the “slope” \\(\\beta_1\\)\nWhen we add predictors, we are adding dimensions to our model. In a model, with two predictors, the predicted values are descibed by a plane\nWith more than two predictors, the predicted values fall along a hyperplane that likely requires some mind altering substances to visualize.\nIn practice, to interpret mutliple regression models, we will produce predicted values for a range of one variable, holding other predictors constant at some typical value (what’s a good typical value?)"
  },
  {
    "objectID": "slides/07-slides.html#producing-predicted-values-in-r",
    "href": "slides/07-slides.html#producing-predicted-values-in-r",
    "title": "Week 07:",
    "section": "Producing Predicted Values in R",
    "text": "Producing Predicted Values in R\nThe basic steps to producing predicted values in R as follows:\n\nFit a model\nProduce a prediction data frame, where one (sometimes two) predictor(s) vary, and others are held constant at a single value\nUse prediction data frame to obtain predicted values from model using the predict() function\nPlot predicted values to help interpret your model\n\nLet’s demo this using the lab data."
  },
  {
    "objectID": "slides/07-slides.html#fit-a-model-that-allows-the-relationship-of-vaccines-to-vary",
    "href": "slides/07-slides.html#fit-a-model-that-allows-the-relationship-of-vaccines-to-vary",
    "title": "Week 07:",
    "section": "Fit a model that allows the relationship of vaccines to vary",
    "text": "Fit a model that allows the relationship of vaccines to vary\nSuppose we think that an additional increase in the percent of population vaccinated has a declining impact on Covid-19 deaths. We could estimate this model as follows:\n\\[\\text{Deaths} = \\beta_0 + \\beta_1 \\text{% Vaxxed} + + \\beta_1 \\text{% Vaxxed}^2 + \\episolon\\]\nLet’s fit two models\n\nm4 &lt;- lm(new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2), \n         data = covid_lab)\nm5 &lt;- lm(new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2)+  \n           rep_voteshare_std + med_age_std + med_income_std, \n         data = covid_lab)\n\nAnd take a quick look at the results:\n\nsummary(m4)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2), \n    data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66897 -0.14227 -0.04084  0.09251  1.21891 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)              7.7077351  2.2796912   3.381  0.00146 **\npercent_vaccinated      -0.2224854  0.0811224  -2.743  0.00860 **\nI(percent_vaccinated^2)  0.0016633  0.0007106   2.341  0.02354 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2996 on 47 degrees of freedom\nMultiple R-squared:  0.4813,    Adjusted R-squared:  0.4592 \nF-statistic:  21.8 on 2 and 47 DF,  p-value: 2.002e-07\n\n\n\nsummary(m5)\n\n\nCall:\nlm(formula = new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2) + \n    rep_voteshare_std + med_age_std + med_income_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54742 -0.14433 -0.01891  0.10051  1.05202 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)              6.3808910  2.3303540   2.738  0.00888 **\npercent_vaccinated      -0.1750396  0.0817174  -2.142  0.03777 * \nI(percent_vaccinated^2)  0.0012475  0.0007173   1.739  0.08901 . \nrep_voteshare_std       -0.0627074  0.0848368  -0.739  0.46374   \nmed_age_std              0.0486490  0.0546727   0.890  0.37840   \nmed_income_std          -0.1114185  0.0665934  -1.673  0.10140   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2888 on 44 degrees of freedom\nMultiple R-squared:  0.5488,    Adjusted R-squared:  0.4976 \nF-statistic: 10.71 on 5 and 44 DF,  p-value: 8.985e-07"
  },
  {
    "objectID": "slides/07-slides.html#interpretting-our-model",
    "href": "slides/07-slides.html#interpretting-our-model",
    "title": "Week 07:",
    "section": "Interpretting our model",
    "text": "Interpretting our model\nSo the coefficient on percent_vaccinated is negative, while the coefficient on I(percent_vaccinated^2) is positive.\nThis implies that the relationship between vaccination rates and Covid-19 deaths is not constant, but changes with the percent of population vaccinated.\nTo help interpreted this model, let’s create a prediction data frame."
  },
  {
    "objectID": "slides/07-slides.html#create-a-prediction-data-frame",
    "href": "slides/07-slides.html#create-a-prediction-data-frame",
    "title": "Week 07:",
    "section": "Create a prediction data frame",
    "text": "Create a prediction data frame\n\npred_df &lt;- expand_grid(\n  percent_vaccinated = seq(\n    min(covid_lab$percent_vaccinated, na.rm =T),\n    max(covid_lab$percent_vaccinated, na.rm =T),\n    length.out = 10\n  ),\n  rep_voteshare_std = 0,\n  med_age_std = 0,\n  med_income_std = 0\n)\n\n\n\n# A tibble: 10 × 4\n   percent_vaccinated rep_voteshare_std med_age_std med_income_std\n                &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n 1               43.5                 0           0              0\n 2               46.7                 0           0              0\n 3               49.9                 0           0              0\n 4               53.1                 0           0              0\n 5               56.3                 0           0              0\n 6               59.5                 0           0              0\n 7               62.6                 0           0              0\n 8               65.8                 0           0              0\n 9               69.0                 0           0              0\n10               72.2                 0           0              0"
  },
  {
    "objectID": "slides/07-slides.html#use-predict-to-obtain-predicted-values",
    "href": "slides/07-slides.html#use-predict-to-obtain-predicted-values",
    "title": "Week 07:",
    "section": "Use predict() to obtain predicted values",
    "text": "Use predict() to obtain predicted values\n\npred_df$m4_pred &lt;- predict(m4, newdata = pred_df)\npred_df$m5_pred &lt;- predict(m5, newdata = pred_df)\n\n\n\n# A tibble: 10 × 6\n   percent_vaccinated rep_voteshare_std med_age_std med_income_std m4_pred\n                &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1               43.5                 0           0              0   1.17 \n 2               46.7                 0           0              0   0.943\n 3               49.9                 0           0              0   0.747\n 4               53.1                 0           0              0   0.584\n 5               56.3                 0           0              0   0.455\n 6               59.5                 0           0              0   0.359\n 7               62.6                 0           0              0   0.298\n 8               65.8                 0           0              0   0.270\n 9               69.0                 0           0              0   0.275\n10               72.2                 0           0              0   0.315\n# ℹ 1 more variable: m5_pred &lt;dbl&gt;\n\n\n\npred_df %&gt;%\n  pivot_longer(\n    cols = c(\"m4_pred\",\"m5_pred\"),\n    names_to = \"Model\",\n    values_to = \"Predicted Values\"\n  )\n\n# A tibble: 20 × 6\n   percent_vaccinated rep_voteshare_std med_age_std med_income_std Model  \n                &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;  \n 1               43.5                 0           0              0 m4_pred\n 2               43.5                 0           0              0 m5_pred\n 3               46.7                 0           0              0 m4_pred\n 4               46.7                 0           0              0 m5_pred\n 5               49.9                 0           0              0 m4_pred\n 6               49.9                 0           0              0 m5_pred\n 7               53.1                 0           0              0 m4_pred\n 8               53.1                 0           0              0 m5_pred\n 9               56.3                 0           0              0 m4_pred\n10               56.3                 0           0              0 m5_pred\n11               59.5                 0           0              0 m4_pred\n12               59.5                 0           0              0 m5_pred\n13               62.6                 0           0              0 m4_pred\n14               62.6                 0           0              0 m5_pred\n15               65.8                 0           0              0 m4_pred\n16               65.8                 0           0              0 m5_pred\n17               69.0                 0           0              0 m4_pred\n18               69.0                 0           0              0 m5_pred\n19               72.2                 0           0              0 m4_pred\n20               72.2                 0           0              0 m5_pred\n# ℹ 1 more variable: `Predicted Values` &lt;dbl&gt;\n\n\n\npred_df %&gt;%\n  pivot_longer(\n    cols = c(\"m4_pred\",\"m5_pred\"),\n    names_to = \"Model\",\n    values_to = \"Predicted Deaths\"\n  )%&gt;%\n  ggplot(aes(percent_vaccinated, `Predicted Deaths`,col = Model))+\n  geom_line()"
  },
  {
    "objectID": "slides/07-slides.html#r2-a-measure-of-the-variance-explained-by-the-model",
    "href": "slides/07-slides.html#r2-a-measure-of-the-variance-explained-by-the-model",
    "title": "Week 07:",
    "section": "R^2: A measure of the variance explained by the model:",
    "text": "R^2: A measure of the variance explained by the model:\nA simple way to compare models is to look at the proportion of the variance explained by the models predictions, relative to total variance in the outcome:\n\\[R^2 = \\frac{\\text{variance(fitted model values)}}{ \\text{variance(response values )}}\\]\nWe call this value the model’s “R-squared” \\((R^2)\\)"
  },
  {
    "objectID": "slides/07-slides.html#r2",
    "href": "slides/07-slides.html#r2",
    "title": "Week 07:",
    "section": "R^2",
    "text": "R^2\nMore formally, you’ll see \\(R^2\\) defined in terms of “Sums of Squares”\n\nTSS = Total Summ of Squares = Variance of the Outcome\nESS = Explained Sum of Squares = Variance of the Predicted Values\nRSS = Sum of Squared Residuals = Variance of the Residuals\n\n\\[R^2 = \\frac{ESS}{TSS}= 1 - \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "slides/07-slides.html#adjusted-r2",
    "href": "slides/07-slides.html#adjusted-r2",
    "title": "Week 07:",
    "section": "Adjusted R^2",
    "text": "Adjusted R^2\n\nAs we will explore in your lab, a models \\(R^2\\) always increases as we add predictors\nThe adjusted \\(R^2\\) is an attempt to adjust for this by weighting a the \\(R^2\\) of a model by the number of predictors\n\n\\[\\text{adj. }R^2 = 1 - \\frac{RSS/(n-k)}{TSS/(n-1)}\\]"
  },
  {
    "objectID": "slides/07-slides.html#comparing-models-with-r2",
    "href": "slides/07-slides.html#comparing-models-with-r2",
    "title": "Week 07:",
    "section": "Comparing models with \\(R^2\\)",
    "text": "Comparing models with \\(R^2\\)\n\nWhen models are nested (larger models contain all the predictors of smaller models), we can ask, does including the additional predictors in the larger model explain more variation in the outcome than we would expect would happen if we just added additional, random variable.\nFormally we call this process an Analysis of Variance (ANOVA)\n\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n7.71**\n2.50***\n6.38**\n\n\n \n(2.28)\n(0.68)\n(2.33)\n\n\npercent_vaccinated\n-0.22**\n-0.03**\n-0.18*\n\n\n \n(0.08)\n(0.01)\n(0.08)\n\n\npercent_vaccinated^2\n0.00*\n \n0.00\n\n\n \n(0.00)\n \n(0.00)\n\n\nrep_voteshare_std\n \n-0.07\n-0.06\n\n\n \n \n(0.09)\n(0.08)\n\n\nmed_age_std\n \n0.07\n0.05\n\n\n \n \n(0.05)\n(0.05)\n\n\nmed_income_std\n \n-0.11\n-0.11\n\n\n \n \n(0.07)\n(0.07)\n\n\nR2\n0.48\n0.52\n0.55\n\n\nAdj. R2\n0.46\n0.47\n0.50\n\n\nNum. obs.\n50\n50\n50\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\n\n# Square term vs Square with controls\nanova(m4,m5)\n\nAnalysis of Variance Table\n\nModel 1: new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2)\nModel 2: new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2) + \n    rep_voteshare_std + med_age_std + med_income_std\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     47 4.2186                           \n2     44 3.6690  3   0.54958 2.1969 0.1018\n\n\n\n# No square vs square with controls \nanova(m5,m6)\n\nAnalysis of Variance Table\n\nModel 1: new_deaths_pc_14da ~ percent_vaccinated + I(percent_vaccinated^2) + \n    rep_voteshare_std + med_age_std + med_income_std\nModel 2: new_deaths_pc_14da ~ percent_vaccinated + rep_voteshare_std + \n    med_age_std + med_income_std\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     44 3.6690                              \n2     45 3.9212 -1   -0.2522 3.0245 0.08901 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-slides.html#summary",
    "href": "slides/07-slides.html#summary",
    "title": "Week 07:",
    "section": "Summary",
    "text": "Summary\n\nR-Squared is a useful heuristic for evaluating models in terms of variance explained\nR-Squared always increases as we add more predictors\n\nEven if they have no relationship with the outcome\nAdjusted R-Squared includes weights the variance explained by number of predictors needed to explain it.\n\nComparing nested models involves assessing whether the added variance explained is more than we would expect by chance using an ANOVA (analysis of variance) to compare models.\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/13-slides.html#general-plan",
    "href": "slides/13-slides.html#general-plan",
    "title": "Week 13:",
    "section": "General Plan",
    "text": "General Plan\n\nCourse Summary\nCourse Feedback\nAMA\nPaper Workshop\n\nclass: bottom, center background-image: url(“https://www.powerthesaurus.org/_images/terms/summery-synonyms-2.png”) bacground-size: contain"
  },
  {
    "objectID": "slides/13-slides.html#what-was-this-course-about",
    "href": "slides/13-slides.html#what-was-this-course-about",
    "title": "Week 13:",
    "section": "What was this course about?",
    "text": "What was this course about?\n–\n\nHow would you know?\n\n–\n\nHow would convince someone who thinks they know different?\n\n–\n\nWhat would it take to change what you think you know?"
  },
  {
    "objectID": "slides/13-slides.html#why-quantitative-social-science",
    "href": "slides/13-slides.html#why-quantitative-social-science",
    "title": "Week 13:",
    "section": "Why Quantitative Social Science?",
    "text": "Why Quantitative Social Science?\nWhat makes for quantitative social science\n\nCompelling?\nUseful?\nHard?\n\n–\nStatistical methods and programming are tools not answers\n–\nProducing knowledge requires us to match theory with empirical design\n–\nBeing “right” is less important than understanding all the ways you might be wrong.\n–\nGood social science is urgent, eclectic, transparent, contingent, and ?"
  },
  {
    "objectID": "slides/13-slides.html#what-youve-learned",
    "href": "slides/13-slides.html#what-youve-learned",
    "title": "Week 13:",
    "section": "What you’ve learned",
    "text": "What you’ve learned\nSkills and concepts to help understand and practice quantitative social science\n\nCausal Inference is about counterfactual comparisons\n\nExperimental designs \\(\\to\\) random assignment\nObservational design \\(\\to\\) identifying assumptions \\(\\to\\) conditional independence\n\nRegression is a tool for describing relationships\n\nOLS provides a linear approximation of the Conditional Expectation Function\nRegression can be used for descriptive, predictive, and causal inference.\n\nStatistical inference is about quantifying uncertainty about what could have happened\n\nConfidence intervals provide a range of plausible values for we observed\nHypothesis tests describe the conditional probability of observing what we did, if some hypothesis were true.\n\n\nclass:inverse, bottom, center background-image: url(“https://i.imgflip.com/6f1aul.jpg”) bacground-size: cover # 📢 ## Feedback"
  },
  {
    "objectID": "slides/13-slides.html#what-we-liked",
    "href": "slides/13-slides.html#what-we-liked",
    "title": "Week 13:",
    "section": "What we liked",
    "text": "What we liked"
  },
  {
    "objectID": "slides/13-slides.html#what-we-disliked",
    "href": "slides/13-slides.html#what-we-disliked",
    "title": "Week 13:",
    "section": "What we disliked",
    "text": "What we disliked"
  },
  {
    "objectID": "slides/13-slides.html#what-we-learned",
    "href": "slides/13-slides.html#what-we-learned",
    "title": "Week 13:",
    "section": "What we learned",
    "text": "What we learned"
  },
  {
    "objectID": "slides/13-slides.html#what-were-still-learning",
    "href": "slides/13-slides.html#what-were-still-learning",
    "title": "Week 13:",
    "section": "What we’re still learning",
    "text": "What we’re still learning"
  },
  {
    "objectID": "slides/13-slides.html#howd-i-do",
    "href": "slides/13-slides.html#howd-i-do",
    "title": "Week 13:",
    "section": "How’d I do?",
    "text": "How’d I do?\n.pull-left[\n]\n–\n.pull-right[\n\n]"
  },
  {
    "objectID": "slides/13-slides.html#howd-you-do",
    "href": "slides/13-slides.html#howd-you-do",
    "title": "Week 13:",
    "section": "How’d you do?",
    "text": "How’d you do?"
  },
  {
    "objectID": "slides/13-slides.html#are-you-happy-you-took-pols-1600",
    "href": "slides/13-slides.html#are-you-happy-you-took-pols-1600",
    "title": "Week 13:",
    "section": "Are you happy you took POLS 1600?",
    "text": "Are you happy you took POLS 1600?"
  },
  {
    "objectID": "slides/13-slides.html#what-ill-do",
    "href": "slides/13-slides.html#what-ill-do",
    "title": "Week 13:",
    "section": "What I’ll Do",
    "text": "What I’ll Do\n\nLectures:\n\nLess is more\nHow much math…\nMore supplemental content (class notes)\nMore active learning\nIntegrating the textbook\n\n\n–\n\nLabs:\n\nSeemed to work ok\nGroup work vs individual learning?\nGreater integration with textbook and lecture\n\n\n–\n\nAssignments and Grading\n\nMostly liked the group project\nTutorials were helpful\nMore individualized assignments/accountability?\n\n\nclass:inverse, middle, center # 🔍 ## AMA"
  },
  {
    "objectID": "slides/13-slides.html#ask-you-anthing-ok-boomer",
    "href": "slides/13-slides.html#ask-you-anthing-ok-boomer",
    "title": "Week 13:",
    "section": "Ask you anthing? Ok boomer…",
    "text": "Ask you anthing? Ok boomer…\n\nsum(df$ama == \"\")\n\n[1] 7\n\n\n\n“how many cats do you have”\n\n–\nWe’ve had three cats, Isla, Abby and, currently Toby.\n\n“What is the craziest moment from your time in undergrad?”\n\n–\n\n\n\n\n\n\n\n\n\n\n\n\n“I am interested to know why you decided to go to graduate school, what other career paths you were deciding between, and whether you believe you made the right decision to go into academia and why?”\n\n–\n\n\n\n\n\n\n\n\n\n\n“What’s your favorite musical”\n\n–\n\n\n\n\n\n\n\n\n\n\n“If we want to, how should we develop our R skills by ourselves at Brown?\n\n\nBest book you ever read?\n\n\nBook you think someone in their early 20s should read?”"
  },
  {
    "objectID": "slides/10-slides.html#general-plan",
    "href": "slides/10-slides.html#general-plan",
    "title": "Week 10:",
    "section": "General Plan",
    "text": "General Plan\n\nCourse Plan\nSetup\nFeedback\nReview\nConfidence Intervals\n\nbackground-image: url(“https://i.kym-cdn.com/entries/icons/original/000/037/873/We’re_All_Trying_To_Find_The_Guy_Who_Did_This_banner_1.jpg”) background-size:contain"
  },
  {
    "objectID": "slides/10-slides.html#two-options",
    "href": "slides/10-slides.html#two-options",
    "title": "Week 10:",
    "section": "Two Options:",
    "text": "Two Options:\n\nProceed with group projects with condensed schedule/assignments\n\n–\n\nReplace group projects with a take home (open book/notes) final exam\n\nPosted April 30\nDue May 7\nMix of theory, concepts, and coding."
  },
  {
    "objectID": "slides/10-slides.html#course-plan-option-1",
    "href": "slides/10-slides.html#course-plan-option-1",
    "title": "Week 10:",
    "section": "Course Plan: Option 1",
    "text": "Course Plan: Option 1\n\nApril 13: No Class, Review Feedback to A2\nApril 18: Lecture – Hypothesis Testing\nApril 20 Workshop on Paper – Inference About Models: Counts as Assignment 3\nApril 25: Lecture – Course Review\nApril 27: Workshop: Paper drafts and Presentations\nApril 30: Upload Presentations\nMay 2: Class Presentations Part 1\nMay 4: Class Presentations Part 2\nMay ?: Tacos?"
  },
  {
    "objectID": "slides/10-slides.html#course-plan-option-2",
    "href": "slides/10-slides.html#course-plan-option-2",
    "title": "Week 10:",
    "section": "Course Plan: Option 2",
    "text": "Course Plan: Option 2\n\nApril 13: No Class,\nApril 18: Lecture – Hypothesis Testing\nApril 20 Lab – Hypothesis Testing and Interval Estimation\nApril 25: Lecture – Course Review\nApril 27: Workshop:\nApril 30: Take Home Final Exam\nMay ?: Tacos or Pizza with POLS 1140?\nMay 7: Take Home Final Exam due"
  },
  {
    "objectID": "slides/10-slides.html#what-do-we-want-to-do",
    "href": "slides/10-slides.html#what-do-we-want-to-do",
    "title": "Week 10:",
    "section": "What do we want to do?",
    "text": "What do we want to do?\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/10-slides.html#packages-for-today",
    "href": "slides/10-slides.html#packages-for-today",
    "title": "Week 10:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\",# &lt;&lt;\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\", \"boot\",\"purrr\"\n)"
  },
  {
    "objectID": "slides/10-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/10-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 10:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/10-slides.html#load-packages-for-today",
    "href": "slides/10-slides.html#load-packages-for-today",
    "title": "Week 10:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled        modelr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign           zoo          boot         purrr \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today\nWe’ll use data from last week’s lab to\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\nclass:inverse, middle, center # 🔍 ## Review: Generalized Linear Models"
  },
  {
    "objectID": "slides/10-slides.html#generalized-linear-models",
    "href": "slides/10-slides.html#generalized-linear-models",
    "title": "Week 10:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nIn last week’s lab we fit two models\n\nOLS\nLogistic regression\n\n\n# OLS\nm1 &lt;- lm(support_war01 ~ age + sex + education_n, df_drww)\n\n# Logisitic \nm2 &lt;- glm(support_war01 ~ age + sex + education_n, df_drww,\n          family = binomial)"
  },
  {
    "objectID": "slides/10-slides.html#generalized-linear-model",
    "href": "slides/10-slides.html#generalized-linear-model",
    "title": "Week 10:",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\n\nLogisitic regression is a type of generalized linear model used to model binary outcomes\nWe estimate logistic regression using Maximum Likelihood, which allows us to model outcomes using different probability distributions\nOther common generalized linear models\n\nProbit regression (binary outcomes)\nPoisson regression (count data)\nNegative binomial regression (count data)\n\nIt’s still “regression”, but interpretation typically requires transforming predictions (inverting the link function)\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.28***\n\n\n-1.36***\n\n\n\n\n \n\n\n(0.05)\n\n\n(0.29)\n\n\n\n\nage\n\n\n0.01***\n\n\n0.05***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nsexMale\n\n\n0.09***\n\n\n0.50***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.13)\n\n\n\n\neducation_n\n\n\n-0.02\n\n\n-0.10\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.06)\n\n\n\n\nR2\n\n\n0.11\n\n\n \n\n\n\n\nAdj. R2\n\n\n0.11\n\n\n \n\n\n\n\nNum. obs.\n\n\n1463\n\n\n1463\n\n\n\n\nAIC\n\n\n \n\n\n1575.54\n\n\n\n\nBIC\n\n\n \n\n\n1596.69\n\n\n\n\nLog Likelihood\n\n\n \n\n\n-783.77\n\n\n\n\nDeviance\n\n\n \n\n\n1567.54\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/10-slides.html#prediction-data-frame",
    "href": "slides/10-slides.html#prediction-data-frame",
    "title": "Week 10:",
    "section": "Prediction Data Frame",
    "text": "Prediction Data Frame\n\npred_df &lt;- expand_grid(\n  age = 18 : 99,\n  sex = \"Female\",\n  education_n = mean(df_drww$education_n, na.rm = T)\n)"
  },
  {
    "objectID": "slides/10-slides.html#predicted-values",
    "href": "slides/10-slides.html#predicted-values",
    "title": "Week 10:",
    "section": "Predicted Values",
    "text": "Predicted Values\n\n# #Predicted values for m1\npred_df$pred_ols &lt;- predict(m1,\n                            newdata = pred_df)\n# Predicted values for m2\n# Remember to add type = \"response\"\npred_df$pred_logit &lt;- predict(m2,\n                            newdata = pred_df,\n                            type = \"response\")\n\n\n# data\npred_df%&gt;%\n  # aesthetics\n  ggplot(aes(age, pred_ols, col = \"OLS\"))+\n  # geometries\n  geom_line()+\n  geom_line(aes(y = pred_logit, col = \"Logistic\"))+\n  geom_jitter(data=df_drww, aes(age, support_war01),\n              col = \"black\",\n              height = .05,\n              size = .5,\n              alpha = .5)+\n  labs(\n    col = \"Model\",\n    x = \"Age\",\n    y = \"Predicted Values\"\n  )\n\n\nclass: inverse, center, middle # 💡 # Confidence Intervals ## The Basics"
  },
  {
    "objectID": "slides/10-slides.html#overview",
    "href": "slides/10-slides.html#overview",
    "title": "Week 10:",
    "section": "Overview:",
    "text": "Overview:\n\nConfidence intervals provide a way of quantifying uncertainty about estimates\nConfidence intervals describe a range of plausible values\nThat range is a function of the standard error of the estimate, and the a critical value determined \\(\\alpha\\), which describes the degree of confidence we want\n\nA 95% confidence interval corresponds to an \\(\\alpha\\) of 0.05\n\nA standard error is the standard deviation of the sampling distribution of our estimate\nWe can obtain the sampling distribution via:\n\nsimulation (bootstrapping)\nasymptotic theory (the CLT)\n\nOur confidence is about the interval, not the estimate."
  },
  {
    "objectID": "slides/10-slides.html#defintions-populations-and-samples",
    "href": "slides/10-slides.html#defintions-populations-and-samples",
    "title": "Week 10:",
    "section": "Defintions: Populations and Samples",
    "text": "Defintions: Populations and Samples\n\nPopulation: All the cases from which you could have sampled\nParameter: A quantity or quantities of interest often generically called \\(\\theta\\) (“theta”). Something we’d like to know about our population\nSample: A (random) draw from that population\nSample Size: The number of observations in your draw (without replacement)"
  },
  {
    "objectID": "slides/10-slides.html#defintions-estimators-estimates-and-statistics",
    "href": "slides/10-slides.html#defintions-estimators-estimates-and-statistics",
    "title": "Week 10:",
    "section": "Defintions: Estimators, Estimates, and Statistics",
    "text": "Defintions: Estimators, Estimates, and Statistics\n\nEstimator: A rule for calculating an estimate of our parameter of interest.\nEstimate: The value produced by some estimator for some parameter from some data. Often called \\(\\hat{\\theta}\\)\nUnbiased estimators: \\(E(\\hat{\\theta})=E(\\theta)\\) On average, the estimates produced by some estimator will be centered around the truth\nConsistent estimates: \\(\\lim_{n\\to \\infty} \\hat{\\theta_N} = \\theta\\) As the sample size increases, the estimates from an estimator converge in probability to the parameter value\nStatistic: A summary of the data (mean, regression coefficient, \\(R^2\\)). An estimator without a specified target of inference"
  },
  {
    "objectID": "slides/10-slides.html#definitions-distrubtions-and-standard-errors",
    "href": "slides/10-slides.html#definitions-distrubtions-and-standard-errors",
    "title": "Week 10:",
    "section": "Definitions: Distrubtions and Standard Errors",
    "text": "Definitions: Distrubtions and Standard Errors\n\nSampling Distribution: How some estimate would vary if you took repeated samples from the population\nStandard Error: The standard deviation of the sampling distribution\nResampling Distribution: How some estimate would vary if you took repeated samples from your sample WITH REPLACEMENT\n\n“Sampling from our sample, as the sample was sampled from the population.”"
  },
  {
    "objectID": "slides/10-slides.html#confidence-intervals-interpretation",
    "href": "slides/10-slides.html#confidence-intervals-interpretation",
    "title": "Week 10:",
    "section": "Confidence Intervals: Interpretation",
    "text": "Confidence Intervals: Interpretation\n\nConfidence intervals give a range of values that are likely to include the true value of the parameter \\(\\theta\\) with probability \\((1-\\alpha) \\times 100\\%\\)\n\n\\(\\alpha = 0.05\\) corresponds to a “95-percent confidence interval”\n\nOur “confidence” is about the interval\nIn repeated sampling, we expect that \\((1-\\alpha) \\times 100\\%\\) of the intervals we construct would contain the truth.\nFor any one interval, the truth, \\(\\theta\\), either falls within in the lower and upper bounds of the interval or it does not."
  },
  {
    "objectID": "slides/10-slides.html#two-approaches-to-calculating-confidence-intervals",
    "href": "slides/10-slides.html#two-approaches-to-calculating-confidence-intervals",
    "title": "Week 10:",
    "section": "Two Approaches to Calculating Confidence Intervals:",
    "text": "Two Approaches to Calculating Confidence Intervals:\nIn general, there are two ways to calculate confidence intervals:\n\nSimulation: Use our computers to simulate the idea of repeated sampling (e.g. bootstrapping)\n\nFlexible, but more computationally intensive\n\nAsymptotic Theory: Use math to derive the properties of the distributions that would arise under repeated sampling\n\nFaster, but requires more assumptions that may not hold\n\n\nWe will consider both.\n\nThe theory of CIs is easier to illustrate via simulation\nThe practice of calculating CIs is (generally) easier using asymptotic theory"
  },
  {
    "objectID": "slides/10-slides.html#steps-to-calculating-a-confidence-interval",
    "href": "slides/10-slides.html#steps-to-calculating-a-confidence-interval",
    "title": "Week 10:",
    "section": "Steps to Calculating a Confidence Interval",
    "text": "Steps to Calculating a Confidence Interval\nFrom QSS (p. 330)\n\nChoose the desired level of confidence \\((1-\\alpha)\\times 100%\\) by specifying a value of α between 0 and 1: the most common choice is= \\(\\alpha = 0.05\\), which gives a 95% confidence level.\nDerive the sampling distribution of the estimator by computing its mean and variance.\nCompute the standard error based on this sampling distribution. (square root of the variance)\nCompute the critical value \\(z_{\\alpha/2}\\) as the \\((1-\\alpha)\\times 100\\) percentile value of the standard normal distribution\nCompute the lower and upper confidence limits as \\(\\hat{\\theta} - z_{\\alpha/2}\\times SE\\) and \\(\\hat{\\theta} + z_{\\alpha/2}\\times SE\\) standard error, respectively.\n\nclass: inverse, center, middle # 💡 # Confidence Intervals ## Simulating the Sampling Distribution through Bootstrapping"
  },
  {
    "objectID": "slides/10-slides.html#populations",
    "href": "slides/10-slides.html#populations",
    "title": "Week 10:",
    "section": "Populations",
    "text": "Populations\nLet’s load the data from the Do Russians Want War survey\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\nTo understand the logic of confidence intervals, let’s treat this data as our population from which we we could draw repeated samples."
  },
  {
    "objectID": "slides/10-slides.html#population-age",
    "href": "slides/10-slides.html#population-age",
    "title": "Week 10:",
    "section": "Population Age",
    "text": "Population Age\nIn our population, there are parameters, true values of things we want to know.\nSuppose we’re interested in the average age of our population.\nIn our population, the true value of \\(\\mu_{age} = E[Age]\\) is\n\nmu_age &lt;- mean(df_drww$age)\nmu_age\n\n[1] 46.64693\n\n\nSimilarly, the true \\(\\sigma_{age} = \\sqrt{E[Age^2] - E[Age]^2}\\)\n\nsd_age &lt;- sqrt(mean((df_drww$age-mean(df_drww$age))^2))\nsd_age\n\n[1] 15.81829"
  },
  {
    "objectID": "slides/10-slides.html#distribution-population-age",
    "href": "slides/10-slides.html#distribution-population-age",
    "title": "Week 10:",
    "section": "Distribution Population Age",
    "text": "Distribution Population Age\n\np_pop &lt;- df_drww %&gt;%\n  ggplot(aes(age))+\n  geom_density(col=\"grey\")+\n  geom_rug()+\n  geom_vline(\n    aes(xintercept = mu_age, \n             col = \"Population Mean\"),\n    linetype=2)+\n  theme_bw()+\n  labs(color = \"Age\")\n\np_pop"
  },
  {
    "objectID": "slides/10-slides.html#sample-estimates-of-average-age-n-25",
    "href": "slides/10-slides.html#sample-estimates-of-average-age-n-25",
    "title": "Week 10:",
    "section": "Sample Estimates of Average Age (N = 25)",
    "text": "Sample Estimates of Average Age (N = 25)\nSuppose we took three samples, without replacement of size 25, and calculated the average age in each sample:\n\nset.seed(123)\nmean_age1 &lt;- mean(sample(df_drww$age, 25, replace = F))\nmean_age2 &lt;- mean(sample(df_drww$age, 25, replace = F))\nmean_age3 &lt;- mean(sample(df_drww$age, 25, replace = F))\n\nmean_age1\n\n[1] 43.36\n\nmean_age2\n\n[1] 39.36\n\nmean_age3\n\n[1] 49.72"
  },
  {
    "objectID": "slides/10-slides.html#repeated-sampling",
    "href": "slides/10-slides.html#repeated-sampling",
    "title": "Week 10:",
    "section": "Repeated Sampling",
    "text": "Repeated Sampling\n\nImagine we could draw a 1,000 or 10,000 or an infinite number of samples of size N=25 from our population.\nHow much would our estimate of the average of age of the population vary?\nLet’s use our computers to simulate this process and find out!"
  },
  {
    "objectID": "slides/10-slides.html#simualting-repeated-sampling",
    "href": "slides/10-slides.html#simualting-repeated-sampling",
    "title": "Week 10:",
    "section": "Simualting Repeated Sampling",
    "text": "Simualting Repeated Sampling\n\nn_sims &lt;- 1000\nsamp_size &lt;- 25\nset.seed(123)\n\nmu_age_samp_dist_n25 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Sampling\",\n  sample = \"Population\"\n) %&gt;%\n  mutate(\n    samp = purrr::map(sim, ~ slice_sample(df_drww, n = samp_size, replace = F)),\n    estimate = purrr::map_dbl(samp, ~ mean(.$age))\n  )\n\n\nmu_age_samp_dist_n25\n\n# A tibble: 1,000 × 5\n     sim distribution sample     samp           estimate\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;list&gt;            &lt;dbl&gt;\n 1     1 Sampling     Population &lt;df [25 × 42]&gt;     43.4\n 2     2 Sampling     Population &lt;df [25 × 42]&gt;     39.4\n 3     3 Sampling     Population &lt;df [25 × 42]&gt;     49.7\n 4     4 Sampling     Population &lt;df [25 × 42]&gt;     46.2\n 5     5 Sampling     Population &lt;df [25 × 42]&gt;     50.6\n 6     6 Sampling     Population &lt;df [25 × 42]&gt;     47.8\n 7     7 Sampling     Population &lt;df [25 × 42]&gt;     46.3\n 8     8 Sampling     Population &lt;df [25 × 42]&gt;     41.4\n 9     9 Sampling     Population &lt;df [25 × 42]&gt;     45.4\n10    10 Sampling     Population &lt;df [25 × 42]&gt;     48.8\n# ℹ 990 more rows"
  },
  {
    "objectID": "slides/10-slides.html#standard-errors",
    "href": "slides/10-slides.html#standard-errors",
    "title": "Week 10:",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nA standard error is simply the standard deviation of the sampling distribution.\nThe standard error for our simulation above:\n\n\nse_age_n25 &lt;- sd(mu_age_samp_dist_n25$estimate)\nse_age_n25\n\n[1] 3.146543"
  },
  {
    "objectID": "slides/10-slides.html#coverage-intervals",
    "href": "slides/10-slides.html#coverage-intervals",
    "title": "Week 10:",
    "section": "Coverage Intervals",
    "text": "Coverage Intervals\n\nFrom the Central Limit Theorem, we know that the distribution of sample means will converge to a normal distribution.\nFrom probability theory, we know that we that roughly 95 percent of the values in a normal distribution fall between Two Standard Deviations of the mean.\n\n\nci_age_ul_n25 &lt;- mu_age + 2*se_age_n25\nci_age_ll_n25 &lt;- mu_age - 2*se_age_n25\n\nmean(mu_age_samp_dist_n25$estimate &gt;ci_age_ll_n25 & \n       mu_age_samp_dist_n25$estimate &lt;ci_age_ul_n25)\n\n[1] 0.954\n\n\n\nmu_age_samp_dist_n25 %&gt;%\n  ggplot(aes(estimate))+\n  geom_density()+\n  geom_rug(\n    aes(col = estimate &gt;ci_age_ll_n25 & \n          estimate &lt;ci_age_ul_n25)\n  )+\n  geom_vline(xintercept = mu_age,\n             col = \"red\",\n             linetype=2)+\n  guides(col=\"none\")+\n  geom_segment(aes(x=ci_age_ll_n25,\n                   xend = ci_age_ul_n25,\n                   y = .15,yend = .15 ),\n               col = \"#00BFC4\")+\n  theme_bw()"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors",
    "href": "slides/10-slides.html#boostrapped-standard-errors",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nA standard error is the standard deviation of a hypothetical sampling distribution\nHow do we calculate a standard error from a single sample?\nIt turns out that a random sample provides unbiased estimates of both the population mean and the standard deviation of the of the sampling distribution (i.e. the standard error).\nWe can estimate this this standard error, by sampling with replacement from our sample to generate a bootstrapped sampling distribution"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-1",
    "href": "slides/10-slides.html#boostrapped-standard-errors-1",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nset.seed(123)\nbs_resamp_1 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 1\",\n) %&gt;%\n  mutate(\n    samp = purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[1]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-2",
    "href": "slides/10-slides.html#boostrapped-standard-errors-2",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nbs_resamp_2 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 2\",\n) %&gt;%\n  mutate(\n    samp =  purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[2]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-3",
    "href": "slides/10-slides.html#boostrapped-standard-errors-3",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nbs_resamp_3 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 3\",\n) %&gt;%\n  mutate(\n    samp =  purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[3]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-4",
    "href": "slides/10-slides.html#boostrapped-standard-errors-4",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nbs_example &lt;- rbind(\n  mu_age_samp_dist_n25,\n  bs_resamp_1,\n  bs_resamp_2,\n  bs_resamp_3\n)\n\ndf_se &lt;- bs_example %&gt;%\n  select(sample, estimate)%&gt;%\n  dplyr::group_by(sample)%&gt;%\n  dplyr::summarise(\n    se = sd(estimate)\n  )\n\n\ndf_ci &lt;- df_mn %&gt;%\n  left_join(df_se)\n\ndf_ci%&gt;%\n  mutate(\n    ll = xint - qt(df=25,.975)*se,\n    ul = xint + qt(df=25,.975)*se,\n    y = .15,\n    xint_pop = xint[1]\n  ) -&gt; df_ci\n\n\nbs_example %&gt;%\n  ggplot(aes(estimate,col = sample))+\n  geom_density(aes(linetype=distribution))+\n  facet_wrap(~sample, ncol=1)+\n   geom_vline(\n    data = df_ci,\n    aes(xintercept = xint, \n             col = sample,\n        linetype=distribution)\n    )+\n    geom_vline(\n    data = df_ci,\n    aes(xintercept = xint_pop), \n             col = \"black\",\n        linetype=2)+\n  geom_segment(\n    data = df_ci,\n    aes(x =ll, xend =ul, y=y, yend=y)\n  )\n\n\n\n\n\n\n\n\n\n\n\nsim_ci_fn&lt;-function(x,\n                    samp_size=100,\n                    n_sims=1000,\n                    level=.95,\n                    bs=F){\n    # Take a sample of size \"nsamp\"\n    y&lt;-sample(x=na.omit(x),size=samp_size,replace=F)\n    # Calculate the mean\n    mu&lt;-mean(y,na.rm=T)\n    # If bs=TRUE do bootstrapped SEs \n    if(bs==T){\n    mu_dist&lt;-rerun(\n      n_sims,\n      mean(sample(y, samp_size, replace = T)))%&gt;%\n      unlist()\n    se&lt;-sd(mu_dist)}else{\n    # Otherwise, just use assymptotic result (Quicker)\n    se&lt;-sd(y,na.rm=T)/sqrt(samp_size-1)\n    }\n    # Significance level\n    the.p&lt;-1-(1-level)/2\n    # Calculate lower and upper limits of interval\n    ll&lt;-mu-qt(p=the.p,df=samp_size-1)*se\n    ul&lt;-mu+qt(p=the.p,df=samp_size-1)*se\n    results&lt;-tibble(mu=mu,ll=ll,ul=ul,se=se)\n    return(results)\n}\n\n\nset.seed(12345)\nsamp25 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 25)) %&gt;%dplyr::mutate(sample = 1:n() )\nsamp50 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 50))%&gt;%dplyr::mutate(sample = 1:n() )\nsamp100 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 100))%&gt;%dplyr::mutate(sample = 1:n() )\nsamp200 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 200))%&gt;%dplyr::mutate(sample = 1:n() )"
  },
  {
    "objectID": "slides/10-slides.html#standard-errors-and-sample-size",
    "href": "slides/10-slides.html#standard-errors-and-sample-size",
    "title": "Week 10:",
    "section": "Standard Errors and Sample Size",
    "text": "Standard Errors and Sample Size\n\n# Standard errors decrease as sample size increases\nc(mean(samp25$se),\nmean(samp50$se),\nmean(samp100$se),\nmean(samp200$se))\n\n[1] 3.193029 2.247602 1.588096 1.121755\n\n# Specifically, by the square root of the sample size\nc(sd_age/sqrt(25),\nsd_age/sqrt(50),\nsd_age/sqrt(100),\nsd_age/sqrt(200))\n\n[1] 3.163659 2.237045 1.581829 1.118522"
  },
  {
    "objectID": "slides/10-slides.html#next-week-standard-errors-for-linear-models",
    "href": "slides/10-slides.html#next-week-standard-errors-for-linear-models",
    "title": "Week 10:",
    "section": "Next Week: Standard Errors for Linear Models",
    "text": "Next Week: Standard Errors for Linear Models\n\nAs you saw in your lab, we can apply the same principles to calculate standard errors for other quantities like the coefficients from a regression\nNext week, we’ll compare these quantities to those obtained from asymptotic theory, and then turn to an alternative approach to quantifying uncertainty: Hypothesis testing.\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/12-slides.html#general-plan",
    "href": "slides/12-slides.html#general-plan",
    "title": "Week 12:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nFeedback\nReview\n\nStatistical Inference\nCausal Inference\nLinear Models\nConfidence intervals and Hypothesis tests\n\nPresentations\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/12-slides.html#new-packages",
    "href": "slides/12-slides.html#new-packages",
    "title": "Week 12:",
    "section": "New packages",
    "text": "New packages\nFirst we’ll install some packages that you will need for your presentations\n\n# Uncomment and run:\n# install.packages(remotes)\n# remotes::install_github('yihui/xaringan')\n# devtools::install_github(\"gadenbuie/xaringanExtra\")\n# install.packages(\"xaringanthemer\")"
  },
  {
    "objectID": "slides/12-slides.html#packages-for-today",
    "href": "slides/12-slides.html#packages-for-today",
    "title": "Week 12:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/12-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/12-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 12:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/12-slides.html#load-packages-for-today",
    "href": "slides/12-slides.html#load-packages-for-today",
    "title": "Week 12:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce scatterplot3d       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE \n\n\nclass:inverse, middle, center # 📢 ## Feedback"
  },
  {
    "objectID": "slides/12-slides.html#feedback-on-drafts",
    "href": "slides/12-slides.html#feedback-on-drafts",
    "title": "Week 12:",
    "section": "Feedback on Drafts",
    "text": "Feedback on Drafts\n\nPosted before class on Thursday.\nIf you haven’t submitted a file on Canvas, do it by COB today.\nFor Thursday’s class:\n\nToday and tomorrow, work on translating your draft into a slide presentation\nCome to class with a set of tasks to work on for your presentation.\n\nSunday, May 1st, upload final presentations to Canvas\nSunday, May 8, upload final papers to Canvas\n\nYou can have a 1-week extension, just email, no questions asked, but must submit by May 15.\n\n\nclass:inverse, middle, center # 🔍 ## Review"
  },
  {
    "objectID": "slides/12-slides.html#what-i-hope-youve-learned",
    "href": "slides/12-slides.html#what-i-hope-youve-learned",
    "title": "Week 12:",
    "section": "What I hope you’ve learned:",
    "text": "What I hope you’ve learned:\nCore Concepts:\n\nStatistical Inference\nCausal Inference\nLinear Models\nConfidence intervals and Hypothesis tests\n\nKey Skills:\n\nHow to load, transform, summarize, and visualize data\nHow to estimate, evaluate, present, and interpret linear models\n\nclass:inverse, middle, center # 🔍 # Core Concepts"
  },
  {
    "objectID": "slides/12-slides.html#statistical-inference",
    "href": "slides/12-slides.html#statistical-inference",
    "title": "Week 12:",
    "section": "Statistical Inference:",
    "text": "Statistical Inference:\n\nStatistical inference involves quantifying uncertainty about what could have happened\nWe describe uncertainty about what could have happened with distributions\nWe can generate these distributions via\n\nsimulation (Bootstrapping and permutations)\nanalytic theory (Limit Theorems)\n\nWe quantify uncertainty using confidence intervals and hypothesis tests"
  },
  {
    "objectID": "slides/12-slides.html#causal-inference",
    "href": "slides/12-slides.html#causal-inference",
    "title": "Week 12:",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal inference involves making counterfactual claims about what would have happened had some causal factor \\((Z)\\) been present or absent.\nThe fundamental problem of causal inference is that for an individual observation, we only observe one of many potential outcomes \\((Y(Z))\\).\nThe statistical solution to this problem moves from individual causal effects (\\(\\tau_i = Y_i(1) - Y_i(0)\\)) to average causal effects \\((\\tau = ATE = E[Y(1)]-E[Y(0)])\\)\nExperimental designs identify the ATE by randomly assigning treatment \\(\\to\\) \\(Y(1), Y(0), X, U, \\perp Z\\)\nObservational designs approximate the experimental ideal based on identifying assumptions that claim conditional independence \\(\\to\\) \\(Y(1), Y(0), X, U, \\perp Z |X\\).\n\nDifference in Difference \\(\\to\\) Parallel Trends\nRegression Discontinuity \\(\\to\\) Continuity at the cut off\nInstrumental Variables \\(\\to\\) The exclusion restriction\nRegression \\(\\to\\) Selection on observable"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression",
    "href": "slides/12-slides.html#linear-regression",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear regression provides a linear estimate of the Conditional Expectation Function\n\nBivariate: \\(E[Y|x] = \\beta_0 + \\beta_1 x +\\epsilon\\)\nMultiple regression: \\(E[Y|X] = X\\beta +\\epsilon = \\beta_0 + \\beta_1 x_1 \\dots \\beta_k x_k +\\epsilon\\)\n\nOrdinary Least Linear regression finds coefficients \\(\\beta\\) by minimizing the sum of squared errors \\((\\sum \\epsilon^2)\\)\nLinear regressions partition variance in the outcome into variance explained by the model \\((X\\beta)\\) and variance not explained by the model (\\(\\epsilon\\)).\n\nA model’s \\(R^2\\) describes the proportion of the overall variance in outcome explained by the predictors"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression-1",
    "href": "slides/12-slides.html#linear-regression-1",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nThe coefficient on a predictor describes how the outcome is expected to change with a 1-unit change in that predictor\nControlling for multiple variables isolates the variation in the outcome explained by one predictor by removing (controlling for) the variation in the outcome and that predictor explained by the other predictors.\n\nWe control for covariates that are common causes of both our key predictor and our outcome to address omitted variable bias (spurious correlation)\nWe avoid controlling for covariates that our common consequences of our outcome and predictor (collider bias)"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression-2",
    "href": "slides/12-slides.html#linear-regression-2",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nWe present linear regression using regression tables where:\n\nEach column corresponds to a model\nEach row corresponds to a coefficient in the model (with standard errors in parentheses and asterisks denoting p&lt;0.05)\n\nWe use generalized linear models to help us incorporate information about our outcomes to improve our models’ predictions\n\nLogistic regression is commonly used to model binary outcomes\nPoisson regression is commonly used to model counts\n\nPlots of predicted values can help us interpret more complicated regression models such as:\n\npolynomial regressions where the marginal effect of one predictor varies non-linearly\ninteraction models, where the marginal effect of one predictor varies with the value of another predictor\ngeneralized linear models"
  },
  {
    "objectID": "slides/12-slides.html#confidence-intervals",
    "href": "slides/12-slides.html#confidence-intervals",
    "title": "Week 12:",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nA confidence interval describes a range of plausible values for the true (population) value of our estimate\nOur confidence is about the interval:\n\n\\((1-\\alpha)\\times 100%\\) of the intervals we could construct in repeated sampling are expected to contain the true (population) value of the thing we’re estimating\n\nTo construct a confidence interval we need:\n\nAn estimate \\((\\hat{\\theta})\\)\nA standard error \\((\\hat{\\sigma_{\\hat{\\theta}}})\\) (the standard deviation of sampling distribution)\nA critical value derived from the hypothetical sampling \\((z_{\\alpha/2})\\)\n\nWith these three components the \\((1-\\alpha)\\times 100%\\) is \\(\\hat{\\theta}\\pm z_{\\alpha/2} \\times \\hat{\\sigma_{\\hat{\\theta}}}\\)\nWe report confidence intervals in text: \\(\\beta = 0.9\\) \\([0.7, 0.11]\\)\nWe interpret estimates as being statistically significant, if 0 is outside the confidence interval"
  },
  {
    "objectID": "slides/12-slides.html#hypothesis-tests",
    "href": "slides/12-slides.html#hypothesis-tests",
    "title": "Week 12:",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\nA hypothesis test quantifies how likely it is that we would observe what we did (our test statistic), if some claim about the world were true (our hypothesis).\nTypically, we test a null hypothesis that expresses our belief that their is no relationship between variables.\n\n\\(\\tau = E[Y|Z=1] - E[Y|Z=0] = 0 \\to\\) No average treatment effect\n\\(\\beta = 0 \\to\\) No relationship between predictor and outcome\n\nIf our claim were true, then under the null, our test statistic would have a distribution centered around the truth.\nWe can describe this distribution via:\n\nsimulation (e.g. permuting the outcome)\nanalytic theory (CLT)\n\nWe quantify our uncertainty using a p-value which describes the probability of observing a test statistic as extreme or more extreme in a world where our null hypothesis was true\n\nIf our p-value is small (p &lt; 0.05), we reject the null hypothesis\nIf our p-value is large (p &gt; 0.05), we fail to reject the null, or retain the null hypothesis\n\n\nclass:inverse, middle, center # 🔍 # Key Skills"
  },
  {
    "objectID": "slides/12-slides.html#how-to-load-explore-and-transform-data",
    "href": "slides/12-slides.html#how-to-load-explore-and-transform-data",
    "title": "Week 12:",
    "section": "How to load, explore and transform data",
    "text": "How to load, explore and transform data\n\n# Load data\nload(\"df.rda\")\ndf &lt;- readr::read_csv(\"df.rda\")\nlibrary(tidyverse)\n## Explore data\nhead(df)\ntable(df$y)\n\n# Transfrom data:\ndf %&gt;%\n  mutate(\n    dv = ifelse(var &lt; 0, NA, y),\n    iv = case_when(\n      x == 1 ~ \"Low\",\n      x == 2 ~ \"Medium\"\n      x == 3 ~ \"High\",\n      T ~ NA_character_\n    ),\n    covar_std = scale(z)\n  ) -&gt; df"
  },
  {
    "objectID": "slides/12-slides.html#how-to-summarize-data",
    "href": "slides/12-slides.html#how-to-summarize-data",
    "title": "Week 12:",
    "section": "How to summarize data",
    "text": "How to summarize data\n\nsummary(df$dv)\n\ndf %&gt;%\n  group_by(iv) %&gt;%\n  summarize(\n    min = min(dv,na.rm = T),\n    median = median(dv,na.rm = T),\n    mean = median(dv,na.rm = T),\n  )"
  },
  {
    "objectID": "slides/12-slides.html#how-to-visualize-data",
    "href": "slides/12-slides.html#how-to-visualize-data",
    "title": "Week 12:",
    "section": "How to visualize data",
    "text": "How to visualize data\n\n# data\ndf %&gt;%\n  # aesthetics\n  ggplot(aes(x = iv, y = dv))+\n  # geometries\n  geom_point() -&gt; figure1"
  },
  {
    "objectID": "slides/12-slides.html#how-to-estimate-and-evaluate",
    "href": "slides/12-slides.html#how-to-estimate-and-evaluate",
    "title": "Week 12:",
    "section": "How to estimate and evaluate",
    "text": "How to estimate and evaluate\n\n# Estimate models\nm1 &lt;- lm(dv ~ iv, df)\nm2 &lt;- lm(dv ~ iv + covar_std, df)\nm3 &lt;- glm(dv ~ iv + covar_std, df, family = binomial)\n\n# evaluate models\nsummary(m1)\nconfint(m2)"
  },
  {
    "objectID": "slides/12-slides.html#how-to-present-and-interpret-linear-models",
    "href": "slides/12-slides.html#how-to-present-and-interpret-linear-models",
    "title": "Week 12:",
    "section": "How to present, and interpret linear models",
    "text": "How to present, and interpret linear models\n\n# Regression Table\ntexreg::htmlreg(list(m1, m2, m3))\n\n\n# Produce Predicted values\n\npred_df &lt;- expand_grid(\n  iv = c(\"Low\",\"Medium\",\"High\"),\n  covar_std = 0\n)\n\npred_df_m2 &lt;- cbind(pred_df, predict(m2, newdata = pred_df), interval = \"confidence\")\n\n# Plot Predicted values\npred_df_m2 %&gt;%\n  ggplot(aes(iv, fit))+\n  geom_pointrange(aes(ymin = lwr, ymax = upr))\n\nclass: inverse, center, middle # 💡 # Final Presentations"
  },
  {
    "objectID": "slides/12-slides.html#final-presentations",
    "href": "slides/12-slides.html#final-presentations",
    "title": "Week 12:",
    "section": "Final Presentations",
    "text": "Final Presentations\n\nNext Tuesday your groups will present some of the findings from your projects\n\n10 Minutes per group\n8-12 slides (15 max)\n2 Minute Q&A\n\nOn Thursday, we will work through the templates you’ve been provided\nDon’t have to present the finished product"
  },
  {
    "objectID": "slides/12-slides.html#final-presentation-structure",
    "href": "slides/12-slides.html#final-presentation-structure",
    "title": "Week 12:",
    "section": "Final Presentation Structure",
    "text": "Final Presentation Structure\n\nMotivation\nResearch Question\nTheory\nExpectations\nData\n\n\nSummary\nDescriptive Table and/or Figure (Optional)\n\n\nDesign\nResults\n\n\nSummary\nTable (Optional)\nFigure (Optional)\n\n\nConclusion\n\n\nAppendices (Extra Slides Optional)"
  },
  {
    "objectID": "slides/12-slides.html#template",
    "href": "slides/12-slides.html#template",
    "title": "Week 12:",
    "section": "Template",
    "text": "Template\nLet’s open up the template and explore.\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "More about this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "class/07-class.html",
    "href": "class/07-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#readings",
    "href": "class/07-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#lecture",
    "href": "class/07-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#lab",
    "href": "class/07-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#assignments",
    "href": "class/07-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/00-class.html",
    "href": "class/00-class.html",
    "title": "Introductions",
    "section": "",
    "text": "Welcome! I’ll see you in class on Thursday.\nTune in via Zoom: https://brown.zoom.us/j/97039852954",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#readings",
    "href": "class/00-class.html#readings",
    "title": "Introductions",
    "section": "Readings",
    "text": "Readings\n\nNone this week",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#lecture",
    "href": "class/00-class.html#lecture",
    "title": "Introductions",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#lab",
    "href": "class/00-class.html#lab",
    "title": "Introductions",
    "section": "Lab",
    "text": "Lab\n\nNone",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#assignments",
    "href": "class/00-class.html#assignments",
    "title": "Introductions",
    "section": "Assignments",
    "text": "Assignments\n\nWork through Software Setup before class next week",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/06-class.html",
    "href": "class/06-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#readings",
    "href": "class/06-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#lecture",
    "href": "class/06-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#lab",
    "href": "class/06-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#assignments",
    "href": "class/06-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/03-class.html",
    "href": "class/03-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#readings",
    "href": "class/03-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#lecture",
    "href": "class/03-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#lab",
    "href": "class/03-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#assignments",
    "href": "class/03-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/11-class.html",
    "href": "class/11-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#readings",
    "href": "class/11-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#lecture",
    "href": "class/11-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#lab",
    "href": "class/11-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#assignments",
    "href": "class/11-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/02-class.html",
    "href": "class/02-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#readings",
    "href": "class/02-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#lecture",
    "href": "class/02-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#lab",
    "href": "class/02-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#assignments",
    "href": "class/02-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/04-class.html",
    "href": "class/04-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#readings",
    "href": "class/04-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#lecture",
    "href": "class/04-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#lab",
    "href": "class/04-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#assignments",
    "href": "class/04-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "files/img/hexlogo.html",
    "href": "files/img/hexlogo.html",
    "title": "Hex Logo for POLS 1600",
    "section": "",
    "text": "Linking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  }
]