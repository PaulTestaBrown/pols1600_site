[
  {
    "objectID": "slides/10-slides.html#class-plan",
    "href": "slides/10-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nTopics:\n\nSampling distributions and standard errors (15 minutes)\nConfidence intervals (15 minutes )\nHypothesis testing (15 minutes)\nQuantifying uncertainty for regression (15 minutes)"
  },
  {
    "objectID": "slides/10-slides.html#announcements",
    "href": "slides/10-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\nFinal lab this week.\nNext week’s lecture:\n\nWorkshop/Review/special topics\nAsk questions in the survey\n\nFeedback on A2 by tomorrow."
  },
  {
    "objectID": "slides/10-slides.html#setup-packages-for-today",
    "href": "slides/10-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\", \"boot\", \"modelr\" ,\"purrr\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo          boot \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       modelr         purrr \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/10-slides.html#feedback",
    "href": "slides/10-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/10-slides.html#what-did-we-like",
    "href": "slides/10-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/10-slides.html#what-did-we-dislike",
    "href": "slides/10-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/10-slides.html#what-are-we-streaming",
    "href": "slides/10-slides.html#what-are-we-streaming",
    "title": "POLS 1600",
    "section": "What are we streaming",
    "text": "What are we streaming"
  },
  {
    "objectID": "slides/10-slides.html#goals",
    "href": "slides/10-slides.html#goals",
    "title": "POLS 1600",
    "section": "Goals:",
    "text": "Goals:\n\nA sampling distribution is a theoretical distribution of estimates obtained in repeated sampling\n\nWhat could have happened?\n\nA standard error (SE) is the standard deviation of the sampling distribution\nWe can calculate SEs via simulation and analytically\nWe can use SEs to construct confidence intervals and conduct hypothesis tests allowing us to quantify uncertainty"
  },
  {
    "objectID": "slides/10-slides.html#populations-and-samples",
    "href": "slides/10-slides.html#populations-and-samples",
    "title": "POLS 1600",
    "section": "Populations and samples",
    "text": "Populations and samples\n\nPopulation: All the cases from which you could have sampled\nParameter: A quantity or quantities of interest often generically called \\(\\theta\\) (“theta”). What we want to learn about our population\nSample: A (random) draw of observations from that population\nSample Size: The number of observations in your draw (without replacement)"
  },
  {
    "objectID": "slides/10-slides.html#estimators-estimates-and-statistics",
    "href": "slides/10-slides.html#estimators-estimates-and-statistics",
    "title": "POLS 1600",
    "section": "Estimators, estimates, and statistics",
    "text": "Estimators, estimates, and statistics\n\nEstimator: A rule for calculating an estimate of our parameter of interest.\nEstimate: The value produced by some estimator for some parameter from some data. Often called \\(\\hat{\\theta}\\)\nUnbiased estimators: \\(E(\\hat{\\theta})=E(\\theta)\\) On average, the estimates produced by some estimator will be centered around the truth\nConsistent estimates: \\(\\lim_{n\\to \\infty} \\hat{\\theta_N} = \\theta\\) As the sample size increases, the estimates from an estimator converge in probability to the parameter value\nStatistic: A summary of the data (mean, regression coefficient, \\(R^2\\)). An estimator without a specified target of inference"
  },
  {
    "objectID": "slides/10-slides.html#distrubtions-and-standard-errors",
    "href": "slides/10-slides.html#distrubtions-and-standard-errors",
    "title": "POLS 1600",
    "section": "Distrubtions and standard errors",
    "text": "Distrubtions and standard errors\n\nSampling Distribution: How some estimate would vary if you took repeated samples from the population\nStandard Error: The standard deviation of the sampling distribution\nResampling Distribution: How some estimate would vary if you took repeated samples from your sample WITH REPLACEMENT\n\n“Sampling from our sample, as the sample was sampled from the population.”"
  },
  {
    "objectID": "slides/10-slides.html#sampling-distributions",
    "href": "slides/10-slides.html#sampling-distributions",
    "title": "POLS 1600",
    "section": "Sampling distributions",
    "text": "Sampling distributions\n\nOverview CodeN = 10N = 30N = 300Comments\n\n\n\n\nTreat the 2024 NES pilot as the population\nTake repeated samples of size N = 10, 30, 300\nFor each sample of size N, calculate the sample mean of age\nPlot the distribution of sample means (i.e. the sampling distribution)\n\n\n\n\n\n# Load Data\nload(url(\"https://pols1600.paultesta.org/files/data/nes24.rda\"))\n\n# ---- Population ----\n\n# Population average\nmu_age &lt;- mean(df$age, na.rm=T)\n# Population standard deviation\nsd_age &lt;- sd(df$age, na.rm = T)\n\n# ---- Function to Take Repeated Samples From Data ----\n\nsample_data_fn &lt;- function(\n    dat=df, var=age, samps=1000, sample_size=10,\n    resample = F){\n  if(resample == F){\n  df &lt;- tibble(\n  sim = 1:samps,\n  distribution = \"Sampling\",\n  size = sample_size,\n  sample_from = \"Population\",\n  pop_mean = dat %&gt;% pull(!!enquo(var)) %&gt;% mean(., na.rm=T),\n  pop_sd = dat %&gt;% pull(!!enquo(var)) %&gt;% sd(., na.rm=T),\n  se_asymp = pop_sd / sqrt(size),\n  ll_asymp = pop_mean - 1.96*se_asymp,\n  ul_asymp = pop_mean + 1.96*se_asymp,\n) %&gt;% \n  mutate(\n    sample = purrr::map(sim, ~ slice_sample(dat %&gt;% select(!!enquo(var)), n = sample_size, replace = F)),\n    sample_mean = purrr::map_dbl(sample, \\(x) x %&gt;% pull(!!enquo(var)) %&gt;% mean(.,na.rm=T)),\n    ll = sample_mean - 1.96*sd(sample_mean),\n    ul = sample_mean + 1.96*sd(sample_mean)\n  )\n  }\n  if(resample == T){\n    df &lt;- tibble(\n  sim = 1:samps,\n  distribution = \"Resampling\",\n  size = sample_size,\n  sample_from = \"Sample\",\n  pop_mean = dat %&gt;% pull(!!enquo(var)) %&gt;% mean(., na.rm=T),\n  pop_sd = dat %&gt;% pull(!!enquo(var)) %&gt;% sd(., na.rm=T),\n  se_asymp = pop_sd / sqrt(size),\n  ll_asymp = pop_mean - 1.96*se_asymp,\n  ul_asymp = pop_mean + 1.96*se_asymp,\n) %&gt;% \n  mutate(\n    sample = purrr::map(sim, ~ slice_sample(dat %&gt;% select(!!enquo(var)), n = sample_size, replace = T)),\n    sample_mean = purrr::map_dbl(sample, \\(x) x %&gt;% pull(!!enquo(var)) %&gt;% mean(.,na.rm=T))\n  )\n  }\n  return(df)\n}\n\n# ---- Plot Single Distribution -----\n\nplot_distribution &lt;- function(the_pop,the_samp, the_var, ...){\n  mu_pop &lt;- the_pop %&gt;% pull(!!enquo(the_var)) %&gt;% mean(., na.rm=T)\n  mu_samp &lt;- the_samp %&gt;% pull(!!enquo(the_var)) %&gt;% mean(., na.rm=T)\n  ll &lt;- the_pop %&gt;% pull(!!enquo(the_var)) %&gt;% as.numeric() %&gt;%  min(., na.rm=T)\n  ul &lt;- the_pop %&gt;% pull(!!enquo(the_var)) %&gt;% as.numeric() %&gt;% max(., na.rm=T)\n  p&lt;- the_samp %&gt;% \n    ggplot(aes(!!enquo(the_var)))+\n    geom_density()+\n    geom_rug()+\n    theme_void()+\n    geom_vline(xintercept = mu_samp, col = \"red\")+\n    geom_vline(xintercept = mu_pop, col = \"grey40\",linetype = \"dashed\")+\n    xlim(ll,ul)\n  return(p)\n}\n\n# ---- Plot multiple distributions ----\n\nplot_samples &lt;- function(pop, x, variable,n_rows = 4, ...){\n  sample_plots &lt;- x$sample[1:(4*n_rows)] %&gt;% \n  purrr::map( \\(x) plot_distribution(the_pop=pop, the_samp = x, \n                                     the_var = !!enquo(variable)))\n  p &lt;- wrap_elements(wrap_plots(sample_plots[1:(4*n_rows)], ncol=4))\n  return(p)\n  \n}\n\n# ---- Plot Combined Figure ----\n\nplot_figure_fn &lt;- function(\n    d=df, \n    v=age, \n    sim=1000, \n    size=10,\n    rows = 4){\n  # Population average\n  mu &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% mean(., na.rm=T)\n  sd &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% sd(., na.rm=T)\n  se &lt;- sd/sqrt(size)\n  # Range\n  ll &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;%  min(., na.rm=T)\n  ul &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;% max(., na.rm=T)\n  # Population standard deviation\n  # Sample data\n  samp_df &lt;- sample_data_fn(dat=d, var = !!enquo(v), samps = sim, sample_size = size)\n  # Plot Population\n  p_pop &lt;- d %&gt;%\n    ggplot(aes(!!enquo(v)))+\n      geom_density(col =\"grey60\")+\n      geom_rug(col = \"grey60\", )+\n      geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n      theme_void()+\n      labs(title =\"Population\")+\n      xlim(ll,ul)+\n      theme(plot.title = element_text(hjust = 0))\n\n  \n  p_samps &lt;- plot_samples(pop=d, x= samp_df,variable = !!enquo(v),\n                          n_rows = rows)\n  p_samps &lt;- p_samps + \n    ggtitle(paste(\"Repeated samples of size N =\",size,\"from the population\"))+\n    theme(plot.title = element_text(hjust = 0.5), \n          plot.background = element_rect(\n            fill = NA, colour = 'black', linewidth = 2)\n          )\n  \n  \n  p_dist &lt;- samp_df %&gt;% \n  ggplot(aes(sample_mean))+\n  geom_density(col=\"red\",aes(y= after_stat(ndensity)))+\n  geom_rug(col=\"red\")+\n  geom_density(data = df, aes(!!enquo(v), y= after_stat(ndensity)),\n               col=\"grey60\")+\n  geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n  xlim(ll,ul)+\n  theme_void()+\n    labs(\n      title = \"Sampling Distribution\"\n    )+  theme(plot.title = element_text(hjust = 0))\n  \n  range_upper_df &lt;- tibble(\n  x = seq( ((ll+ul)/2 -5), ((ll+ul)/2 +5), length.out = 20),\n  xend = seq(ll-5, ul+5, length.out = 20),\n  y = rep(9, 20),\n  yend = rep(1, 20)\n)\np_upper &lt;- range_upper_df %&gt;% \n  ggplot(aes(x=x, xend = xend, y=y,yend=yend))+\n  geom_segment(\n    arrow = arrow(length = unit(0.05, \"npc\"))\n  )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),\n              xlim =c(ll-5,ul+5),clip=\"off\")\n  # Lower\n  range_df &lt;- samp_df %&gt;% \n  summarise(\n    min = min(sample_mean),\n    max = max(sample_mean),\n    mean = mean(sample_mean)\n  )\n  \n  plot_df &lt;- tibble(\n  id = 1:50,\n  # x = sort(rnorm(50, mu, sd)),\n  x = sort(runif(50, ll, ul)),\n  xend = sort(rnorm(50, mu, se)),\n  y = 9,\n  yend = 1\n)\n\np_lower &lt;- plot_df %&gt;%\n  ggplot(aes(x,y, group =id))+\n  geom_segment(aes(xend=xend, yend=yend),\n               col = \"red\",arrow = arrow(length = unit(0.05, \"npc\"))\n               )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),xlim = c(ll,ul),clip=\"off\")\n\n  \n  design &lt;-\"##AAAA##\n            ##AAAA##\n            ##AAAA##\n            BBBBBBBB\n            BBBBBBBB\n            #CCCCCC#\n            #CCCCCC#\n            #CCCCCC#\n            #CCCCCC#\n            DDDDDDDD\n            DDDDDDDD\n            ##EEEE##\n            ##EEEE##\n            ##EEEE##\"\n  \n  fig &lt;- p_pop / p_upper / p_samps / p_lower / p_dist +\n    plot_layout(design = design)\n  return(fig)\n\n\n  \n  \n  \n}\n\n# ---- Samples and Figures Varying Sample Size ----\n## N = 10\nset.seed(1234)\nsamp_n10 &lt;- sample_data_fn(sample_size  = 10, samps = 1000)\nset.seed(1234)\nfig_n10 &lt;- plot_figure_fn(v=age,size = 10)\n\n## N = 30\nset.seed(1234)\nsamp_n30 &lt;- sample_data_fn(sample_size  = 30, samps = 1000)\nset.seed(1234)\nfig_n30 &lt;- plot_figure_fn(size = 30,rows=4)\n\n## N = 300\nset.seed(1234)\nsamp_n300 &lt;- sample_data_fn(sample_size  = 300, samps = 1000)\nset.seed(1234)\nfig_n300 &lt;- plot_figure_fn(size = 300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the sample sample size increases:\n\nThe width of the sampling distribution decreases (LLN)\nThe shape of the sampling distribution approximates a Normal distribution (CLT)"
  },
  {
    "objectID": "slides/10-slides.html#standard-errors",
    "href": "slides/10-slides.html#standard-errors",
    "title": "POLS 1600",
    "section": "Standard errors",
    "text": "Standard errors\n\nOverview Code SEs Coverage\n\n\n\n\nThe standard error (SE) is simply the standard deviation of the sampling distribution.\nThe SE decreases as the sample size increases (by the LLN):\nApproximately 95% of the sample means will be within 2 SEs of the population mean (CLT)\n\n\n\n\n\nse_df &lt;- tibble(\n  `Sample Size` = factor(paste(\"N =\",c(10,30, 300))),\n  se = c(sd(samp_n10$sample_mean),\n         sd(samp_n30$sample_mean),\n         sd(samp_n300$sample_mean)),\n  SE = paste(\"SE =\", round(se,2)),\n  ll = mu_age,\n  ul = mu_age + se,\n  y = c(.3,.3,.45),\n  yend = y\n)\n\nci_df &lt;- tibble(\n  `Sample Size` = factor(paste(\"N =\",c(10,30, 300))),\n  se = c(sd(samp_n10$sample_mean),\n         sd(samp_n30$sample_mean),\n         sd(samp_n300$sample_mean)),\n  mu = mu_age,\n  ll = round(mu_age - 1.96 *se,2),\n  ul = round(mu_age + 1.96 *se,2),\n  ci = paste(\"95 % Coverage Interval [\",ll,\";\",ul,\"]\",sep=\"\"),\n  y = c(.3,.3,.45),\n  yend = y\n)\nsim_df &lt;- samp_n10 %&gt;% \n  bind_rows(samp_n30) %&gt;% \n  bind_rows(samp_n300) %&gt;% \n  mutate(\n    `Sample Size` = factor(paste(\"N =\",size))\n    ) %&gt;% \n  left_join(ci_df) %&gt;% \n  mutate(\n    Coverage = case_when(\n      sample_mean &gt; ll_asymp & sample_mean &lt; ul_asymp  & size == 10~ \"#F8766D\",\n      sample_mean &gt; ll_asymp & sample_mean &lt; ul_asymp  & size == 30~ \"#00BA38\",\n      sample_mean &gt; ll_asymp & sample_mean &lt; ul_asymp  & size == 300~ \"#619CFF\",\n      T ~ \"grey\"\n    )\n  )\n\n\n\nfig_se &lt;- sim_df %&gt;% \n  ggplot(aes(sample_mean, col = `Sample Size`))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = mu_age, linetype = \"dashed\")+\n  theme_minimal()+\n  facet_wrap(~`Sample Size`, ncol=1)+\n  ylim(0,.5)+\n  guides(col=\"none\")+\n  geom_segment(\n    data = se_df,\n    aes(x= ll, xend =ul, y = y, yend = yend)\n  )+\n  geom_text(\n    data = se_df,\n    aes(x = ul, y =y, label = SE),\n    hjust = -.25\n  ) +\n  labs(\n    y = \"\",\n    x = \"Sampling Distributions of Sample Means\",\n    title = \"Standard Errors decrease with Sample Size\"\n  )\n\nfig_coverage &lt;- sim_df %&gt;% \n  ggplot(aes(sample_mean,col=`Sample Size`))+\n  geom_density()+\n  geom_rug(col=sim_df$Coverage)+\n  geom_vline(xintercept = mu_age, linetype = \"dashed\")+\n  theme_minimal()+\n  facet_wrap(~`Sample Size`, ncol=1)+\n  ylim(0,.55)+\n  guides(col=\"none\")+\n  geom_segment(\n    data = ci_df,\n    aes(x= ll, xend =ul, y = y, yend = yend)\n  )+\n  geom_text(\n    data = ci_df,\n    aes(x = mu, y =y, label = ci),\n    hjust = .5,\n    nudge_y =.1\n  ) +\n  labs(\n    y = \"\",\n    x = \"Sampling Distributions of Sample Means\",\n    title = \"Approximately 95% of sample means are within 2 SE of the population mean\"\n  )"
  },
  {
    "objectID": "slides/10-slides.html#how-do-we-calculate-a-standard-error-from-a-single-sample",
    "href": "slides/10-slides.html#how-do-we-calculate-a-standard-error-from-a-single-sample",
    "title": "POLS 1600",
    "section": "How do we calculate a standard error from a single sample?",
    "text": "How do we calculate a standard error from a single sample?"
  },
  {
    "objectID": "slides/10-slides.html#calculating-standard-errors",
    "href": "slides/10-slides.html#calculating-standard-errors",
    "title": "POLS 1600",
    "section": "Calculating standard errors",
    "text": "Calculating standard errors\n\nTwo ApproachesN = 10N = 30N = 300Simulation vs Analytic\n\n\n\n\nSimulation:\n\nTreat sample as population\nSample with replacement (“bootstrapping”)\nEstimate SE from standard deviation of resampling distribution (“plug-in principle”)\n\nAnalytic\n\nCharacterize sampling distribution from sample mean and variance via asymptotic theory (the LLT and CLT)\nFor a sample mean, \\(\\bar{x}\\)\n\n\n\\[\nSE_{\\bar{x}} = \\frac{\\sigma_x}{\\sqrt(n)}\n\\]\n\n\n\n\nplot_resampling_fn &lt;- function(d=df, v=age, sim=1000, size=10,rows=3){\n  # Population average\n  mu &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% mean(., na.rm=T)\n  # Population standard deviation and SE\n  sd &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% sd(., na.rm=T)\n  se &lt;- sd/sqrt(size)\n  # Range\n  ll &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;%  min(., na.rm=T)\n  ul &lt;- d %&gt;% pull(!!enquo(v)) %&gt;% as.numeric() %&gt;% max(., na.rm=T)\n  # Resampling with replace\n  # Draw 1 Sample\n  sample &lt;- sample_data_fn(dat=d, var = !!enquo(v), samps = 1, sample_size = size, resample = F)\n  samp_df &lt;- as.data.frame(sample$sample)\n  # Resample from sample with replacement\n  resamp_df &lt;- sample_data_fn(dat=samp_df, var = !!enquo(v), samps = sim, sample_size = size, resample = T)\n  # Plot Population\n  p_pop &lt;- d %&gt;%\n    ggplot(aes(!!enquo(v)))+\n      geom_density(col =\"grey60\")+\n      geom_rug(col = \"grey60\", )+\n      geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n      theme_void()+\n      labs(title =\"Population\")+\n      xlim(ll,ul)+\n      theme(plot.title = element_text(hjust = 0))\n\n  p_samp &lt;- plot_distribution(the_pop = d,\n                              the_samp = samp_df,\n                              the_var = age)+\n    labs(title =\"Sample\")+\n      xlim(ll,ul)+\n      theme(plot.title = element_text(hjust = 0))\n  \n  p_samps &lt;- plot_samples(pop=d, x= resamp_df,variable = !!enquo(v), n_rows =rows)\n  p_samps &lt;- p_samps + \n    ggtitle(paste(\"Repeated samples with replacement\\nof size N =\",size,\"from sample\"))+\n    theme(plot.title = element_text(hjust = 0.5), \n          plot.background = element_rect(\n            fill = NA, colour = 'black', linewidth = 2)\n          )\n  \n  # Resampling Distribution\n  \n  \n  p_dist &lt;- resamp_df %&gt;% \n  ggplot(aes(sample_mean))+\n  geom_density(col=\"red\",aes(y= after_stat(ndensity)))+\n  geom_rug(col=\"red\")+\n  geom_density(data = df, aes(!!enquo(v), y= after_stat(ndensity)),\n               col=\"grey60\")+\n  geom_vline(xintercept = unique(resamp_df$pop_mean), col=\"red\", linetype=\"solid\")+\n  geom_vline(xintercept = mu, col=\"grey40\", linetype=\"dashed\")+\n  xlim(ll,ul)+\n  theme_void()+\n    labs(\n      title = \"Reampling Distribution\"\n    )+  theme(plot.title = element_text(hjust = 0))\n  \n   range_upper_df &lt;- tibble(\n  x = seq( ((ll+ul)/2 -5), ((ll+ul)/2 +5), length.out = 20),\n  xend = seq(ll-5, ul+5, length.out = 20),\n  y = rep(9, 20),\n  yend = rep(1, 20)\n)\np_upper &lt;- range_upper_df %&gt;% \n  ggplot(aes(x=x, xend = xend, y=y,yend=yend))+\n  geom_segment(\n    arrow = arrow(length = unit(0.05, \"npc\"))\n  )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),\n              xlim =c(ll-5,ul+5),clip=\"off\")\n  # Lower\n  range_df &lt;- resamp_df %&gt;% \n  summarise(\n    min = min(sample_mean),\n    max = max(sample_mean),\n    mean = mean(sample_mean)\n  )\n  \n  plot_df &lt;- tibble(\n  id = 1:50,\n  # x = sort(rnorm(50, mu, sd)),\n  x = sort(runif(50, ll, ul)),\n  xend = sort(rnorm(50, unique(resamp_df$pop_mean), se)),\n  y = 9,\n  yend = 1\n)\n\np_lower &lt;- plot_df %&gt;%\n  ggplot(aes(x,y, group =id))+\n  geom_segment(aes(xend=xend, yend=yend),\n               col = \"red\",arrow = arrow(length = unit(0.05, \"npc\"))\n               )+\n  theme_void()+\n  coord_fixed(ylim=c(0,10),xlim = c(ll,ul),clip=\"off\")\n\n  \n  design &lt;-\"##AAAA##\n            ##AAAA##\n            ##AAAA##\n            ##BBBB##\n            ##BBBB##\n            ##BBBB##            \n            CCCCCCCC\n            CCCCCCCC\n            #DDDDDD#\n            #DDDDDD#\n            #DDDDDD#\n            #DDDDDD#\n            EEEEEEEE\n            EEEEEEEE\n            ##FFFF##\n            ##FFFF##\n            ##FFFF##\"\n  \n  fig &lt;- p_pop / p_samp /p_upper / p_samps / p_lower / p_dist +\n    plot_layout(design = design)\n  return(fig)\n\n\n  \n  \n  \n}\nset.seed(123)\nresamp_n10 &lt;- sample_data_fn(\n  dat = sample_data_fn(samps = 1, sample_size = 10, resample = T)$sample %&gt;%  as.data.frame(),\n  sample_size = 10, \n  resample = T)\nset.seed(123)\nfig_n10_bs &lt;- plot_resampling_fn(size=10)\n\nset.seed(12345)\nresamp_n30 &lt;- sample_data_fn(\n  dat = sample_data_fn(samps = 1, sample_size = 30, resample = T)$sample %&gt;%  as.data.frame(),\n  samps = 1000, sample_size = 30, resample = T)\n\nset.seed(12345)\nfig_n30_bs &lt;- plot_resampling_fn(size=30)\n\nset.seed(1234)\nresamp_n300 &lt;- sample_data_fn(\n  dat = sample_data_fn(samps = 1, sample_size = 300, resample = T)$sample %&gt;%  as.data.frame(),\n  samps = 1000, sample_size = 300, resample = T)\nset.seed(1234)\nfig_n300_bs &lt;- plot_resampling_fn(size=300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap SE\nAnalytic SE\n\n\n\n\n5.74\n5.61\n\n\n2.75\n3.24\n\n\n1.07\n1.02"
  },
  {
    "objectID": "slides/10-slides.html#confidence-intervals-1",
    "href": "slides/10-slides.html#confidence-intervals-1",
    "title": "POLS 1600",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nConfidence intervals:\n\nprovide a way of quantifying uncertainty about estimates\ndescribe a range of plausible values for an estimate\nare a function of the standard error of the estimate, and the a critical value determined by \\(\\alpha\\), which describes the degree of confidence we want"
  },
  {
    "objectID": "slides/10-slides.html#section-1",
    "href": "slides/10-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Calculating a confidence interval\n\nSteps Code Fig 1 Fig 2 Fig 3Comments\n\n\n\n\nChoose level of confidence \\((1-\\alpha)\\times 100%\\)\n\n\\(\\alpha = 0.05\\), corresponds to a 95% confidence level.\n\nDerive the sampling distribution of the estimator\n\nSimulation: bootstrap re-sampling\nAnalytically: computing its mean and variance.\n\nCompute the standard error\nCompute the critical value \\(z_{\\alpha/2}\\)\n\nas the \\(1.96 = \\Phi(z_{0.5/2})\\) for a 95% CI\n\nCompute the lower and upper confidence limits\n\nlower limit = \\(\\hat{\\theta} - z_{\\alpha/2}\\times SE\\)\nupper limit = \\(\\hat{\\theta} + z_{\\alpha/2}\\times SE\\)\n\n\n\n\n\n\nresamp_df &lt;- \n  resamp_n10 %&gt;% \n  bind_rows(resamp_n30) %&gt;% \n  bind_rows(resamp_n300) %&gt;% \n  mutate(\n    `Sample Size` = factor(paste(\"N =\",size))\n    )\n\nresamp_ci_df &lt;- tibble(\n  `Sample Size` = factor(paste(\"N =\",c(10,30,300))),\n  mu = unique(resamp_df$pop_mean),\n  ll = unique(resamp_df$ll_asymp),\n  ul = unique(resamp_df$ul_asymp),\n  y = c(.3, .3,.5)\n)\n\nfig_ci1 &lt;- resamp_df %&gt;% \n  ggplot(aes(sample_mean,\n             col = `Sample Size`))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = mu_age, linetype = \"dashed\")+\n  geom_vline(data = resamp_ci_df,\n             aes(xintercept = mu,\n                 col = `Sample Size`))+\n  geom_segment(data = resamp_ci_df,\n               aes(x = ll, xend =ul, y = y, yend =y,\n                   col = `Sample Size`))+\n  facet_wrap(~`Sample Size`, ncol=1)+\n  theme_minimal()+\n  labs(\n    y = \"\",\n    x = \"Resampling Distribution\",\n    title = \"95% Confidence Intervals\"\n  )\n  \n\nsamp_ci_df &lt;- samp_n10 %&gt;% \n  bind_rows(samp_n30) %&gt;% \n  bind_rows(samp_n300) %&gt;% \n  mutate(\n    `Sample Size` = factor(paste(\"N =\",size))\n    ) %&gt;% \n  mutate(\n    Coverage = case_when(\n      pop_mean &gt; ll & pop_mean &lt; ul ~ \"red\",\n      T ~ \"black\"\n    )\n  )\n\nfig_ci2 &lt;- samp_ci_df %&gt;% \n  filter(sim %in% 1:100) %&gt;% \n  filter(size == 10) %&gt;% \n  ggplot(aes(y = sample_mean, x= sim))+\n  geom_pointrange(aes(ymin = ll, ymax =ul, col=Coverage))+\n  geom_hline(yintercept = mu_age, linetype = \"dashed\")+\n  coord_flip()+\n  theme_minimal()+\n  guides(col = \"none\")+\n  facet_wrap(~`Sample Size`)\n\nfig_ci3 &lt;- samp_ci_df %&gt;% \n  filter(sim %in% 1:100) %&gt;% \n  ggplot(aes(y = sample_mean, x= sim))+\n  geom_pointrange(aes(ymin = ll, ymax =ul, col=Coverage))+\n  geom_hline(yintercept = mu_age, linetype = \"dashed\")+\n  coord_flip()+\n  theme_minimal()+\n  guides(col = \"none\")+\n  facet_wrap(~`Sample Size`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 shows 3 confidences intervals for 3 samples of different sizes (N = 10, 30, 300). The CIs for N = 10 and N = 300, intervals contain the truth (include the population mean). By chance, the CI for N=30 falls outside of the truth.\nFigure 2 shows that our confidence is about the property of the interval. Over repeated sampling, 95% of the intervals would contain the truth, 5% percent would not.\n\nIn any one sample, the population parameter either is or is not within the interval.\n\nFigure 3, shows that while the width of the interval declines with the sample size, the coverage properties remains the same."
  },
  {
    "objectID": "slides/10-slides.html#interpreting-confidence-intervals",
    "href": "slides/10-slides.html#interpreting-confidence-intervals",
    "title": "POLS 1600",
    "section": "Interpreting confidence intervals",
    "text": "Interpreting confidence intervals\n\nConfidence intervals give a range of values that are likely to include the true value of the parameter \\(\\theta\\) with probability \\((1-\\alpha) \\times 100\\%\\)\n\n\\(\\alpha = 0.05\\) corresponds to a “95-percent confidence interval”\n\nOur “confidence” is about the interval\nIn repeated sampling, we expect that \\((1-\\alpha) \\times 100\\%\\) of the intervals we construct would contain the truth.\nFor any one interval, the truth, \\(\\theta\\), either falls within in the lower and upper bounds of the interval or it does not."
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-hypothesis-test",
    "href": "slides/10-slides.html#what-is-a-hypothesis-test",
    "title": "POLS 1600",
    "section": "What is a hypothesis test",
    "text": "What is a hypothesis test\n\nA formal way of assessing statistical evidence. Combines\n\nDeductive reasoning distribution of a test statistic, if the a null hypothesis were true\nInductive reasoning based on the test statistic we observed, how likely is it that we would observe it if the null were true?"
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-test-statistic",
    "href": "slides/10-slides.html#what-is-a-test-statistic",
    "title": "POLS 1600",
    "section": "What is a test statistic?",
    "text": "What is a test statistic?\n\nA way of summarizing data\n\ndifference of means\ncoefficients from a linear model\ncoefficients from a linear model divided by their standard errors\nR^2\nSums of ranks\n\n\n\n\n\n\n\n\n\nNote\n\n\nDifferent test statistics may be more or less appropriate depending on your data and questions."
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-null-hypothesis",
    "href": "slides/10-slides.html#what-is-a-null-hypothesis",
    "title": "POLS 1600",
    "section": "What is a null hypothesis?",
    "text": "What is a null hypothesis?\n\nA statement about the world\n\nOnly interesting if we reject it\nWould yield a distribution of test statistics under the null\nTypically something like “X has no effect on Y” (Null = no effect)\nNever accept the null can only reject"
  },
  {
    "objectID": "slides/10-slides.html#what-is-a-p-value",
    "href": "slides/10-slides.html#what-is-a-p-value",
    "title": "POLS 1600",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nA p-value is a conditional probability summarizing the likelihood of observing a test statistic as far from our hypothesis or farther, if our hypothesis were true."
  },
  {
    "objectID": "slides/10-slides.html#how-do-we-do-hypothesis-testing",
    "href": "slides/10-slides.html#how-do-we-do-hypothesis-testing",
    "title": "POLS 1600",
    "section": "How do we do hypothesis testing?",
    "text": "How do we do hypothesis testing?\n\nPosit a hypothesis (e.g. \\(\\beta = 0\\))\nCalculate the test statistic (e.g. \\((\\hat{\\beta}-\\beta)/se_\\beta\\))\nDerive the distribution of the test statistic under the null via simulation or asymptotic theory\nCompare the test statistic to the distribution under the null\nCalculate p-value (Two Sided vs One sided tests)\nReject or fail to reject/retain our hypothesis based on some threshold of statistical significance (e.g. p &lt; 0.05)"
  },
  {
    "objectID": "slides/10-slides.html#outcomes-of-hypothesis-tests",
    "href": "slides/10-slides.html#outcomes-of-hypothesis-tests",
    "title": "POLS 1600",
    "section": "Outcomes of hypothesis tests",
    "text": "Outcomes of hypothesis tests\n\n\nTwo conclusions from of a hypothesis test: we can reject or fail to reject a hypothesis test.\nWe never “accept” a hypothesis, since there are, in theory, an infinite number of other hypotheses we could have tested.\n\nOur decision can produce four outcomes and two types of error:\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative\n\n\n\n\nType 1 Errors: False Positive Rate (p &lt; 0.05)\nType 2 Errors: False negative rate (1 - Power of test)"
  },
  {
    "objectID": "slides/10-slides.html#quantifying-uncertainty-in-regression-1",
    "href": "slides/10-slides.html#quantifying-uncertainty-in-regression-1",
    "title": "POLS 1600",
    "section": "Quantifying uncertainty in regression",
    "text": "Quantifying uncertainty in regression\n\nOverview Raw SEs CIs Coefficient plot\n\n\nHow do income and education shape political participation?\nLet’s fit the following model\n\\[\ny = \\beta_0 + \\beta_1\\text{income} + \\beta_2 \\text{education} + \\epsilon\n\\]\n\nm1 &lt;- lm_robust(dv_participation ~   education + income, df)\n\nAnd unpack the output\n\n\n\ntidy(m1) %&gt;% \n  mutate_if(is.numeric, \\(x) round(x, 3)) -&gt; m1_sum\nm1_sum\n\n         term estimate std.error statistic p.value conf.low conf.high   df\n1 (Intercept)    0.312     0.080     3.910   0.000    0.155     0.468 1684\n2   education    0.167     0.024     6.891   0.000    0.119     0.214 1684\n3      income    0.007     0.010     0.671   0.502   -0.014     0.028 1684\n           outcome\n1 dv_participation\n2 dv_participation\n3 dv_participation\n\n\n\n\nhtmlreg(m1,include.ci=F) \n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.31***\n\n\n\n\n \n\n\n(0.08)\n\n\n\n\neducation\n\n\n0.17***\n\n\n\n\n \n\n\n(0.02)\n\n\n\n\nincome\n\n\n0.01\n\n\n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.29\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\nhtmlreg(m1,include.ci=T) \n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.31*\n\n\n\n\n \n\n\n[ 0.16; 0.47]\n\n\n\n\neducation\n\n\n0.17*\n\n\n\n\n \n\n\n[ 0.12; 0.21]\n\n\n\n\nincome\n\n\n0.01\n\n\n\n\n \n\n\n[-0.01; 0.03]\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.29\n\n\n\n\n\n\n* 0 outside the confidence interval.\n\n\n\n\n\n\n\nm1_coefplot &lt;- m1_sum %&gt;% \n  ggplot(aes(term, estimate))+\n  geom_pointrange(aes(ymin = conf.low, ymax =conf.high))+\n  geom_hline(yintercept = 0, linetype = \"dashed\")+\n  coord_flip()+\n  labs(\n    y = \"Estimate\",\n    x = \"\",\n    title = \"Coefficient plot\"\n  )+\n  theme_minimal()"
  },
  {
    "objectID": "slides/10-slides.html#estimates",
    "href": "slides/10-slides.html#estimates",
    "title": "POLS 1600",
    "section": "Estimates",
    "text": "Estimates\n\nEstimateComments\n\n\nThe estimate column are the regression coefficients, \\(\\beta\\)\nRecall, lm_robust() calculates these:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\n\n\n\n\n\n\nTip\n\n\n\\(\\beta\\)s describe substantive relationships between predictors (income, education) and the outcome (political participation)\n\n\n\n\n\n\ncoef(m1)\n\n(Intercept)   education      income \n0.311609712 0.166755964 0.007034253 \n\nX &lt;- model.matrix(m1,data=df)\ny &lt;- model.frame(m1)$dv_participation\nbetas &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbetas\n\n                   [,1]\n(Intercept) 0.311609712\neducation   0.166755964\nincome      0.007034253\n\n\n\n\nA unit increases in education is associated with about 0.16 more acts of political participation, while a unit increase in income is associated with 0.007 more acts of participation.\nNote that both income and education are measured with ordinal scales\n\nget_value_labels(df$educ)\n\n    No HS credential High school graduate         Some college \n                   1                    2                    3 \n       2-year degree        4-year degree            Post-grad \n                   4                    5                    6 \n\n\nSuch that it might be unreasonable to assume cardinality (going from a 1 to 2 is the same as going from a 3 to 4)\n\nConsider treating as factor / recoding variable"
  },
  {
    "objectID": "slides/10-slides.html#section-4",
    "href": "slides/10-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Standard errors & confidence intervals\n\nSEs and CI Code SEs SEsComments\n\n\nThe default standard errors from lm_robust() are calculated as follows\n\\[\nSE_{\\beta} = (X'X)^{-1}X'\\text{diag}\\left[\\frac{e_i^2}{1-h_{ii}}\\right]X(X'X)^{-1}\n\\]\nWhich we could also obtain via bootstrapping.\nThe confidence intervals are calculated as follows:\n\\[\nCI = \\beta \\pm 1.96\\times SE_\\beta\n\\]\n\n\n\n# 0 Set seed\nset.seed(123)\n\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df %&gt;% select(dv_participation, income, education), 1000)\n# Estimate Boostrapped Models\nm1_bs &lt;- purrr::map(boot$strap, ~ lm_robust(dv_participation ~  income + education, data = .))\n\n# Tidy coefficients\nm1_bs_df &lt;- map_df(m1_bs, tidy, .id = \"id\")\nm1_asymp_df &lt;- tidy(m1) %&gt;% \n  mutate(\n    term = factor(term)\n  ) %&gt;% \n  select(term,estimate, std.error,conf.low, conf.high) %&gt;% \n  mutate(\n    ll = conf.low,\n    ul = conf.high,\n    y = 1.1,\n    type = \"Analytic\"\n  )\n\nm1_bs_ci_df &lt;- m1_bs_df %&gt;%\n  mutate(\n    term = factor(term)\n  ) %&gt;% \n  group_by(term) %&gt;% \n  summarise(\n  beta = mean(estimate,na.rm=T),\n  se = sd(estimate,na.rm=T)\n  ) %&gt;% \n  mutate(\n  ll = beta - 1.96*se,\n  ul = beta + 1.96*se,\n  y = 1.05,\n  type = \"Bootstrap\"\n) \n\n# Compare SEs\n\ncompare_m1_se_tab &lt;-\n  tibble(\n    `Predictor` = m1_bs_ci_df$term,\n    Estimate = m1_asymp_df$estimate,\n    `SE` = m1_asymp_df$std.error,\n     `CI` = paste(\"[\", round(m1_asymp_df$ll,2),\n                  \"; \", round(m1_asymp_df$ul,2),\"]\",\n                  sep =\"\"),\n    `SE ` = m1_bs_ci_df$se,\n    `CI ` = paste(\"[\", round(m1_bs_ci_df$ll,2),\n                  \"; \", round(m1_bs_ci_df$ul,2),\"]\",\n                  sep =\"\"),\n  )\n\n\n# Figure\nfig_m1_bs &lt;- m1_bs_df %&gt;% \n  ggplot(aes(estimate))+\n  geom_density(aes(y=after_stat(ndensity)))+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype = \"dashed\")+\n  facet_wrap(~term,scales = \"free\")+\n  theme_minimal()+\n  ylim(0, 1.2)+\n  geom_vline(\n    data = m1_asymp_df,\n    aes(xintercept = estimate)\n  ) +\n  geom_segment(\n    data = m1_bs_ci_df,\n    aes(x = ll, xend = ul,\n        y = y, yend = y,\n        col = \"Bootstrap\")\n    \n  ) +\n  geom_segment(\n    data = m1_asymp_df,\n    aes(x = ll, xend = ul,\n        y = y, yend = y,\n        col = \"Analytic\")\n    \n  ) +\n  labs(\n    col = \"Confidence Interval\",\n    x = \"Bootstrapped Sampling Distribution\\n of Coefficients\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytic\n\n\nBootstrap\n\n\n\nPredictor\nEstimate\nSE\nCI\nSE\nCI\n\n\n\n\n(Intercept)\n0.3116\n0.0797\n[0.16; 0.47]\n0.0805\n[0.15; 0.47]\n\n\neducation\n0.1668\n0.0242\n[0.12; 0.21]\n0.0248\n[0.12; 0.22]\n\n\nincome\n0.0070\n0.0105\n[-0.01; 0.03]\n0.0107\n[-0.01; 0.03]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe main takeaway here is that for linear models, bootstrapped SEs and CIs are quite similar to those obtained via analytically (via math and asymptotic theory)\nFor common estimators and large samples, we’ll generally use analytic SEs (quicker)\nFor less common estimators (ratios of estimates), analytic estimates of the SEs may not exist. Bootstrapping will still provide valid SEs, provided we “sample from the sample, as the sample was drawn from the population”"
  },
  {
    "objectID": "slides/10-slides.html#test-statistics-and-p-values",
    "href": "slides/10-slides.html#test-statistics-and-p-values",
    "title": "POLS 1600",
    "section": "Test statistics and p-values",
    "text": "Test statistics and p-values\n\nOverview CodeComments\n\n\nThe test statistic (“t-stat”) reported by lm() and lm_robust() is our observed coefficient, \\(\\hat{\\beta}\\) minus our hypothesized value \\(\\beta\\) (e.g. 0), divided by the standard error of \\(\\hat{\\beta}\\).\n\\[t= \\frac{\\hat\\beta-\\beta}{\\widehat{SE}_{\\hat{\\beta}}} \\sim \\text{Students's } t \\text{ with } n-k \\text{ degrees of freedom}\\] Which follows a \\(t\\) distribution – like a Normal with “heavier tails” (e.g. more probability assigned to extreme values)\n\n\n\n# Calculate t-stats\n\nt_stat_df &lt;- tibble(\n  x= seq(-3,3,length.out = 20),\n  p = dt(x,df=m1$df[1] )\n)\n\n\nm1_tstat_educ &lt;- t_stat_df %&gt;% \n  ggplot(aes(x=x,y=p))+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"line\",\n    xlim = c(\n      min(c(-3, abs(m1$statistic[2])*-1 -1)),\n      max(c(3, abs(m1$statistic[2])+1))\n      )\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(m1$statistic[2],4)\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(-4, abs(m1$statistic[2])*-1)\n  )+\n  geom_vline(xintercept = m1$statistic[2],\n             col = \"blue\",\n             linetype = \"dashed\")+\n   geom_vline(xintercept = m1$statistic[2]*-1,\n             col = \"blue\",\n             linetype = \"dashed\")+\n  theme_minimal()+\n  labs(\n    title = \"Education\",\n    subtitle = paste(\"t-stat = \",round(m1$statistic[2],3),\n    \"\\nPr(&gt;|t|) = \",\n    format(round(m1$p.value[2],3),nsmall = 3),\n    sep = \"\"\n    ),\n    x = \"Distribution of t-stat under the Null\"\n  )\n\nm1_tstat_income &lt;- t_stat_df %&gt;% \n  ggplot(aes(x=x,y=p))+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"line\",\n    xlim = c(\n      min(c(-3, abs(m1$statistic[3])*-1 -1)),\n      max(c(3, abs(m1$statistic[3])+1))\n      )\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(m1$statistic[3],4)\n  )+\n  stat_function(\n    fun= dt, \n    args = list(df = m1$df[1]),\n    geom = \"area\",\n    fill = \"blue\",\n    alpha = .5,\n    xlim = c(-4, abs(m1$statistic[3])*-1)\n  )+\n  geom_vline(xintercept = m1$statistic[3],\n             col = \"blue\",\n             linetype = \"dashed\")+\n   geom_vline(xintercept = m1$statistic[3]*-1,\n             col = \"blue\",\n             linetype = \"dashed\")+\n  theme_minimal()+\n  labs(\n    title = \"Income\",\n    subtitle = paste(\"t-stat = \",round(m1$statistic[3],3),\n    \"\\nPr(&gt;|t|) = \",\n    format(round(m1$p.value[3],3),nsmall = 3),\n    sep = \"\"\n    ),\n    x = \"Distribution of t-stat under the Null\"\n  )\n\nfig_pvalue &lt;- m1_tstat_educ + m1_tstat_income\n\n# Compare Pvalues\n\ncompare_m1_pvalue &lt;-\n  tibble(\n    `Predictor` = m1_bs_ci_df$term,\n    Estimate = m1_asymp_df$estimate,\n    SE = m1_sum$std.error,\n    `t-stat` = m1_sum$statistic,\n     `Pr(&gt;abs(t))` = format(round(m1_sum$p.value,3), nsmall=3)\n  )\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nSE\nt-stat\nPr(&gt;abs(t))\n\n\n\n\n(Intercept)\n0.312\n0.080\n3.910\n0.000\n\n\neducation\n0.167\n0.024\n6.891\n0.000\n\n\nincome\n0.007\n0.010\n0.671\n0.502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 4\n\n\n\n\n\n\nThe p-value for the coefficient on education is less than 0.05, while the p-value for income is 0.50.\nIf there was no relationship between education and participation (\\(H_0:\\beta_2=0\\)), it would be quite unlikely that we would observed a test statistic of 6.89 or larger.\nSimilarly, test statistics as larger or larger than 0.671 occurs quite frequently in a world where there is no relationship (\\(H_0:\\beta_3=0\\)) between income and participation.\nThus we reject the null hypothesis for education, but fail to reject the null hypothesis for income in this model."
  },
  {
    "objectID": "slides/10-slides.html#predicted-values",
    "href": "slides/10-slides.html#predicted-values",
    "title": "POLS 1600",
    "section": "Predicted values",
    "text": "Predicted values\n\nOverview Code Table FigComments\n\n\nLet’s explore whether income and education condition each other’s relationship with participation using the following interaction model\n\\[\ny = \\beta_0 +\\beta_1 \\text{educ} + \\beta_2 \\text{inc} + \\beta_3\\text{educ}\\times\\text{inc} + \\epsilon\n\\]\nTo help our interpretations we’ll produce plots of predicted values of participation, at varying levels of income and education.\n\n\n\n# Fit model\nm2 &lt;- lm_robust(dv_participation ~ education*income, df)\n\n\n# Regression Table\nm2_tab &lt;- htmlreg(\n  m2, \n  include.ci = F,\n  digits = 3,\n  stars = c(0.05, 0.10)\n                    )\n\n# Predicted values\n\n# Data frame of values we want to make predictions at\npred_df &lt;-expand_grid(\n  income = sort(unique(df$income)),\n  education = quantile(df$education, na.rm = T)[c(2,4)]\n)\n\n# Combine model predictions\npred_df &lt;- cbind(pred_df, predict(m2, pred_df,\n                                  interval = \"confidence\")$fit)\n\n# Plot predicted values\nfig_m2_pred &lt;- pred_df %&gt;% \n  mutate(\n    Education = ifelse(education == 2, \"High school\",\"College\")\n  ) %&gt;% \n  ggplot(aes(income, fit, group=Education))+\n  geom_ribbon(aes(ymin = lwr, ymax = upr,\n                  fill = Education),\n              alpha=.5)+\n  geom_line()+\n  theme_minimal()+\n  labs(y = \"Predicted Participation\",\n       x = \"Income\",\n       title = \"\")\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.060\n\n\n\n\n \n\n\n(0.151)\n\n\n\n\neducation\n\n\n0.242**\n\n\n\n\n \n\n\n(0.050)\n\n\n\n\nincome\n\n\n0.048**\n\n\n\n\n \n\n\n(0.024)\n\n\n\n\neducation:income\n\n\n-0.011*\n\n\n\n\n \n\n\n(0.006)\n\n\n\n\nR2\n\n\n0.042\n\n\n\n\nAdj. R2\n\n\n0.040\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.286\n\n\n\n\n\n\n**p &lt; 0.05; *p &lt; 0.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow income individuals with a college degree participate at significantly higher rates than individuals with a similar levels of income with only a high school diploma.\nAlternatively, we might say that the college educated tend to participate at similar levels, regardless of their level of income, while income has a marginally positive relationship with participation for those without college degrees.\n\n\n\n\n\n\nNote\n\n\nIs this a causal relationship? What assumptions would we need to make a causal claim about the effects of education on participation?"
  },
  {
    "objectID": "slides/10-slides.html#references",
    "href": "slides/10-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/11-slides.html#class-plan",
    "href": "slides/11-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nCourse Review\nStatistical Inference\nFinal Projects"
  },
  {
    "objectID": "slides/11-slides.html#annoucements",
    "href": "slides/11-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nDo we need to start taking attendance?\nLab 11/Assignment 3 this week\nNo tutorial this week\nNext Tuesday, April 23, Work on Presentations/Drafts\nAssignment 4 now due April 25.\nApril 30, Final Workshop\nMay 2 Class Presentations"
  },
  {
    "objectID": "slides/11-slides.html#pie-me",
    "href": "slides/11-slides.html#pie-me",
    "title": "POLS 1600",
    "section": "Pie me",
    "text": "Pie me"
  },
  {
    "objectID": "slides/11-slides.html#setup-packages-for-today",
    "href": "slides/11-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/11-slides.html#three-modes-of-inference",
    "href": "slides/11-slides.html#three-modes-of-inference",
    "title": "POLS 1600",
    "section": "Three Modes of Inference",
    "text": "Three Modes of Inference\n\nDescriptive\nCausal\nPredictive"
  },
  {
    "objectID": "slides/11-slides.html#descriptive-inference",
    "href": "slides/11-slides.html#descriptive-inference",
    "title": "POLS 1600",
    "section": "Descriptive Inference",
    "text": "Descriptive Inference\n\nSummarize distributions and relationships in data\n\n\nYou should know how to:\n\nCalculate and interpret measures:\nCentral Tendency\nDispersion\nAssociation\n\nLoad, look at, wrangle, and describe data using:\n\nTables\nFigures"
  },
  {
    "objectID": "slides/11-slides.html#data-wrangling",
    "href": "slides/11-slides.html#data-wrangling",
    "title": "POLS 1600",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe process of transforming data into a useable format\n\nYou should know how to:\n\nLoad, look at,and transform data into R\nGet a HLO of the raw data:\n\nUnit of analysis\nDimensions of the data\nQuickly summarize the distributions and values of variables\n\nRecode the data to:\n\nReplace values as NAs\nCreate categories, indicators (0,1), and factors\nTransform predictors (e.g. standardizing predictors)\n\nReshape the data\n\nPivoting columns and rows\nJoining data sets together.\n\nAggregate the data into summaries"
  },
  {
    "objectID": "slides/11-slides.html#data-visualization",
    "href": "slides/11-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nA tool for describing distributions and relationships\n\nYou should know:\n\nThe grammar of graphics:\n\nData\nAesthetic mappings\nGeometries\n\nHow to generate common plots to describe:\n\nDistributions\nRelationships"
  },
  {
    "objectID": "slides/11-slides.html#causal-inference",
    "href": "slides/11-slides.html#causal-inference",
    "title": "POLS 1600",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal Inference requires counterfactual comparisons\n\nYou should know:\n\nPotential outcomes and DAGs\nThe fundamental problem of causal inference\nBias caused by:\n\nConfounding (Coffee and Cancer)\nColliding (Dating Jerks)\n\nCasual Identification in:\n\nExperimental designs\nObservational designs"
  },
  {
    "objectID": "slides/11-slides.html#prediction-with-linear-models",
    "href": "slides/11-slides.html#prediction-with-linear-models",
    "title": "POLS 1600",
    "section": "Prediction with Linear Models",
    "text": "Prediction with Linear Models\n\nLinear regression provides a linear estimate of the conditional expectation function\n\nYou should know:\n\nHow linear regression works\nWhat it means to control for predictors in a multiple regression\nWhen and why we should control for predictors.\nHow to translate substantive claims into empirical expectations for our models\nHow to estimate and interpret these models using tables and figures\nHow to quantify uncertainty about these estimates using confidence intervals and hypothesis tests."
  },
  {
    "objectID": "slides/11-slides.html#probability",
    "href": "slides/11-slides.html#probability",
    "title": "POLS 1600",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event\nRandom variables assign numeric values to all the events that could occur.\nProbability distributions assign probabilities to every value of a random variable. Can be:\n\ndiscrete\ncontinuous\ncharacterized by their expected values and variances\nused to:\ndescribe the data generating process\nquantify uncertainty about estimates"
  },
  {
    "objectID": "slides/11-slides.html#sampling-distributions-and-standard-errors",
    "href": "slides/11-slides.html#sampling-distributions-and-standard-errors",
    "title": "POLS 1600",
    "section": "Sampling Distributions and Standard Errors",
    "text": "Sampling Distributions and Standard Errors\n\nA sampling distribution is a theoretical probability distribution of estimates obtained from taking repeated samples of size \\(n\\) from some population\n\nA distribution of what we could have seen\n\nA standard errors is simply the standard deviation (\\(\\sigma\\)) of the sampling distribution\n\nA measure of how much our estimate could have varied.\n\nLaw of Large Numbers: As \\(N \\to \\infty\\) \\(\\bar{x} \\to \\mu\\)\nCentral Limit Theorem: As \\(N \\to \\infty\\) \\(\\bar{x} \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals",
    "href": "slides/11-slides.html#confidence-intervals",
    "title": "POLS 1600",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nConfidence intervals provide a range of plausible values for our estimate\n\n\nThree components:\n\nPoint Estimate (i.e. a mean, or coefficient)\nConfidence Level (Often 95 percent by convention)\nMargin of Error (+/- some range (typically 2*SD for 95 percent CI))\n\nConfidence is about the interval\n\n95 percent of the intervals construct in this manner would contain the truth."
  },
  {
    "objectID": "slides/11-slides.html#hypothesis-testing",
    "href": "slides/11-slides.html#hypothesis-testing",
    "title": "POLS 1600",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nA hypothesis test quantifies how likely it is that we would observe what we did (our test statistic), if some claim about the world were true (our hypothesis, typically a null ).\nIf our claim were true, then under this null hypothesis, our test statistic would have a distribution centered around the truth.\nA p-value which describes the probability of observing a test statistic as extreme or more extreme in a world where our null hypothesis was true\n\nIf our p-value is small (\\(p &lt; 0.05\\)), we reject the null hypothesis\nIf our p-value is large (\\(p &gt; 0.05\\)), we fail to reject the null, or retain the null hypothesis"
  },
  {
    "objectID": "slides/11-slides.html#section",
    "href": "slides/11-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Relationship between CIs and Hypothsis Testing\n\nConcept Code Table Figure\n\n\nWe can think of a confidence interval as a range of hypotheses we would fail to reject with \\(p &lt; \\alpha\\)\n\n\n\n# Load Data\nload(url(\"https://pols1600.paultesta.org/files/data/nes24.rda\"))\n\n# Fit Model\nm1 &lt;- lm_robust(dv_participation ~   education + income, df,\n                se = \"classical\")\n\n# Range of hypotheses for education\npval_ci_df &lt;- tibble(\n  # Hypothesized Betas for Education\n  Hypothesis = seq(0, .32, length.out = 100),\n  # Test Statistics\n  Statistic = (m1$coefficients[\"education\"] - Hypothesis) /\n  m1$std.error[\"education\"],\n  # P-value for two sided test\n  `p-value` = 2*pt(abs(Statistic), df = m1$df,lower.tail = F)\n)\n\nfig_pval_ci &lt;- pval_ci_df %&gt;% \n  ggplot(aes(Hypothesis, `p-value`))+\n  geom_line()+\n  geom_vline(xintercept = m1$coefficients[\"education\"],\n             linetype = \"solid\",\n             col = \"red\")+\n  geom_vline(xintercept = m1$conf.low[\"education\"],\n             linetype = \"dotted\")+\n  geom_vline(xintercept = m1$conf.high[\"education\"],\n             linetype = \"dotted\")+\n  geom_hline(yintercept = 0.05,\n             linetype = \"dashed\")+\n  labs(\n    x = \"Hypothesized Education, Coefficent\",\n    title = \"Confidence intervals are a range\\nof plausible hypotheses\"\n  )+\n  theme_minimal()\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.31*\n\n\n\n\n \n\n\n[ 0.14; 0.48]\n\n\n\n\neducation\n\n\n0.17*\n\n\n\n\n \n\n\n[ 0.12; 0.21]\n\n\n\n\nincome\n\n\n0.01\n\n\n\n\n \n\n\n[-0.01; 0.03]\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1687\n\n\n\n\nRMSE\n\n\n1.29\n\n\n\n\n\n\n* 0 outside the confidence interval."
  },
  {
    "objectID": "slides/11-slides.html#four-possible-outcomes-of-a-hypothesis-test",
    "href": "slides/11-slides.html#four-possible-outcomes-of-a-hypothesis-test",
    "title": "POLS 1600",
    "section": "Four Possible Outcomes of a hypothesis Test",
    "text": "Four Possible Outcomes of a hypothesis Test\n\n\n\nFalse Positive: (Type I Error)\n\nRejecting a True \\(H_0\\).\n\\(\\tau = 0\\), but \\(\\hat{\\tau}\\) has a \\(p&lt;0.05\\)\nProbability=\\(\\alpha\\)\n\nTrue Positive: (Correct Decision)\n\nRejecting a false \\(H_0\\):\n\\(\\tau \\neq 0\\), and \\(\\hat{\\tau}\\) has a \\(p&lt;0.05\\)\nOccurs with Probability = \\(1-\\beta\\)\n\n\n\n\nTrue Negative: (Correct Decision) \n\nFailing to reject a True \\(H_0\\):\n\\(\\tau = 0\\), and \\(\\hat{\\tau}\\) has a \\(p&gt;0.05\\)\nOccurs with Probability = \\(1-\\alpha\\)\n\nFalse Negative: (Type II Error)\n\nFailing to reject a false \\(H_0\\).\n\\(\\tau \\neq 0\\) but \\(\\hat{\\tau}\\) has a \\(p&gt;0.05\\)\nOccurs with Probability= \\(\\beta\\)"
  },
  {
    "objectID": "slides/11-slides.html#type-1-and-2-errors",
    "href": "slides/11-slides.html#type-1-and-2-errors",
    "title": "POLS 1600",
    "section": "Type 1 and 2 Errors",
    "text": "Type 1 and 2 Errors\n\nSource"
  },
  {
    "objectID": "slides/11-slides.html#statistical-power",
    "href": "slides/11-slides.html#statistical-power",
    "title": "POLS 1600",
    "section": "Statistical Power",
    "text": "Statistical Power\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nConceptPowerComments\n\n\n\n\nConsider two distributions of statistics under\n\na null of no effect (\\(H_0\\))\nan effect of \\(\\tau\\) (\\(H_1\\))\n\nFor a significance threshold of \\(\\alpha\\) we would:\n\nFail to reject the null \\(\\beta\\) (Type II Errors)\nCorrectly reject the null \\(1 -\\beta\\) (Statistical Power)\n\n\n\n\n\nTry changing \\(\\tau\\) (the effect size), and se (the standard deviation of the effect)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPower is a function of:\n\nSample size (\\(N\\))\n\nLarger samples, smaller standard errors (LLN)\n\nEffect size (\\(\\tau\\))\n\nBigger effects less overlap\n\nSignificance threshold (\\(\\alpha\\))\n\nDecrease Type 1 (False Positives) error leads to increased Type 2 (False Negatives)\n\nThe distribution of the data\n\nVariance, asympotitc approximations"
  },
  {
    "objectID": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "href": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "title": "POLS 1600",
    "section": "Strucutre of Final Paper and Drafts",
    "text": "Strucutre of Final Paper and Drafts\nAssignment 4: Seven sections\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides.html#for-thursday",
    "href": "slides/11-slides.html#for-thursday",
    "title": "POLS 1600",
    "section": "For Thursday",
    "text": "For Thursday\n\nAssignment 3\nDownload template\nCreate shared google drive.\nMake progress on:\n\n\nData (20 percent ~ 4+ paragraphs)\n\n\nDesign (25 percent ~ 5+ paragraphs)\n\n\nResults (25 percent ~ 5+ paragraphs)"
  },
  {
    "objectID": "slides/11-slides.html#motivating-questions",
    "href": "slides/11-slides.html#motivating-questions",
    "title": "POLS 1600",
    "section": "Motivating Questions",
    "text": "Motivating Questions\nIn the reset of today’s class, we’ll get some practice putting together the various skills you need for your drafts by exploring the following:\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\nHave these perceptions about vaccines changed over time?"
  },
  {
    "objectID": "slides/11-slides.html#tasks",
    "href": "slides/11-slides.html#tasks",
    "title": "POLS 1600",
    "section": "Tasks:",
    "text": "Tasks:\nTo explore these questions, we need to\n\nGet setup to work\nLoad our data\nRecode our data\nSummarize our data\nSpecify our expectations\nEstimate models to test these expectations\nPresent and interpret results using\n\nTables\nFigures\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "slides/11-slides.html#new-packages",
    "href": "slides/11-slides.html#new-packages",
    "title": "POLS 1600",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# # Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anesr)"
  },
  {
    "objectID": "slides/11-slides.html#packages-for-today",
    "href": "slides/11-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"patchwork\",\n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr     patchwork \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       GGally        scales       dagitty         ggdag       ggforce \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/11-slides.html#data",
    "href": "slides/11-slides.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nNow that we have anesr installed, let’s load data from the 2016 and 2020 National Election Studies:\n\n# Load data\ndata(timeseries_2016, package = \"anesr\")\ndata(timeseries_2020, package = \"anesr\")\n\nAnd copy those data frames into new dataframes with shorter names\n\n# Rename datasets\nnes16 &lt;- timeseries_2016\nnes20 &lt;- timeseries_2020"
  },
  {
    "objectID": "slides/11-slides.html#finding-variables-outcomes",
    "href": "slides/11-slides.html#finding-variables-outcomes",
    "title": "POLS 1600",
    "section": "Finding variables: Outcomes",
    "text": "Finding variables: Outcomes\nOur primary outcome of interest are beliefs about vaccines.\nVariables V162162x in the 2016 NES and V202383x in the 2020 NES will serve as our primary outcome of interest, summarizing respondents answer to the following question:\n\nDo the health benefits of vaccinations generally outweigh the risks, do the risks outweigh the benefits, or is there no difference?"
  },
  {
    "objectID": "slides/11-slides.html#finding-variables-predictors",
    "href": "slides/11-slides.html#finding-variables-predictors",
    "title": "POLS 1600",
    "section": "Finding variables: Predictors",
    "text": "Finding variables: Predictors\nSimilarly, V161158x in the 2016 NES and V201231x in the 2020 NES will serve our key predictor (respondent’s partisanship).\nFinally, we’ll control respondents’ age, using V161267 in the 2016 NES and V201507x in the 2020 NES"
  },
  {
    "objectID": "slides/11-slides.html#examine-distributions-vaccine-beliefs",
    "href": "slides/11-slides.html#examine-distributions-vaccine-beliefs",
    "title": "POLS 1600",
    "section": "Examine Distributions: Vaccine Beliefs",
    "text": "Examine Distributions: Vaccine Beliefs\nThe variables in the NES datasets are of a class labelled which allows numeric values to have substantive labels\n\nclass(nes16$V162162x)\n\n[1] \"haven_labelled\"\n\n\nOur outcome variable has the following labels:\n\nlabelled::val_labels(nes16$V162162x)\n\n                                   -9. Refused \n                                            -9 \n                                -8. Don't know \n                                            -8 \n-7. No post data, deleted due to incomplete IW \n                                            -7 \n                -6. No post-election interview \n                                            -6 \n                      1. Benefits much greater \n                                             1 \n                2. Benefits moderately greater \n                                             2 \n                  3. Benefits slightly greater \n                                             3 \n                              4. No difference \n                                             4 \n                     5. Risks slightly greater \n                                             5 \n                   6. Risks moderately greater \n                                             6 \n                         7. Risks much greater \n                                             7 \n\n\nAnd distribution of responses:\n\ntable(nes16$V162162x)\n\n\n  -9   -8   -7   -6    1    2    3    4    5    6    7 \n  21   28   86  536 1687  726  258  539   96  211   82"
  },
  {
    "objectID": "slides/11-slides.html#recoding-outcome-variables",
    "href": "slides/11-slides.html#recoding-outcome-variables",
    "title": "POLS 1600",
    "section": "Recoding outcome variables",
    "text": "Recoding outcome variables\n\nTasks Code\n\n\nWhat transformations do we need to make to V162162x in nes16 and V202383x in nes20 so that these variables are suitable for analysis?\n\nRecode negative values to be NA\nReverse code so that higher values indicate greater belief vaccines benefits\nCreate an indicator of people who are vaccine skeptics\n\n\n\n\nnes16 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V162162x &lt; 0, NA, (V162162x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes16 # Save recodes to nes16\n\nnes20 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V202383x &lt; 0, NA, (V202383x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes20 # Save recodes to nes20"
  },
  {
    "objectID": "slides/11-slides.html#recoding-predictors",
    "href": "slides/11-slides.html#recoding-predictors",
    "title": "POLS 1600",
    "section": "Recoding Predictors",
    "text": "Recoding Predictors\n\nTasks Code\n\n\n\nNow we repeat this process for our key predictor, partisanship.\n\nRecode partisanship variables V161158x in nes16 and V201231x in nes20\nCreate indicators from this recoded variable that classify partisanship as categorical variable (with Democrats as the reference category)\n\nAnd our covariate, age variables V161267 in nes16 and V201507x in nes20\n\nRecode negative values to be NA\n\n\n\n\n\nnes16 %&gt;%\n  mutate(\n    pid = ifelse(V161158x &lt; 0, NA, V161158x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V161267 &lt; 0, NA, V161267)\n  ) -&gt; nes16\n\n## Recoding Partisanship (V201231x) in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    pid = ifelse(V201231x &lt; 0, NA, V201231x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V201507x &lt; 0, NA, V201507x)\n  ) -&gt; nes20"
  },
  {
    "objectID": "slides/11-slides.html#progress-report",
    "href": "slides/11-slides.html#progress-report",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data📥\nSpecify our expectations\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "slides/11-slides.html#descriptive-statistics-2016",
    "href": "slides/11-slides.html#descriptive-statistics-2016",
    "title": "POLS 1600",
    "section": "Descriptive statistics (2016)",
    "text": "Descriptive statistics (2016)\n\nTasks Code Table\n\n\n\nCreate the_vars\nSelect these variables\nPivot the data\nCalculate summary statistics\nFormat as an html table\n\n\n\n\n# 1. Create a object with the names of the variables you want to summarize\nthe_vars &lt;- c(\"vaccine_skeptic01\",\"pid\",\"age\")\n# 2. Select these variables\nnes16 %&gt;%\n  select(all_of(the_vars)) %&gt;%\n# 3. Pivot the data\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  # 3. Calculate summary statistics\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) -&gt; sum_df \n\nsum_tab &lt;- \nknitr::kable(sum_df,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =2) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 3, end_row =3)\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\nVariable\nmin\np25\nMedian\nmean\np75\nmax\nmissing\n\n\n\n\nOutcome\n\n\nvaccine_skeptic01\n0\n0\n0\n0.26\n0\n1\n671\n\n\nKey Predictors\n\n\npid\n1\n2\n4\n3.86\n2\n7\n23\n\n\nCovariates\n\n\nage\n18\n34\n50\n49.58\n34\n90\n121"
  },
  {
    "objectID": "slides/11-slides.html#progress-report-1",
    "href": "slides/11-slides.html#progress-report-1",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data ✅\nSpecify our expectations 📥\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations",
    "href": "slides/11-slides.html#specificying-expecations",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\n\nConsider our first two motivating questions\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\n\nAnd some illustrative stereotypes:\n\n“Republicans are anti-science”\n“Liberal always fall for Goopy pseudo-science”\n“Independents love to do their own research”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-1",
    "href": "slides/11-slides.html#specificying-expecations-1",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\n\nSimilarly, consider our third question:\n\nHave these perceptions about vaccines changed over time?\n\nAnd some similar simplified claims:\n\n“The Covid-19 vaccine is a miracle of modern science”\n“Social media is rife with misinformation about the Covid-19 vaccine”\n“Politicians are politicizing vaccine politics for political benefits”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-2",
    "href": "slides/11-slides.html#specificying-expecations-2",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nOur goal is to take claims/conventional wisdom/theories, and derive their empirical implications:\n\nH1: Partisan Differences in Vaccine Skepticism\n\nH1a: Republicans will be the most skeptical of vaccines\nH1b: Democrats will be the most skeptical of vaccines\nH1c: Independents will be the most skeptical of vaccines"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-3",
    "href": "slides/11-slides.html#specificying-expecations-3",
    "title": "POLS 1600",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\n\nH2: Temporal Differences in Vaccine Skepticism\n\nH2a: Vaccine skepticism will decrease from 2016 to 2020 with the widespread roll out of the Covid-19 vaccine\nH2b: Vaccine skepticism will increase from 2016 to 2020 with increased amounts of misinformation about the Covid-19 vaccine\n\nH3: Partisan Difference in Vaccine Skepticism Over Time Partisan differences in Vaccine Skepticism will increase from 2016 to 2020 with the politicization of Covid-19 policies"
  },
  {
    "objectID": "slides/11-slides.html#motivating-your-expectations",
    "href": "slides/11-slides.html#motivating-your-expectations",
    "title": "POLS 1600",
    "section": "Motivating your expectations",
    "text": "Motivating your expectations\nIn your final papers, unlike in these slides, your expectations should be grounded in existing theory, research, and evidence. For the present question, we might cite sources such as:\n\nEnders, Adam M., and Steven M. Smallpage. “Informational cues, partisan-motivated reasoning, and the manipulation of conspiracy beliefs.” Political Communication 36.1 (2019): 83-102.\nStecula, Dominik A., and Mark Pickup. “How populism and conservative media fuel conspiracy beliefs about COVID-19 and what it means for COVID-19 behaviors.” Research & Politics 8.1 (2021): 2053168021993979.\nJennings, Will, et al. “Lack of trust, conspiracy beliefs, and social media use predict COVID-19 vaccine hesitancy.” Vaccines 9.6 (2021): 593.\nHollander, Barry A. “Partisanship, individual differences, and news media exposure as predictors of conspiracy beliefs.” Journalism & Mass Communication Quarterly 95.3 (2018): 691-713."
  },
  {
    "objectID": "slides/11-slides.html#model-specification",
    "href": "slides/11-slides.html#model-specification",
    "title": "POLS 1600",
    "section": "Model Specification",
    "text": "Model Specification\nTranslate these expectations into empirical models requires choices about how to specify our models\n\nHow should we measure/operationalize our outcome\n\nShould we measure beliefs about vaccines with 7-point ordinal scale or as a binary indicator of vaccine skepticism\n\nHow should we measure/operationalize our key predictor(s)\n\nShould we measure partisanship using a 7 point scale or as categorical variable?\n\nWhat should we control for in our model?\n\nFactors likely to predict both our outcome and our key predictor of interest\n\nThere are rarely definitive answers to these questions. Instead, we will often estimate multiple models to try and show that our findings are robust to alternative specifications"
  },
  {
    "objectID": "slides/11-slides.html#model-specification-1",
    "href": "slides/11-slides.html#model-specification-1",
    "title": "POLS 1600",
    "section": "Model Specification",
    "text": "Model Specification\nFor your projects, every group will almost surely estimate some form of the following:\n\nBaseline bivariate model: The simplest test of the relationship between your outcome and key predictor\nMultiple regression model: A test of the robustness of this relationship, controlling for alternative explanations"
  },
  {
    "objectID": "slides/11-slides.html#model-specification-2",
    "href": "slides/11-slides.html#model-specification-2",
    "title": "POLS 1600",
    "section": "Model Specification",
    "text": "Model Specification\nIn practice, I suspect you may estimate multiple regression models such as:\n\nAlternative specifications/operationalizations of outcomes and predictors\nInteraction models to test conditional relationships\nPolynomial models to test non-linear relationships"
  },
  {
    "objectID": "slides/11-slides.html#section-1",
    "href": "slides/11-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Translating Theoretical Claims into Empirical Expectations\nBefore we estimate our models in R, we will write down our models formally and empirical implications of our theoretical expectations in terms of the coefficients of our model.\nFor example, our baseline model might be:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} + X\\beta + \\epsilon\\]\nIf \\(\\beta_1\\) is positive this is consistent with H1a (greater skepticism among Republicans), - If \\(\\beta_2\\) is negative this is consistent with H1b (greater skepticism among Democrats),\n\nBut how could we test H1c – greater skepticism among Independents, who are “4s” on \\(\\text{PID}_{7pt}\\)?"
  },
  {
    "objectID": "slides/11-slides.html#section-2",
    "href": "slides/11-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Translating Theoretical Claims into Empirical Expectations\nWe could fit a polynomial regression, including both partisanship and “partissanship squared” to allow the relationship between partisanship and vaccine skepticism to vary non-linearly\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} +  \\beta_2 \\text{PID}_{7pt}^2+ X\\beta+ \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides.html#section-3",
    "href": "slides/11-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Translating Theoretical Claims into Empirical Expectations\nOr we could estimate a model treating Partisanship as a categorical variable rather than an ordinal interval variable.\nIn our recoding, we set \"Democrat\" to be the first level of the variable pid3cat, so the model R will estimate by default is:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{Ind} +  \\beta_2 \\text{PID}_{Rep}+ X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides.html#testing-differences-over-time",
    "href": "slides/11-slides.html#testing-differences-over-time",
    "title": "POLS 1600",
    "section": "Testing differences over time",
    "text": "Testing differences over time\nTesting Hypotheses 2 and 3 involve making comparisons across models estimated on data from different surveys.\nFormally, testing these expectations is a little more complicated\n\nwe could pool our two surveys together include an interaction term for survey year\n\nFor our purposes, we’ll treat these as more qualitative/exploratory hypotheses:\n\nH2a/b implies overall rates of vaccine skepticism will be lower/higher in 2020 compared to 2016\nH3 implies that whatever partisan differences we find in 2016 should be larger in 2020."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-2",
    "href": "slides/11-slides.html#progress-report-2",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations 📥\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models",
    "href": "slides/11-slides.html#estimating-empirical-models",
    "title": "POLS 1600",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nHaving derived empirical implications of our theoretical expectations expressed in terms of linear regressions, now we simply have to estimate our models in R.\nWhen estimating the same model on different datasets we can write the formulas once\n\nf1 &lt;- formula(vaccine_skeptic01 ~ pid + age)\nf2 &lt;- formula(vaccine_skeptic01 ~ pid + I(pid^2) + age)\nf3 &lt;- formula(vaccine_skeptic01 ~ pid3cat + age)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-1",
    "href": "slides/11-slides.html#estimating-empirical-models-1",
    "title": "POLS 1600",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nAnd then pass it to lm() with different data arguments:\n\nm1_2016 &lt;- lm(formula = f1, data = nes16)\nm1_2020 &lt;- lm(formula = f1, data = nes20)\nm2_2016 &lt;- lm(formula = f2, data = nes16)\nm2_2020 &lt;- lm(formula = f2, data = nes20)\nm3_2016 &lt;- lm(formula = f3, data = nes16)\nm3_2020 &lt;- lm(formula = f3, data = nes20)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-2",
    "href": "slides/11-slides.html#estimating-empirical-models-2",
    "title": "POLS 1600",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\n\nIf you’ve:\n\ncoded your data correctly\ndeveloped clear testable implications from your theoretical expectations\n\nSpecifying and estimating empirical models is straightforward. Literally a few lines of code."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-3",
    "href": "slides/11-slides.html#progress-report-3",
    "title": "POLS 1600",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations ✅\nPresent our results 📥\n\nTables\nFigures\nConfidence intervals\nHypothesis testing"
  },
  {
    "objectID": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "href": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "title": "POLS 1600",
    "section": "Presenting and Interpreting Your Results",
    "text": "Presenting and Interpreting Your Results\nPresenting and interpreting your results is requires both art and science.\nYour goal is to tell a story with your results,\nLet’s start by producing a regression table, which provides a concise summary of multiple regression models."
  },
  {
    "objectID": "slides/11-slides.html#regression-tables",
    "href": "slides/11-slides.html#regression-tables",
    "title": "POLS 1600",
    "section": "Regression Tables",
    "text": "Regression Tables\n\nTasks Code Basic Fetch\n\n\n\n\nGiving the variables in substantive names\nReporting coefficients to 3 decimal places\nUsing a single significance threshold of \\(p &lt; 0.05\\)\nGiving the models custom names\nAdding a header to group models by year\nChanging the caption of the table\n\n\n\n\n\n# Basic\ntab_basic &lt;- texreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020)\n)\n\n# Formatted\ntab_fetch &lt;- texreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020),\n  # Reporting coefficients to 3 decimal places\n  digits = 3,\n  # Using a single significance threshold \n  stars = 0.05,\n  # Giving the variables in substantive names\n  custom.coef.names = c(\n    \"(Intercept)\",\n    \"PID (7pt)\",\n    \"Age\",\n    \"PID&lt;sup&gt;2&lt;/sup&gt; (7pt)\",\n    \"Independent\",\n    \"Republican\"\n  ),\n  # Use SE instead o CIs\n  include.ci = F,\n  # Giving the models custom names\n  custom.model.names = paste(\"(\",c(1:6),\")\", sep=\"\"),\n  # Adding a header to group models by year\n  custom.header = list(\"NES 2016\" = 1:3, \"NES 2020\" = 4:6),\n  # Changing the caption of the table\n  caption = \"Partisanship and Vaccine Skepticism\"\n)\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.46***\n\n\n0.35***\n\n\n0.42***\n\n\n0.34***\n\n\n0.32***\n\n\n0.35***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\npid\n\n\n-0.00\n\n\n0.06***\n\n\n \n\n\n0.02***\n\n\n0.04***\n\n\n \n\n\n\n\n \n\n\n(0.00)\n\n\n(0.02)\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n \n\n\n\n\nage\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\npid^2\n\n\n \n\n\n-0.01***\n\n\n \n\n\n \n\n\n-0.00\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n\n\npid3catIndependent\n\n\n \n\n\n \n\n\n0.17***\n\n\n \n\n\n \n\n\n0.20***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\npid3catRepublican\n\n\n \n\n\n \n\n\n-0.02\n\n\n \n\n\n \n\n\n0.10***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#telling-a-story-with-regression",
    "href": "slides/11-slides.html#telling-a-story-with-regression",
    "title": "POLS 1600",
    "section": "Telling a Story with Regression",
    "text": "Telling a Story with Regression\n\nFirst, provide an overview the models presented in the table\n\nExplain what each model is doing conceptually\n\nThen start with your simplest model (first column)\n\nUse this as a chance to explain core concepts from the course\n\nWhat is regression\nHow should I interpret a coefficient substantively\nHow should I interepret the statistical signficance of a give coefficient\n\nAs you move from left to right (simple to more complex)\n\nyou need not interpret every single coefficient in the model\ninstead highlight the factors that are important for the reader to note (e.g. a comparison between one coefficient in model or another.)"
  },
  {
    "objectID": "slides/11-slides.html#example",
    "href": "slides/11-slides.html#example",
    "title": "POLS 1600",
    "section": "Example",
    "text": "Example\nTable 1 presents the results of three specifications exploring the relationship between partisanship and vaccine skepticism using data from the 2016 (Models 1-3) and 2020 (Models 4-5) National Election Studies.\nModels 1 and 4 operationalize partisanship as a 7-point scale, where 1 corresponds to Strong Democrats, 4 to Indepndents, and 7 to Strong Republicans in the 2016 (Model 1) and 2020 (Model 2) surveys.\nModels 2 and 5 allow the relationship between partisanship and vaccine skepticism to vary non-linear again for the 2016 (Model 2) and 2020 (Model 5) elections.\nModels 3 and 6 treat partisanship as categorical variable, describing how Independents and Republicans differ from Democrats, the reference category in these models.\nAll models control age, since (put in substantive justification for controlling for age here)"
  },
  {
    "objectID": "slides/11-slides.html#story-testing-for-partisan-differences",
    "href": "slides/11-slides.html#story-testing-for-partisan-differences",
    "title": "POLS 1600",
    "section": "Story: Testing for Partisan Differences",
    "text": "Story: Testing for Partisan Differences\n\nThe results from Model 1 provide little initial evidence for partisan differences in vaccine skepticism in the 2016 Election.\n\nThe coefficient on the partisanship variable is -0.005, suggesting that a unit increase in partisanship (going from being a Strong Democrat to just a Democrat, or an Independent to an independent who leans Republican), is associated with just a 0.5 percentage point increase in the probability of being a vaccine skeptic (believing that the risks of vaccination outweigh the benefits or that their is no difference in the risks versus benefits).\n\nFurthermore the 95-percent confidence interval for this estimate (-0.011, 0.002) brackets 0, suggesting the true population estimate from this model could be either positive or negative. Similarly, we fail to reject the null hypothesis that the true coefficient on partisanship in this model is 0 as the test statistic for this estimate ( -1.38) corresponds to a p-value of 0.168 suggesting that we would see test statistics this large or larger fairly often when the true relationship was 0.\nIn sum, the results from Model 1 provide little support for any of the expectations described by H1"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "href": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "title": "POLS 1600",
    "section": "Testing for Partisan Differences: Model 2",
    "text": "Testing for Partisan Differences: Model 2\n\nWhile coefficients from Model 1 suggest little evidence of partisan differences in vaccine skepticism, the coefficients on both partisanship, and partisanship squared are statistically significant (p &lt; 0.05)."
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2",
    "href": "slides/11-slides.html#interpreting-model-2",
    "title": "POLS 1600",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nTask FigureInterpretation\n\n\n\nThe coefficients from polynomial regressions can be difficult to interpret jointly and so Figure 1 presents the predicted values from Model 2, holding age constant at its sample mean.\n\n\n\n\npred_df_m2 &lt;- expand_grid(\n  pid = 1:7,\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m2 &lt;- cbind(pred_df_m2, predict(m2_2016,pred_df_m2, interval =\"confidence\"))\n\nfig_m2 &lt;- pred_df_m2 %&gt;%\n  ggplot(aes(pid, fit, ymin =lwr, ymax =upr))+\n  geom_line()+\n  geom_ribbon(alpha=.2, fill=\"grey\")+\n  theme_bw()+\n  labs(x = \"Partisanship\",\n       y = \"Predicted Vaccine Skepticism\",\n       title = \"Independents are the most skeptical of vaccines\",\n       subtitle = \"Data: 2016 NES\"\n       )\n\n\n\n\nfig_m2\n\n\n\n\n\n\n\n\n\n\nWe see from Model 2 that 29.7 percent [27.3%, 32.1%] of Independents in the 2016 NES were predicted to be vaccine skeptics compared to 23.7 percent [20.8%, 26.5%] of Strong Democrats and only 20.1 percent [16.9%, 23.3%] of Strong Republicans."
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-3",
    "href": "slides/11-slides.html#interpreting-model-3",
    "title": "POLS 1600",
    "section": "Interpreting Model 3",
    "text": "Interpreting Model 3\n\nInterpretation Code\n\n\nModel 3 tells a similar story to model 2. Again, adjusting for differences in vaccine skepticism explained by age, Model 3 predicts that 41.7 percent [37.7%, 45.6%] of Independents in the 2016 NES are vaccine skeptics compared to 24.2 percent [22.1%, 26.2%] of Democrats, and 22.6 percent [20.4%, 24.8%] of Republicans.\nNote the coefficients from Model 3 imply that the differences between Independents and Democrats are statistically significant (\\(\\beta_{Ind} = 0.175, p &lt; 0.05\\)), the differences between Republicans and Democrats are not (\\(\\beta_{Rep} = -0.004, p = 0.31\\))\n\n\n\npred_df_m3 &lt;- expand_grid(\n  pid3cat = c(\"Democrat\", \"Independent\",\"Republican\"),\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m3 &lt;- cbind(pred_df_m3, predict(m3_2016,pred_df_m3, interval =\"confidence\"))\npred_df_m3\n\n      pid3cat      age       fit       lwr       upr\n1    Democrat 49.58231 0.2419547 0.2211228 0.2627867\n2 Independent 49.58231 0.4169043 0.3773539 0.4564547\n3  Republican 49.58231 0.2261496 0.2038046 0.2484947"
  },
  {
    "objectID": "slides/11-slides.html#section-5",
    "href": "slides/11-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "tab_fetch\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-differences-over-time",
    "href": "slides/11-slides.html#testing-for-differences-over-time",
    "title": "POLS 1600",
    "section": "Testing for Differences Over Time",
    "text": "Testing for Differences Over Time\nThe results for the 2016 NES suggest political independents are most skeptical of vaccines.\nThe results for 2020 suggest the relationship between partisanship and vaccine skepticism has changed overtime.\n\nThe coefficient on partisanship in model 4 is now positive and statistically significant (p &lt; 0.05), suggesting that as respondents become more Republican, they are more likely to be skeptical of vaccines\nThe coefficients from Model 5 suggest the relationship between partisanship skepticism is non linear, which is confirmed by model 6.\nIn Model 6, we see that independents remain the most skeptical of vaccines in 2020 \\((\\beta = 0.20,\\, p &lt;0.05)\\), but that Republicans now tend to be more skeptical of vaccines than Democrats \\((\\beta = 0.10,\\, p &lt;0.05)\\)\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "assignments/a4.html",
    "href": "assignments/a4.html",
    "title": "A4: Project Drafts",
    "section": "",
    "text": "You final will paper consists of seven sections. There is no minimum (or maximum) length for the final paper, but I’ve included rough estimates in parentheses of how long I think each section should be along with the relative contribution of each section to your final grade.\nSpecific guidance, expectations, and grading criteria for each section are provided below.\nYou need not have every section completely finished in this draft, but the more you have done the more feedback you will get. Remember, the data, design, and results section make up 70 percent of your grade, so while a good introduction, compelling theory, and thoughtful conclusion are important, you should budget your time accordingly.\nYour actual grade for the draft will reflect a general assessment of the status of your project\nAdditionally, I will assign points to each section as if it were you final paper to give you a sense of where to focus your efforts. This is not your actual grade for this assignment, but can be though of as a rough estimate of the grade you would receive on a section if you were to submit it, as is, for your final paper. Your grade for that section on the final paper may increase or decrease depending on the changes you make to your draft. Additionally, I will assign points to each section as if it were you final paper to give you a sense of where to focus your efforts. This is not your actual grade for this assignment, but can be though of as a rough estimate of the grade you would receive on a section if you were to submit it, as is, for your final paper. Your grade for that section on the final paper may increase or decrease depending on the changes you make to your draft.\nI have also posted a  template for the final paper and an example of a final paper from a previous version of POLS 1600.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-5-points",
    "href": "assignments/a4.html#grading-criteria-5-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (5 points)",
    "text": "Grading Criteria (5 points)\n\n5 points\n\nArticulates clear and compelling research question\nOffers a strong motivation for pursuing this question tied to existing social science theory\nDescribes a convincing empirical strategy with appropriate data and research design\nPreviews interesting results with substantive interpretation\nMakes reader excited to read the rest of the paper\nWriting throughout is clear, precise, and engaging, with correct spelling and grammar and consistent citations\n\n4 points\n\nArticulates research question\nLink between research question and social science theory could be more developed\nDescribes empirical strategy with data and research design\nPreview of results with little to no interpretation\nLeaves reader moderately interested in the rest of the paper.\nWriting throughout is clear and precise, with correct spelling and grammar and consistent citations\n\n&lt; 4 points\n\nResearch question unclear or absent\nLittle to no discussion of why this research question is of interest to social scientists and citizens\nLittle to no discussion of the data and research design used to pursue this question.\nNo discussion of empirical results\nDoes little to pique reader’s interest in the rest of the paper.\nWriting throughout is muddled and vague, with uncorrekt spellung and grammur and consistent citations",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-10-points",
    "href": "assignments/a4.html#grading-criteria-10-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (10 points)",
    "text": "Grading Criteria (10 points)\n\n9-10 points:\n\nCareful discussion of relevant theoretical, empirical, and/or substantive contexts that motivates research question.\nCites at least three relevant pieces of academic literature on research topic. Uses this literature to articulate theoretical framework for thinking about research question.\nTheoretical framework clearly defines:\nthe outcome (or outcomes) of interest\nthe factors expected to explain variation in that outcome\nsome potential alternative explanations/omitted variables, that might explain variation in both the outcome and predictors\nDevelops a clear set of expectations informed by theoretical framework for how these explanatory factors should relate to the outcome (and possibly each other)\n\n8 points:\n\nBrief discussion of theoretical, empirical, and/or substantive contexts that motivates research question.\nCites less than three relevant pieces of academic literature on research topic. Uses this literature to articulate theoretical framework for thinking about research question.\nTheoretical framework broadly defines:\nthe outcome (or outcomes) of interest\nthe factors expected to explain variation in that outcome\nsome potential alternative explanations/omitted variables, that might explain variation in both the outcome and predictors\nDevelops a set of expectations for how these explanatory factors should relate to the outcome (and possibly each other, but connection to larger theoretical framework is unclear\n\n&lt;8 points:\n\nLittle to no discussion of theoretical, empirical, and/or substantive contexts that motivates research question.\nNo citation of relevant pieces of academic literature on research topic. Broader theoretical framework absent.\nFails to clearly define:\nthe outcome (or outcomes) of interest\nthe factors expected to explain variation in that outcome\nsome potential alternative explanations/omitted variables, that might explain variation in both the outcome and predictors\nFails to articulate a clear set of expectations for how these explanatory factors should relate to the outcome",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-20-points",
    "href": "assignments/a4.html#grading-criteria-20-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (20 points)",
    "text": "Grading Criteria (20 points)\n\n18-20 points:\n\nData sources are clear and appropriate\nUnit of analysis is clear and appropriate\nOutcomes, key predictors, and covariates are clearly defined and carefully described\nDescriptive tables and figures are thoroughly discussed in text with correct interpretations\nData appear to be clean and tidy\n\n16-17 points:\n\nData sources are clear and appropriate\nUnit of analysis is unclear\nOutcomes, key predictors, and covariates are briefly defined and described\nDescriptive tables and figures are briefly discussed in text with mostly correct interpretations\nData appear to be clean and tidy\n\n&lt; 16 points:\n\nData sources are not identified\nUnit of analysis is is not discussed or incorrect\nOutcomes, key predictors, and covariates are not defined and described\nDescriptive tables and figures absent, not discussed or interpreted incorrectly\nClear errors in coding (e.g. failing to recode values as NA)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-30-points",
    "href": "assignments/a4.html#grading-criteria-30-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (30 points)",
    "text": "Grading Criteria (30 points)\n\n23-25 points:\n\nCorrectly specifies multiple models to test empirical expectations\nExpresses empirical expectations in terms of coefficients from linear models\nExplains motivation and interpretation of specified models\nConveys a clear understanding of\n\nIdentifying assumptions of empirical design\nWhat linear regression is\nWhat it means to control for variables\nHow to interpret coefficients from regression\nHow to quantify uncertainty around regression estimates, using both confidence intervals and hypothesis tests\n\n\n20-22 points:\n\nCorrectly specifies two models to test empirical expectations\nExpresses empirical expectations in terms of coefficients from linear models\nExplains motivation and interpretation of specified models\nConveys some understanding of\n\nIdentifying assumptions of empirical design\nWhat linear regression is\nWhat it means to control for variables\nHow to interpret coefficients from regression\nHow to quantify uncertainty around regression estimates, using both confidence intervals and hypothesis tests\n\n\n&lt; 20 points:\n\nIncorrectly specifies one models to test empirical expectations\nDoes not express empirical expectations in terms of coefficients from linear models\nLittle to no discussion of the motivation and interpretation of specified models\nConveys little to no understanding of\n\nIdentifying assumptions of empirical design\nWhat linear regression is\nWhat it means to control for variables\nHow to interpret coefficients from regression\nHow to quantify uncertainty around regression estimates, using both confidence intervals and hypothesis tests",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-25-points",
    "href": "assignments/a4.html#grading-criteria-25-points",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (25 points)",
    "text": "Grading Criteria (25 points)\n\n23-25 points:\n\nEmpirical results are\n\nclearly presented in tables and figures appropriate variable names and labels\ncorrectly interpreted in terms of substantive and statistical significance\nused to evaluate theoretical expectations\ninterpreted to demonstrate mastery of the concepts of\n\nbivariate and multiple regression\nconfidence intervals\nhypothesis testing\n\ntell a coherent and compelling story that provides insights into research question\n\n\n20-22 points:\n\nEmpirical results are\n\nroughly presented in tables and figures with awkward variable names and labels\ninterpreted mostly correctly in terms of substantive and statistical significance\nused to evaluate theoretical expectations\ninterpreted to demonstrate general understanding of the concepts of\n\nbivariate and multiple regression\nconfidence intervals\nhypothesis testing\n\ntell a coherent story that provides partial insights into research question\n\n\n&lt; 20 points:\n\nEmpirical results are\n\npoorly presented in tables and figures with awkward variable names and labels\ninterpreted mostly incorrectly in terms of substantive and statistical significance\nare not used to evaluate theoretical expectations\ninterpreted to demonstrate little to no understanding of the concepts of\n\nbivariate and multiple regression\nconfidence intervals\nhypothesis testing\n\nfail to tell a coherent story that provides little to no insights into research question",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-25-points-1",
    "href": "assignments/a4.html#grading-criteria-25-points-1",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (25 points)",
    "text": "Grading Criteria (25 points)\n\n5 points\n\nClear summary of the project’s findings\nDiscussion of the strengths and weaknesses of the project\nInteresting suggestions for further research that seem likely to add additional insights and/or address limitations of the present study.\nCorrectly formatted references/works cited\n\n4 points\n\nCursory summary of the project’s findings\nDiscussion of the strengths and weaknesses of the project\nSome suggestions for further research informed by the present project\nCorrectly formatted references/works cited\n\n&lt; 4 points\n\nLittle to no summary of the project’s findings\nLittle to no discussion of the strengths and weaknesses of the project\nNo suggestions for further research informed by the present project\nNo references/works cited",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a4.html#grading-criteria-25-points-2",
    "href": "assignments/a4.html#grading-criteria-25-points-2",
    "title": "A4: Project Drafts",
    "section": "Grading Criteria (25 points)",
    "text": "Grading Criteria (25 points)\n\n10 points\n\nCodebook is\n\neasy to read and informative\ndescribes all the variables used in the analysis.\n\nCode appendix is\n\nnicely formatted\nwell-commented\nable to to reproduces all the analysis in text without errors\n\nNo R code, raw output, error messages appears in the main text of the paper.\n\n8-9 points\n\nCodebook is\n\ndescribes all of the variables used in the analysis.\n\nCode appendix:\n\nCould be more cleanly formatted and needs more comments\nable to reproduces all the analysis in text without errors or only minor errors\n\nNo R code, raw output, error messages appears in the main text of the paper.\n\n&lt; 8 points\n\nCodebook is absent, incomplete, and/or incorrect\nCode appendix is:\n\nPoorly formatted\nContains no explanatory comments\nunable to reproduce most of the analysis in the text.\nContains clear errors\n\nR code, raw R output, error messages appear throughout the main text of the paper.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/POLS1600_paper_template.html",
    "href": "assignments/POLS1600_paper_template.html",
    "title": "POLS 1600: Final Paper Template",
    "section": "",
    "text": "This document provides a template for the structure of your final paper for POLS 1600.\nReplace this markdown text with the introduction to your paper that:\n\nClearly articulates your group’s research question\nLays out the theoretical framework that motivates your inquiry\nDescribes your empirical strategy\nProvides an outline of the rest of the paper and previews your results."
  },
  {
    "objectID": "assignments/POLS1600_paper_template.html#references",
    "href": "assignments/POLS1600_paper_template.html#references",
    "title": "POLS 1600: Final Paper Template",
    "section": "6.1 References",
    "text": "6.1 References\nI’d use a bulleted list to format your references. Aim for at least three, academic citations.\n\nCitrin, J. (1974). Comment: The political relevance of trust in government. American Political Science Review, 68(3), 973-988.\nLerman, Amy E., and Vesla M. Weaver. (2014) Arresting Citizenship. University of Chicago Press, 2014.\nJeong, J., & Han, S. (2020). Trust in police as an influencing factor on trust in government: 2SLS analysis using perception of safety. Policing: An International Journal."
  },
  {
    "objectID": "assignments/POLS1600_paper_template.html#code-book",
    "href": "assignments/POLS1600_paper_template.html#code-book",
    "title": "POLS 1600: Final Paper Template",
    "section": "7.1 Code book",
    "text": "7.1 Code book\nYour code book should describe be organized conceptually by variable type:\n\nOutcome\nKey Predictors\nCovariates\n\nFor each variable, provide:\n\nvariable_name_used_in_code | Conceptual name\n\nDescription of variable/Survey Question Wording\nOriginal name in raw data.\nSummary of any recoding, transformations (e.g. Collapsing categories, or reverse coding endpoints of scales)\nSummary of range of values (e.g. 1= Strong Democrat … 7 = Strong Republican)\n\ntrust_in_gov | Trust in Government\n\nQuestion: “How often can you trust the federal government in Washington to do what is right?”\nOriginal variable: V201233\nRecoding: V201233 reverse coded so that in trust_in_gov, 0 = “Never”, 1 = “Some of the Time”, 2 = “About half the time”, 3 = “Most of the time” and 4 = “Always”\nRange: 0 (“Never) to 4 (”Always)"
  },
  {
    "objectID": "assignments/POLS1600_paper_template.html#code-appendix",
    "href": "assignments/POLS1600_paper_template.html#code-appendix",
    "title": "POLS 1600: Final Paper Template",
    "section": "7.2 Code Appendix",
    "text": "7.2 Code Appendix\nFinally, in your code appendix you will display the all the code from the previous code chunks in one single code chunk by:\n\nsetting echo = T, eval = F in the code chunk’s header\ntyping the &lt;&lt;chunk_label&gt;&gt; for each of your code chunks sequentially where chunk_label is replaced with the of the corresponding code chunk (e.g. `&lt;&gt;)\n\n\n# Pacakges used in analysis\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\", \"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"boot\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n# Uncomment to install | Recomment after installing\n# remotes::install_github(\"jamesmartherus/anesr\")\nlibrary(anesr)\n\n# Load 2020 NES\ndata(\"timeseries_2020\", package =\"anesr\")\n\n# rename timeseries_2020 to df\ndf &lt;- timeseries_2020\n\n\ndf %&gt;%\n  mutate(\n    # ---- OUTCOMES ----\n    \n    ## V201233 PRE: HOW OFTEN TRUST GOVERNMENT IN WASHINGTON TO DO WHAT IS RIGHT\n    ### Reverse code so 0 = Never, 4 = Always\n    trust_gov = ifelse(V201233 &lt; 0, NA, (V201233-5)*-1),\n    \n    # ---- KEY PREDICTORs ----\n    \n    ## V202457 POST: HAS R EVER BEEN ARRESTED\n    been_arrested = case_when(\n      V202457 == 2 ~ 0,\n      V202457 == 1 ~ 1,\n      T ~ NA_real_\n    ),\n    ## V202456: DURING PAST 12 MONTHS, R OR ANY FAMILY MEMBERS STOPPED BY POLICE\n    police_stop_12mo = case_when(\n      V202456 == 2 ~ 0,\n      V202456 == 1 ~ 1,\n      T ~ NA_real_\n    ),\n    \n    # ---- COVARIATES ----\n    \n    ## V201549x  PRE: SUMMARY: R self‐identified race/ethnicity\n    race_f = ifelse(\n      V201549x &lt; 0, NA,\n      # Remove numbers from race variable labels\n      gsub(\"^[[:graph:]]* \", \"\", as_factor(V201549x))\n    ),\n    # Relevel race_f so white is reference category\n    race_f = forcats::fct_relevel(race_f,\"White, non-Hispanic\"),\n    # Create label variable with line breaks for plotting\n    race_l = stringr::str_wrap(race_f, width = 20),\n    is_white = ifelse(race_f == \"White, non-Hispanic\", 1, 0),\n    is_black = ifelse(race_f == \"Black, non-Hispanic\", 1, 0),\n    is_asian = ifelse(race_f == \"Asian or Native Hawaiian/other Pacific Islander, non-Hispanic alone\", 1, 0),\n    is_hispanic = ifelse(race_f == \"Hispanic\", 1, 0),\n    is_multiracial = ifelse(race_f == \"Multiple races, non-Hispanic\", 1, 0),\n    is_NA_AN_other = ifelse(race_f == \"Native American/Alaska Native or other race, non-Hispanic alone\", 1, 0),\n    is_nonwhite = ifelse(is_white == 1, 0, 1),\n    ## V201617x PRE: SUMMARY: Total (family) income\n    income  = ifelse(V201617x &lt; 0, NA, V201617x),\n    income_class = case_when(\n      income &lt;= 6 ~ \"Low income\",\n      income &gt; 6 & income &lt; 17 ~ \"Middle income\",\n      income &gt;= 17 ~ \"High income\",\n      T ~ NA_character_\n    ) %&gt;% factor(., levels = c(\"Middle income\",\"Low income\",\"High income\")) # Make middle income reference category\n    \n  ) -&gt; df\n\n\n\n# ---- Check Recodes ----\n\n## Trust\ntable(df$V201233, df$trust_gov, useNA = \"ifany\")\n## CJS Contact\n\n### Respondented Arrested Ever\ntable(df$V202457, df$been_arrested, useNA = \"ifany\")\n\n### Respondent or peers/family been stopped by police in past year\ntable(df$V202456, df$police_stop_12mo, useNA = \"ifany\")\n\n## Race\ntable(df$race_f, df$V201549x, useNA = \"ifany\")\n### White indicator\ntable(df$race_f, df$is_white, useNA = \"ifany\")\n\n### Non-White indicator\ntable(df$race_f, df$is_nonwhite, useNA = \"ifany\")\n\n## Income\ntable(df$income, df$income_class)\n\n# ---- Descriptive Statistics ----\n\n# Variables for table of descriptive statistics:\nthe_vars &lt;- c(\"trust_gov\",\n              \"been_arrested\", \"police_stop_12mo\",\n              \"income\", df%&gt;%select(starts_with(\"is_\"))%&gt;%names())\n\n# Create table of summary statistics\n\ndf %&gt;%\n  select(all_of(the_vars))%&gt;%\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) %&gt;%\n  mutate(\n    Variable = case_when(\n      Variable == \"been_arrested\" ~ \"Ever Arrested\",\n      Variable == \"income\" ~ \"Income\",\n      Variable == \"police_stop_12mo\" ~ \"Stopped by Police in Past Year\",\n      Variable == \"is_nonwhite\" ~ \"Non-White\",\n      Variable == \"is_white\" ~ \"White\",\n      Variable == \"is_black\" ~ \"Black\",\n      Variable == \"is_hispanic\" ~ \"Hispanic\",\n      Variable == \"is_asian\" ~ \"Asian\",\n      Variable == \"is_NA_AN_other\" ~ \"Native American, Alaskan Native, or Other\",\n      Variable == \"is_multiracial\" ~ \"Multiracial\",\n      Variable == \"trust_gov\" ~ \"Trust in Government\"\n    )\n  ) -&gt; sum_df\n\n\n\n# ---- Descriptive Figures ----\n\n\n## Distribution of trust in Government\nfig1 &lt;- df %&gt;%\n  ggplot(aes(trust_gov))+\n  geom_histogram()+\n  labs(\n    x = \"Trust in Government\"\n  )\n\n## Average proportion of respendents reporting Police Stops by Race\n\nfig2 &lt;- df %&gt;%\n  filter(!is.na(race_l))%&gt;%\n  group_by(race_l)%&gt;%\n  summarise(\n    stop = mean(police_stop_12mo, na.rm=T)\n  )%&gt;%\n  mutate(\n    race_l = fct_reorder(race_l,stop)\n  )%&gt;%\n  ggplot(aes(race_l, stop, \n             fill = race_l,\n             label = scales::percent(stop),\n             ))+\n  geom_bar(stat = \"identity\")+\n  scale_y_continuous(labels = scales::percent,\n                     expand = expansion(mult = c(0,0.5))\n                     )+\n  labs(\n    y = \"% Experiencing Police Stop\\nin Past Year\",\n    x= \"\"\n  )+\n  geom_text_repel(direction = \"x\", hjust = 1)+\n  coord_flip()+\n  guides(fill = \"none\")+\n\n  theme_bw()\n\n## Average trust by Arrest Status\nfig3 &lt;- df %&gt;%\n  mutate(\n    `Ever Arrested` = ifelse(been_arrested == 1, \"Yes\", \"No\")\n  ) %&gt;%\n  filter(!is.na(`Ever Arrested`))%&gt;%\n  ggplot(aes(`Ever Arrested`,trust_gov, \n             fill = `Ever Arrested`,\n             group = `Ever Arrested`))+\n  stat_summary()+\n  labs(\n    y = \"Trust in Government\\n(0 = Never, 4 = Always)\"\n  )\n\n\n# Table of descriptive statistics\nknitr::kable(sum_df,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =3) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 4, end_row =11)\n\n\nfig1\n\nfig2\n\n# Figure 3\nfig3\n\n# ---- Regression Models ----\n\n\n# Bivariate\nm1 &lt;- lm(trust_gov ~ been_arrested, df)\nm2 &lt;- lm(trust_gov ~ police_stop_12mo, df)\nm3 &lt;- lm(trust_gov ~ race_f, df)\nm4 &lt;- lm(trust_gov ~ income_class, df)\n\n# Mutliple regression\nm5 &lt;- lm(trust_gov ~ been_arrested + police_stop_12mo + race_f + income_class, df)\n\n\n\ntexreg::htmlreg(\n  list(m1, m2, m3,m4),\n  custom.coef.names = c(\"(Intercept)\",\n                        \"Ever Arrested\", \"Police Stopped in Past Yr\", \n                        \"Asian\",\n                        \"Black\",\n                        \"Hispanic\",\n                        \"Multiracial\",\n                        \"NA, AN, or Other\",\n                        \"Low Income\",\n                        \"High Income\"),\n  custom.model.names = c(\"Arrests\",\"Stops\",\"Race\",\"Class\"),\n  custom.header = list(\"Outcome: Trust in Government\" = 1:4),\n  caption = \"Baseline Models\"\n  \n)\n\ntexreg::htmlreg(\n  list(m5),\n  custom.coef.names = c(\"(Intercept)\",\n                        \"Ever Arrested\", \"Police Stopped in Past Yr\", \n                        \"Asian\",\n                        \"Black\",\n                        \"Hispanic\",\n                        \"Multiracial\",\n                        \"NA, AN, or Other\",\n                        \"Low Income\",\n                        \"High Income\"),\n  ci.force = T,\n  caption = \"Multiple regression\"\n  \n)\n\n\n# Figure 4\nfig4"
  }
]