[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This class is an introduction to applied statistics as practiced in political science. It is computing intensive, and, as such, will enable students to execute basic quantitative analyses of social science data using the linear model with statistical inference arising from re-sampling and permutation based techniques as applied in the R statistical computing language with RStudio. By the end of the course, a successful student will be able to find social science data online, download it, analyze it, and write about how the analyses bear on focused social science or policy questions.\n\n\n\nMore than anything I assume a willingness to engage with mathematics, data analysis, computer programming, and the practice of social science thinking and writing. I also assume you’ve taken at least one class in algebra at the level taught in most high schools in the United States and have used a personal computer to read and type email and other documents and have some experience with the Internet.\nI also assume that you will read the syllabus and that you keep up to date on changes in the syllabus which will be announced in class. You should not expect a response to emails that ask a question already answered in the syllabus.\nThis is an experimental class so you should expect that the syllabus will change throughout the term. Make sure you have the syllabus with the latest date stamp. I will announce syllabus changes via the emails sent from Canvas.\n\n\n\nNeither the University nor I tolerate cheating or plagiarism. The Brown Writing Center defines plagiarism as ``appropriating another person’s ideas or words (spoken or written) without attributing those word or ideas to their true source.’’ The consequences for plagiarism are often severe, and can include suspension or expulsion. This course will follow the guidelines in the Academic Code for determining what is and isn’t plagiarism:\n\nIn preparing assignments a student often needs or is required to employ outside sources of information or opinion. All such sources should be listed in the bibliography. Citations and footnote references are required for all specific facts that are not common knowledge and about which there is not general agreement. New discoveries or debatable opinions must be credited to the source, with specific references to edition and page even when the student restates the matter in his or her own words. Word-for-word inclusion of any part of someone else’s written or oral sentence, even if only a phrase or sentence, requires citation in quotation marks and use of the appropriate conventions for attribution. Citations should normally include author, title, edition, and page. (Quotations longer than one sentence are generally indented from the text of the essay, without quotation marks, and identified by author, title, edition, and page.) Paraphrasing or summarizing the contents of another’s work is not dishonest if the source or sources are clearly identified (author, title, edition, and page), but such paraphrasing does not constitute independent work and may be rejected by the instructor. Students who have questions about accurate and proper citation methods are expected to consult reference guides as well as course instructors.\n\nWe will discuss specific information about your written work in class in more detail, but if you are unsure of how to properly cite material, please ask for clarification. If you are having difficulty with writing or would like more information or assistance, consult the Writing Center, the Brown library and/or the Academic Code for more information.\n\n\n\nAll students and the instructor must be respectful of others in the classroom. If you ever feel that the classroom environment is discouraging your participation or problematic in any way, please contact me.\n\n\n\nBrown University is committed to full inclusion of all students. Please inform me if you have a disability or other condition that might require accommodations or modification of any of these course procedures. You may speak with me after class or during office hours. For more information contact Student and Employee Accessibility Services at 401-863-9588 or SEAS@brown.edu.\n\n\n\nAny student with a documented disability is welcome to contact me as early in the semester as possible so that we may arrange reasonable accommodations. As part of this process, please be in touch with Student Accessibility Services by calling 401-863-9588 or online\n\n\n\nThis course is designed to support an inclusive learning environment where diverse perspectives are recognized, respected and seen as a source of strength. It is my intent to provide materials and activities that are respectful of various levels of diversity: mathematical background, previous computing skills, gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Toward that goal:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official Brown records, please let me know!\nIf there are things going on inside or outside of class that are affecting your performance in class, please don’t hesitate to talk to me, provide anonymous feedback through our course survey, or contact one of Brown’s Academic Deans."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "This class is an introduction to applied statistics as practiced in political science. It is computing intensive, and, as such, will enable students to execute basic quantitative analyses of social science data using the linear model with statistical inference arising from re-sampling and permutation based techniques as applied in the R statistical computing language with RStudio. By the end of the course, a successful student will be able to find social science data online, download it, analyze it, and write about how the analyses bear on focused social science or policy questions."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "",
    "text": "More than anything I assume a willingness to engage with mathematics, data analysis, computer programming, and the practice of social science thinking and writing. I also assume you’ve taken at least one class in algebra at the level taught in most high schools in the United States and have used a personal computer to read and type email and other documents and have some experience with the Internet.\nI also assume that you will read the syllabus and that you keep up to date on changes in the syllabus which will be announced in class. You should not expect a response to emails that ask a question already answered in the syllabus.\nThis is an experimental class so you should expect that the syllabus will change throughout the term. Make sure you have the syllabus with the latest date stamp. I will announce syllabus changes via the emails sent from Canvas."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "",
    "text": "Neither the University nor I tolerate cheating or plagiarism. The Brown Writing Center defines plagiarism as ``appropriating another person’s ideas or words (spoken or written) without attributing those word or ideas to their true source.’’ The consequences for plagiarism are often severe, and can include suspension or expulsion. This course will follow the guidelines in the Academic Code for determining what is and isn’t plagiarism:\n\nIn preparing assignments a student often needs or is required to employ outside sources of information or opinion. All such sources should be listed in the bibliography. Citations and footnote references are required for all specific facts that are not common knowledge and about which there is not general agreement. New discoveries or debatable opinions must be credited to the source, with specific references to edition and page even when the student restates the matter in his or her own words. Word-for-word inclusion of any part of someone else’s written or oral sentence, even if only a phrase or sentence, requires citation in quotation marks and use of the appropriate conventions for attribution. Citations should normally include author, title, edition, and page. (Quotations longer than one sentence are generally indented from the text of the essay, without quotation marks, and identified by author, title, edition, and page.) Paraphrasing or summarizing the contents of another’s work is not dishonest if the source or sources are clearly identified (author, title, edition, and page), but such paraphrasing does not constitute independent work and may be rejected by the instructor. Students who have questions about accurate and proper citation methods are expected to consult reference guides as well as course instructors.\n\nWe will discuss specific information about your written work in class in more detail, but if you are unsure of how to properly cite material, please ask for clarification. If you are having difficulty with writing or would like more information or assistance, consult the Writing Center, the Brown library and/or the Academic Code for more information."
  },
  {
    "objectID": "syllabus.html#community-standards",
    "href": "syllabus.html#community-standards",
    "title": "Syllabus",
    "section": "",
    "text": "All students and the instructor must be respectful of others in the classroom. If you ever feel that the classroom environment is discouraging your participation or problematic in any way, please contact me."
  },
  {
    "objectID": "syllabus.html#accessibility",
    "href": "syllabus.html#accessibility",
    "title": "Syllabus",
    "section": "",
    "text": "Brown University is committed to full inclusion of all students. Please inform me if you have a disability or other condition that might require accommodations or modification of any of these course procedures. You may speak with me after class or during office hours. For more information contact Student and Employee Accessibility Services at 401-863-9588 or SEAS@brown.edu."
  },
  {
    "objectID": "syllabus.html#academic-accommodations",
    "href": "syllabus.html#academic-accommodations",
    "title": "Syllabus",
    "section": "",
    "text": "Any student with a documented disability is welcome to contact me as early in the semester as possible so that we may arrange reasonable accommodations. As part of this process, please be in touch with Student Accessibility Services by calling 401-863-9588 or online"
  },
  {
    "objectID": "syllabus.html#diversity-and-inclusion",
    "href": "syllabus.html#diversity-and-inclusion",
    "title": "Syllabus",
    "section": "",
    "text": "This course is designed to support an inclusive learning environment where diverse perspectives are recognized, respected and seen as a source of strength. It is my intent to provide materials and activities that are respectful of various levels of diversity: mathematical background, previous computing skills, gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Toward that goal:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official Brown records, please let me know!\nIf there are things going on inside or outside of class that are affecting your performance in class, please don’t hesitate to talk to me, provide anonymous feedback through our course survey, or contact one of Brown’s Academic Deans."
  },
  {
    "objectID": "syllabus.html#requirements",
    "href": "syllabus.html#requirements",
    "title": "Syllabus",
    "section": "Requirements",
    "text": "Requirements\nTo accomplish this metamorphosis, we’ll need the following:\n\nSome math\nSome programming and computing skills\nSome general life skills"
  },
  {
    "objectID": "syllabus.html#math",
    "href": "syllabus.html#math",
    "title": "Syllabus",
    "section": "Math",
    "text": "Math\nYou either already know, or will learn, all the math you need to take this course.1 We’ll go over some key theorems of probability and statistics in class, emphasizing conceptual understanding (often illustrated via simulation) over formal proofs.2. Along the way, we’ll need some calculus and linear algebra to make our lives easier, and so we’ll briefly review this material together in class."
  },
  {
    "objectID": "syllabus.html#computing",
    "href": "syllabus.html#computing",
    "title": "Syllabus",
    "section": "Computing",
    "text": "Computing\nDoing quantitative, empirical social science research requires working with data. Today, working with data requires a computer and statistical software. I assume that you have, or will acquire, a laptop that you will bring to class. In terms of software, there are many possible options. In this class,R.3.\nAll the slides, notes, and assignments in this class are produced using R Markdown, a free, open-source tool for creating reproducible research. It’s a short but steep learning curve, the benefits of which (pretty documents, nicely formatted tables and figures, easy integration with citation managers) far outweigh the costs (finicky syntax)"
  },
  {
    "objectID": "syllabus.html#general",
    "href": "syllabus.html#general",
    "title": "Syllabus",
    "section": "General",
    "text": "General\nLike any course, success in this class requires preparation, participation and perseverance. In terms of preparation, I expect that you will have done the readings and submitted your assignments on time (more on that below). In short, you’ll get out of this class what you put in. In terms of participation, I expect that you will come to class eager to learn and engage with that week’s topics. If you have a question, ask it. If you’re getting an error, share it. In some ways, your job is to make errors. To paraphrase Joyce: people of genius make no mistakes. Our errors are volitional and portals of discovery. While this experience can be challenging and frustrating, it is also incredibly rewarding. I fully expect you persevere through the problems and difficulties that inevitably arise in this course, and will do everything I can to help in this process."
  },
  {
    "objectID": "syllabus.html#class",
    "href": "syllabus.html#class",
    "title": "Syllabus",
    "section": "Class",
    "text": "Class\nThis course meets two times a week for 80 minutes on Tuesdays and Thursdays. Tuesday’s class will be devoted to lecture, demonstration and review. Recorded versions of these lectures will be provided on Canvas after class. Thursday’s class will focus on applications of these concepts through brief labs where you’ll work with real data from a variety of sources. I assume that you will come to class having done each week’s assigned readings and reviewed material from the previous week’s lectures and labs. Slides and labs are available on Canvas and https://pols1600.paultesta.org"
  },
  {
    "objectID": "syllabus.html#attendance",
    "href": "syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nYou may miss two classes without it having any effect on the attendance portion of your grade. After two absences, each additional absence (without a written note from the University) will reduce your final grade by 1 percent."
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Syllabus",
    "section": "Readings",
    "text": "Readings\nImai (2022) required textbooks for the course (Estimated cost: \\(\\sim\\)$38.50 for the ebook, $55.00 for the paperback):\n\nImai, Kosuke. 2022. Quantitative Social Science An Introduction in Tidyverse. Princeton University Press.\n\nMost chapters are spread over multiple weeks. You should read this text with your laptop and R Studio open. Execute the code in the main text and ideally try to complete the assignments and exercises at the end of the chapter.4\nAdditional readings will be listed below and available to download on the course website and Canvas."
  },
  {
    "objectID": "syllabus.html#labs",
    "href": "syllabus.html#labs",
    "title": "Syllabus",
    "section": "Labs",
    "text": "Labs\nThe bulk of the work and learning you’ll do in the course comes in the form of weekly labs in which you’ll explore a given data set or paper using R. You’ll be given an R Markdown document that will guide you through a set of exercises to teach concepts covered in the lectures and reading. You’ll code in R and summaries of your findings in R Markdown. You will compile your document to produce an html document which you will submit on Canvas by the end of each class.\nAll work in this class MUST BE SUBMITTED ONLINE VIA CANVAS.\nYou will work in groups on these labs. One member of your group will submit a lab. One question from the lab will be randomly selected for grading."
  },
  {
    "objectID": "syllabus.html#tutorials",
    "href": "syllabus.html#tutorials",
    "title": "Syllabus",
    "section": "Tutorials",
    "text": "Tutorials\nIn addition to weekly labs, you will also work through weekly tutorials made available to you through the `qsslearnr’ package. These tutorials provide you with an opportunity to practice your programming and review concepts from the text and lecture. After completing each tutorial, you will download your progress report and upload this file to Canvas by midnight on Friday each week a tutorial is assigned. If you upload a report by Friday, you receive a grade of 100 % on that Tutorial. If you upload a report after Friday, you receive a grade of 50 %. If you do not upload a report, you receive a grade of 0 %. There are 11 total tutorials for the course. Your lowest grade on the Tutorials will be dropped.\nThese Tutorials are for your personal benefit. You may collaborate with peers, but you must submit your own file."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nIn addition to weekly labs, you will complete periodic group assignments developing an original research presentation applying skills you have learned in this class to a topic of your choosing. All assignments are due the Friday after the class with which they are associated.\nThe timeline of assignments for your final paper is as follows:\n\nWeek 4: Research Topics\nWeek 6: Identifying Datasets\nWeek 8: Data Explorations\nWeek 11: Draft of Research Presentation\nWeek 12: Research Presentations\nWeek 13: Final Paper\n\nAssignments must be submitted on time to Canvas. No late work will be accepted without prior approval of the instructor or a note from the University."
  },
  {
    "objectID": "syllabus.html#grades",
    "href": "syllabus.html#grades",
    "title": "Syllabus",
    "section": "Grades",
    "text": "Grades\nYour final grade for this course will be calculated as follows:\n\n5 % Class attendance\n10 % Class involvement and participation\n10 % Tutorials\n30 % Labs\n20 % Assignments not including the final Paper\n25 % Final Project\n\nLabs, assignments excluding the final presentation, will be graded graded out of 100 roughly on a ✓ + (100, completed on time, acceptable), ✓ (85, completed on time, passable), ✓ - (0 not submitted on time, unacceptable). The lowest two lab grades will be dropped from your final lab grade. Tutorials are graded on pass (submitted on time = 100 % ) - fail (not submitted =0) based submitting your progress report from the tutorial by Friday each week. If you submit a Tutorial after the week it’s do, you will receive partial credit (50 %). Your final projects will be graded on 100-point scales with rubrics provided beforehand.\nIncomplete Work Assignments not turned in will be counted as zero in the calculation of the final grade.\nComputers in class Please bring your laptops if you have them. We will install R and RStudio together. If you do not own a laptop, you can still work in a group of other people who have laptops and will be able to complete the in-class worksheets without a problem. In fact, it is ideal if each group of 2-4 people works with one laptop and then shares the work among themselves. Of course, feel free to work on your own outside of class."
  },
  {
    "objectID": "syllabus.html#time",
    "href": "syllabus.html#time",
    "title": "Syllabus",
    "section": "Time",
    "text": "Time\nThis course meets 27 times over 13 weeks in the semester. Each class is 80 minutes long so you should expect to spend approximately 36 hours total in class; approximately 4 hours per week reading the textbook and reviewing material (42 hours total); approximately 22 hours on tutorials each week, approximately 30 hours on assignments for the final paper; approximately 50 hours researching, writing, and revising your final presentation; and at least .5 hours meeting with me in person to discuss your work (Estimated Total Time: 180.5 hours)"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not the same as all the math you need to know be a successful, methodologically sophisticated political scientist. But it’s a start, and one that will hopefully help you figure out what additional training you’ll need.↩︎\nWe’ll do the proofs as well, but your focus should be on making sure you understand concepts and implications rather than specific derivations↩︎\nAvailable for free at https://cran.r-project.org/. Python is also increasingly common among social scientists.↩︎\nSeriously. Working carefully through these examples will be incredibly helpful and rewarding. If you’re taking the time to read this footnote, send me picture of a cute animal and I’ll add 1 point of extra credit to your final paper grade. See, your hard work is already paying off.↩︎"
  },
  {
    "objectID": "class/09-class.html",
    "href": "class/09-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#readings",
    "href": "class/09-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#lecture",
    "href": "class/09-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#lab",
    "href": "class/09-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/09-class.html#assignments",
    "href": "class/09-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "9: Probability - Limit Theorems"
    ]
  },
  {
    "objectID": "class/10-class.html",
    "href": "class/10-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#readings",
    "href": "class/10-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#lecture",
    "href": "class/10-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#lab",
    "href": "class/10-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/10-class.html#assignments",
    "href": "class/10-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "10: Inference -- Confidence Intervals"
    ]
  },
  {
    "objectID": "class/08-class.html",
    "href": "class/08-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#readings",
    "href": "class/08-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#lecture",
    "href": "class/08-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#lab",
    "href": "class/08-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/08-class.html#assignments",
    "href": "class/08-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "8: Probability - Random Variables & Distributions"
    ]
  },
  {
    "objectID": "class/05-class.html",
    "href": "class/05-class.html",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 2 & 4\n Red Covid New York Times, 27 September, 2021",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#readings",
    "href": "class/05-class.html#readings",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 2 & 4\n Red Covid New York Times, 27 September, 2021",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#lecture",
    "href": "class/05-class.html#lecture",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#lab",
    "href": "class/05-class.html#lab",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/05-class.html#assignments",
    "href": "class/05-class.html#assignments",
    "title": "Casual Inference inObservational Designs & Simple Linear Regression",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\nlearnr::run_tutorial(\"05-prediction1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "5: Bivariate Regression"
    ]
  },
  {
    "objectID": "class/index.html",
    "href": "class/index.html",
    "title": "General Workflow for POLS 1600",
    "section": "",
    "text": "Before class on Tuesday\n\nDo the assigned readings\nReview last week’s lab and slides\n\nTuesday: Come to class ready to engage with lecture and slides\nWednesday:\n\nStart the tutorials for that week\nDownload, render, and skim Thursday’s lab\n\nThursday: Come to class ready to work through that week’s lab\nFriday: Upload that week’s tutorials to Canvas",
    "crumbs": [
      "Class",
      "Overview",
      "General Workflow for POLS 1600"
    ]
  },
  {
    "objectID": "class/12-class.html",
    "href": "class/12-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#readings",
    "href": "class/12-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#lecture",
    "href": "class/12-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#lab",
    "href": "class/12-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/12-class.html#assignments",
    "href": "class/12-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "12: Workshop"
    ]
  },
  {
    "objectID": "class/13-class.html",
    "href": "class/13-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#readings",
    "href": "class/13-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#lecture",
    "href": "class/13-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#lab",
    "href": "class/13-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/13-class.html#assignments",
    "href": "class/13-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "13: Presentations"
    ]
  },
  {
    "objectID": "class/01-class.html",
    "href": "class/01-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#readings",
    "href": "class/01-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#slides",
    "href": "class/01-class.html#slides",
    "title": "Data and Measurement",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#lecture",
    "href": "class/01-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#lab",
    "href": "class/01-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "class/01-class.html#assignments",
    "href": "class/01-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "1: Data & Measurement"
    ]
  },
  {
    "objectID": "slides/06-slides.html#class-plan",
    "href": "slides/06-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements (20)\n\nAssignment 1: Research Questions due Tuesday, March 12\nAssignment 2 Data: due Friday March 22\n\nFeedback\nReview: Simple Linear Regression and Lab 5 (15-20 min)\nPreview: Setup for Lab 6 (10-15 min)\nEstimating and Interpreting Multiple Regression (25-30 min)\nDifference-in-Differences (15-20 min)"
  },
  {
    "objectID": "slides/06-slides.html#annoucements",
    "href": "slides/06-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/06-slides.html#assignment-1",
    "href": "slides/06-slides.html#assignment-1",
    "title": "POLS 1600",
    "section": "Assignment 1",
    "text": "Assignment 1\n\nPrompt posted on website here\n3 possible research questions uploaded to Canvas by Next Tuesday, March 12\nThe ideal experiment = “What would you have to randomly assign to create a meaningful counterfactual comparison”\nObservational study = “What could you say with data that likely exist?”\nFeedback will help you narrow down a topic for final project"
  },
  {
    "objectID": "slides/06-slides.html#assignment-2",
    "href": "slides/06-slides.html#assignment-2",
    "title": "POLS 1600",
    "section": "Assignment 2",
    "text": "Assignment 2\n\nPrompt posted on website [here]{https://pols1600.paultesta.org/assignments/a2}\nUploaded to Canvas by Friday, March 22\nRevised Research Question\nImplied Linear Model\nCode to load possible data"
  },
  {
    "objectID": "slides/06-slides.html#feedback",
    "href": "slides/06-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback\n\nOnly 9 people completed the survey last week…\nSo let’s try to get those numbers up and will summarize the results of the survey next week"
  },
  {
    "objectID": "slides/06-slides.html#packages-for-today",
    "href": "slides/06-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/06-slides.html#review-key-concepts-from-the-lab",
    "href": "slides/06-slides.html#review-key-concepts-from-the-lab",
    "title": "POLS 1600",
    "section": "Review: Key Concepts from the Lab",
    "text": "Review: Key Concepts from the Lab"
  },
  {
    "objectID": "slides/06-slides.html#review-simple-linear-regression",
    "href": "slides/06-slides.html#review-simple-linear-regression",
    "title": "POLS 1600",
    "section": "Review: Simple Linear Regression",
    "text": "Review: Simple Linear Regression\nLet’s pick up where we left off on Thursday. First we’ll need to run some code to get to recreate our data\n\n# ---- Load data ----\n## Covid-19 Data\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n## Presidential Election Data\nload(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n# ---- Recode Covid Data ----\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n# Recode Deaths\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# ---- Recode Presidential Election Data ----\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df\n\n# ---- Merge Data ----\n\ndim(covid_us)\n\n[1] 53678    60\n\ndim(pres2020_df)\n\n[1] 51  9\n\ncovid_us &lt;- covid_us %&gt;% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us) # Same number of rows as covid_us w/ 8 additional columns\n\n[1] 53678    68"
  },
  {
    "objectID": "slides/06-slides.html#section",
    "href": "slides/06-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Calculating Conditional Means\n\nQ6DataTable-Fig-Fig\n\n\nNow let’s revisit question 6, which asked you to calculate some conditional means:\n\nOverall\nBefore the vaccine was widely available\nAfter the vaccine was widely available\n\n\n\n\n# ---- Deaths: Overall ----\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  ) %&gt;% \n  mutate(\n    comparison = \"Overall\"\n  ) -&gt; deaths_overall\n\n# ---- Deaths: Pre Vaccine ----\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  ) %&gt;% \n  mutate(\n    comparison = \"Pre Vaccine\"\n  ) -&gt; deaths_pre_vax\n\n# ---- Deaths: Post Vaccine ----\ncovid_us %&gt;%\n  filter(date &gt;= \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  ) %&gt;% \n  mutate(\n    comparison = \"Post Vaccine\"\n  ) -&gt; deaths_post_vax\n\n# ---- Tidy outputs for display ----\n\ndeaths_tab &lt;- deaths_overall %&gt;% \n  bind_rows(\n  deaths_pre_vax,\n  deaths_post_vax\n) %&gt;% \n  mutate(\n    comparison = factor(\n      comparison,\n      levels = c(\"Overall\",\"Pre Vaccine\", \"Post Vaccine\")\n      )\n  )\n\n\n\n\nknitr::kable(deaths_tab) %&gt;% \n  kableExtra::kable_styling()\n\n\n\n\n\nwinner\nnew_deaths\nnew_deaths_pc_7day\ncomparison\n\n\n\n\nTrump\n19.34867\n0.3402479\nOverall\n\n\nBiden\n22.04853\n0.2871843\nOverall\n\n\nTrump\n22.82048\n0.4000294\nPre Vaccine\n\n\nBiden\n30.61051\n0.3801906\nPre Vaccine\n\n\nTrump\n17.07402\n0.3016571\nPost Vaccine\n\n\nBiden\n16.30683\n0.2257111\nPost Vaccine\n\n\n\n\n\n\n\n\n\n\n\n# 1. Data\ndeaths_tab %&gt;% \n  # 2. Aesthetics\n  ggplot(\n    aes(winner, new_deaths_pc_7day,\n        fill = winner)\n  ) +\n  # 3. Geometries\n  geom_bar(stat = \"identity\") +\n  # 4. Facets\n  facet_grid(~ comparison) +\n  # 5. Labels and Themes\n  guides(fill = \"none\") +\n  labs(\n    x = \"State's won by\",\n    y = \"Average # of Deaths per 100k\\n(7-day rolling average)\",\n    title = \"Red States have more Covid-19 deaths per capita after vaccine\"\n  )+\n  theme_minimal()+\n  theme(title = element_text(size = 10,face = \"bold\")) -&gt; fig_q6"
  },
  {
    "objectID": "slides/06-slides.html#section-1",
    "href": "slides/06-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Using OLS to estimate conditional means\n\nQ7OutputTable\n\n\nQuestion 7 asked you estimate the following OLS models:\n\\[ \\text{New Deaths} = \\beta_0 + \\beta_1 \\text{Election Winner} + \\epsilon \\]\n\\[ \\text{7-day average of New Deaths (per 100k)} = \\beta_0 + \\beta_1 \\text{Election Winner} + \\epsilon \\]\n\n\n\n\n\n\nNote\n\n\nRecall winner is a factor whose levels we set to be c(\"Trump\",\"Biden\").\nlm() converts factors into binary indicators. Here 0=\"Trump\" and 1=\"Biden\"\n\n\n\n\n\n\nm1_lab &lt;- lm(new_deaths ~ winner, covid_us)\nm2_lab &lt;- lm(new_deaths_pc_7day ~ winner, covid_us)\n\n\n\n\nm1_lab\n\n\nCall:\nlm(formula = new_deaths ~ winner, data = covid_us)\n\nCoefficients:\n(Intercept)  winnerBiden  \n      19.35         2.70  \n\n# Just the coefficients\ncoef(m2_lab)\n\n(Intercept) winnerBiden \n 0.34024787 -0.05306357 \n\n# Coefficients with summary stats (for later)\nsummary(m2_lab)\n\n\nCall:\nlm(formula = new_deaths_pc_7day ~ winner, data = covid_us)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0016 -0.2205 -0.1340  0.0966  5.8550 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.340248   0.002487  136.83   &lt;2e-16 ***\nwinnerBiden -0.053064   0.003475  -15.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3978 on 52447 degrees of freedom\n  (1229 observations deleted due to missingness)\nMultiple R-squared:  0.004427,  Adjusted R-squared:  0.004408 \nF-statistic: 233.2 on 1 and 52447 DF,  p-value: &lt; 2.2e-16\n\n\n\n\ntexreg::htmlreg(list(m1_lab,m2_lab),\n                custom.header = list(\n                  \"DV:\" = 1:2\n                ),\n                custom.model.names = c(\n                  \"new_deaths\",\n                  \"new_deaths_pc_7day\"\n                ))\n\n\nStatistical models\n\n\n\n\n \n\n\nDV:\n\n\n\n\n \n\n\nnew_deaths\n\n\nnew_deaths_pc_7day\n\n\n\n\n\n\n(Intercept)\n\n\n19.35***\n\n\n0.34***\n\n\n\n\n \n\n\n(0.39)\n\n\n(0.00)\n\n\n\n\nwinnerBiden\n\n\n2.70***\n\n\n-0.05***\n\n\n\n\n \n\n\n(0.55)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n\n\nNum. obs.\n\n\n52755\n\n\n52449\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/06-slides.html#section-3",
    "href": "slides/06-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Q7: When the CEF is Linear, OLS = CEF\n\n -CEF-CEF-OLS-OLS\n\n\nIf the CEF is linear, then OLS = CEF\nIn a saturated linear model, every group in the data can be represented by a coefficient or combination of coefficients in the model.\n\n\n\ncovid_us %&gt;%\n  mutate(\n    biden01 = ifelse(winner == \"Biden\",1,0)\n  ) %&gt;% \n  ggplot(aes(biden01, new_deaths_pc_7day))+\n  geom_jitter(size=.25,alpha=.15)+\n  stat_summary(col=\"red\")+\n  stat_summary(aes(label=round(..y..,2)),\n               geom = \"text\", \n               position = position_nudge(y = 1),\n               col = \"red\",\n               fun = mean) +\n  theme_minimal()-&gt; fig_q7cef\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig_q7cef +\n  stat_smooth(method = \"lm\",se = F)+\n  stat_summary(aes(label=round(..y..,2)),\n               geom = \"text\", \n               position = position_nudge(y = .05),\n               col = \"red\",\n               fun = mean)+\n  coord_cartesian(ylim=c(0,.45))+\n  geom_segment(aes(\n    x = 0,\n    xend = 0,\n    y = coef(m2_lab)[1],\n    yend = coef(m2_lab)[1] + coef(m2_lab)[2]\n  ),\n  col = \"blue\",\n  linetype = \"dashed\")+\n  geom_segment(aes(\n    x = 0,\n    xend = 1,\n    y = coef(m2_lab)[1] + coef(m2_lab)[2],\n    yend = coef(m2_lab)[1] + coef(m2_lab)[2]\n  ),\n  col = \"blue\",\n  linetype = \"dashed\")+\n  annotate(\"text\",\n           label = expression(paste(beta[0],\" = \" )),\n           x = 0, \n           y = coef(m2_lab)[1] + 0.075,\n           hjust = \"center\",\n           col = \"blue\"\n           )+\n    annotate(\"text\",\n           label = expression(paste(beta[1],\" = 0.29 - 0.34 = -0.05\" )),\n           x = 0, \n           y = coef(m2_lab)[1] +coef(m2_lab)[2] - 0.05,\n           hjust = \"left\",\n           col = \"blue\"\n           )+\n  annotate(\"text\",\n           label = expression(\n             paste(beta[0],\" + \", beta[1], \" = \" )),\n           x = 1, \n           y = coef(m2_lab)[1] + 0.05,\n           hjust = \"center\",\n           col = \"blue\"\n           ) -&gt; fig_q7ols"
  },
  {
    "objectID": "slides/06-slides.html#section-5",
    "href": "slides/06-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "Q8: Vote shares, vaccinations, and deaths\n\nQ9-OLSResults\n\n\nQ9 asked you to fit three models exploring the relationship between:\n\\[ \\text{m3} =\\text{14-day average of New Deaths (per 100k)} = \\beta_0 + \\beta_1 \\text{Percent Vaccinated} \\] \\[ \\text{m4} =\\text{Percent Vaccinated} = \\beta_0 + \\beta_1 \\text{Republican Vote Share} \\] \\[ \\text{m5} =\\text{14-day average of New Deaths (per 100k)} = \\beta_0 + \\beta_1 \\text{Republican Vote Share} \\]\nOn September 23, 2021, when Leonhardt was writing\n\n\n\n# Deaths modeled by percent vaccinated on 2021-09-23\nm3_lab &lt;- lm(new_deaths_pc_14day ~ percent_vaccinated, \n         covid_us,\n         subset = date == \"2021-09-23\")\n\n#  Percent vaccinated modeled by Republican vote share on 2021-09-23\nm4_lab &lt;- lm(percent_vaccinated ~ rep_voteshare, \n         covid_us,\n         subset = date == \"2021-09-23\")\n\n# Deaths modeled by Republican vote share on 2021-09-23\nm5_lab &lt;- lm(new_deaths_pc_14day ~ rep_voteshare, \n         covid_us,\n         subset = date == \"2021-09-23\")\n\n\n\n\ncoef(m3_lab)\n\n       (Intercept) percent_vaccinated \n         2.4232235         -0.0330773 \n\ncoef(m4_lab)\n\n  (Intercept) rep_voteshare \n   84.3326265    -0.5731175 \n\ncoef(m5_lab)\n\n  (Intercept) rep_voteshare \n  -0.36296799    0.01888996"
  },
  {
    "objectID": "slides/06-slides.html#section-6",
    "href": "slides/06-slides.html#section-6",
    "title": "POLS 1600",
    "section": "",
    "text": "Q9: Visualizing Vote Shares and Deaths\n\nQ9-Basic-Basic-Fetch-Fetch\n\n\nQ9 asks you to visualize the results of the model m5\nLet’s take a basic figure, and make it fetch!\n\n\n\ncovid_us %&gt;%\n  # Only use observations from September 23, 2021\n  filter(date == \"2021-09-23\") %&gt;%\n  # Exclude DC\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14day))+\n  # Set geometries\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # Include the linear regression of lm(new_deaths_pc_14day ~ rep_voteshare)\n  geom_smooth(method = \"lm\", se=F,\n              col = \"grey\", linetype =2) -&gt; fig_m5_lab\n\n\n\n\nfig_m5_lab\n\n\n\n\n\n\n\n\n\n\n\nfig_m5_lab +\n  # two way gradient, blue states -&gt; blue, red states -&gt; red, swing -&gt; grey\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # Vertical line at 50% threshold\n  geom_vline(xintercept = 50, \n             col = \"grey\",linetype = 3)+\n  # Add labels\n  ggrepel::geom_text_repel(aes(label = state_po), size=2)+\n  # theme with minimal lines\n  theme_classic()+\n  # Labels\n  labs(\n    x = \"Republican Vote Share\\n 2020 Presidential Election\",\n    y = \"New Covid-19 Deaths per 100kresidents\\n (14-day average)\",\n    title = \"Partisan Gaps in Covid-19 Deaths at the State Level\",\n    subtitle = \"Data from Sept. 23, 2021\"\n  ) -&gt; fig_m5_lab_fetch\n\n\n\n\nfig_m5_lab_fetch"
  },
  {
    "objectID": "slides/06-slides.html#q10-alternative-explanations",
    "href": "slides/06-slides.html#q10-alternative-explanations",
    "title": "POLS 1600",
    "section": "Q10: Alternative Explanations",
    "text": "Q10: Alternative Explanations\n\nFinally Q10 asked you to consider some possible alternative explanations or omitted variables that might explain the positive relationship, between Republican Vote Share and Covid-19 deaths.\nIn this week’s lab, we’ll see how we can use multiple regression to evaluate these claims."
  },
  {
    "objectID": "slides/06-slides.html#red-covid",
    "href": "slides/06-slides.html#red-covid",
    "title": "POLS 1600",
    "section": "Red Covid",
    "text": "Red Covid\nThe core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States."
  },
  {
    "objectID": "slides/06-slides.html#section-7",
    "href": "slides/06-slides.html#section-7",
    "title": "POLS 1600",
    "section": "",
    "text": "Testing Alternative Explanations of Red Covid\n\nA skeptic might argue that this relationship is spurious.\nThere are lots of ways that Red States differ from Blue States – demographics, economics, geography, culture – that might explain the differences in Covid-19 deaths."
  },
  {
    "objectID": "slides/06-slides.html#section-8",
    "href": "slides/06-slides.html#section-8",
    "title": "POLS 1600",
    "section": "",
    "text": "Testing Alternative Explanations of Red Covid\n\nIn Lab 6, we use multiple regression to try and control for the following alternative explanations:\n\nDifferences in median age\nDifferences in median income\n\nTo do this, we need to merge in some additional state level data from the census."
  },
  {
    "objectID": "slides/06-slides.html#loading-data-from-the-census",
    "href": "slides/06-slides.html#loading-data-from-the-census",
    "title": "POLS 1600",
    "section": "Loading data from the Census",
    "text": "Loading data from the Census\nIf you worked through the instructions here and installed tidycensus and a Census API key on your machine, you should be able to run the following:\n\nacs_df &lt;- tidycensus::get_acs(geography = \"state\", \n              variables = c(med_income = \"B19013_001\",\n                            med_age = \"B01002_001\"), \n              year = 2019)\n\nIf not, no worries, just uncomment the code below:\n\n# Uncomment if get_acs() doesn't work:\n# load(url(\"https://pols1600.paultesta.org/files/data/acs_df.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#tidy-census-data",
    "href": "slides/06-slides.html#tidy-census-data",
    "title": "POLS 1600",
    "section": "Tidy Census Data",
    "text": "Tidy Census Data\n\nTask-Old-Tidy-New\n\n\nget_acs() returns data whose unit of analysis is roughly a state-variable\nWe need to reshape acs_df using pivot_wider() so that the unit of analysis is just a state, and each column variable is a column\n\n\n\nacs_df\n\n# A tibble: 104 × 5\n   GEOID NAME       variable   estimate    moe\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1 01    Alabama    med_age        39      0.2\n 2 01    Alabama    med_income  50536    304  \n 3 02    Alaska     med_age        34.3    0.1\n 4 02    Alaska     med_income  77640   1015  \n 5 04    Arizona    med_age        37.7    0.2\n 6 04    Arizona    med_income  58945    266  \n 7 05    Arkansas   med_age        38.1    0.1\n 8 05    Arkansas   med_income  47597    328  \n 9 06    California med_age        36.5    0.1\n10 06    California med_income  75235    232  \n# ℹ 94 more rows\n\n\n\n\n\nacs_df %&gt;%\n  mutate(\n    state = NAME,\n  ) %&gt;%\n  select(state, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable,\n              values_from = estimate) -&gt; acs_df\n\n\n\n\nacs_df\n\n# A tibble: 52 × 3\n   state                med_age med_income\n   &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alabama                 39        50536\n 2 Alaska                  34.3      77640\n 3 Arizona                 37.7      58945\n 4 Arkansas                38.1      47597\n 5 California              36.5      75235\n 6 Colorado                36.7      72331\n 7 Connecticut             41        78444\n 8 Delaware                40.6      68287\n 9 District of Columbia    34        86420\n10 Florida                 42        55660\n# ℹ 42 more rows"
  },
  {
    "objectID": "slides/06-slides.html#merge-census-data-into-covid-data",
    "href": "slides/06-slides.html#merge-census-data-into-covid-data",
    "title": "POLS 1600",
    "section": "Merge Census data into Covid data",
    "text": "Merge Census data into Covid data\n\nTask-Merge\n\n\n\nNow we can merge acs_df into covid_us using the left_join() function\n\n\n\n\n\n\nNote\n\n\n\nIn the code, we’ll save the output of joining acs_df to covid_us to a new dataframe called covid_df to avoid potentially duplicating columns\n\n\n\n\n\n\n\n\n\ndim(acs_df)\n\n[1] 52  3\n\ndim(acs_df)\n\n[1] 52  3\n\ndim(covid_us)\n\n[1] 53678    68\n\n# Merge tmp with acs_df and save as final covid_df file\ncovid_df &lt;- covid_us %&gt;% left_join(\n  acs_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_df)  # Same number of rows as tmp w/ 2 additional columns\n\n[1] 53678    70"
  },
  {
    "objectID": "slides/06-slides.html#subset-data",
    "href": "slides/06-slides.html#subset-data",
    "title": "POLS 1600",
    "section": "Subset Data",
    "text": "Subset Data\n\nTask-Subset-Subsetted Data\n\n\n\nNext, we’ll subset the data to include just the values from variables we’ll use in the lab on from September 23, 2021 using:\n\nfilter()\nselect()\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, we’ll save this output to a new object called covid_lab\n\n\n\n\n\n\n\n\n\nthe_vars &lt;- c(\n  # Covid variables\n  \"state\",\"state_po\",\"date\",\"new_deaths_pc_14day\", \"percent_vaccinated\",\n  # Election variables\n  \"winner\",\"rep_voteshare\",\n  # Demographic variables\n  \"med_age\",\"med_income\",\"population\")\n\n\ncovid_lab &lt;- covid_df %&gt;%\n  filter( date == \"2021-09-23\") %&gt;%\n  select(all_of(the_vars)) %&gt;%\n  ungroup()\n\nlength(the_vars)\n\n[1] 10\n\ndim(covid_lab)\n\n[1] 51 10\n\n\n\n\n\ncovid_lab\n\n# A tibble: 51 × 10\n   state       state_po date       new_deaths_pc_14day percent_vaccinated winner\n   &lt;chr&gt;       &lt;I&lt;chr&gt;&gt; &lt;date&gt;                   &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt; \n 1 Minnesota   MN       2021-09-23               0.222               61.0 Biden \n 2 California  CA       2021-09-23               0.295               60.8 Biden \n 3 Florida     FL       2021-09-23               1.61                58.4 Trump \n 4 Wyoming     WY       2021-09-23               0.938               43.9 Trump \n 5 South Dako… SD       2021-09-23               0.291               52.4 Trump \n 6 Kansas      KS       2021-09-23               0.559               53.9 Trump \n 7 Nevada      NV       2021-09-23               0.700               51.8 Biden \n 8 Virginia    VA       2021-09-23               0.379               62.5 Biden \n 9 Washington  WA       2021-09-23               0.532               63.2 Biden \n10 Oregon      OR       2021-09-23               0.439               62.3 Biden \n# ℹ 41 more rows\n# ℹ 4 more variables: rep_voteshare &lt;dbl&gt;, med_age &lt;dbl&gt;, med_income &lt;dbl&gt;,\n#   population &lt;int&gt;"
  },
  {
    "objectID": "slides/06-slides.html#standardized-variables",
    "href": "slides/06-slides.html#standardized-variables",
    "title": "POLS 1600",
    "section": "Standardized Variables",
    "text": "Standardized Variables\n\nWhen numeric variables are measured on different scales, it can be useful to construct standardized measures, sometimes called z-scores\n\\[z\\text{-scores of x} = \\frac{x_i - \\mu_{x}}{\\sigma_x}\\]\n\nThe z-score of Age is\n\n\\[z\\text{-scores of Age} = \\frac{\\text{Age}_i - \\text{Average Age}}{\\text{Standard Deviation of Age}}\\]\n\n\n\n\n\n\nNote\n\n\n\nStandardized variables all have a mean of 0 and a standard deviation of 1\nStandardizing variables helps us interpret coefficients in multiple regressions with predictors measured on different scales"
  },
  {
    "objectID": "slides/06-slides.html#standardizing-variables-for-the-lab",
    "href": "slides/06-slides.html#standardizing-variables-for-the-lab",
    "title": "POLS 1600",
    "section": "Standardizing variables for the lab",
    "text": "Standardizing variables for the lab\n\nTask-Standardize-Compare-Compare\n\n\n\nCreate standardized versions of\n\nrep_voteshare\nmed_age\nmed_income\npercent_vaccinated\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll use the suffix _std to distinguish the standardized variables from the original variables\n\n\n\n\n\n\n\n\ncovid_lab %&gt;%\n  mutate(\n    rep_voteshare_std = (rep_voteshare - mean(rep_voteshare)) / sd(rep_voteshare),\n    med_age_std = ( med_age - mean( med_age)) / sd( med_age),\n    med_income_std = (med_income - mean(med_income)) / sd(med_income),\n    percent_vaccinated_std = (percent_vaccinated - mean(percent_vaccinated)) / sd(percent_vaccinated)\n  ) -&gt; covid_lab\n\n\n\n\ncovid_lab %&gt;%\n  summarise(\n    mn_rep_vote = mean(rep_voteshare),\n    mn_rep_vote_std = round(mean(rep_voteshare_std),2),\n    sd_rep_vote = sd(rep_voteshare),\n    sd_rep_vote_std = sd(rep_voteshare_std)\n  )\n\n# A tibble: 1 × 4\n  mn_rep_vote mn_rep_vote_std sd_rep_vote sd_rep_vote_std\n        &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1        49.2               0        12.0               1\n\n\n\n\n\ncovid_lab %&gt;% \n  ggplot(aes(rep_voteshare_std,rep_voteshare))+\n  geom_point()"
  },
  {
    "objectID": "slides/06-slides.html#save-data",
    "href": "slides/06-slides.html#save-data",
    "title": "POLS 1600",
    "section": "Save Data",
    "text": "Save Data\nFinally, I’ll save the data for Thursday’s lab\n\n# Don't run this code\nsave(covid_lab, file = \"../files/data/06_lab.rda\")\n\nAnd on Thursday, we’ll be able to load the covid_lab just by running:\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))"
  },
  {
    "objectID": "slides/06-slides.html#conceptual-multiple-regression",
    "href": "slides/06-slides.html#conceptual-multiple-regression",
    "title": "POLS 1600",
    "section": "Conceptual: Multiple Regression",
    "text": "Conceptual: Multiple Regression\n\n\nMultiple linear regression generalizes simple regression to models with multiple predictors\n\n\\[y = \\beta_0 + \\beta_1x_{1} +\\beta_2x_{2} \\dots \\beta_kx_k + \\epsilon\\] More compactly written in matrix notation:\n\\[y = X\\beta + \\epsilon\\] Where:\n\\[\n\\overbrace{\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}}^{y}\n=\n\\overbrace{\n\\begin{bmatrix}\n     1  & x_{1,1}&\\dots & x_{1,k}\\\\\n1  & x_{2,1}&\\dots & x_{2,k}\\\\\n\\vdots&\\vdots&  & \\vdots \\\\\n1  & x_{n,1}&\\dots & x_{n,k}\\\\\n\\end{bmatrix}}^{X}\n\\overbrace{\\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1\\\\\n\\vdots \\\\\n\\beta_k\\\\\n\\end{bmatrix}}^{\\beta}\n+\n\\overbrace{\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{bmatrix}}^{\\epsilon}\n\\]"
  },
  {
    "objectID": "slides/06-slides.html#conceptual-multiple-regression-1",
    "href": "slides/06-slides.html#conceptual-multiple-regression-1",
    "title": "POLS 1600",
    "section": "Conceptual: Multiple Regression",
    "text": "Conceptual: Multiple Regression\n\nRegression models partition variance\nSeparate variation in the outcome (\\(y\\)) into variation explained by the predictors in the model and the residual variation not explained by these predictors\nRegression coefficients tell us how the outcome \\(y\\) is expected to change if \\(x\\) changes by one unit, holding constant or controlling for other predictors in the model."
  },
  {
    "objectID": "slides/06-slides.html#practical-multiple-regression",
    "href": "slides/06-slides.html#practical-multiple-regression",
    "title": "POLS 1600",
    "section": "Practical: Multiple Regression",
    "text": "Practical: Multiple Regression\n\n\nWe estimate linear models in R using the lm() function using the + to add predictors\nWe use the * to include the main effects \\((\\beta_1 x, \\beta_2z)\\) and interactions \\((\\beta_3 (x\\cdot z))\\)of two predictors\n\n\nlm(y ~ x + z, data = df)\nlm(y ~ x*z, data = df) # Is a shortcut for:\nlm(y ~ x + z + x:z, data = df)"
  },
  {
    "objectID": "slides/06-slides.html#technical-multiple-regression",
    "href": "slides/06-slides.html#technical-multiple-regression",
    "title": "POLS 1600",
    "section": "Technical: Multiple Regression",
    "text": "Technical: Multiple Regression\n\n\nSimple linear regression chooses a \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize the Sum of Squared Residuals (SSR):\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\hat{\\beta_0}+\\hat{\\beta_1}x_i))^2\\]\n\nMultiple linear regression chooses a vector of coefficients \\(\\hat{\\beta}\\) to minimize the Sum of Squared Residuals (SSR):\n\n\\[\\textrm{Find }\\widehat{\\beta} \\text{ argmin }_{\\widehat{\\beta}} \\sum \\epsilon^2=\\epsilon^\\prime\\epsilon=(y-X\\widehat{\\beta})^\\prime(y-X\\widehat{\\beta})\\]"
  },
  {
    "objectID": "slides/06-slides.html#theoretical-multiple-regression",
    "href": "slides/06-slides.html#theoretical-multiple-regression",
    "title": "POLS 1600",
    "section": "Theoretical: Multiple Regression",
    "text": "Theoretical: Multiple Regression\n\n\nMultiple Linear regression still provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\) where \\(Y\\) is now a function of multiple predictors, \\(X\\)"
  },
  {
    "objectID": "slides/06-slides.html#section-9",
    "href": "slides/06-slides.html#section-9",
    "title": "POLS 1600",
    "section": "",
    "text": "Estimating and Interpretting Multiple Regressions\n\nTask NES HLO Wrangle\n\n\n\nLet’s load, inspect, and recode some data from the 2016 NES and explore the relationship between political interest and evaluations of presidential candidates:\n\nPolitical Interest: “How interested are you in in politics?\n\nVery Interested\nSomewhat interested\nNot very Interested\nNot at all Interested\n\nFeeling Thermometer: “… On the feeling thermometer scale of 0 to 100, how would you rate”\n\nDonald Trump\nHillary Clinton\n\n\n\n\n\n\n# Load data from 2016 NES\nload(url(\"https://pols1600.paultesta.org/files/data/nes.rda\"))\n\n\n\n\n# Take a quick look at the data\ndim(nes)\n\n[1] 1200   14\n\nhead(nes)\n\n  caseid    state age gender educ faminc pid7 ideo5 pol_interest church_atd\n1    745  Alabama  19      2    3     97    5     3            1          2\n2   1115  Alabama  46      1    3      3    1     1            3          6\n3    258  Alabama  59      2    2      6    2     3            2          1\n4    126  Alabama  55      2    4      6    1     2            3          5\n5    414  Alabama  66      1    3      8    7     4            3          3\n6    523  Alabama  61      1    2     97    1     2            3          6\n  bornagain01 ft_trump ft_hrc vote_choice\n1           0        3     19       Other\n2           0        0     36         HRC\n3           1       22      2        &lt;NA&gt;\n4           0        1     80         HRC\n5           1      100      3        &lt;NA&gt;\n6           1        0    100        &lt;NA&gt;\n\ntable(nes$pol_interest)\n\n\n  0   1   2   3 \n 78 178 348 568 \n\nsummary(nes$ft_trump)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    2.00   30.00   38.38   72.00  100.00       3 \n\nsummary(nes$ft_hrc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    3.00   44.00   42.99   76.00  100.00       1 \n\n\n\n\n\nnes %&gt;%\n  mutate(\n    income = ifelse(faminc &gt; 16, NA, faminc),\n    interested = ifelse(pol_interest==3,T,F),\n    pol_interest_f = factor(case_when(\n      pol_interest == 0 ~ \"Not at all Interested\",\n      pol_interest == 1 ~ \"Not very Interested\",\n      pol_interest == 2 ~ \"Somewhat Interested\",\n      pol_interest == 3 ~ \"Very Interested\"\n    )),\n    tc_diff = abs(ft_trump - ft_hrc)\n  ) -&gt; nes"
  },
  {
    "objectID": "slides/06-slides.html#section-10",
    "href": "slides/06-slides.html#section-10",
    "title": "POLS 1600",
    "section": "",
    "text": "Estimating and Interpreting Models\n\nModelsSkills\n\n\nLet’s estimate the following models:\n\\[\\text{m1: tc_diff} = \\beta_0 + \\beta_1\\text{interested}+\\epsilon \\]\n\\[\\text{m2: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest}+\\epsilon\\]\n\\[\\text{m3: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest_f} +\\epsilon\\]\n\\[\\text{m4: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} +\\epsilon\\]\n\\[\\text{m5: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} + \\beta_3\\text{interested} \\times \\text{age}+\\epsilon\\]\n\\[\\text{m6: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} +\\epsilon\\]\n\\[\\text{m7: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} + \\beta_4\\text{age}\\times \\text{income}+\\epsilon \\]\n\n\n\nWe’ll use:\n\nlm() to estimate models\ncoef() and summary() to examine our results\nhtmlreg()to format our results\ndata.frame() to create prediction dataframes\npredict() to produce predicted values and cbind() to combine these predicted back into the prediction dataframes for plotting\nggplot() to display and interpret our models’ predictions"
  },
  {
    "objectID": "slides/06-slides.html#regression-tables",
    "href": "slides/06-slides.html#regression-tables",
    "title": "POLS 1600",
    "section": "Regression Tables",
    "text": "Regression Tables\n\nAcademic articles are littered with regression tables.\nBelow we’ll see how to:\n\nproduce regression tables in R\nuse some heuristics to interpret regression tables\n\nWe’ll cover the why with greater depth and care later in the course, for now, let’s focus on the what and how"
  },
  {
    "objectID": "slides/06-slides.html#making-regression-tables-in-r",
    "href": "slides/06-slides.html#making-regression-tables-in-r",
    "title": "POLS 1600",
    "section": "Making Regression Tables in R",
    "text": "Making Regression Tables in R\n\nTask\n\n\nWe can make a very simple regression table using the htmlreg function from the texreg package\n\n\n\n\n\n\nNote\n\n\nTo properly display your results you need to add #| results: asis the following to the code chunk header\n\n\n\n\n\n```{r}\n#| echo: false\n#| results: asis\ntexreg::htmlreg(\n  list(m1_lab,m2_lab, m3_lab,m4_lab,m5_lab)\n)\n```\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\n\n\n\n\n(Intercept)\n\n\n19.35***\n\n\n0.34***\n\n\n2.42***\n\n\n84.33***\n\n\n-0.36\n\n\n\n\n \n\n\n(0.39)\n\n\n(0.00)\n\n\n(0.30)\n\n\n(2.66)\n\n\n(0.20)\n\n\n\n\nwinnerBiden\n\n\n2.70***\n\n\n-0.05***\n\n\n \n\n\n \n\n\n \n\n\n\n\n \n\n\n(0.55)\n\n\n(0.00)\n\n\n \n\n\n \n\n\n \n\n\n\n\npercent_vaccinated\n\n\n \n\n\n \n\n\n-0.03***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.01)\n\n\n \n\n\n \n\n\n\n\nrep_voteshare\n\n\n \n\n\n \n\n\n \n\n\n-0.57***\n\n\n0.02***\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.00)\n\n\n\n\nR2\n\n\n0.00\n\n\n0.00\n\n\n0.44\n\n\n0.71\n\n\n0.31\n\n\n\n\nAdj. R2\n\n\n0.00\n\n\n0.00\n\n\n0.43\n\n\n0.70\n\n\n0.29\n\n\n\n\nNum. obs.\n\n\n52755\n\n\n52449\n\n\n51\n\n\n51\n\n\n51\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/06-slides.html#interpreting-regression-tables-stargazing",
    "href": "slides/06-slides.html#interpreting-regression-tables-stargazing",
    "title": "POLS 1600",
    "section": "Interpreting Regression Tables (Stargazing)",
    "text": "Interpreting Regression Tables (Stargazing)\n\n\nEach column is a model\nEach row is a coefficient from that model with its standard error (more to come) in parentheses below\nWe interpret coefficients by looking at their sign, size, and significance\n\nCoefficients with asterisks * are statistically significant (more to come)\nIt is unlikely that we would see a coefficient this big or bigger if the true coefficient were 0\n\nRule of thumb:\n\n\\[\\text{If }\\frac{\\beta}{se} &gt; 2 \\to \\text{Statistically Significant}\\]"
  },
  {
    "objectID": "slides/06-slides.html#m1-a-binary-indicator",
    "href": "slides/06-slides.html#m1-a-binary-indicator",
    "title": "POLS 1600",
    "section": "m1: A binary indicator",
    "text": "m1: A binary indicator\n\nm1-Table-Figure-m1\n\n\n\\[\\text{m1: tc_diff} = \\beta_0 + \\beta_1\\text{interested}+\\epsilon \\]\n\nm1 &lt;- lm(tc_diff ~ interested, nes)\ncoef(m1)\n\n   (Intercept) interestedTRUE \n      47.84500       12.74655 \n\nmean(nes$tc_diff[nes$interested == F], na.rm=T)\n\n[1] 47.845\n\nmean(nes$tc_diff[nes$interested == T], na.rm=T)\n\n[1] 60.59155\n\nmean(nes$tc_diff[nes$interested == T], na.rm=T) -\n  mean(nes$tc_diff[nes$interested == F], na.rm=T)\n\n[1] 12.74655\n\n\n\n\nhtmlreg(m1)\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n\n\n \n\n\n(1.26)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n\n\n \n\n\n(1.81)\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df1 &lt;- expand_grid(\n  interested = c(F,T)\n)\n\npred_df1  &lt;- cbind(\n  pred_df1 ,\n  fit = predict(m1, pred_df1 )\n)\n\n# Produce figure\n\npred_df1 %&gt;% \n  ggplot(aes(interested, fit))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(interested)),\n    aes(x= interested,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point(col=\"red\") -&gt; fig_tc_m1"
  },
  {
    "objectID": "slides/06-slides.html#m2-a-numerical-predictor",
    "href": "slides/06-slides.html#m2-a-numerical-predictor",
    "title": "POLS 1600",
    "section": "m2: A numerical predictor",
    "text": "m2: A numerical predictor\n\nm2-Table-Figure-m2\n\n\n\\[\\text{m2: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest}+\\epsilon\\]\n\nm2 &lt;- lm(tc_diff ~ pol_interest, nes)\nround(coef(m2),2)\n\n (Intercept) pol_interest \n       40.80         6.01 \n\n\n\n\nhtmlreg(list(m1,m2))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n40.80***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.35)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n \n\n\n\n\n \n\n\n(1.81)\n\n\n \n\n\n\n\npol_interest\n\n\n \n\n\n6.01***\n\n\n\n\n \n\n\n \n\n\n(0.98)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.03\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.03\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df2 &lt;- expand_grid(\n  pol_interest = na.omit(sort(unique(nes$pol_interest)))\n)\n\npred_df2  &lt;- cbind(\n  pred_df2 ,\n  fit = predict(m2, pred_df2 )\n)\n\n# Produce figure\n\npred_df2 %&gt;% \n  ggplot(aes(pol_interest, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(pol_interest)),\n    aes(x= pol_interest,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_line(col=\"red\") +\n  geom_point(col=\"red\") -&gt; fig_tc_m2"
  },
  {
    "objectID": "slides/06-slides.html#m3-a-categorical-indicator",
    "href": "slides/06-slides.html#m3-a-categorical-indicator",
    "title": "POLS 1600",
    "section": "m3: A categorical indicator",
    "text": "m3: A categorical indicator\n\nm3 lm()-Table-Figure-m3-m2 vs m3\n\n\n\\[\\text{m3: tc_diff} = \\beta_0 + \\beta_1\\text{pol_interest_f} +\\epsilon\\]\n\nm3 &lt;- lm(tc_diff ~ pol_interest_f, nes)\nround(coef(m3),2)\n\n                      (Intercept) pol_interest_fNot very Interested \n                            46.10                              1.73 \npol_interest_fSomewhat Interested     pol_interest_fVery Interested \n                             2.14                             14.49 \n\nnes %&gt;% \n  group_by(pol_interest_f) %&gt;%\n  filter(!is.na(pol_interest_f)) %&gt;% \n  summarise(\n    mean = mean(tc_diff,na.rm = T),\n    beta = round(mean - coef(m3)[1],3)\n  )\n\n# A tibble: 4 × 3\n  pol_interest_f         mean  beta\n  &lt;fct&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n1 Not at all Interested  46.1  0   \n2 Not very Interested    47.8  1.73\n3 Somewhat Interested    48.2  2.14\n4 Very Interested        60.6 14.5 \n\n\n\n\nlm() converts the factor pol_interest_f into binary indicators for every value of pol_interest_f excluding “Not at all Interested”, the first level of the factor.\n“Not at all Interested” is the reference category because all the other coefficients describe how the means for other levels of pol_interest_f differ from “Not at all Interested”\n\ncbind(\nm3$model[26:30,],\nmodel.matrix(m3)[26:30,]\n)\n\n   tc_diff        pol_interest_f (Intercept) pol_interest_fNot very Interested\n26      96 Not at all Interested           1                                 0\n27      93   Somewhat Interested           1                                 0\n28      95       Very Interested           1                                 0\n29      75   Somewhat Interested           1                                 0\n30       1   Not very Interested           1                                 1\n   pol_interest_fSomewhat Interested pol_interest_fVery Interested\n26                                 0                             0\n27                                 1                             0\n28                                 0                             1\n29                                 1                             0\n30                                 0                             0\n\n\n\n\nhtmlreg(list(m1,m2,m3))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n40.80***\n\n\n46.10***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.35)\n\n\n(3.53)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n \n\n\n \n\n\n\n\n \n\n\n(1.81)\n\n\n \n\n\n \n\n\n\n\npol_interest\n\n\n \n\n\n6.01***\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.98)\n\n\n \n\n\n\n\npol_interest_fNot very Interested\n\n\n \n\n\n \n\n\n1.73\n\n\n\n\n \n\n\n \n\n\n \n\n\n(4.23)\n\n\n\n\npol_interest_fSomewhat Interested\n\n\n \n\n\n \n\n\n2.14\n\n\n\n\n \n\n\n \n\n\n \n\n\n(3.90)\n\n\n\n\npol_interest_fVery Interested\n\n\n \n\n\n \n\n\n14.49***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(3.76)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.03\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df3 &lt;- expand_grid(\n  pol_interest_f = na.omit(sort(unique(nes$pol_interest_f)))\n)\n\npred_df3  &lt;- cbind(\n  pred_df3 ,\n  fit = predict(m3, pred_df3 )\n)\n\n# Produce figure\n\npred_df3 %&gt;% \n  ggplot(aes(pol_interest_f, fit,\n             col = pol_interest_f))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(pol_interest_f)),\n    aes(x= pol_interest_f,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point() -&gt; fig_tc_m3"
  },
  {
    "objectID": "slides/06-slides.html#m4-a-binary-and-numerical-predictor",
    "href": "slides/06-slides.html#m4-a-binary-and-numerical-predictor",
    "title": "POLS 1600",
    "section": "m4: A binary and numerical predictor",
    "text": "m4: A binary and numerical predictor\n\nm4-Table-Figure-m4-m4 in 3d\n\n\n\\[\\text{m4: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} +\\epsilon\\]\n\nm4 &lt;- lm(tc_diff ~ interested + age, nes)\ncoef(m4)\n\n   (Intercept) interestedTRUE            age \n    32.1757944      9.7739375      0.3533876 \n\n\n\n\nhtmlreg(list(m1, m4))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n32.18***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.71)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n9.77***\n\n\n\n\n \n\n\n(1.81)\n\n\n(1.84)\n\n\n\n\nage\n\n\n \n\n\n0.35***\n\n\n\n\n \n\n\n \n\n\n(0.05)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.07\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.07\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df4 &lt;- expand_grid(\n  interested = c(F,T),\n  age = seq(min(nes$age, na.rm = T),\n            max(nes$age, na.rm=T),\n            length.out = 10)\n)\n\npred_df4  &lt;- cbind(\n  pred_df4 ,\n  fit = predict(m4, pred_df4 )\n)\n\n# Produce figure\n\npred_df4 %&gt;% \n  ggplot(aes(age, fit,\n             col = interested,\n             group = interested))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(interested)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() -&gt; fig_tc_m4"
  },
  {
    "objectID": "slides/06-slides.html#section-13",
    "href": "slides/06-slides.html#section-13",
    "title": "POLS 1600",
    "section": "",
    "text": "m5: An interaction between a binary and numerical predictor\n\nm5-Table-Figure-m5-m5 in 3d\n\n\n\\[\\text{m5: tc_diff} = \\beta_0 + \\beta_1\\text{interested} + \\beta_2\\text{age} + \\beta_3\\text{interested} \\times \\text{age}+\\epsilon\\]\n\nm5 &lt;- lm(tc_diff ~ interested*age, nes)\ncoef(m5)\n\n       (Intercept)     interestedTRUE                age interestedTRUE:age \n        39.7626421         -6.7040163          0.1822814          0.3396523 \n\n\n\n\nhtmlreg(list(m1, m4, m5))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\n(Intercept)\n\n\n47.84***\n\n\n32.18***\n\n\n39.76***\n\n\n\n\n \n\n\n(1.26)\n\n\n(2.71)\n\n\n(3.62)\n\n\n\n\ninterestedTRUE\n\n\n12.75***\n\n\n9.77***\n\n\n-6.70\n\n\n\n\n \n\n\n(1.81)\n\n\n(1.84)\n\n\n(5.55)\n\n\n\n\nage\n\n\n \n\n\n0.35***\n\n\n0.18*\n\n\n\n\n \n\n\n \n\n\n(0.05)\n\n\n(0.08)\n\n\n\n\ninterestedTRUE:age\n\n\n \n\n\n \n\n\n0.34**\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.11)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.07\n\n\n0.08\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.07\n\n\n0.08\n\n\n\n\nNum. obs.\n\n\n1168\n\n\n1168\n\n\n1168\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df5 &lt;- expand_grid(\n  interested = c(F,T),\n  age = seq(min(nes$age, na.rm = T),\n            max(nes$age, na.rm=T),\n            length.out = 10)\n)\n\npred_df5  &lt;- cbind(\n  pred_df5 ,\n  fit = predict(m5, pred_df5 )\n)\n\n# Produce figure\n\npred_df5 %&gt;% \n  ggplot(aes(age, fit,\n             col = interested,\n             group = interested))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(interested)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() -&gt; fig_tc_m5"
  },
  {
    "objectID": "slides/06-slides.html#section-14",
    "href": "slides/06-slides.html#section-14",
    "title": "POLS 1600",
    "section": "",
    "text": "m6: Two numerical predictors\n\nm6-Table-Figure-m6-m6 in 3d\n\n\n\\[\\text{m6: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} +\\epsilon\\]\n\nm6 &lt;- lm(tc_diff ~ age + income, nes)\ncoef(m6)\n\n(Intercept)         age      income \n 33.1994121   0.4002682   0.1573334 \n\n\n\n\nhtmlreg(list( m6))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n33.20***\n\n\n\n\n \n\n\n(3.35)\n\n\n\n\nage\n\n\n0.40***\n\n\n\n\n \n\n\n(0.06)\n\n\n\n\nincome\n\n\n0.16\n\n\n\n\n \n\n\n(0.29)\n\n\n\n\nR2\n\n\n0.04\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1049\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df6_age &lt;- expand_grid(\n  age = seq(min(nes$age, na.rm = T),\n            max(nes$age, na.rm=T),\n            length.out = 10),\n  # Hold income constant at mean value\n  income = mean(nes$income,na.rm=T)\n)\n\npred_df6_income &lt;- expand_grid(\n  income = seq(min(nes$income, na.rm = T),\n            max(nes$income, na.rm=T),\n            length.out = 16),\n  # Hold income constant at mean value\n  age = mean(nes$age,na.rm=T)\n)\n\npred_df6_age  &lt;- cbind(\n  pred_df6_age ,\n  fit = predict(m6, pred_df6_age )\n)\n\npred_df6_income  &lt;- cbind(\n  pred_df6_income ,\n  fit = predict(m6, pred_df6_income )\n)\n# Produce figure\n\npred_df6_age %&gt;% \n  ggplot(aes(age, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() +\n  labs(title =\"Age holding income constant\") -&gt; fig_tc_m6_age\n\npred_df6_income %&gt;% \n  ggplot(aes(income, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= income,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point()+\n  geom_line() +\n  labs(title = \"Income holding age constant\") -&gt; fig_tc_m6_income\n\n\nfig_tc_m6 &lt;- ggpubr::ggarrange(fig_tc_m6_age, fig_tc_m6_income)"
  },
  {
    "objectID": "slides/06-slides.html#section-15",
    "href": "slides/06-slides.html#section-15",
    "title": "POLS 1600",
    "section": "",
    "text": "m7: Interaction between two numerical predictors\n\nm7-Table-Figure-m7-m7 in 3d\n\n\n\\[\\text{m7: tc_diff} = \\beta_0 + \\beta_1\\text{age} + \\beta_2\\text{income} + \\beta_4\\text{age}\\times \\text{income}+\\epsilon \\]\n\nm7 &lt;- lm(tc_diff ~ age*income, nes)\ncoef(m7)\n\n(Intercept)         age      income  age:income \n38.22449671  0.29406374 -0.78888643  0.01993978 \n\n\n\n\nhtmlreg(list( m6, m7))\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n33.20***\n\n\n38.22***\n\n\n\n\n \n\n\n(3.35)\n\n\n(5.80)\n\n\n\n\nage\n\n\n0.40***\n\n\n0.29*\n\n\n\n\n \n\n\n(0.06)\n\n\n(0.12)\n\n\n\n\nincome\n\n\n0.16\n\n\n-0.79\n\n\n\n\n \n\n\n(0.29)\n\n\n(0.94)\n\n\n\n\nage:income\n\n\n \n\n\n0.02\n\n\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\nR2\n\n\n0.04\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.04\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n1049\n\n\n1049\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n# Create Prediction Data Frame\n\npred_df7 &lt;- expand_grid(\n  age = seq(19, 95,\n            by = 4),\n  # Hold income constant at mean value\n  income = seq(min(nes$income, na.rm = T),\n            max(nes$income, na.rm=T),\n            length.out = 16)\n)\n\n\n\npred_df7  &lt;- cbind(\n  pred_df7 ,\n  fit = predict(m7, pred_df7 )\n)\n\n# Produce figure\n\n# Marginal effect of age at min, median, and max values of income\npred_df7 %&gt;%\n  mutate(\n    income_at = case_when(\n      income == 1 ~ \"01\",\n      income == median(nes$income,na.rm=T) ~ \"05\",\n      income == 16 ~ \"16\",\n      T ~ NA_character_\n    )\n  ) %&gt;% \n  filter(!is.na(income_at)) %&gt;% \n  ggplot(aes(age, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= age,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point(aes(col =income_at,\n                 group = income_at))+\n  geom_line(aes(col =income_at,\n                group = income_at)\n            ) -&gt; fig_tc_m7_age\n\n# Marginal effect of income at min, median, and max values of age\npred_df7 %&gt;%\n  mutate(\n    age_at = case_when(\n      age == min(nes$age, na.rm=T) ~ \"19\",\n      age == 47 ~ \"47\", # close enough ...\n      age ==  max(nes$age, na.rm=T) ~ \"95\",\n      T ~ NA_character_\n    )\n  ) %&gt;% \n  filter(!is.na(age_at)) %&gt;% \n  ggplot(aes(income, fit,\n             ))+\n  # Add raw data\n  geom_point(\n    data = nes %&gt;% \n      filter(!is.na(income)) %&gt;%\n      filter(!is.na(age))\n      ,\n    aes(x= income,\n        y= tc_diff),\n    alpha = .1,\n    size = .2\n  ) +\n  geom_point(aes(col =age_at,\n                 group = age_at))+\n  geom_line(aes(col =age_at,\n                group = age_at)\n            ) -&gt; fig_tc_m7_income\n\n\nfig_tc_m7 &lt;- ggpubr::ggarrange(fig_tc_m7_age, \n                               fig_tc_m7_income,\n                               legend = \"bottom\")"
  },
  {
    "objectID": "slides/06-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/06-slides.html#motivating-example-what-causes-cholera",
    "title": "POLS 1600",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/06-slides.html#notation",
    "href": "slides/06-slides.html#notation",
    "title": "POLS 1600",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/06-slides.html#causal-effects",
    "href": "slides/06-slides.html#causal-effects",
    "title": "POLS 1600",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/06-slides.html#average-treatment-on-treated",
    "href": "slides/06-slides.html#average-treatment-on-treated",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/06-slides.html#average-treatment-on-treated-1",
    "href": "slides/06-slides.html#average-treatment-on-treated-1",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n\n\n\n\n\n\n\n\n\nPre-Period (T=0)\nPost-Period (T=1)\n\n\n\n\nTreated \\(D_{i}=1\\)\n\\(E[Y_{0i}(0)\\vert D_i = 1]\\)\n\\(E[Y_{1i}(1)\\vert D_i = 1]\\)\n\n\nControl \\(D_i=0\\)\n\\(E[Y_{0i}(0)\\vert D_i = 0]\\)\n\\(E[Y_{0i}(1)\\vert D_i = 0]\\)"
  },
  {
    "objectID": "slides/06-slides.html#data-1",
    "href": "slides/06-slides.html#data-1",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/06-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/06-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "POLS 1600",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/06-slides.html#before-vs-after-comparisons",
    "href": "slides/06-slides.html#before-vs-after-comparisons",
    "title": "POLS 1600",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854\n\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/06-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/06-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "POLS 1600",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V\n\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-1",
    "href": "slides/06-slides.html#difference-in-differences-1",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-2",
    "href": "slides/06-slides.html#difference-in-differences-2",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\] Is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/06-slides.html#difference-in-differences-3",
    "href": "slides/06-slides.html#difference-in-differences-3",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/06-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/06-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "POLS 1600",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V"
  },
  {
    "objectID": "slides/06-slides.html#parallel-trends",
    "href": "slides/06-slides.html#parallel-trends",
    "title": "POLS 1600",
    "section": "Parallel Trends",
    "text": "Parallel Trends"
  },
  {
    "objectID": "slides/06-slides.html#using-lm-to-estimate-diff-in-diff",
    "href": "slides/06-slides.html#using-lm-to-estimate-diff-in-diff",
    "title": "POLS 1600",
    "section": "Using lm to estimate Diff-in-Diff",
    "text": "Using lm to estimate Diff-in-Diff\n\n Data lm() Diff-in-Diff\n\n\n\ncholera_df &lt;- tibble(\n  Location = c(\"S&V\",\"Lambeth\",\"S&V\",\"Lambeth\"),\n  Treated = c(0,1,0, 1),\n  Time = c(0,0,1,1),\n  Deaths = c(135,85,147,19)\n)\ncholera_df\n\n# A tibble: 4 × 4\n  Location Treated  Time Deaths\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 S&V            0     0    135\n2 Lambeth        1     0     85\n3 S&V            0     1    147\n4 Lambeth        1     1     19\n\n\n\n\n\\[\n\\text{Deaths} = \\beta_0 + \\beta_1\\text{Treated} + \\beta_2\\text{Time} + \\beta_3 \\text{Treated}\\times\\text{Time}\n\\]\n\ndiff_in_diff &lt;- lm(Deaths ~ Treated + Time + Treated:Time, cholera_df)\ndiff_in_diff\n\n\nCall:\nlm(formula = Deaths ~ Treated + Time + Treated:Time, data = cholera_df)\n\nCoefficients:\n (Intercept)       Treated          Time  Treated:Time  \n         135           -50            12           -78  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_0=\\) Outcome in control (S&V) before treatment\n\\(\\beta_1=\\) Fixed, time invariant differences between treated and control\n\\(\\beta_2=\\) Fixed, unit invariant differences between pre and post periods\n\\(\\beta_3=\\) Difference-in-Differences = \\(E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\)"
  },
  {
    "objectID": "slides/06-slides.html#summary",
    "href": "slides/06-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\nDifferencing twice accounts for fixed differences across units and between periods\n\nBut not time varying differences across units…\n\nThe key identifying assumption of a DiD design is the assumption of parallel trends\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to test"
  },
  {
    "objectID": "slides/06-slides.html#extensions-and-limitations",
    "href": "slides/06-slides.html#extensions-and-limitations",
    "title": "POLS 1600",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nDiff-in-Diff easy to estimate with linear regression\nGeneralizes to multiple periods and treatment interventions\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\n\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/06-slides.html#applications",
    "href": "slides/06-slides.html#applications",
    "title": "POLS 1600",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?"
  },
  {
    "objectID": "slides/06-slides.html#summary---linear-regression",
    "href": "slides/06-slides.html#summary---linear-regression",
    "title": "POLS 1600",
    "section": "Summary - Linear Regression",
    "text": "Summary - Linear Regression\n\nLinear regression produces linear estimates of the Conditional Expectation Function\nWe estimate linear regression using lm()\n\n\n\nm1 &lt;- lm( y ~ x1 + x2 + x3, data = df)\n\n\n\nWe interpret linear regression by: looking at the sign, size, and, eventually, significance of coefficients\n\nthe intercept \\((\\beta_0)\\) corresponds to the model’s prediction when every other predictor is zero\nthe other \\(\\beta\\)s describe how \\(y\\) changes with a [unit change] in \\(x\\), controlling for other predictors in the model\n\nWe present our results using regression tables and figures showing predicted values"
  },
  {
    "objectID": "slides/06-slides.html#summary---difference-in-differences",
    "href": "slides/06-slides.html#summary---difference-in-differences",
    "title": "POLS 1600",
    "section": "Summary - Difference-in-Differences",
    "text": "Summary - Difference-in-Differences\n\nDifference-in-Differences (DiD) is a powerful research design for observational data that combines a pre-post comparisons with a treated and control comparisons\nDifferencing twice accounts for fixed differences across units and between periods\nDiD relies on an assumption of parallel trends\nWe can use linear regression to estimate and generalize DiD designs"
  },
  {
    "objectID": "slides/06-slides.html#references",
    "href": "slides/06-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/03-slides.html#class-plan",
    "href": "slides/03-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan\n\nCausal Inference\nNotation for Causal Inference\nCausal Identification\nCausal Identification in Experiments\nresume data from QSS\nExplore Broockman and Kalla (2016)"
  },
  {
    "objectID": "slides/03-slides.html#annoucements",
    "href": "slides/03-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nRevised Schedule\n\nLecture 3 today\nRead  Broockman and Kalla (2016) pdf\nLab 3 next week\nWeek 4 content spread out over weeks 5/6/7 condensed\nMore updates to come"
  },
  {
    "objectID": "slides/03-slides.html#group-assignments",
    "href": "slides/03-slides.html#group-assignments",
    "title": "POLS 1600",
    "section": "Group Assignments",
    "text": "Group Assignments"
  },
  {
    "objectID": "slides/03-slides.html#what-did-we-like",
    "href": "slides/03-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/03-slides.html#what-did-we-dislike",
    "href": "slides/03-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/03-slides.html#setup",
    "href": "slides/03-slides.html#setup",
    "title": "POLS 1600",
    "section": "Setup",
    "text": "Setup\nEvery time you work in R\n\nSave your file to your course or project folder\nSet your working directory\nLoad, and if needed, install packages\nMaybe change some global options in your .Rmd file"
  },
  {
    "objectID": "slides/03-slides.html#setting-your-working-directory",
    "href": "slides/03-slides.html#setting-your-working-directory",
    "title": "POLS 1600",
    "section": "Setting your working directory:",
    "text": "Setting your working directory:\n\nMy default code for setting a working directory is:\n\n\nThis is really just a reminder to someone else using my code that they need to have their working directories set up correctly\nR Studio sets the working directory automatically, when you knit the file\nWhen I work on a file, I set the working directory manually"
  },
  {
    "objectID": "slides/03-slides.html#setting-your-working-directory-when-working-live",
    "href": "slides/03-slides.html#setting-your-working-directory-when-working-live",
    "title": "POLS 1600",
    "section": "Setting your working directory when working “Live”",
    "text": "Setting your working directory when working “Live”"
  },
  {
    "objectID": "slides/03-slides.html#packages-for-today",
    "href": "slides/03-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\n  \n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \n  \"haven\", \"labelled\",\n  \n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \n  \"ggthemes\", \"ggpubr\", \"GGally\",\n  \"scales\", \"dagitty\", \"ggdag\", #&lt;&lt;\n  \n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\n  \"qss\" #&lt;&lt;\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\nkableExtra         DT  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally     scales \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n   dagitty      ggdag    COVID19       maps    mapdata        qss \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE"
  },
  {
    "objectID": "slides/03-slides.html#review-data-wrangling",
    "href": "slides/03-slides.html#review-data-wrangling",
    "title": "POLS 1600",
    "section": "Review: Data wrangling",
    "text": "Review: Data wrangling"
  },
  {
    "objectID": "slides/03-slides.html#data-transformations",
    "href": "slides/03-slides.html#data-transformations",
    "title": "POLS 1600",
    "section": "Data transformations",
    "text": "Data transformations\n\n\nYou want to:\n\nLoad some data\nCombine multiple functions\nLook at your data\nRecode your data\nTransform your data\n\n\n\nYou could use\n\nread_* functions\n%&gt;% the “pipe” operator\nglimpse() head(), filter(), select(), arrange()\nmutate(), case_when(), ifelse()\nsummarize(), group_by()"
  },
  {
    "objectID": "slides/03-slides.html#data-visualization",
    "href": "slides/03-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nThe grammar of graphics\nAt minimum you need:\n\ndata\naesthetic mappings\ngeometries\n\nHey Jude, make a sad plot and make it better by:\n\nlabels\nthemes\nstatistics\ncooridnates\nfacets\ntransforming your data before plotting"
  },
  {
    "objectID": "slides/03-slides.html#causal-claims-imply-counterfactual-comparisons",
    "href": "slides/03-slides.html#causal-claims-imply-counterfactual-comparisons",
    "title": "POLS 1600",
    "section": "Causal claims imply counterfactual comparisons",
    "text": "Causal claims imply counterfactual comparisons\n\n\n\nCausal claims imply claims about counterfactuals\nWhat would have happened if we were to change some aspect of the world?"
  },
  {
    "objectID": "slides/03-slides.html#whats-the-counter-factual-for-these-claims",
    "href": "slides/03-slides.html#whats-the-counter-factual-for-these-claims",
    "title": "POLS 1600",
    "section": "What’s the counter factual for these claims:",
    "text": "What’s the counter factual for these claims:\n\nForeign aid increases develop\nWikileaks cost Hillary Clinton the 2016 election\nDemocracies don’t fight wars with other democracies\nUniversal Pre-K improves child development"
  },
  {
    "objectID": "slides/03-slides.html#casual-claims-are-are-all-around-us",
    "href": "slides/03-slides.html#casual-claims-are-are-all-around-us",
    "title": "POLS 1600",
    "section": "Casual claims are are all around us",
    "text": "Casual claims are are all around us"
  },
  {
    "objectID": "slides/03-slides.html#casual-claims-are-are-all-around-us-1",
    "href": "slides/03-slides.html#casual-claims-are-are-all-around-us-1",
    "title": "POLS 1600",
    "section": "Casual claims are are all around us",
    "text": "Casual claims are are all around us\nWhat are some questions that interest you?\nWhat are the counterfactual comparisons they imply?"
  },
  {
    "objectID": "slides/03-slides.html#how-to-represent-causal-claims",
    "href": "slides/03-slides.html#how-to-represent-causal-claims",
    "title": "POLS 1600",
    "section": "How to represent causal claims",
    "text": "How to represent causal claims\nIn this course, we will use two forms of notation to describe our causal claims.\n\nDirected Acyclic Graphs (DAGs, next lecture)\nPotential Outcomes Notation"
  },
  {
    "objectID": "slides/03-slides.html#general-notation-variables",
    "href": "slides/03-slides.html#general-notation-variables",
    "title": "POLS 1600",
    "section": "General Notation: Variables",
    "text": "General Notation: Variables\n\nY our outcome of interest\nD an indicator of treatment status\n\nD=1 \\(\\to\\) treated\nD=0 \\(\\to\\) not treated (control)\n\nZ an of assignment status\n\nZ=1 \\(\\to\\) assigned to treatment\nZ=0 \\(\\to\\) assigned to control\n\nX a covariate or predictor we can measure/observe\nU unmeasured covariates"
  },
  {
    "objectID": "slides/03-slides.html#expected-values",
    "href": "slides/03-slides.html#expected-values",
    "title": "POLS 1600",
    "section": "Expected Values",
    "text": "Expected Values\n\n\nThe \\(E[Y]\\) reads as “the expected value of Y”\n\\(E[Y]\\) is defined as a probability weighted average based on the unconditional probability of Y ( \\(f(y)\\) )\n\n\\[\\operatorname{E}[Y] = \\int_{-\\infty}^\\infty y f(y)\\, dy\\]"
  },
  {
    "objectID": "slides/03-slides.html#conditional-expectations",
    "href": "slides/03-slides.html#conditional-expectations",
    "title": "POLS 1600",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\n\n\nThe \\(E[Y|X=x]\\) reads as “the expected value of Y conditional on the value of X”\n\\(E[Y|X=x]\\) is defined as a probability weighted average of Y based on the conditional probability of Y given X ( \\(y f_{Y|X}(y|x)\\) )\n\n\\[\\operatorname{E}[Y \\vert X=x] = \\int_{-\\infty}^\\infty y f (y\\vert x) \\, dy\\]"
  },
  {
    "objectID": "slides/03-slides.html#estimands-estimators-and-estimates",
    "href": "slides/03-slides.html#estimands-estimators-and-estimates",
    "title": "POLS 1600",
    "section": "Estimands, Estimators and Estimates",
    "text": "Estimands, Estimators and Estimates\n\nEstimand the thing we want to know.\n\nSometimes called a parameter (\\(\\theta\\), “theta”) or quantity of interest\nThe expected value of heights in POLS 1600 (\\(\\theta =E[X]\\))\n\nEstimator a rule or method for calculating an estimate of our estimand\n\nAn average of a sample of 10 student’s heights in POLS 1600\n\\(\\hat{\\theta} = \\bar{x} = 1/n*\\sum_1^{n} x_i\\)\n\nEstimate: a value produced by our estimator for some data\n\nThe average of our 10 person sample is 5'10''"
  },
  {
    "objectID": "slides/03-slides.html#error-and-bias",
    "href": "slides/03-slides.html#error-and-bias",
    "title": "POLS 1600",
    "section": "Error and Bias",
    "text": "Error and Bias\nWe’ll talk about lots of types of bias throughout this course.\nFormally, we’ll say an estimate, \\(\\hat{\\theta}\\) (“theta hat”) is an unbiased estimator of a parameter, \\(\\theta\\) (“theta”) if:\n\\[\nE[\\hat{\\theta}] = \\theta\n\\]\nBias or error, \\(\\epsilon\\), is the difference between our estimate and the truth\n\\[\n\\epsilon = \\hat{\\theta} -\\theta\n\\]\nAn estimator is unbiased if, on average, the errors equal 0\n\\[\nE[\\epsilon] = E[\\hat{\\theta} -\\theta] = 0\n\\]"
  },
  {
    "objectID": "slides/03-slides.html#bias-vs.-variance",
    "href": "slides/03-slides.html#bias-vs.-variance",
    "title": "POLS 1600",
    "section": "Bias vs. variance",
    "text": "Bias vs. variance"
  },
  {
    "objectID": "slides/03-slides.html#the-bias-variance-tradeoff",
    "href": "slides/03-slides.html#the-bias-variance-tradeoff",
    "title": "POLS 1600",
    "section": "The bias-variance tradeoff",
    "text": "The bias-variance tradeoff"
  },
  {
    "objectID": "slides/03-slides.html#potential-outcomes-notation",
    "href": "slides/03-slides.html#potential-outcomes-notation",
    "title": "POLS 1600",
    "section": "Potential outcomes notation",
    "text": "Potential outcomes notation\n\n\\(Y_i(1)\\) describes individual \\(i\\)’s outcome, \\(Y_i\\) if they received the treatment \\((D_i = 1)\\)\n\nShorthand for \\(Y_i(D_i=1)\\)\nPaul’s Covid-19 status (\\(Y_i\\)) with the vaccine (\\(D_i = 1\\))\n\n\\(Y_i(0)\\) describes individual \\(i\\)’s outcome, \\(Y_i\\) if they did not receive the treatment \\((D_i = 0)_i\\)\n\nShorthand for \\(Y_i(D_i=0)\\)\nPaul’s Covid-19 status (\\(Y_i\\)) without the vaccine (\\(D_i=0\\))\n\n\n\nThe treatment received determines which potential outcome we actually observe:\n\\[\nY_i = (1 - D_i)*Y_i(0) + D_i*Y_i(1)\n\\]\nPotential outcomes are fixed, but we only observe one (of many) potential outcomes \\(\\to\\) Fundamental Problem of Causal Inference"
  },
  {
    "objectID": "slides/03-slides.html#fundamental-problem-of-causal-inference",
    "href": "slides/03-slides.html#fundamental-problem-of-causal-inference",
    "title": "POLS 1600",
    "section": "Fundamental Problem of Causal Inference",
    "text": "Fundamental Problem of Causal Inference\nThe individual causal effect (ICE), \\(\\tau_i\\), is defined as\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\n\nThe fundamental problem of causal inference is that we only ever see one potential outcome for an individual, and so it’s impossible to know the causal effect of some intervention for that individual\nThe ICE is unidentified"
  },
  {
    "objectID": "slides/03-slides.html#identification",
    "href": "slides/03-slides.html#identification",
    "title": "POLS 1600",
    "section": "Identification",
    "text": "Identification\n\nIdentification refers to what we can learn from the data available\nA quantity of interest is identified if, with infinite data it can only take one value\nMathematically, we’ll sometimes say a coefficient in an equation is unidentified if\n\nWe have more predictors than observations, or\nSome of predictors are linear combinations of other predictors."
  },
  {
    "objectID": "slides/03-slides.html#causal-identification-1",
    "href": "slides/03-slides.html#causal-identification-1",
    "title": "POLS 1600",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCasual Identification refers to “the assumptions needed for statistical estimates to be given a causal interpretation” Keele (2015)\nWhat’s Your Casual Identification Strategy What are the assumptions that make your research design credible?\nIdentification &gt; Estimation"
  },
  {
    "objectID": "slides/03-slides.html#observational-vs-experimental-designs",
    "href": "slides/03-slides.html#observational-vs-experimental-designs",
    "title": "POLS 1600",
    "section": "Observational vs Experimental Designs",
    "text": "Observational vs Experimental Designs\n\nExperimental designs are studies in which a causal variable of interest, the treatement, is manipulated by the researcher to examine its causal effects on some outcome of interest\nObservational designs are studies in which a causal variable of interest is determined by someone/thing other than the researcher (nature, governments, people, etc.)"
  },
  {
    "objectID": "slides/03-slides.html#the-fpoci-is-a-problem-of-missing-data",
    "href": "slides/03-slides.html#the-fpoci-is-a-problem-of-missing-data",
    "title": "POLS 1600",
    "section": "The FPoCI is a problem of missing data",
    "text": "The FPoCI is a problem of missing data\nRecall that an individual causal effect \\(\\tau_i\\), is defined as:\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\nThe problem is that for any one individual, we only observe \\(Y_i(1)\\) or \\(Y_i(0)\\), but never both.\n\nIf Paul got the vaccine \\((Y_{Paul}(Vaxxed)=\\text{Covid Free})\\), then we don’t know what Paul’s health status would have been, had he not got the vaccine \\((Y_{Paul}(Unvaxxed) =???)\\)"
  },
  {
    "objectID": "slides/03-slides.html#a-statistical-solution-to-the-fpoci",
    "href": "slides/03-slides.html#a-statistical-solution-to-the-fpoci",
    "title": "POLS 1600",
    "section": "A statistical solution to the FPoCI",
    "text": "A statistical solution to the FPoCI\nRather than focus individual causal effects:\n\\[\n\\tau_i \\equiv Y_i(1) - Y_i(0)\n\\]\nWe focus on average causal effects (Average Treatment Effects [ATEs]):\n\\[\nE[\\tau_i] = \\overbrace{E[Y_i(1) - Y_i(0)]}^{\\text{Average of a difference}} = \\overbrace{E[Y_i(1)] - E[Y_i(0)]}^{\\text{Difference of Averages}}\n\\]\nWhen does the difference of averages provide us with a good estimate of the average difference?\nLet’s consider a simple example"
  },
  {
    "objectID": "slides/03-slides.html#does-eating-chocolate-make-you-happy",
    "href": "slides/03-slides.html#does-eating-chocolate-make-you-happy",
    "title": "POLS 1600",
    "section": "Does eating chocolate make you happy?",
    "text": "Does eating chocolate make you happy?\n\n\\(Y_i\\) happiness measured on a 0-10 scale\n\\(D_i\\) whether a person ate chocolate \\((D=1)\\) or fruit \\((D = 0)\\)\n\\(Y_i(1)\\) a person’s happiness eating chocolate\n\\(Y_i(0)\\) a person’s happiness eating fruit\n\\(X_i\\) a person’s self-reported preference \\((X_i \\in\\) {chocolate, fruit })"
  },
  {
    "objectID": "slides/03-slides.html#section",
    "href": "slides/03-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "Potential Outcomes:\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(\\tau_i\\)\n\n\n\n\n7\n3\n4\n\n\n8\n6\n2\n\n\n5\n4\n1\n\n\n4\n3\n1\n\n\n6\n10\n-4\n\n\n8\n9\n-1\n\n\n5\n4\n1\n\n\n7\n8\n-1\n\n\n4\n3\n1\n\n\n6\n0\n6\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i(1)]\\)\n\\(E[Y_i(0)]\\)\n\\(E[\\tau_i]\\)\n\n\n\n\n6\n5\n1\n\n\n\n\n\n\n\nIf we could observe everyone’s potential outcomes, we could calculate the ICE\nOn average eating chocolate increases happiness by 1 point on our 10-point scale (ATE = 1)\nSuppose we conducted a study and let folks select what they wanted to eat."
  },
  {
    "objectID": "slides/03-slides.html#section-1",
    "href": "slides/03-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Potential Outcomes:\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(\\tau_i\\)\n\n\n\n\n7\n3\n4\n\n\n8\n6\n2\n\n\n5\n4\n1\n\n\n4\n3\n1\n\n\n6\n10\n-4\n\n\n8\n9\n-1\n\n\n5\n4\n1\n\n\n7\n8\n-1\n\n\n4\n3\n1\n\n\n6\n0\n6\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i(1)]\\)\n\\(E[Y_i(0)]\\)\n\\(ATE\\)\n\n\n\n\n6\n5\n1\n\n\n\n\n\n\nObserved Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n1\n5\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n0\n9\n\n\nchocolate\n1\n5\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n1\n6\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n5.57\n9\n-3.43"
  },
  {
    "objectID": "slides/03-slides.html#section-2",
    "href": "slides/03-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Observed Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n1\n5\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n0\n9\n\n\nchocolate\n1\n5\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n1\n6\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n5.57\n9\n-3.43\n\n\n\n\n\n\nSelection Bias\n\nOur estimate of the ATE is biased by the fact that folks who prefer fruit seem to be happier than folks who prefer chocolate in this example\nIn general, selection bias occurs when folks who receive the treatment differ systematically from folks who don’t\nWhat if instead of letting people pick and choose, we randomly assigned half our respondents to chocolate and half to receive fruit"
  },
  {
    "objectID": "slides/03-slides.html#section-3",
    "href": "slides/03-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Potential Outcomes:\n\n\n\n\n\n\n\n\n\n\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(\\tau_i\\)\n\n\n\n\n7\n3\n4\n\n\n8\n6\n2\n\n\n5\n4\n1\n\n\n4\n3\n1\n\n\n6\n10\n-4\n\n\n8\n9\n-1\n\n\n5\n4\n1\n\n\n7\n8\n-1\n\n\n4\n3\n1\n\n\n6\n0\n6\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i(1)]\\)\n\\(E[Y_i(0)]\\)\n\\(ATE\\)\n\n\n\n\n6\n5\n1\n\n\n\n\n\n\nRandomly Assigned Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.2\n5.2\n1"
  },
  {
    "objectID": "slides/03-slides.html#section-4",
    "href": "slides/03-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Randomly Assigned Treatment:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.2\n5.2\n1\n\n\n\n\n\n\nRandom Assignment\n\nWhen treatment has been randomly assigned, a difference in sample means provides an unbiased estimate of the ATE\nThe fact that our \\(\\hat{ATE} = ATE\\) in this example is pure coincidence.\nIf we randomly assigned treatment a different way, we’d get a different estimate.\nIn general unbiased estimators will tend to be neither too high nor too low (e.g. \\(E[\\hat{\\theta} - \\theta] = 0\\)])"
  },
  {
    "objectID": "slides/03-slides.html#estimating-an-average-treatment-effect",
    "href": "slides/03-slides.html#estimating-an-average-treatment-effect",
    "title": "POLS 1600",
    "section": "Estimating an Average Treatment Effect",
    "text": "Estimating an Average Treatment Effect\nIf we treatment has been randomly assigned, we can estimate the ATE by taking the difference of means between treatment and control:\n\\[\n\\begin{align*}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{align*}\n\\]\nThat is, the ATE is causally identified by the difference of means estimator in an experimental design"
  },
  {
    "objectID": "slides/03-slides.html#section-5",
    "href": "slides/03-slides.html#section-5",
    "title": "POLS 1600",
    "section": "",
    "text": "Random Assignment 1\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n0\n8\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.2\n5.2\n1\n\n\n\n\n\n\nRandom Assignment 2\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n0\n3\n\n\nchocolate\n1\n8\n\n\nchocolate\n0\n4\n\n\nchocolate\n1\n4\n\n\nfruit\n1\n6\n\n\nfruit\n1\n8\n\n\nchocolate\n0\n4\n\n\nfruit\n1\n7\n\n\nchocolate\n0\n3\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n6.6\n2.8\n3.8\n\n\n\n\n\n\nRandom Assignment 3\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(d_i\\)\n\\(y_i\\)\n\n\n\n\nchocolate\n1\n7\n\n\nchocolate\n0\n6\n\n\nchocolate\n1\n5\n\n\nchocolate\n1\n4\n\n\nfruit\n0\n10\n\n\nfruit\n0\n9\n\n\nchocolate\n0\n4\n\n\nfruit\n1\n7\n\n\nchocolate\n1\n4\n\n\nchocolate\n0\n0\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{y}_{d=1}\\)\n\\(\\bar{y}_{d=0}\\)\n\\(\\hat{ATE}\\)\n\n\n\n\n5.4\n5.8\n-0.4"
  },
  {
    "objectID": "slides/03-slides.html#distribution-of-sample-ates",
    "href": "slides/03-slides.html#distribution-of-sample-ates",
    "title": "POLS 1600",
    "section": "Distribution of Sample ATEs",
    "text": "Distribution of Sample ATEs"
  },
  {
    "objectID": "slides/03-slides.html#why-random-assignment-matters",
    "href": "slides/03-slides.html#why-random-assignment-matters",
    "title": "POLS 1600",
    "section": "Why Random Assignment Matters?",
    "text": "Why Random Assignment Matters?\nFormally, randomly assigning treatments creates statistical independence \\((\\unicode{x2AEB})\\) between treatment ( \\(D\\) ) and potential outcomes ( \\(Y(1),Y(0)\\) ) as well as any observed ( \\(X\\) ) or unobserved confounders ( \\(U\\) ):\n\\[Y_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\\]\nPractically, what this means is that what we can observe ( differences in conditional means for treated and control ), provide good (unbiased) estimates of what we’re trying to learn about (Average Treatment Effects)"
  },
  {
    "objectID": "slides/03-slides.html#causal-identification-with-experimental-designs",
    "href": "slides/03-slides.html#causal-identification-with-experimental-designs",
    "title": "POLS 1600",
    "section": "Causal Identification with Experimental Designs",
    "text": "Causal Identification with Experimental Designs\nCausal identification for experimental designs requires very few assumptions:\n\nIndependence (Satisfied by Randomization)\n\n\\(Y(1), Y(0),X,U, \\perp D\\)\n\nSUTVA Stable Unit Treatment Value Assumption (Depends on features of the design)\n\nNo interference between units \\(Y_i(d_1, d_2, \\dots, d_N) = Y_i(d_i)\\)\nNo hidden values of the treatment/Variation in the treatment"
  },
  {
    "objectID": "slides/03-slides.html#random-assignment-creates-testable-implications",
    "href": "slides/03-slides.html#random-assignment-creates-testable-implications",
    "title": "POLS 1600",
    "section": "Random assignment creates testable implications",
    "text": "Random assignment creates testable implications\n\nIf treatment has been randomly assigned, we would expect treatment and control groups to look similar in terms of pre-treatment covariates\n\nWe can show covariate balance by comparing the means in each treatment group\n\nIf the treatment had an effect, than we can credibly claim that that effect was due to the presence or absence of the treatment, and not some alternative explanation.\nThis type of clean apples-to-apples counterfactual comparison is what people mean when they talk about an experimental ideal"
  },
  {
    "objectID": "slides/03-slides.html#no-causation-without-manipulation",
    "href": "slides/03-slides.html#no-causation-without-manipulation",
    "title": "POLS 1600",
    "section": "No Causation without Manipulation?",
    "text": "No Causation without Manipulation?\n\n“No causation without manipulation” - Holland (1986)\nCausal effects are well defined when we can imagine manipulating (changing) the value of \\(D_i\\) and only \\(D_i\\)\nBut what about the “effects” of things like:\n\nRace\nSex\nDemocracy\n\nStudying the effects of these factors requires strong theory and clever design Sen and Wasow (2016)"
  },
  {
    "objectID": "slides/03-slides.html#the-resume-experimento",
    "href": "slides/03-slides.html#the-resume-experimento",
    "title": "POLS 1600",
    "section": "The Resume Experimento",
    "text": "The Resume Experimento\nLet’s take a look at the resume experiment from your text book and compare some of Imai’s code to its tidyverse equivalent\n\n# make sure qss package is loaded\nlibrary(qss)\ndata(\"resume\")"
  },
  {
    "objectID": "slides/03-slides.html#high-level-overview-p.-34",
    "href": "slides/03-slides.html#high-level-overview-p.-34",
    "title": "POLS 1600",
    "section": "High level Overview (p. 34)",
    "text": "High level Overview (p. 34)\n\ndim(resume)\n\n[1] 4870    4\n\nhead(resume)\n\n  firstname    sex  race call\n1   Allison female white    0\n2   Kristen female white    0\n3   Lakisha female black    0\n4   Latonya female black    0\n5    Carrie female white    0\n6       Jay   male white    0"
  },
  {
    "objectID": "slides/03-slides.html#high-level-overview-p.-34-1",
    "href": "slides/03-slides.html#high-level-overview-p.-34-1",
    "title": "POLS 1600",
    "section": "High level Overview (p. 34)",
    "text": "High level Overview (p. 34)\n\nsummary(resume)\n\n  firstname             sex                race                call        \n Length:4870        Length:4870        Length:4870        Min.   :0.00000  \n Class :character   Class :character   Class :character   1st Qu.:0.00000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.00000  \n                                                          Mean   :0.08049  \n                                                          3rd Qu.:0.00000  \n                                                          Max.   :1.00000"
  },
  {
    "objectID": "slides/03-slides.html#crosstabs",
    "href": "slides/03-slides.html#crosstabs",
    "title": "POLS 1600",
    "section": "Crosstabs",
    "text": "Crosstabs\n\nrace.call.tab &lt;- table(race = resume$race,\n                       call = resume$call)\nrace.call.tab\n\n       call\nrace       0    1\n  black 2278  157\n  white 2200  235\n\naddmargins(race.call.tab)\n\n       call\nrace       0    1  Sum\n  black 2278  157 2435\n  white 2200  235 2435\n  Sum   4478  392 4870"
  },
  {
    "objectID": "slides/03-slides.html#tidy-crosstab",
    "href": "slides/03-slides.html#tidy-crosstab",
    "title": "POLS 1600",
    "section": "Tidy crosstab",
    "text": "Tidy crosstab\n\nresume %&gt;%\n  group_by(race, call)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  pivot_wider(names_from = call, values_from = n)\n\n# A tibble: 2 × 3\n# Groups:   race [2]\n  race    `0`   `1`\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 black  2278   157\n2 white  2200   235"
  },
  {
    "objectID": "slides/03-slides.html#calculating-call-back-rates",
    "href": "slides/03-slides.html#calculating-call-back-rates",
    "title": "POLS 1600",
    "section": "Calculating Call Back Rates",
    "text": "Calculating Call Back Rates\n\n# Overall\nsum(race.call.tab[,2])/nrow(resume)\n\n[1] 0.08049281\n\n# Black names\ncb_bl &lt;- sum(race.call.tab[1,2])/sum(race.call.tab[1,])\n# White Names\ncb_wh &lt;- sum(race.call.tab[2,2])/sum(race.call.tab[2,])\n\n# ATE\ncb_wh - cb_bl\n\n[1] 0.03203285"
  },
  {
    "objectID": "slides/03-slides.html#calculating-call-back-rates-with-group_by",
    "href": "slides/03-slides.html#calculating-call-back-rates-with-group_by",
    "title": "POLS 1600",
    "section": "Calculating Call Back Rates with group_by()",
    "text": "Calculating Call Back Rates with group_by()\n\nresume %&gt;%\n  group_by(race)%&gt;%\n  summarise(\n    call_back = mean(call)\n  )\n\n# A tibble: 2 × 2\n  race  call_back\n  &lt;chr&gt;     &lt;dbl&gt;\n1 black    0.0645\n2 white    0.0965"
  },
  {
    "objectID": "slides/03-slides.html#factor-variables-in-base-r",
    "href": "slides/03-slides.html#factor-variables-in-base-r",
    "title": "POLS 1600",
    "section": "Factor variables in Base R",
    "text": "Factor variables in Base R\n\nresume$type &lt;- NA\nresume$type[resume$race == \"black\" & resume$sex == \"female\"] &lt;- \"BlackFemale\"\nresume$type[resume$race == \"black\" & resume$sex == \"male\"] &lt;- \"BlackMale\"\nresume$type[resume$race == \"white\" & resume$sex == \"female\"] &lt;- \"WhiteFemale\"\nresume$type[resume$race == \"white\" & resume$sex == \"male\"] &lt;- \"WhiteMale\""
  },
  {
    "objectID": "slides/03-slides.html#factor-variables-in-tidy-r",
    "href": "slides/03-slides.html#factor-variables-in-tidy-r",
    "title": "POLS 1600",
    "section": "Factor variables in Tidy R",
    "text": "Factor variables in Tidy R\n\nresume %&gt;%\n  mutate(\n    type_tidy = case_when(\n      race == \"black\" & sex == \"female\" ~ \"BlackFemale\",\n      race == \"black\" & sex == \"male\" ~ \"BlackMale\",\n      race == \"white\" & sex == \"female\" ~ \"WhiteFemale\",\n      race == \"white\" & sex == \"male\" ~ \"WhiteMale\"\n    )\n  ) -&gt; resume"
  },
  {
    "objectID": "slides/03-slides.html#comparing-approaches",
    "href": "slides/03-slides.html#comparing-approaches",
    "title": "POLS 1600",
    "section": "Comparing approaches",
    "text": "Comparing approaches\n\ntable(base= resume$type, tidy= resume$type_tidy)\n\n             tidy\nbase          BlackFemale BlackMale WhiteFemale WhiteMale\n  BlackFemale        1886         0           0         0\n  BlackMale             0       549           0         0\n  WhiteFemale           0         0        1860         0\n  WhiteMale             0         0           0       575"
  },
  {
    "objectID": "slides/03-slides.html#visualizing-call-back-rates-by-name",
    "href": "slides/03-slides.html#visualizing-call-back-rates-by-name",
    "title": "POLS 1600",
    "section": "Visualizing Call Back Rates by Name",
    "text": "Visualizing Call Back Rates by Name\n\nCodeFigure\n\n\n\nresume %&gt;%\n  group_by(race, sex,firstname)%&gt;%\n  summarize(\n    Y = mean(call),\n    n = n()\n  )%&gt;%\n  arrange(desc(Y)) %&gt;%\n  mutate(\n    firstname = forcats::fct_reorder(firstname,Y)\n  )%&gt;%\n  ggplot(aes(Y, firstname,col=race, size=n))+\n  geom_point() + \n  facet_grid(sex~.,scales = \"free_y\")"
  },
  {
    "objectID": "slides/03-slides.html#reading-academic-papers",
    "href": "slides/03-slides.html#reading-academic-papers",
    "title": "POLS 1600",
    "section": "Reading Academic Papers",
    "text": "Reading Academic Papers\n\nReading academic papers is a skill and takes practice.\nYou should aim to answer the following:\n\nWhat’s the research question?\nWhat’s the theoretical framework?\nWhat’s the empirical design?\nWhat’s are the main results?"
  },
  {
    "objectID": "slides/03-slides.html#study-design-a-placebo-controlled-field-experiment",
    "href": "slides/03-slides.html#study-design-a-placebo-controlled-field-experiment",
    "title": "POLS 1600",
    "section": "Study Design :A placebo-controlled field experiment",
    "text": "Study Design :A placebo-controlled field experiment\n\nRecruited from voter files to complete a baseline survey\nAmong those who complete the survey, half are assigned to receive an intervention and half are assigned to receive a placebo\nOnly some are actually home or open the door when the canvassers knock.\nThese people are then recruited to participate in a series of surveys 3 days, 3 weeks, 6 weeks, and 3 months after the initial intervention."
  },
  {
    "objectID": "slides/03-slides.html#data-for-thursday",
    "href": "slides/03-slides.html#data-for-thursday",
    "title": "POLS 1600",
    "section": "Data for Thursday",
    "text": "Data for Thursday\nLet’s load the data from the orginal study\n\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))"
  },
  {
    "objectID": "slides/03-slides.html#codebook",
    "href": "slides/03-slides.html#codebook",
    "title": "POLS 1600",
    "section": "Codebook",
    "text": "Codebook\n\ncompleted_baseline whether someone completed the baseline survey (“Survey”) or not (“No Survey”)\ntreatment_assigned what intervention someone who completed the baseline survey was assigned two (treatment= “Trans-Equality”, placebo = “Recycling”)\nanswered_door whether someone answered the door (“Yes”) or not (“No”) when a canvasser came to their door\ntreatment_group the treatment assignments of those who answered the door (treatment= “Trans-Equality”, placebo = “Recycling”)\nvf_age the age of the person in years\nvf_female the respondent’s sex (female = 1, male = 0)\nvf_democrat whether the person was a registered Democract (Democrat=1, 0 otherwise)\nvf_white whether the person was white (White=1, 0 otherwise)\nvf_vg_12 whether the person voted in the 2012 general election (voted = 1, 0 otherwise)"
  },
  {
    "objectID": "slides/03-slides.html#hlo",
    "href": "slides/03-slides.html#hlo",
    "title": "POLS 1600",
    "section": "HLO",
    "text": "HLO\n\nglimpse(df)\n\nRows: 68,378\nColumns: 14\n$ completed_baseline &lt;chr&gt; \"No Survey\", \"No Survey\", \"No Survey\", \"No Survey\",…\n$ treatment_assigned &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ answered_door      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ treatment_group    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ vf_age             &lt;dbl&gt; 23.00000, 38.00000, 48.00000, 49.20192, 49.20192, 4…\n$ vf_female          &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, …\n$ vf_democrat        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ vf_white           &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ vf_vg_12           &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, …\n$ therm_trans_t0     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t1     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t2     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t3     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ therm_trans_t4     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…"
  },
  {
    "objectID": "slides/03-slides.html#study-design",
    "href": "slides/03-slides.html#study-design",
    "title": "POLS 1600",
    "section": "Study Design",
    "text": "Study Design\n\ntable(df$completed_baseline)\n\n\nNo Survey    Survey \n    66553      1825 \n\ntable(df$treatment_assigned)\n\n\n     Recycling Trans-Equality \n           913            912 \n\ntable(df$answered_door)\n\n\n  No  Yes \n1324  501 \n\ntable(df$treatment_group)\n\n\n     Recycling Trans-Equality \n           255            246"
  },
  {
    "objectID": "slides/03-slides.html#assessing-balance-in-covariates",
    "href": "slides/03-slides.html#assessing-balance-in-covariates",
    "title": "POLS 1600",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\ndf %&gt;%\n  filter(completed_baseline == \"Survey\") %&gt;%\n  select(treatment_assigned,\n         starts_with(\"vf_\"))%&gt;%\n  group_by(treatment_assigned)%&gt;%\n  summarise(\n    across(starts_with(\"vf_\"), mean)\n    ) -&gt; pretreatment_balance"
  },
  {
    "objectID": "slides/03-slides.html#assessing-balance-in-covariates-1",
    "href": "slides/03-slides.html#assessing-balance-in-covariates-1",
    "title": "POLS 1600",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\npretreatment_balance\n\n# A tibble: 2 × 6\n  treatment_assigned vf_age vf_female vf_democrat vf_white vf_vg_12\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Recycling            46.3     0.593       0.463    0.209    0.757\n2 Trans-Equality       47.7     0.582       0.488    0.217    0.719"
  },
  {
    "objectID": "slides/03-slides.html#assessing-balance-in-covariates-2",
    "href": "slides/03-slides.html#assessing-balance-in-covariates-2",
    "title": "POLS 1600",
    "section": "Assessing balance in covariates",
    "text": "Assessing balance in covariates\n\nCodeBalance\n\n\n\n# Rearrange data\npretreatment_balance %&gt;%\n  # Pivot columns except treatement assigned\n  pivot_longer(names_to = \"covariate\", values_to =  \"value\", -treatment_assigned) %&gt;%\n  # Pivot rows two two columns for treatment and placebo\n  pivot_wider(names_from = treatment_assigned) %&gt;%\n  # Calculate covariate balance\n  mutate(\n    Difference = `Trans-Equality` - Recycling\n  )\n\n\n\n\n\n# A tibble: 5 × 4\n  covariate   Recycling `Trans-Equality` Difference\n  &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 vf_age         46.3             47.7      1.40   \n2 vf_female       0.593            0.582   -0.0103 \n3 vf_democrat     0.463            0.488    0.0246 \n4 vf_white        0.209            0.217    0.00790\n5 vf_vg_12        0.757            0.719   -0.0375"
  },
  {
    "objectID": "slides/03-slides.html#summary-1",
    "href": "slides/03-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nCausal Claims involve counterfactual comparisons\nThe fundamental problem of causal inference is that for an individual only observe one of many potential outcomes\nCausal identification refers to the assumptions necessary to generate credible causal estimates\nIdentification for experimental designs follows from the random assignment of treatment which allows us to produce unbiased estimates of the Average Treatment Effect"
  },
  {
    "objectID": "slides/03-slides.html#references",
    "href": "slides/03-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nBroockman, David, and Joshua Kalla. 2016. “Durably reducing transphobia: A field experiment on door-to-door canvassing.” Science 352 (6282): 220–24."
  },
  {
    "objectID": "slides/10-slides.html#general-plan",
    "href": "slides/10-slides.html#general-plan",
    "title": "Week 10:",
    "section": "General Plan",
    "text": "General Plan\n\nCourse Plan\nSetup\nFeedback\nReview\nConfidence Intervals\n\nbackground-image: url(“https://i.kym-cdn.com/entries/icons/original/000/037/873/We’re_All_Trying_To_Find_The_Guy_Who_Did_This_banner_1.jpg”) background-size:contain"
  },
  {
    "objectID": "slides/10-slides.html#two-options",
    "href": "slides/10-slides.html#two-options",
    "title": "Week 10:",
    "section": "Two Options:",
    "text": "Two Options:\n\nProceed with group projects with condensed schedule/assignments\n\n–\n\nReplace group projects with a take home (open book/notes) final exam\n\nPosted April 30\nDue May 7\nMix of theory, concepts, and coding."
  },
  {
    "objectID": "slides/10-slides.html#course-plan-option-1",
    "href": "slides/10-slides.html#course-plan-option-1",
    "title": "Week 10:",
    "section": "Course Plan: Option 1",
    "text": "Course Plan: Option 1\n\nApril 13: No Class, Review Feedback to A2\nApril 18: Lecture – Hypothesis Testing\nApril 20 Workshop on Paper – Inference About Models: Counts as Assignment 3\nApril 25: Lecture – Course Review\nApril 27: Workshop: Paper drafts and Presentations\nApril 30: Upload Presentations\nMay 2: Class Presentations Part 1\nMay 4: Class Presentations Part 2\nMay ?: Tacos?"
  },
  {
    "objectID": "slides/10-slides.html#course-plan-option-2",
    "href": "slides/10-slides.html#course-plan-option-2",
    "title": "Week 10:",
    "section": "Course Plan: Option 2",
    "text": "Course Plan: Option 2\n\nApril 13: No Class,\nApril 18: Lecture – Hypothesis Testing\nApril 20 Lab – Hypothesis Testing and Interval Estimation\nApril 25: Lecture – Course Review\nApril 27: Workshop:\nApril 30: Take Home Final Exam\nMay ?: Tacos or Pizza with POLS 1140?\nMay 7: Take Home Final Exam due"
  },
  {
    "objectID": "slides/10-slides.html#what-do-we-want-to-do",
    "href": "slides/10-slides.html#what-do-we-want-to-do",
    "title": "Week 10:",
    "section": "What do we want to do?",
    "text": "What do we want to do?\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/10-slides.html#packages-for-today",
    "href": "slides/10-slides.html#packages-for-today",
    "title": "Week 10:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\",# &lt;&lt;\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\", \"boot\",\"purrr\"\n)"
  },
  {
    "objectID": "slides/10-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/10-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 10:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/10-slides.html#load-packages-for-today",
    "href": "slides/10-slides.html#load-packages-for-today",
    "title": "Week 10:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled        modelr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign           zoo          boot         purrr \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today\nWe’ll use data from last week’s lab to\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\nclass:inverse, middle, center # 🔍 ## Review: Generalized Linear Models"
  },
  {
    "objectID": "slides/10-slides.html#generalized-linear-models",
    "href": "slides/10-slides.html#generalized-linear-models",
    "title": "Week 10:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nIn last week’s lab we fit two models\n\nOLS\nLogistic regression\n\n\n# OLS\nm1 &lt;- lm(support_war01 ~ age + sex + education_n, df_drww)\n\n# Logisitic \nm2 &lt;- glm(support_war01 ~ age + sex + education_n, df_drww,\n          family = binomial)"
  },
  {
    "objectID": "slides/10-slides.html#generalized-linear-model",
    "href": "slides/10-slides.html#generalized-linear-model",
    "title": "Week 10:",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\n\nLogisitic regression is a type of generalized linear model used to model binary outcomes\nWe estimate logistic regression using Maximum Likelihood, which allows us to model outcomes using different probability distributions\nOther common generalized linear models\n\nProbit regression (binary outcomes)\nPoisson regression (count data)\nNegative binomial regression (count data)\n\nIt’s still “regression”, but interpretation typically requires transforming predictions (inverting the link function)\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.28***\n\n\n-1.36***\n\n\n\n\n \n\n\n(0.05)\n\n\n(0.29)\n\n\n\n\nage\n\n\n0.01***\n\n\n0.05***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\nsexMale\n\n\n0.09***\n\n\n0.50***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.13)\n\n\n\n\neducation_n\n\n\n-0.02\n\n\n-0.10\n\n\n\n\n \n\n\n(0.01)\n\n\n(0.06)\n\n\n\n\nR2\n\n\n0.11\n\n\n \n\n\n\n\nAdj. R2\n\n\n0.11\n\n\n \n\n\n\n\nNum. obs.\n\n\n1463\n\n\n1463\n\n\n\n\nAIC\n\n\n \n\n\n1575.54\n\n\n\n\nBIC\n\n\n \n\n\n1596.69\n\n\n\n\nLog Likelihood\n\n\n \n\n\n-783.77\n\n\n\n\nDeviance\n\n\n \n\n\n1567.54\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/10-slides.html#prediction-data-frame",
    "href": "slides/10-slides.html#prediction-data-frame",
    "title": "Week 10:",
    "section": "Prediction Data Frame",
    "text": "Prediction Data Frame\n\npred_df &lt;- expand_grid(\n  age = 18 : 99,\n  sex = \"Female\",\n  education_n = mean(df_drww$education_n, na.rm = T)\n)"
  },
  {
    "objectID": "slides/10-slides.html#predicted-values",
    "href": "slides/10-slides.html#predicted-values",
    "title": "Week 10:",
    "section": "Predicted Values",
    "text": "Predicted Values\n\n# #Predicted values for m1\npred_df$pred_ols &lt;- predict(m1,\n                            newdata = pred_df)\n# Predicted values for m2\n# Remember to add type = \"response\"\npred_df$pred_logit &lt;- predict(m2,\n                            newdata = pred_df,\n                            type = \"response\")\n\n\n# data\npred_df%&gt;%\n  # aesthetics\n  ggplot(aes(age, pred_ols, col = \"OLS\"))+\n  # geometries\n  geom_line()+\n  geom_line(aes(y = pred_logit, col = \"Logistic\"))+\n  geom_jitter(data=df_drww, aes(age, support_war01),\n              col = \"black\",\n              height = .05,\n              size = .5,\n              alpha = .5)+\n  labs(\n    col = \"Model\",\n    x = \"Age\",\n    y = \"Predicted Values\"\n  )\n\n\nclass: inverse, center, middle # 💡 # Confidence Intervals ## The Basics"
  },
  {
    "objectID": "slides/10-slides.html#overview",
    "href": "slides/10-slides.html#overview",
    "title": "Week 10:",
    "section": "Overview:",
    "text": "Overview:\n\nConfidence intervals provide a way of quantifying uncertainty about estimates\nConfidence intervals describe a range of plausible values\nThat range is a function of the standard error of the estimate, and the a critical value determined \\(\\alpha\\), which describes the degree of confidence we want\n\nA 95% confidence interval corresponds to an \\(\\alpha\\) of 0.05\n\nA standard error is the standard deviation of the sampling distribution of our estimate\nWe can obtain the sampling distribution via:\n\nsimulation (bootstrapping)\nasymptotic theory (the CLT)\n\nOur confidence is about the interval, not the estimate."
  },
  {
    "objectID": "slides/10-slides.html#defintions-populations-and-samples",
    "href": "slides/10-slides.html#defintions-populations-and-samples",
    "title": "Week 10:",
    "section": "Defintions: Populations and Samples",
    "text": "Defintions: Populations and Samples\n\nPopulation: All the cases from which you could have sampled\nParameter: A quantity or quantities of interest often generically called \\(\\theta\\) (“theta”). Something we’d like to know about our population\nSample: A (random) draw from that population\nSample Size: The number of observations in your draw (without replacement)"
  },
  {
    "objectID": "slides/10-slides.html#defintions-estimators-estimates-and-statistics",
    "href": "slides/10-slides.html#defintions-estimators-estimates-and-statistics",
    "title": "Week 10:",
    "section": "Defintions: Estimators, Estimates, and Statistics",
    "text": "Defintions: Estimators, Estimates, and Statistics\n\nEstimator: A rule for calculating an estimate of our parameter of interest.\nEstimate: The value produced by some estimator for some parameter from some data. Often called \\(\\hat{\\theta}\\)\nUnbiased estimators: \\(E(\\hat{\\theta})=E(\\theta)\\) On average, the estimates produced by some estimator will be centered around the truth\nConsistent estimates: \\(\\lim_{n\\to \\infty} \\hat{\\theta_N} = \\theta\\) As the sample size increases, the estimates from an estimator converge in probability to the parameter value\nStatistic: A summary of the data (mean, regression coefficient, \\(R^2\\)). An estimator without a specified target of inference"
  },
  {
    "objectID": "slides/10-slides.html#definitions-distrubtions-and-standard-errors",
    "href": "slides/10-slides.html#definitions-distrubtions-and-standard-errors",
    "title": "Week 10:",
    "section": "Definitions: Distrubtions and Standard Errors",
    "text": "Definitions: Distrubtions and Standard Errors\n\nSampling Distribution: How some estimate would vary if you took repeated samples from the population\nStandard Error: The standard deviation of the sampling distribution\nResampling Distribution: How some estimate would vary if you took repeated samples from your sample WITH REPLACEMENT\n\n“Sampling from our sample, as the sample was sampled from the population.”"
  },
  {
    "objectID": "slides/10-slides.html#confidence-intervals-interpretation",
    "href": "slides/10-slides.html#confidence-intervals-interpretation",
    "title": "Week 10:",
    "section": "Confidence Intervals: Interpretation",
    "text": "Confidence Intervals: Interpretation\n\nConfidence intervals give a range of values that are likely to include the true value of the parameter \\(\\theta\\) with probability \\((1-\\alpha) \\times 100\\%\\)\n\n\\(\\alpha = 0.05\\) corresponds to a “95-percent confidence interval”\n\nOur “confidence” is about the interval\nIn repeated sampling, we expect that \\((1-\\alpha) \\times 100\\%\\) of the intervals we construct would contain the truth.\nFor any one interval, the truth, \\(\\theta\\), either falls within in the lower and upper bounds of the interval or it does not."
  },
  {
    "objectID": "slides/10-slides.html#two-approaches-to-calculating-confidence-intervals",
    "href": "slides/10-slides.html#two-approaches-to-calculating-confidence-intervals",
    "title": "Week 10:",
    "section": "Two Approaches to Calculating Confidence Intervals:",
    "text": "Two Approaches to Calculating Confidence Intervals:\nIn general, there are two ways to calculate confidence intervals:\n\nSimulation: Use our computers to simulate the idea of repeated sampling (e.g. bootstrapping)\n\nFlexible, but more computationally intensive\n\nAsymptotic Theory: Use math to derive the properties of the distributions that would arise under repeated sampling\n\nFaster, but requires more assumptions that may not hold\n\n\nWe will consider both.\n\nThe theory of CIs is easier to illustrate via simulation\nThe practice of calculating CIs is (generally) easier using asymptotic theory"
  },
  {
    "objectID": "slides/10-slides.html#steps-to-calculating-a-confidence-interval",
    "href": "slides/10-slides.html#steps-to-calculating-a-confidence-interval",
    "title": "Week 10:",
    "section": "Steps to Calculating a Confidence Interval",
    "text": "Steps to Calculating a Confidence Interval\nFrom QSS (p. 330)\n\nChoose the desired level of confidence \\((1-\\alpha)\\times 100%\\) by specifying a value of α between 0 and 1: the most common choice is= \\(\\alpha = 0.05\\), which gives a 95% confidence level.\nDerive the sampling distribution of the estimator by computing its mean and variance.\nCompute the standard error based on this sampling distribution. (square root of the variance)\nCompute the critical value \\(z_{\\alpha/2}\\) as the \\((1-\\alpha)\\times 100\\) percentile value of the standard normal distribution\nCompute the lower and upper confidence limits as \\(\\hat{\\theta} - z_{\\alpha/2}\\times SE\\) and \\(\\hat{\\theta} + z_{\\alpha/2}\\times SE\\) standard error, respectively.\n\nclass: inverse, center, middle # 💡 # Confidence Intervals ## Simulating the Sampling Distribution through Bootstrapping"
  },
  {
    "objectID": "slides/10-slides.html#populations",
    "href": "slides/10-slides.html#populations",
    "title": "Week 10:",
    "section": "Populations",
    "text": "Populations\nLet’s load the data from the Do Russians Want War survey\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\nTo understand the logic of confidence intervals, let’s treat this data as our population from which we we could draw repeated samples."
  },
  {
    "objectID": "slides/10-slides.html#population-age",
    "href": "slides/10-slides.html#population-age",
    "title": "Week 10:",
    "section": "Population Age",
    "text": "Population Age\nIn our population, there are parameters, true values of things we want to know.\nSuppose we’re interested in the average age of our population.\nIn our population, the true value of \\(\\mu_{age} = E[Age]\\) is\n\nmu_age &lt;- mean(df_drww$age)\nmu_age\n\n[1] 46.64693\n\n\nSimilarly, the true \\(\\sigma_{age} = \\sqrt{E[Age^2] - E[Age]^2}\\)\n\nsd_age &lt;- sqrt(mean((df_drww$age-mean(df_drww$age))^2))\nsd_age\n\n[1] 15.81829"
  },
  {
    "objectID": "slides/10-slides.html#distribution-population-age",
    "href": "slides/10-slides.html#distribution-population-age",
    "title": "Week 10:",
    "section": "Distribution Population Age",
    "text": "Distribution Population Age\n\np_pop &lt;- df_drww %&gt;%\n  ggplot(aes(age))+\n  geom_density(col=\"grey\")+\n  geom_rug()+\n  geom_vline(\n    aes(xintercept = mu_age, \n             col = \"Population Mean\"),\n    linetype=2)+\n  theme_bw()+\n  labs(color = \"Age\")\n\np_pop"
  },
  {
    "objectID": "slides/10-slides.html#sample-estimates-of-average-age-n-25",
    "href": "slides/10-slides.html#sample-estimates-of-average-age-n-25",
    "title": "Week 10:",
    "section": "Sample Estimates of Average Age (N = 25)",
    "text": "Sample Estimates of Average Age (N = 25)\nSuppose we took three samples, without replacement of size 25, and calculated the average age in each sample:\n\nset.seed(123)\nmean_age1 &lt;- mean(sample(df_drww$age, 25, replace = F))\nmean_age2 &lt;- mean(sample(df_drww$age, 25, replace = F))\nmean_age3 &lt;- mean(sample(df_drww$age, 25, replace = F))\n\nmean_age1\n\n[1] 43.36\n\nmean_age2\n\n[1] 39.36\n\nmean_age3\n\n[1] 49.72"
  },
  {
    "objectID": "slides/10-slides.html#repeated-sampling",
    "href": "slides/10-slides.html#repeated-sampling",
    "title": "Week 10:",
    "section": "Repeated Sampling",
    "text": "Repeated Sampling\n\nImagine we could draw a 1,000 or 10,000 or an infinite number of samples of size N=25 from our population.\nHow much would our estimate of the average of age of the population vary?\nLet’s use our computers to simulate this process and find out!"
  },
  {
    "objectID": "slides/10-slides.html#simualting-repeated-sampling",
    "href": "slides/10-slides.html#simualting-repeated-sampling",
    "title": "Week 10:",
    "section": "Simualting Repeated Sampling",
    "text": "Simualting Repeated Sampling\n\nn_sims &lt;- 1000\nsamp_size &lt;- 25\nset.seed(123)\n\nmu_age_samp_dist_n25 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Sampling\",\n  sample = \"Population\"\n) %&gt;%\n  mutate(\n    samp = purrr::map(sim, ~ slice_sample(df_drww, n = samp_size, replace = F)),\n    estimate = purrr::map_dbl(samp, ~ mean(.$age))\n  )\n\n\nmu_age_samp_dist_n25\n\n# A tibble: 1,000 × 5\n     sim distribution sample     samp           estimate\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;list&gt;            &lt;dbl&gt;\n 1     1 Sampling     Population &lt;df [25 × 42]&gt;     43.4\n 2     2 Sampling     Population &lt;df [25 × 42]&gt;     39.4\n 3     3 Sampling     Population &lt;df [25 × 42]&gt;     49.7\n 4     4 Sampling     Population &lt;df [25 × 42]&gt;     46.2\n 5     5 Sampling     Population &lt;df [25 × 42]&gt;     50.6\n 6     6 Sampling     Population &lt;df [25 × 42]&gt;     47.8\n 7     7 Sampling     Population &lt;df [25 × 42]&gt;     46.3\n 8     8 Sampling     Population &lt;df [25 × 42]&gt;     41.4\n 9     9 Sampling     Population &lt;df [25 × 42]&gt;     45.4\n10    10 Sampling     Population &lt;df [25 × 42]&gt;     48.8\n# ℹ 990 more rows"
  },
  {
    "objectID": "slides/10-slides.html#standard-errors",
    "href": "slides/10-slides.html#standard-errors",
    "title": "Week 10:",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nA standard error is simply the standard deviation of the sampling distribution.\nThe standard error for our simulation above:\n\n\nse_age_n25 &lt;- sd(mu_age_samp_dist_n25$estimate)\nse_age_n25\n\n[1] 3.146543"
  },
  {
    "objectID": "slides/10-slides.html#coverage-intervals",
    "href": "slides/10-slides.html#coverage-intervals",
    "title": "Week 10:",
    "section": "Coverage Intervals",
    "text": "Coverage Intervals\n\nFrom the Central Limit Theorem, we know that the distribution of sample means will converge to a normal distribution.\nFrom probability theory, we know that we that roughly 95 percent of the values in a normal distribution fall between Two Standard Deviations of the mean.\n\n\nci_age_ul_n25 &lt;- mu_age + 2*se_age_n25\nci_age_ll_n25 &lt;- mu_age - 2*se_age_n25\n\nmean(mu_age_samp_dist_n25$estimate &gt;ci_age_ll_n25 & \n       mu_age_samp_dist_n25$estimate &lt;ci_age_ul_n25)\n\n[1] 0.954\n\n\n\nmu_age_samp_dist_n25 %&gt;%\n  ggplot(aes(estimate))+\n  geom_density()+\n  geom_rug(\n    aes(col = estimate &gt;ci_age_ll_n25 & \n          estimate &lt;ci_age_ul_n25)\n  )+\n  geom_vline(xintercept = mu_age,\n             col = \"red\",\n             linetype=2)+\n  guides(col=\"none\")+\n  geom_segment(aes(x=ci_age_ll_n25,\n                   xend = ci_age_ul_n25,\n                   y = .15,yend = .15 ),\n               col = \"#00BFC4\")+\n  theme_bw()"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors",
    "href": "slides/10-slides.html#boostrapped-standard-errors",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nA standard error is the standard deviation of a hypothetical sampling distribution\nHow do we calculate a standard error from a single sample?\nIt turns out that a random sample provides unbiased estimates of both the population mean and the standard deviation of the of the sampling distribution (i.e. the standard error).\nWe can estimate this this standard error, by sampling with replacement from our sample to generate a bootstrapped sampling distribution"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-1",
    "href": "slides/10-slides.html#boostrapped-standard-errors-1",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nset.seed(123)\nbs_resamp_1 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 1\",\n) %&gt;%\n  mutate(\n    samp = purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[1]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-2",
    "href": "slides/10-slides.html#boostrapped-standard-errors-2",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nbs_resamp_2 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 2\",\n) %&gt;%\n  mutate(\n    samp =  purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[2]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-3",
    "href": "slides/10-slides.html#boostrapped-standard-errors-3",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nbs_resamp_3 &lt;- tibble(\n  sim = 1:n_sims,\n  distribution = \"Bootstrap\",\n  sample = \"Sample 3\",\n) %&gt;%\n  mutate(\n    samp =  purrr::map(sim, ~ slice_sample(\n      mu_age_samp_dist_n25$samp[[3]], n = samp_size, replace = T)),\n    estimate =  purrr::map_dbl(samp, ~ mean(.$age))\n  )"
  },
  {
    "objectID": "slides/10-slides.html#boostrapped-standard-errors-4",
    "href": "slides/10-slides.html#boostrapped-standard-errors-4",
    "title": "Week 10:",
    "section": "Boostrapped Standard Errors",
    "text": "Boostrapped Standard Errors\n\nbs_example &lt;- rbind(\n  mu_age_samp_dist_n25,\n  bs_resamp_1,\n  bs_resamp_2,\n  bs_resamp_3\n)\n\ndf_se &lt;- bs_example %&gt;%\n  select(sample, estimate)%&gt;%\n  dplyr::group_by(sample)%&gt;%\n  dplyr::summarise(\n    se = sd(estimate)\n  )\n\n\ndf_ci &lt;- df_mn %&gt;%\n  left_join(df_se)\n\ndf_ci%&gt;%\n  mutate(\n    ll = xint - qt(df=25,.975)*se,\n    ul = xint + qt(df=25,.975)*se,\n    y = .15,\n    xint_pop = xint[1]\n  ) -&gt; df_ci\n\n\nbs_example %&gt;%\n  ggplot(aes(estimate,col = sample))+\n  geom_density(aes(linetype=distribution))+\n  facet_wrap(~sample, ncol=1)+\n   geom_vline(\n    data = df_ci,\n    aes(xintercept = xint, \n             col = sample,\n        linetype=distribution)\n    )+\n    geom_vline(\n    data = df_ci,\n    aes(xintercept = xint_pop), \n             col = \"black\",\n        linetype=2)+\n  geom_segment(\n    data = df_ci,\n    aes(x =ll, xend =ul, y=y, yend=y)\n  )\n\n\n\n\n\n\n\n\n\n\n\nsim_ci_fn&lt;-function(x,\n                    samp_size=100,\n                    n_sims=1000,\n                    level=.95,\n                    bs=F){\n    # Take a sample of size \"nsamp\"\n    y&lt;-sample(x=na.omit(x),size=samp_size,replace=F)\n    # Calculate the mean\n    mu&lt;-mean(y,na.rm=T)\n    # If bs=TRUE do bootstrapped SEs \n    if(bs==T){\n    mu_dist&lt;-rerun(\n      n_sims,\n      mean(sample(y, samp_size, replace = T)))%&gt;%\n      unlist()\n    se&lt;-sd(mu_dist)}else{\n    # Otherwise, just use assymptotic result (Quicker)\n    se&lt;-sd(y,na.rm=T)/sqrt(samp_size-1)\n    }\n    # Significance level\n    the.p&lt;-1-(1-level)/2\n    # Calculate lower and upper limits of interval\n    ll&lt;-mu-qt(p=the.p,df=samp_size-1)*se\n    ul&lt;-mu+qt(p=the.p,df=samp_size-1)*se\n    results&lt;-tibble(mu=mu,ll=ll,ul=ul,se=se)\n    return(results)\n}\n\n\nset.seed(12345)\nsamp25 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 25)) %&gt;%dplyr::mutate(sample = 1:n() )\nsamp50 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 50))%&gt;%dplyr::mutate(sample = 1:n() )\nsamp100 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 100))%&gt;%dplyr::mutate(sample = 1:n() )\nsamp200 &lt;-  purrr::map_df(1:1000, ~sim_ci_fn(df_drww$age, samp_size = 200))%&gt;%dplyr::mutate(sample = 1:n() )"
  },
  {
    "objectID": "slides/10-slides.html#standard-errors-and-sample-size",
    "href": "slides/10-slides.html#standard-errors-and-sample-size",
    "title": "Week 10:",
    "section": "Standard Errors and Sample Size",
    "text": "Standard Errors and Sample Size\n\n# Standard errors decrease as sample size increases\nc(mean(samp25$se),\nmean(samp50$se),\nmean(samp100$se),\nmean(samp200$se))\n\n[1] 3.193029 2.247602 1.588096 1.121755\n\n# Specifically, by the square root of the sample size\nc(sd_age/sqrt(25),\nsd_age/sqrt(50),\nsd_age/sqrt(100),\nsd_age/sqrt(200))\n\n[1] 3.163659 2.237045 1.581829 1.118522"
  },
  {
    "objectID": "slides/10-slides.html#next-week-standard-errors-for-linear-models",
    "href": "slides/10-slides.html#next-week-standard-errors-for-linear-models",
    "title": "Week 10:",
    "section": "Next Week: Standard Errors for Linear Models",
    "text": "Next Week: Standard Errors for Linear Models\n\nAs you saw in your lab, we can apply the same principles to calculate standard errors for other quantities like the coefficients from a regression\nNext week, we’ll compare these quantities to those obtained from asymptotic theory, and then turn to an alternative approach to quantifying uncertainty: Hypothesis testing.\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/13-slides.html#general-plan",
    "href": "slides/13-slides.html#general-plan",
    "title": "Week 13:",
    "section": "General Plan",
    "text": "General Plan\n\nCourse Summary\nCourse Feedback\nAMA\nPaper Workshop\n\nclass: bottom, center background-image: url(“https://www.powerthesaurus.org/_images/terms/summery-synonyms-2.png”) bacground-size: contain"
  },
  {
    "objectID": "slides/13-slides.html#what-was-this-course-about",
    "href": "slides/13-slides.html#what-was-this-course-about",
    "title": "Week 13:",
    "section": "What was this course about?",
    "text": "What was this course about?\n–\n\nHow would you know?\n\n–\n\nHow would convince someone who thinks they know different?\n\n–\n\nWhat would it take to change what you think you know?"
  },
  {
    "objectID": "slides/13-slides.html#why-quantitative-social-science",
    "href": "slides/13-slides.html#why-quantitative-social-science",
    "title": "Week 13:",
    "section": "Why Quantitative Social Science?",
    "text": "Why Quantitative Social Science?\nWhat makes for quantitative social science\n\nCompelling?\nUseful?\nHard?\n\n–\nStatistical methods and programming are tools not answers\n–\nProducing knowledge requires us to match theory with empirical design\n–\nBeing “right” is less important than understanding all the ways you might be wrong.\n–\nGood social science is urgent, eclectic, transparent, contingent, and ?"
  },
  {
    "objectID": "slides/13-slides.html#what-youve-learned",
    "href": "slides/13-slides.html#what-youve-learned",
    "title": "Week 13:",
    "section": "What you’ve learned",
    "text": "What you’ve learned\nSkills and concepts to help understand and practice quantitative social science\n\nCausal Inference is about counterfactual comparisons\n\nExperimental designs \\(\\to\\) random assignment\nObservational design \\(\\to\\) identifying assumptions \\(\\to\\) conditional independence\n\nRegression is a tool for describing relationships\n\nOLS provides a linear approximation of the Conditional Expectation Function\nRegression can be used for descriptive, predictive, and causal inference.\n\nStatistical inference is about quantifying uncertainty about what could have happened\n\nConfidence intervals provide a range of plausible values for we observed\nHypothesis tests describe the conditional probability of observing what we did, if some hypothesis were true.\n\n\nclass:inverse, bottom, center background-image: url(“https://i.imgflip.com/6f1aul.jpg”) bacground-size: cover # 📢 ## Feedback"
  },
  {
    "objectID": "slides/13-slides.html#what-we-liked",
    "href": "slides/13-slides.html#what-we-liked",
    "title": "Week 13:",
    "section": "What we liked",
    "text": "What we liked"
  },
  {
    "objectID": "slides/13-slides.html#what-we-disliked",
    "href": "slides/13-slides.html#what-we-disliked",
    "title": "Week 13:",
    "section": "What we disliked",
    "text": "What we disliked"
  },
  {
    "objectID": "slides/13-slides.html#what-we-learned",
    "href": "slides/13-slides.html#what-we-learned",
    "title": "Week 13:",
    "section": "What we learned",
    "text": "What we learned"
  },
  {
    "objectID": "slides/13-slides.html#what-were-still-learning",
    "href": "slides/13-slides.html#what-were-still-learning",
    "title": "Week 13:",
    "section": "What we’re still learning",
    "text": "What we’re still learning"
  },
  {
    "objectID": "slides/13-slides.html#howd-i-do",
    "href": "slides/13-slides.html#howd-i-do",
    "title": "Week 13:",
    "section": "How’d I do?",
    "text": "How’d I do?\n.pull-left[\n]\n–\n.pull-right[\n\n]"
  },
  {
    "objectID": "slides/13-slides.html#howd-you-do",
    "href": "slides/13-slides.html#howd-you-do",
    "title": "Week 13:",
    "section": "How’d you do?",
    "text": "How’d you do?"
  },
  {
    "objectID": "slides/13-slides.html#are-you-happy-you-took-pols-1600",
    "href": "slides/13-slides.html#are-you-happy-you-took-pols-1600",
    "title": "Week 13:",
    "section": "Are you happy you took POLS 1600?",
    "text": "Are you happy you took POLS 1600?"
  },
  {
    "objectID": "slides/13-slides.html#what-ill-do",
    "href": "slides/13-slides.html#what-ill-do",
    "title": "Week 13:",
    "section": "What I’ll Do",
    "text": "What I’ll Do\n\nLectures:\n\nLess is more\nHow much math…\nMore supplemental content (class notes)\nMore active learning\nIntegrating the textbook\n\n\n–\n\nLabs:\n\nSeemed to work ok\nGroup work vs individual learning?\nGreater integration with textbook and lecture\n\n\n–\n\nAssignments and Grading\n\nMostly liked the group project\nTutorials were helpful\nMore individualized assignments/accountability?\n\n\nclass:inverse, middle, center # 🔍 ## AMA"
  },
  {
    "objectID": "slides/13-slides.html#ask-you-anthing-ok-boomer",
    "href": "slides/13-slides.html#ask-you-anthing-ok-boomer",
    "title": "Week 13:",
    "section": "Ask you anthing? Ok boomer…",
    "text": "Ask you anthing? Ok boomer…\n\nsum(df$ama == \"\")\n\n[1] 7\n\n\n\n“how many cats do you have”\n\n–\nWe’ve had three cats, Isla, Abby and, currently Toby.\n\n“What is the craziest moment from your time in undergrad?”\n\n–\n\n\n\n\n\n\n\n\n\n\n\n\n“I am interested to know why you decided to go to graduate school, what other career paths you were deciding between, and whether you believe you made the right decision to go into academia and why?”\n\n–\n\n\n\n\n\n\n\n\n\n\n“What’s your favorite musical”\n\n–\n\n\n\n\n\n\n\n\n\n\n“If we want to, how should we develop our R skills by ourselves at Brown?\n\n\nBest book you ever read?\n\n\nBook you think someone in their early 20s should read?”"
  },
  {
    "objectID": "slides/07-slides.html#class-plan",
    "href": "slides/07-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements (2-3 min)\nSetup (2-3 min)\nFeedback (15 min)\nTopics:\n\nWhat does it mean to control for X\nHow to make predictions with regression\nEvaluating model fit\nDifference-in-Differences\nSet up for Lab 7"
  },
  {
    "objectID": "slides/07-slides.html#goals",
    "href": "slides/07-slides.html#goals",
    "title": "POLS 1600",
    "section": "Goals",
    "text": "Goals\n\nRegression models partition variation in an outcome into variation explained by the model and not explained by the model\nIndividual regression coefficients reflect the variation explained by that predictor, and only that predictor\nPredicted values for regression models aid in substantive interpretation\nMeasures of model fit like \\(R^2\\) can be useful for comparing different regression models\nDifference-in-differences designs combine pre-post and treatment-control comparisons to make stronger causal claims."
  },
  {
    "objectID": "slides/07-slides.html#annoucements",
    "href": "slides/07-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nAssignment 1: Research Questions due Tuesday, March 12\n\nFeedback by class on Thursday\n\nAssignment 2 Data: due Friday March 22"
  },
  {
    "objectID": "slides/07-slides.html#setup-packages-for-today",
    "href": "slides/07-slides.html#setup-packages-for-today",
    "title": "POLS 1600",
    "section": "Setup: Packages for today",
    "text": "Setup: Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "slides/07-slides.html#feedback",
    "href": "slides/07-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/07-slides.html#what-did-we-like",
    "href": "slides/07-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/07-slides.html#what-did-we-dislike",
    "href": "slides/07-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/07-slides.html#what-were-good-at",
    "href": "slides/07-slides.html#what-were-good-at",
    "title": "POLS 1600",
    "section": "What we’re good at",
    "text": "What we’re good at"
  },
  {
    "objectID": "slides/07-slides.html#what-were-working-on",
    "href": "slides/07-slides.html#what-were-working-on",
    "title": "POLS 1600",
    "section": "What we’re working on",
    "text": "What we’re working on"
  },
  {
    "objectID": "slides/07-slides.html#how-are-we-doing",
    "href": "slides/07-slides.html#how-are-we-doing",
    "title": "POLS 1600",
    "section": "How are we doing?",
    "text": "How are we doing?"
  },
  {
    "objectID": "slides/07-slides.html#dont-trust-the-polls",
    "href": "slides/07-slides.html#dont-trust-the-polls",
    "title": "POLS 1600",
    "section": "Don’t trust the polls",
    "text": "Don’t trust the polls"
  },
  {
    "objectID": "slides/07-slides.html#what-should-we-do-going-forward",
    "href": "slides/07-slides.html#what-should-we-do-going-forward",
    "title": "POLS 1600",
    "section": "What should we do going forward?",
    "text": "What should we do going forward?\n\nFrom me:\n\nShorter labs\nShorter slides\nMore coding, less copy-pasting\n\nFrom you?"
  },
  {
    "objectID": "slides/07-slides.html#regression-models-partition-variance",
    "href": "slides/07-slides.html#regression-models-partition-variance",
    "title": "POLS 1600",
    "section": "Regression models partition variance",
    "text": "Regression models partition variance\nRegression models partition variance, separating the variation in the outcome into variation explained by the predictors in our model and the remaining variation not explained by these predictors\n\\[\\begin{aligned}\n\\textrm{Total Variance} &= \\textrm{Variance Explained by Model} + \\textrm{Unexplained Variance} \\\\\n\\textrm{Observed} &= \\textrm{Predicted Value} + \\textrm{Error}\\\\\n\\textrm{Y} &=  E[Y|X] + \\epsilon\\\\\n\\textrm{Y} &=  X\\hat{\\beta} + \\hat{\\epsilon}\\\\\n\\textrm{Y} &= \\hat{Y} + \\hat{\\epsilon}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/07-slides.html#section-1",
    "href": "slides/07-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Coefficients describe the unique variance in Y explained by X (and only X)\n\nTask\n\n\nThe coefficients in a regression model describe the variation in the outcome explained by that predictor, and only that predictor.\nLet’s fit three models from last week’s lab and look at how the coefficients change from model to model\n\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\nm1 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)\nm2 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, covid_lab)\nm3 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)\n\n\n\n\nhtmlreg(list(m1, m2, m3)) %&gt;% HTML() %&gt;% browsable()\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n0.07\n\n\n \n(0.05)\n(0.05)\n(0.07)\n\n\nmed_age_std\n \n0.03\n-0.02\n\n\n \n \n(0.05)\n(0.05)\n\n\nmed_income_std\n \n \n-0.22**\n\n\n \n \n \n(0.07)\n\n\nR2\n0.31\n0.31\n0.44\n\n\nAdj. R2\n0.29\n0.28\n0.40\n\n\nNum. obs.\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#why-do-coefficients-change-when-we-control-for-variables",
    "href": "slides/07-slides.html#why-do-coefficients-change-when-we-control-for-variables",
    "title": "POLS 1600",
    "section": "Why do coefficients change when we control for variables?",
    "text": "Why do coefficients change when we control for variables?"
  },
  {
    "objectID": "slides/07-slides.html#residualized-regression",
    "href": "slides/07-slides.html#residualized-regression",
    "title": "POLS 1600",
    "section": "Residualized Regression",
    "text": "Residualized Regression\nResidualized regression is way of understanding what it means to control for variables in a regression.\nResidualized regression provides a way of illustrating what we mean when say the coefficients describe the unique variance in Y explained by some predictor \\(x\\) (and only \\(x\\))"
  },
  {
    "objectID": "slides/07-slides.html#whats-a-residual",
    "href": "slides/07-slides.html#whats-a-residual",
    "title": "POLS 1600",
    "section": "What’s a residual",
    "text": "What’s a residual\n\nResiduals represent the part of the outcome variable, not explained by the predictors in a model\n\nDifference between the observed \\(y\\) and the predicted \\(\\hat{y}\\)\n\n\n\\[y = \\overbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2  + \\dots \\beta_j x_j}^{\\text{Predictors}} + \\underbrace{\\epsilon}_{\\text{Residuals}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#residuals-are-uncorrelated-with-x-and-haty",
    "href": "slides/07-slides.html#residuals-are-uncorrelated-with-x-and-haty",
    "title": "POLS 1600",
    "section": "Residuals are uncorrelated with \\(X\\) and \\(\\hat{y}\\)",
    "text": "Residuals are uncorrelated with \\(X\\) and \\(\\hat{y}\\)\n\n\nResiduals are uncorrelated with (orthogonal to) the predictors \\(X\\), and predicted values \\(X\\beta\\)\n\n# Trust but verify\ncor(resid(m2),covid_lab$rep_voteshare_std) \n\n[1] 3.600777e-17\n\ncor(resid(m2),covid_lab$med_age_std)\n\n[1] 7.021125e-17\n\ncor(resid(m2),fitted(m2))\n\n[1] -1.036606e-17"
  },
  {
    "objectID": "slides/07-slides.html#section-4",
    "href": "slides/07-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Residualized Regression\n\n m2 m3\n\n\n\nFor a model like m2 we can recover the coefficient on rep_voteshare_std by:\n\nRegressing new_deaths_pc_14day on med_age_std to get the residual variation in Covid-19 deaths not explained by median age\nRegressing rep_voteshare_std on med_age_std to get the residual variation in Republican Vote Share not explained by median age\nRegressing the residuals from 1. (Deaths not explained by age) on the residuals from 2. (Vote share not explained by age) to obtain the same coefficient from m2 for rep_voteshare_std\n\nThe same principle holds for m3\n\n\n\n\n# 1. Regressing `new_deaths_pc_14da` on `med_age_std`\nm2_death_by_age &lt;- lm(new_deaths_pc_14day ~ med_age_std, covid_lab)\n# Save residuals\ncovid_lab$res_death_no_age &lt;- resid(m2_death_by_age)\n\n# 2. Regressing `rep_voteshare_std` on `med_age_std` \nm2_repvs_by_age &lt;- lm(rep_voteshare_std ~ med_age_std, covid_lab)\n# Save residuals\ncovid_lab$res_repvs_no_age &lt;- resid(m2_repvs_by_age)\n\n# 3. Residualized regression of deaths on Rep Vote Share\nm2_res &lt;- lm(res_death_no_age ~ res_repvs_no_age, covid_lab)\n\n# Mutliple regression\ncoef(m2)[2]\n\nrep_voteshare_std \n         0.230745 \n\n# Residualized regression\ncoef(m2_res)[2]\n\nres_repvs_no_age \n        0.230745 \n\n\n\n\n\n# 1. Regressing `new_deaths_pc_14da` on `med_age_std` and med_income_std\nm3_death_by_age_income &lt;- lm(new_deaths_pc_14day ~ med_age_std + med_income_std, covid_lab)\n# Save residuals\ncovid_lab$res_death_no_age_income &lt;- resid(m3_death_by_age_income)\n\n# 2. Regressing `rep_voteshare_std` on `med_age_std` and med_income_std\nm3_repvs_by_age_income &lt;- lm(rep_voteshare_std ~ med_age_std + med_income_std, covid_lab)\n# Save residuals\ncovid_lab$res_repvs_no_age_income &lt;- resid(m3_repvs_by_age_income)\n\n# 3. Residualized regression of deaths on Rep Vote Share\nm3_res &lt;- lm(res_death_no_age_income ~ res_repvs_no_age_income, covid_lab)\n\n# multiple regression coefficient\ncoef(m3)[2]\n\nrep_voteshare_std \n       0.07140446 \n\n# Same as  residualized regression coefficient\ncoef(m3_res)[2]\n\nres_repvs_no_age_income \n             0.07140446"
  },
  {
    "objectID": "slides/07-slides.html#why-did-the-coefficient-on-rep-vote-share-change-in-m3-but-not-m2",
    "href": "slides/07-slides.html#why-did-the-coefficient-on-rep-vote-share-change-in-m3-but-not-m2",
    "title": "POLS 1600",
    "section": "Why did the coefficient on Rep Vote Share change in m3 but not m2?",
    "text": "Why did the coefficient on Rep Vote Share change in m3 but not m2?"
  },
  {
    "objectID": "slides/07-slides.html#section-7",
    "href": "slides/07-slides.html#section-7",
    "title": "POLS 1600",
    "section": "",
    "text": "Statistical models\n\n\n \nDV: Death\n\n\n \nBaseline\n\n\n\n\n(Intercept)\n0.57***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#section-8",
    "href": "slides/07-slides.html#section-8",
    "title": "POLS 1600",
    "section": "",
    "text": "\\[\n\\text{Covid-19 Deaths} = \\beta_0 + \\beta_1 \\text{Rep Vote Share}\n\\]"
  },
  {
    "objectID": "slides/07-slides.html#section-9",
    "href": "slides/07-slides.html#section-9",
    "title": "POLS 1600",
    "section": "",
    "text": "Statistical models\n\n\n \nDV: Death\nDV: Vote Share\nDV: Res. Deaths\n\n\n \nBaseline\nMutliple\nDeaths\nVote Share\nDeaths\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n-0.00\n-0.00\n\n\n \n(0.05)\n(0.05)\n(0.06)\n(0.14)\n(0.05)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n \n \n \n\n\n \n(0.05)\n(0.05)\n \n \n \n\n\nmed_age_std\n \n0.03\n0.00\n-0.12\n \n\n\n \n \n(0.05)\n(0.06)\n(0.14)\n \n\n\nres_repvs_no_age\n \n \n \n \n0.23***\n\n\n \n \n \n \n \n(0.05)\n\n\nR2\n0.31\n0.31\n0.00\n0.02\n0.31\n\n\nAdj. R2\n0.29\n0.28\n-0.02\n-0.00\n0.30\n\n\nNum. obs.\n51\n51\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#section-10",
    "href": "slides/07-slides.html#section-10",
    "title": "POLS 1600",
    "section": "",
    "text": "\\[\n\\text{Deaths} = \\beta_0 + \\beta_1 \\text{Rep VS} + \\beta_2 \\text{Age}\n\\]\n\n\\(\\beta_1\\) doesn’t change because age has no relationship to deaths in these data"
  },
  {
    "objectID": "slides/07-slides.html#section-11",
    "href": "slides/07-slides.html#section-11",
    "title": "POLS 1600",
    "section": "",
    "text": "Statistical models\n\n\n \nDV: Death\nDV: Vote Share\nDV: Res. Death\n\n\n \nBaseline\nMutliple\nDeaths\nVote Share\nDeaths\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n-0.00\n0.00\n\n\n \n(0.05)\n(0.04)\n(0.04)\n(0.10)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.07\n \n \n \n\n\n \n(0.05)\n(0.07)\n \n \n \n\n\nmed_age_std\n \n-0.02\n-0.03\n-0.22*\n \n\n\n \n \n(0.05)\n(0.05)\n(0.10)\n \n\n\nmed_income_std\n \n-0.22**\n-0.27***\n-0.74***\n \n\n\n \n \n(0.07)\n(0.05)\n(0.10)\n \n\n\nres_repvs_no_age_income\n \n \n \n \n0.07\n\n\n \n \n \n \n \n(0.07)\n\n\nR2\n0.31\n0.44\n0.43\n0.55\n0.02\n\n\nAdj. R2\n0.29\n0.40\n0.40\n0.53\n0.00\n\n\nNum. obs.\n51\n51\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/07-slides.html#section-12",
    "href": "slides/07-slides.html#section-12",
    "title": "POLS 1600",
    "section": "",
    "text": "\\[\n\\text{Deaths} = \\beta_0 + \\beta_1 \\text{Rep VS} + \\beta_2 \\text{Age} + \\beta_3 \\text{Income}\n\\]\n\n\\(\\beta_1\\) decreases because after controlling for income there is less unique variation explained only by republican vote share"
  },
  {
    "objectID": "slides/07-slides.html#using-regression-to-produce-predicted-values",
    "href": "slides/07-slides.html#using-regression-to-produce-predicted-values",
    "title": "POLS 1600",
    "section": "Using regression to produce predicted values",
    "text": "Using regression to produce predicted values\nCoefficients in a regression define a formula which produces a predicted value of the outcome \\(y\\) when the predictors \\(X\\) take particular values.\n\\[\n\\begin{aligned}y &= \\overbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2  + \\dots \\beta_j x_j}^{\\text{Predictors}} + \\underbrace{\\epsilon}_{\\text{Residuals}} &\\\\\ny &= \\beta_0 + \\beta_1x_{rvs} + \\beta_2x_{age}+ \\beta_3x_{inc} + \\epsilon & \\text{m3}\\\\\ny &= 0.56 + 0.07x_{rvs} - 0.02 x_{age} - 0.22 x_{inc} + \\hat{\\epsilon} & \\text{estimated m3}\\\\\ny &= 0.56 + 0.07(-0.87) - 0.02(0.62) - 0.22(0.38) + \\hat{\\epsilon} & \\text{prediction for RI} \\\\\n\\overbrace{0.22}^{\\text{Observed}} &= \\underbrace{0.41}_{\\text{Predicted}} + \\overbrace{(-0.19)}^{\\text{Residual}} &\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-slides.html#producing-predicted-values-in-r",
    "href": "slides/07-slides.html#producing-predicted-values-in-r",
    "title": "POLS 1600",
    "section": "Producing Predicted Values in R",
    "text": "Producing Predicted Values in R\nThe basic steps to producing predicted values in R as follows:\n\nFit a model using lm()\nCreate a prediction data frame using expand_grid():\n\nvary the values of the predictor you’re interested in\nhold all the other predictors in your model constant at some typical value.\n\nInput the model from lm() and the prediction data frame, into the predict() function to obtain predicted values.\n\nSave predictions as a new column in your prediction data frame (I generally call them fit)\n\nPlot predicted values in your prediction data frame to help interpret your model"
  },
  {
    "objectID": "slides/07-slides.html#section-13",
    "href": "slides/07-slides.html#section-13",
    "title": "POLS 1600",
    "section": "",
    "text": "Are there decreasing returns to vaccination?\n\nTask m4 Table Predict Fig\n\n\nSuppose we thought the marginal effect – (here, predicted change in deaths from a 1 percent increase in the percent of the population vaccinated) of vaccines varied.\nThere might be large gains from going to low to average rates of vaccination, but after a certain threshold, the decreases in deaths would taper off.\nWe could test this by including a polynomial term I(percent_vaccinated)^2 in our model.\nIncluding a polynomial term, allows the marginal effect to vary, based on the value of the predictor.\nIt’s hard to interpret the coefficients on polynomial terms (or interaction terms) just by looking at coefficients in a table\nInstead, we’ll produce a plot of predicted values to test these claims\n\n\n\nm4 &lt;- lm(new_deaths_pc_14day ~ percent_vaccinated + I(percent_vaccinated^2) + rep_voteshare_std + med_age_std + med_income_std, covid_lab\n           )\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n6.194**\n\n\n\n\n \n\n\n(2.186)\n\n\n\n\npercent_vaccinated\n\n\n-0.169*\n\n\n\n\n \n\n\n(0.077)\n\n\n\n\npercent_vaccinated^2\n\n\n0.001\n\n\n\n\n \n\n\n(0.001)\n\n\n\n\nrep_voteshare_std\n\n\n-0.062\n\n\n\n\n \n\n\n(0.081)\n\n\n\n\nmed_age_std\n\n\n0.053\n\n\n\n\n \n\n\n(0.053)\n\n\n\n\nmed_income_std\n\n\n-0.114\n\n\n\n\n \n\n\n(0.068)\n\n\n\n\nR2\n\n\n0.561\n\n\n\n\nAdj. R2\n\n\n0.512\n\n\n\n\nNum. obs.\n\n\n51\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\npred_df &lt;- expand_grid(\n  percent_vaccinated = sort(unique(covid_lab$percent_vaccinated)),\n  # Set standardized predictors to their means of 0\n  rep_voteshare_std = 0,\n  med_age_std = 0,\n  med_income_std = 0\n)\n\npred_df$fit &lt;- predict(m4, newdata = pred_df)\n\npred_df %&gt;% \n  ggplot(aes(percent_vaccinated, fit))+\n  geom_line()+\n  labs(\n    y = \"Predicted Covid-19 Deaths\\n(per capita, 14-day average)\",\n    x = \"Percent of State's population that's Vaccinated\"\n  ) + \n  theme_minimal() -&gt; fig_m4\n\n\n\nFor a typical state, early increases in vaccination rate are associated with larger declines in predicted deaths from Covid-19"
  },
  {
    "objectID": "slides/07-slides.html#evaluating-model-fit-1",
    "href": "slides/07-slides.html#evaluating-model-fit-1",
    "title": "POLS 1600",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\nModels partition variance. We can summarize the overall fit of our model using measures like \\(R^2\\)\n\\[R^2 = \\frac{\\text{variance(predicted values )}}{ \\text{variance(observed values )}}\\]"
  },
  {
    "objectID": "slides/07-slides.html#r2",
    "href": "slides/07-slides.html#r2",
    "title": "POLS 1600",
    "section": "R^2",
    "text": "R^2\nMore formally, you’ll see \\(R^2\\) defined in terms of “Sums of Squares”\n\nTSS = Total Sum of Squares = Variance of the Outcome\nESS = Explained Sum of Squares = Variance of the Predicted Values\nRSS = Sum of Squared Residuals = Variance of the Residuals\n\n\\[R^2 = \\frac{ESS}{TSS}= 1 - \\frac{RSS}{TSS}\\]"
  },
  {
    "objectID": "slides/07-slides.html#calculating-r2-in-r",
    "href": "slides/07-slides.html#calculating-r2-in-r",
    "title": "POLS 1600",
    "section": "Calculating \\(R^2\\) in R",
    "text": "Calculating \\(R^2\\) in R\nWe could do it by hand, finding that our model explained about 43 percent of the observed variation deaths.\n\n# ESS / TSS\nvar(m3$fitted.values)/var(m3$model$new_deaths_pc_14day)\n\n[1] 0.4393655\n\n# 1 - RSS/TSS\n1 - var(m3$residuals)/var(m3$model$new_deaths_pc_14day)\n\n[1] 0.4393655\n\n\nBut generally we let the summary() function do it for us:\n\nsummary(m3)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50751 -0.19703 -0.06278  0.20024  0.92320 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04425  12.782  &lt; 2e-16 ***\nrep_voteshare_std  0.07140    0.06654   1.073  0.28869    \nmed_age_std       -0.01692    0.04744  -0.357  0.72296    \nmed_income_std    -0.21669    0.06660  -3.254  0.00211 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.316 on 47 degrees of freedom\nMultiple R-squared:  0.4394,    Adjusted R-squared:  0.4036 \nF-statistic: 12.28 on 3 and 47 DF,  p-value: 4.689e-06"
  },
  {
    "objectID": "slides/07-slides.html#adjusted-r2",
    "href": "slides/07-slides.html#adjusted-r2",
    "title": "POLS 1600",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) Example Figure\n\n\n\n\nOne can show that a models \\(R^2\\) always increases as we add predictors, even when they’re unrelated to the outcome\nThe adjusted \\(R^2\\) adjusts for this by weighting the \\(R^2\\) of a model by the number of predictors\n\n\\[\\text{adj. }R^2 = 1 - \\frac{RSS/(n-k)}{TSS/(n-1)}\\]\n\n\n\n\nex_df &lt;- data.frame(\n  y = rnorm(100) \n  ) %&gt;%\n    bind_cols(\n      data.frame(matrix(rnorm(10000), ncol=100))\n    ) %&gt;% janitor::clean_names()\n\n\nthe_formulas &lt;- list()\nfor(i in 2:51){\n  vars &lt;- names(ex_df)[2:i]\n  the_formulas[[i-1]] &lt;- paste(\"y~\",paste(vars,collapse = \"+\"))\n}\n\nthe_formulas %&gt;% \n  purrr::map(as.formula) %&gt;% \n  purrr::map(lm, data=ex_df) %&gt;% \n  purrr::map(summary) %&gt;% \n  purrr::map_df(glance) -&gt; r2_df\n\nr2_df %&gt;% \n  ggplot(aes(df, r.squared))+\n  geom_point(aes(col = \"R^2\"))+\n  geom_line()+\n  geom_point(aes(y=adj.r.squared,col = \"Adjusted R^2\"))+\n  geom_line(aes(y=adj.r.squared))+\n  labs(\n    x = \"Number of predictors\",\n    y = \"Proportion of Variance Explained\",\n    title = \"Adding unrelated predictors increases a model's R^2\\nwhile the Adjusted R^2 provides a better indicator of poor fit \",\n    col =\"Model fit\"\n  ) -&gt; fig_r2"
  },
  {
    "objectID": "slides/07-slides.html#section-14",
    "href": "slides/07-slides.html#section-14",
    "title": "POLS 1600",
    "section": "",
    "text": "Using \\(R^2\\) to compare models\n\nANOVA m5 Table Anova\n\n\nWhen models are nested (larger models contain all the predictors of smaller models), we can ask, does including the additional predictors in the larger model explain more variation in the outcome than we would expect would happen if we just added additional, random variable.\nFormally we call this process an Analysis of Variance (ANOVA)\nLet’s assess the added predictive power of I(percent_vaccinated^2) by estimating a model without it and comparing models using ANOVA\n\n\n\n# Estimate model without polynomial\nm5 &lt;- lm(new_deaths_pc_14day ~ percent_vaccinated  + rep_voteshare_std + med_age_std + med_income_std, covid_lab\n           )\n\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n6.194**\n\n\n2.532***\n\n\n\n\n \n\n\n(2.186)\n\n\n(0.657)\n\n\n\n\npercent_vaccinated\n\n\n-0.169*\n\n\n-0.035**\n\n\n\n\n \n\n\n(0.077)\n\n\n(0.012)\n\n\n\n\npercent_vaccinated^2\n\n\n0.001\n\n\n \n\n\n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nrep_voteshare_std\n\n\n-0.062\n\n\n-0.089\n\n\n\n\n \n\n\n(0.081)\n\n\n(0.082)\n\n\n\n\nmed_age_std\n\n\n0.053\n\n\n0.071\n\n\n\n\n \n\n\n(0.053)\n\n\n(0.053)\n\n\n\n\nmed_income_std\n\n\n-0.114\n\n\n-0.119\n\n\n\n\n \n\n\n(0.068)\n\n\n(0.070)\n\n\n\n\nR2\n\n\n0.561\n\n\n0.531\n\n\n\n\nAdj. R2\n\n\n0.512\n\n\n0.490\n\n\n\n\nNum. obs.\n\n\n51\n\n\n51\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\nThe anova suggests that including a polynomial provides a marginal improvement to fit (p &lt; 0.10)\n\nanova(m5, m4)\n\nAnalysis of Variance Table\n\nModel 1: new_deaths_pc_14day ~ percent_vaccinated + rep_voteshare_std + \n    med_age_std + med_income_std\nModel 2: new_deaths_pc_14day ~ percent_vaccinated + I(percent_vaccinated^2) + \n    rep_voteshare_std + med_age_std + med_income_std\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     46 3.9268                              \n2     45 3.6758  1   0.25098 3.0725 0.08644 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/07-slides.html#motivating-example-what-causes-cholera",
    "title": "POLS 1600",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/07-slides.html#notation",
    "href": "slides/07-slides.html#notation",
    "title": "POLS 1600",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/07-slides.html#causal-effects",
    "href": "slides/07-slides.html#causal-effects",
    "title": "POLS 1600",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/07-slides.html#average-treatment-on-treated",
    "href": "slides/07-slides.html#average-treatment-on-treated",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/07-slides.html#average-treatment-on-treated-1",
    "href": "slides/07-slides.html#average-treatment-on-treated-1",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n\n\n\n\n\n\n\n\n\nPre-Period (T=0)\nPost-Period (T=1)\n\n\n\n\nTreated \\(D_{i}=1\\)\n\\(E[Y_{0i}(0)\\vert D_i = 1]\\)\n\\(E[Y_{1i}(1)\\vert D_i = 1]\\)\n\n\nControl \\(D_i=0\\)\n\\(E[Y_{0i}(0)\\vert D_i = 0]\\)\n\\(E[Y_{0i}(1)\\vert D_i = 0]\\)"
  },
  {
    "objectID": "slides/07-slides.html#data",
    "href": "slides/07-slides.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/07-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/07-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "POLS 1600",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/07-slides.html#before-vs-after-comparisons",
    "href": "slides/07-slides.html#before-vs-after-comparisons",
    "title": "POLS 1600",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854\n\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/07-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/07-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "POLS 1600",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V\n\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/07-slides.html#difference-in-differences-1",
    "href": "slides/07-slides.html#difference-in-differences-1",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period"
  },
  {
    "objectID": "slides/07-slides.html#difference-in-differences-2",
    "href": "slides/07-slides.html#difference-in-differences-2",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\] Is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/07-slides.html#difference-in-differences-3",
    "href": "slides/07-slides.html#difference-in-differences-3",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/07-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/07-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "POLS 1600",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V"
  },
  {
    "objectID": "slides/07-slides.html#parallel-trends",
    "href": "slides/07-slides.html#parallel-trends",
    "title": "POLS 1600",
    "section": "Parallel Trends",
    "text": "Parallel Trends"
  },
  {
    "objectID": "slides/07-slides.html#using-lm-to-estimate-diff-in-diff",
    "href": "slides/07-slides.html#using-lm-to-estimate-diff-in-diff",
    "title": "POLS 1600",
    "section": "Using lm to estimate Diff-in-Diff",
    "text": "Using lm to estimate Diff-in-Diff\n\n Data lm() Diff-in-Diff\n\n\n\ncholera_df &lt;- tibble(\n  Location = c(\"S&V\",\"Lambeth\",\"S&V\",\"Lambeth\"),\n  Treated = c(0,1,0, 1),\n  Time = c(0,0,1,1),\n  Deaths = c(135,85,147,19)\n)\ncholera_df\n\n# A tibble: 4 × 4\n  Location Treated  Time Deaths\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 S&V            0     0    135\n2 Lambeth        1     0     85\n3 S&V            0     1    147\n4 Lambeth        1     1     19\n\n\n\n\n\\[\n\\text{Deaths} = \\beta_0 + \\beta_1\\text{Treated} + \\beta_2\\text{Time} + \\beta_3 \\text{Treated}\\times\\text{Time}\n\\]\n\ndiff_in_diff &lt;- lm(Deaths ~ Treated + Time + Treated:Time, cholera_df)\ndiff_in_diff\n\n\nCall:\nlm(formula = Deaths ~ Treated + Time + Treated:Time, data = cholera_df)\n\nCoefficients:\n (Intercept)       Treated          Time  Treated:Time  \n         135           -50            12           -78  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_0=\\) Outcome in control (S&V) before treatment\n\\(\\beta_1=\\) Fixed, time invariant differences between treated and control\n\\(\\beta_2=\\) Fixed, unit invariant differences between pre and post periods\n\\(\\beta_3=\\) Difference-in-Differences = \\(E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\)"
  },
  {
    "objectID": "slides/07-slides.html#summary",
    "href": "slides/07-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\nDifferencing twice accounts for fixed differences across units and between periods\n\nBut not time varying differences across units…\n\nThe key identifying assumption of a DiD design is the assumption of parallel trends\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to assess if we have multiple periods of pre-treatment observations"
  },
  {
    "objectID": "slides/07-slides.html#generalizing-diff-in-diff-with-linear-regression",
    "href": "slides/07-slides.html#generalizing-diff-in-diff-with-linear-regression",
    "title": "POLS 1600",
    "section": "Generalizing Diff-in-Diff with Linear Regression",
    "text": "Generalizing Diff-in-Diff with Linear Regression\n\nLinear regression allows us to generalizes Diff-in-Diff to multiple periods and treatment interventions, with fixed effects\n\n\n\\[\ny_{it} = \\overbrace{\\alpha_i}^{\\text{Unit FE}} + \\underbrace{\\gamma_t}_{\\text{Period FE}} + \\overbrace{\\tau*d_{it}}^{\\text{Treatment}} + \\underbrace{X\\beta}_{\\text{Covariates}} + \\epsilon_{it}\n\\]\n\n\nUnit fixed effects \\((\\alpha_i)\\)control for time-invariant differences across units\nPeriod fixed effects \\((\\gamma_i)\\) control for unit-invariant differences across periods\n\\(\\tau\\) corresponds the Difference-in-Difference estimate for a two-way fixed effects regression"
  },
  {
    "objectID": "slides/07-slides.html#extensions-and-limitations",
    "href": "slides/07-slides.html#extensions-and-limitations",
    "title": "POLS 1600",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nInterpretation of two-way fixed effects DiD estimator is complicated…\n\nGoodman-Bacon (2021)\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/07-slides.html#applications",
    "href": "slides/07-slides.html#applications",
    "title": "POLS 1600",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?"
  },
  {
    "objectID": "slides/07-slides.html#replicating-grumbach-and-hill-2022",
    "href": "slides/07-slides.html#replicating-grumbach-and-hill-2022",
    "title": "POLS 1600",
    "section": "Replicating Grumbach and Hill (2022)",
    "text": "Replicating Grumbach and Hill (2022)\n\nIn this week’s lab, we’ll be conducting a partial replication of Grumbach and Hill (2022) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nOn Thursday, we’ll walk through\n\nthe paper’s design and argument\nsetting up and exploring the data\nreproducing some descriptive figures\n\nNext Thursday, we’ll focus on replicating and understanding the main results"
  },
  {
    "objectID": "slides/07-slides.html#general-structure-of-labs-7-8",
    "href": "slides/07-slides.html#general-structure-of-labs-7-8",
    "title": "POLS 1600",
    "section": "General Structure of Labs 7-8",
    "text": "General Structure of Labs 7-8\nLab 7:\n\nSummarize the study\nDownload and load the data\nRecode the data\nMerge the data\nRecreate Figures 1 and 2\n\nLab 8:\n\nEstimate some baseline models to understand Two-Way Fixed Effects\nEstimate some of the models in Figure 3\nExtend the study, perhaps considering SDR by race or gender"
  },
  {
    "objectID": "slides/07-slides.html#reading-grumbach-and-hill-2022",
    "href": "slides/07-slides.html#reading-grumbach-and-hill-2022",
    "title": "POLS 1600",
    "section": "Reading Grumbach and Hill (2022)",
    "text": "Reading Grumbach and Hill (2022)\nReading Grumbach and Hill (2022), focus on being able to answer the following:\n\nWhat’s the research question?\n\nGeneral RQ: First sentence, second paragraph, p. 405\nSpecific RQs: p. 405-406\n\nWhat’s the theoretical framework?\n\nIntro and Theory of Registration, p. 407-409\n\nWhat’s the empirical design?\n\nMethods pp. 409-410\n\nWhat’s are the main results?\n\nResults pp. 410-413\nFigure 3 in particular"
  },
  {
    "objectID": "slides/07-slides.html#q1-download-the-replication-files",
    "href": "slides/07-slides.html#q1-download-the-replication-files",
    "title": "POLS 1600",
    "section": "Q1: Download the replication files",
    "text": "Q1: Download the replication files\nRather than downloading the files directly from the paper’s replication archives, in this lab, we will download the replication files to your computers and then load the data into R from where they’re saved\nPlease click here and let’s download the files together."
  },
  {
    "objectID": "slides/07-slides.html#go-to-the-papers-dataverse",
    "href": "slides/07-slides.html#go-to-the-papers-dataverse",
    "title": "POLS 1600",
    "section": "1. Go to the paper’s dataverse",
    "text": "1. Go to the paper’s dataverse\nPlease click here"
  },
  {
    "objectID": "slides/07-slides.html#log-in-through-brown",
    "href": "slides/07-slides.html#log-in-through-brown",
    "title": "POLS 1600",
    "section": "2. Log in through Brown",
    "text": "2. Log in through Brown"
  },
  {
    "objectID": "slides/07-slides.html#select-all-of-the-files",
    "href": "slides/07-slides.html#select-all-of-the-files",
    "title": "POLS 1600",
    "section": "3. Select all of the files",
    "text": "3. Select all of the files\nMake sure to Select all 11 files in this dataset"
  },
  {
    "objectID": "slides/07-slides.html#download-the-files-in-their-original-format",
    "href": "slides/07-slides.html#download-the-files-in-their-original-format",
    "title": "POLS 1600",
    "section": "4. Download the files in their original format",
    "text": "4. Download the files in their original format"
  },
  {
    "objectID": "slides/07-slides.html#section-16",
    "href": "slides/07-slides.html#section-16",
    "title": "POLS 1600",
    "section": "",
    "text": "5. Save and unzip the downloaded files into your course folder where your labs are saved"
  },
  {
    "objectID": "slides/07-slides.html#q3-load-the-data-into-r",
    "href": "slides/07-slides.html#q3-load-the-data-into-r",
    "title": "POLS 1600",
    "section": "Q3: Load the data into R",
    "text": "Q3: Load the data into R\nIf you’ve saved the dataverse_files into the folder where your lab is saved, you should be able to run the following code after setting the working directory to source file location:\n\n# Remember to set working directory:\n# Session &gt; Set working directory &gt; Source file location\n\n# Load fips_codes\nfips_codes &lt;- read_csv(\"dataverse_files/fips_codes_website.csv\")%&gt;%\n  janitor::clean_names()\n\n# Load policy data\ndata &lt;- readRDS(\"dataverse_files/policy_data_updated.RDS\")%&gt;%\n  janitor::clean_names()\n\n# Load CPS data\ncps &lt;- read_csv(\"dataverse_files/cps_00021.csv\") %&gt;%\n  janitor::clean_names()"
  },
  {
    "objectID": "slides/07-slides.html#summary-2",
    "href": "slides/07-slides.html#summary-2",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/07-slides.html#references",
    "href": "slides/07-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-differences with variation in treatment timing.” Journal of Econometrics 225 (2): 254–77.\n\n\nGrumbach, Jacob M, and Charlotte Hill. 2022. “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.” The Journal of Politics 84 (1): 405–17."
  },
  {
    "objectID": "slides/00-slides.html#overview",
    "href": "slides/00-slides.html#overview",
    "title": "Welcome to POLS 1600",
    "section": "Overview",
    "text": "Overview\n\n\nGoals and Expectations\n\n\n\n\nCourse Structure\n\n\n\n\nCourse Policies\n\n\n\n\nA Few Fundamental Truths"
  },
  {
    "objectID": "slides/00-slides.html#what-you-will-learn",
    "href": "slides/00-slides.html#what-you-will-learn",
    "title": "Welcome to POLS 1600",
    "section": "What you will learn",
    "text": "What you will learn\nYou will learn\n\n\nhow to think like a social scientist\n\n\n\n\nhow to use data to make descriptive, predictive, and causal claims\n\n\n\n\nhow to quantify uncertainty about these claims\n\n\n\n\nhow to present, interpret, and critique these claims"
  },
  {
    "objectID": "slides/00-slides.html#reasons-to-take-this-class",
    "href": "slides/00-slides.html#reasons-to-take-this-class",
    "title": "Welcome to POLS 1600",
    "section": "Reasons to take this class",
    "text": "Reasons to take this class\n\nYou want to change the world"
  },
  {
    "objectID": "slides/00-slides.html#why-is-this-study-important",
    "href": "slides/00-slides.html#why-is-this-study-important",
    "title": "Welcome to POLS 1600",
    "section": "Why is this study important?",
    "text": "Why is this study important?\n\n\nFindings provide evidence of benefits of social spending/universal basic income"
  },
  {
    "objectID": "slides/00-slides.html#why-should-we-believe-these-results",
    "href": "slides/00-slides.html#why-should-we-believe-these-results",
    "title": "Welcome to POLS 1600",
    "section": "Why should we believe these results",
    "text": "Why should we believe these results\n\n\nBecause it’s in the Times?\nBecause the authors are professors at good schools?\nBecause of how the study was done!\n\nRandom assignment provides a reasoned basis for inference\nCreates informative counter-factual comparisons\nPre-registered hypotheses ensure that we’re not cherry-picking results"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\n\nHow strong are the effects?\n\nIs a fifth of a standard deviation a lot?\n\nWhy do we care about brain waves?\nWhat’s the mechanism?\nHow confident are we that these results couldn’t have happened just by chance"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-1",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-1",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-2",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-2",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-3",
    "href": "slides/00-slides.html#why-might-we-be-skeptical-of-these-results-3",
    "title": "Welcome to POLS 1600",
    "section": "Why might we be skeptical of these results?",
    "text": "Why might we be skeptical of these results?\n\nSource: Andrew Gelman"
  },
  {
    "objectID": "slides/00-slides.html#reasons-to-take-this-class-1",
    "href": "slides/00-slides.html#reasons-to-take-this-class-1",
    "title": "Welcome to POLS 1600",
    "section": "Reasons to take this class",
    "text": "Reasons to take this class\n\n\nYou want to change the world\n\nData, design, and analysis are incredlibly powerful tools\nYou want to understand their strengths and limits\n\nYou want to be a better consumer of data and knowledge\nYou want to be a better consumer of data and knowledge\nYou want to get a job / go to grad school\nYou have to\nYou’re just in it for the memes"
  },
  {
    "objectID": "slides/00-slides.html#great-expectations",
    "href": "slides/00-slides.html#great-expectations",
    "title": "Welcome to POLS 1600",
    "section": "Great expectations",
    "text": "Great expectations\n\nI expect that you will come to class ready to engage with:\n\nsocial science\ndata\nprogramming\nmath"
  },
  {
    "objectID": "slides/00-slides.html#requirements",
    "href": "slides/00-slides.html#requirements",
    "title": "Welcome to POLS 1600",
    "section": "Requirements",
    "text": "Requirements\nI assume that you will\n\n\nDo the readings\n\n\n\n\nBring your computers 1\n\n\n\n\nWork through classwork\n\n\n\n\nAsk questions\n\n\nIf you only have a desktop/or tablet let me know and we’ll figure out a solution."
  },
  {
    "objectID": "slides/00-slides.html#course-structure",
    "href": "slides/00-slides.html#course-structure",
    "title": "Welcome to POLS 1600",
    "section": "Course structure",
    "text": "Course structure"
  },
  {
    "objectID": "slides/00-slides.html#class",
    "href": "slides/00-slides.html#class",
    "title": "Welcome to POLS 1600",
    "section": "Class",
    "text": "Class\n\nTuesday: Lecture/Demonstration\nThursday: Lab/Exploration"
  },
  {
    "objectID": "slides/00-slides.html#class-websites",
    "href": "slides/00-slides.html#class-websites",
    "title": "Welcome to POLS 1600",
    "section": "Class websites",
    "text": "Class websites\n\nSlides, labsm and additional resources available here: pols1600.paultesta.org\nAssignments uploaded here: Canvas"
  },
  {
    "objectID": "slides/00-slides.html#software-and-computing",
    "href": "slides/00-slides.html#software-and-computing",
    "title": "Welcome to POLS 1600",
    "section": "Software and computing",
    "text": "Software and computing\n\nStatistics done using R\n\nOpen source (free) statistical language\n\nThrough R Studio\n\nAn integrated development environment for R\n\nResults written up using R Markdown\n\nLanguage for combing R code with html Markdown"
  },
  {
    "objectID": "slides/00-slides.html#r",
    "href": "slides/00-slides.html#r",
    "title": "Welcome to POLS 1600",
    "section": "R",
    "text": "R"
  },
  {
    "objectID": "slides/00-slides.html#r-studio",
    "href": "slides/00-slides.html#r-studio",
    "title": "Welcome to POLS 1600",
    "section": "R Studio",
    "text": "R Studio"
  },
  {
    "objectID": "slides/00-slides.html#quarto",
    "href": "slides/00-slides.html#quarto",
    "title": "Welcome to POLS 1600",
    "section": "Quarto",
    "text": "Quarto\n\n\n\nProject options in YAML\nCode in triple backtick chunks:\n\nChunk options set with “#|” (hashpipe)\n\n\n```{r}\n#| label = \"simulate_data\"\nx &lt;- rnorm(100)\ny &lt;- 2*x + rnorm(100)\n```\n\nWrite up in Markdown\nOutput rendered as an html file"
  },
  {
    "objectID": "slides/00-slides.html#getting-set-up-for-the-course",
    "href": "slides/00-slides.html#getting-set-up-for-the-course",
    "title": "Welcome to POLS 1600",
    "section": "Getting set up for the course:",
    "text": "Getting set up for the course:\nHere’s a link to a guide to get you setup for the course.\nTake a crack at it after class, over the weekend.\nEmail me with any issues (there are always issues), and drop by my office hours on Tuesday so we can trouble shoot."
  },
  {
    "objectID": "slides/00-slides.html#textbook",
    "href": "slides/00-slides.html#textbook",
    "title": "Welcome to POLS 1600",
    "section": "Textbook",
    "text": "Textbook\n\nhttps://press.princeton.edu/books/paperback/9780691222288/quantitative-social-science"
  },
  {
    "objectID": "slides/00-slides.html#how-to-read-imai",
    "href": "slides/00-slides.html#how-to-read-imai",
    "title": "Welcome to POLS 1600",
    "section": "How to Read Imai",
    "text": "How to Read Imai\n\nActive reading\nCopy and run the code in the text. To do so, do the following:\n\n\nif (!require(\"devtools\")){\n  install.packages(\"devtools\")\n  }\nlibrary(\"devtools\")\ninstall_github(\"kosukeimai/qss-package\",  \n               build_vignettes  =  TRUE)"
  },
  {
    "objectID": "slides/00-slides.html#how-to-read-imai-1",
    "href": "slides/00-slides.html#how-to-read-imai-1",
    "title": "Welcome to POLS 1600",
    "section": "How to Read Imai",
    "text": "How to Read Imai\nOnce you’ve rune the following\n\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\nAnywhere the text loads data:\n\nafghan &lt;- read_csv(\"afgahn.csv\")\n\nYou can do\n\nlibrary(\"qss\")\ndata(\"afghan\")\nsummary(afghan$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.00   22.00   30.00   32.39   40.00   80.00"
  },
  {
    "objectID": "slides/00-slides.html#additional-readings",
    "href": "slides/00-slides.html#additional-readings",
    "title": "Welcome to POLS 1600",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nOccasionally, I will provide additional readings, available on both Canvas and pols1600.paultesta.org"
  },
  {
    "objectID": "slides/00-slides.html#assignments-1",
    "href": "slides/00-slides.html#assignments-1",
    "title": "Welcome to POLS 1600",
    "section": "Assignments",
    "text": "Assignments\nYou have three types of assignments in this course\n\nLabs\nTutorials\nFinal Project"
  },
  {
    "objectID": "slides/00-slides.html#labs",
    "href": "slides/00-slides.html#labs",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs\n\nEach Thursday we will work in groups to complete an in-class lab\nThe labs are designed to reinforce and extend concepts from lecture using real world data."
  },
  {
    "objectID": "slides/00-slides.html#labs-1",
    "href": "slides/00-slides.html#labs-1",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "slides/00-slides.html#labs-2",
    "href": "slides/00-slides.html#labs-2",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs"
  },
  {
    "objectID": "slides/00-slides.html#labs-3",
    "href": "slides/00-slides.html#labs-3",
    "title": "Welcome to POLS 1600",
    "section": "Labs",
    "text": "Labs\n\nWeeks 1 and 2 we’ll work collectively\nWeeks 3 on, you’ll be assigned to small groups\nEach week:\n\nLog on to the Canvas, download the lab .qmd file\nOpen R Studio\nRender the qmd file to get ready to work\nComplete the lab\nUpload the rendered html file to Canvas by the end of class\n\nOne question randomly graded\n\n100% if correct\n85% if incorrect, but you tried\n0% if you did not try/absent for the lab\n\nComments/Answers posted immediately after class"
  },
  {
    "objectID": "slides/00-slides.html#problem-setstutorials",
    "href": "slides/00-slides.html#problem-setstutorials",
    "title": "Welcome to POLS 1600",
    "section": "Problem Sets/Tutorials",
    "text": "Problem Sets/Tutorials\n\nCoding tutorials to reinforce concepts from lecture and textbook.\nAccessed by running\n\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\n\nComplete the tutorial. Save output as “LASTNAME_TutorialNumber.pdf”\nUpload output to Canvas by Friday by 11:59 pm\nGrades:\n\n100% any upload\n0% no upload"
  },
  {
    "objectID": "slides/00-slides.html#final-project",
    "href": "slides/00-slides.html#final-project",
    "title": "Welcome to POLS 1600",
    "section": "Final Project",
    "text": "Final Project"
  },
  {
    "objectID": "slides/00-slides.html#your-first-assignment",
    "href": "slides/00-slides.html#your-first-assignment",
    "title": "Welcome to POLS 1600",
    "section": "Your First Assignment:",
    "text": "Your First Assignment:\n\nDownload and install R and R Studio\n\nEmail me if you have troubles\nTroubleshoot by Zoom or in-person (111 Thayer Room 339)\n\nWork through 00-software_setup before next class."
  },
  {
    "objectID": "slides/00-slides.html#portals-of-discovery",
    "href": "slides/00-slides.html#portals-of-discovery",
    "title": "Welcome to POLS 1600",
    "section": "Portals of Discovery",
    "text": "Portals of Discovery"
  },
  {
    "objectID": "slides/00-slides.html#errors",
    "href": "slides/00-slides.html#errors",
    "title": "Welcome to POLS 1600",
    "section": "Errors",
    "text": "Errors\n\nish happens\nSeeing red is a good thing\nWe learn by making errors"
  },
  {
    "objectID": "slides/00-slides.html#final-reports",
    "href": "slides/00-slides.html#final-reports",
    "title": "Welcome to POLS 1600",
    "section": "Final Reports",
    "text": "Final Reports\n\nCan be on any topic you like\nMore info to come\nDue dates:\n\nWeek 2 Groups assigned\nWeek 3 Research Topics\nWeek 6 Data Proposal\nWeek 8 Data Explorations\nWeek 11 Drafts\nWeek 12 Presentations\nWeek 13 Final Paper"
  },
  {
    "objectID": "slides/00-slides.html#grading",
    "href": "slides/00-slides.html#grading",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-1",
    "href": "slides/00-slides.html#grading-1",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-2",
    "href": "slides/00-slides.html#grading-2",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-3",
    "href": "slides/00-slides.html#grading-3",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "slides/00-slides.html#grading-4",
    "href": "slides/00-slides.html#grading-4",
    "title": "Welcome to POLS 1600",
    "section": "Grading",
    "text": "Grading\n\n5% Attendance\n10% Class involvement and participation\n10% Tutorials\n30% Labs\n20% Assignments for final paper\n20% Final paper"
  },
  {
    "objectID": "slides/00-slides.html#course-policies",
    "href": "slides/00-slides.html#course-policies",
    "title": "Welcome to POLS 1600",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nCommunity standards\nIncomplete/late work"
  },
  {
    "objectID": "slides/00-slides.html#two-fundamental-truths",
    "href": "slides/00-slides.html#two-fundamental-truths",
    "title": "Welcome to POLS 1600",
    "section": "Two Fundamental Truths",
    "text": "Two Fundamental Truths"
  },
  {
    "objectID": "slides/00-slides.html#testas-first-fundamental-truth",
    "href": "slides/00-slides.html#testas-first-fundamental-truth",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s first fundamental truth",
    "text": "Testa’s first fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#testas-first-fundamental-truth-1",
    "href": "slides/00-slides.html#testas-first-fundamental-truth-1",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s first fundamental truth",
    "text": "Testa’s first fundamental truth\nWhy would I profess my utter ignorance on the first day of class?\nFour possible reasons…"
  },
  {
    "objectID": "slides/00-slides.html#expectation-management",
    "href": "slides/00-slides.html#expectation-management",
    "title": "Welcome to POLS 1600",
    "section": "1. Expectation Management",
    "text": "1. Expectation Management"
  },
  {
    "objectID": "slides/00-slides.html#pedagogical-tomfoolery",
    "href": "slides/00-slides.html#pedagogical-tomfoolery",
    "title": "Welcome to POLS 1600",
    "section": "2. Pedagogical Tomfoolery",
    "text": "2. Pedagogical Tomfoolery"
  },
  {
    "objectID": "slides/00-slides.html#positionality",
    "href": "slides/00-slides.html#positionality",
    "title": "Welcome to POLS 1600",
    "section": "3. Positionality",
    "text": "3. Positionality"
  },
  {
    "objectID": "slides/00-slides.html#epistemology",
    "href": "slides/00-slides.html#epistemology",
    "title": "Welcome to POLS 1600",
    "section": "4. Epistemology",
    "text": "4. Epistemology"
  },
  {
    "objectID": "slides/00-slides.html#testas-second-fundamental-truth",
    "href": "slides/00-slides.html#testas-second-fundamental-truth",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s second fundamental truth",
    "text": "Testa’s second fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#testas-second-fundamental-truth-1",
    "href": "slides/00-slides.html#testas-second-fundamental-truth-1",
    "title": "Welcome to POLS 1600",
    "section": "Testa’s second fundamental truth",
    "text": "Testa’s second fundamental truth"
  },
  {
    "objectID": "slides/00-slides.html#two-kinds-of-people-in-this-world",
    "href": "slides/00-slides.html#two-kinds-of-people-in-this-world",
    "title": "Welcome to POLS 1600",
    "section": "Two kinds of people in this world",
    "text": "Two kinds of people in this world"
  },
  {
    "objectID": "slides/00-slides.html#what-is-it-that-we-say-we-do-here",
    "href": "slides/00-slides.html#what-is-it-that-we-say-we-do-here",
    "title": "Welcome to POLS 1600",
    "section": "What is it that we say we do here",
    "text": "What is it that we say we do here"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do",
    "href": "slides/00-slides.html#what-does-quantitative-research-do",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions"
  },
  {
    "objectID": "slides/00-slides.html#descriptions",
    "href": "slides/00-slides.html#descriptions",
    "title": "Welcome to POLS 1600",
    "section": "Descriptions",
    "text": "Descriptions"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-1",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-1",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations"
  },
  {
    "objectID": "slides/00-slides.html#explanations",
    "href": "slides/00-slides.html#explanations",
    "title": "Welcome to POLS 1600",
    "section": "Explanations",
    "text": "Explanations"
  },
  {
    "objectID": "slides/00-slides.html#explanations-1",
    "href": "slides/00-slides.html#explanations-1",
    "title": "Welcome to POLS 1600",
    "section": "Explanations",
    "text": "Explanations"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-2",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-2",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations\nPredictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty",
    "href": "slides/00-slides.html#predictions-and-uncertainty",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty-1",
    "href": "slides/00-slides.html#predictions-and-uncertainty-1",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#predictions-and-uncertainty-2",
    "href": "slides/00-slides.html#predictions-and-uncertainty-2",
    "title": "Welcome to POLS 1600",
    "section": "Predictions and Uncertainty",
    "text": "Predictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#what-does-quantitative-research-do-3",
    "href": "slides/00-slides.html#what-does-quantitative-research-do-3",
    "title": "Welcome to POLS 1600",
    "section": "What does quantitative research do?",
    "text": "What does quantitative research do?\n\nDescriptions\nExplanations\nPredictions and Uncertainty"
  },
  {
    "objectID": "slides/00-slides.html#two-kinds-of-people-in-this-world-1",
    "href": "slides/00-slides.html#two-kinds-of-people-in-this-world-1",
    "title": "Welcome to POLS 1600",
    "section": "Two kinds of people in this world",
    "text": "Two kinds of people in this world"
  },
  {
    "objectID": "slides/00-slides.html#introductions-1",
    "href": "slides/00-slides.html#introductions-1",
    "title": "Welcome to POLS 1600",
    "section": "Introductions",
    "text": "Introductions"
  },
  {
    "objectID": "slides/00-slides.html#my-research",
    "href": "slides/00-slides.html#my-research",
    "title": "Welcome to POLS 1600",
    "section": "My research",
    "text": "My research\n\nI study American Poltical Behavior with focus on poltics of race and criminal justice\nHow do we break cycles of inequality when those most affected by injustice are the least likely to participate and those unaffected are the least likely to care?\nHow can we use methodological tools to better answer these questions?"
  },
  {
    "objectID": "slides/00-slides.html#but-enough-about-me",
    "href": "slides/00-slides.html#but-enough-about-me",
    "title": "Welcome to POLS 1600",
    "section": "But enough about me",
    "text": "But enough about me"
  },
  {
    "objectID": "slides/00-slides.html#class-survey",
    "href": "slides/00-slides.html#class-survey",
    "title": "Welcome to POLS 1600",
    "section": "Class survey",
    "text": "Class survey\nPlease click here to take a brief survey that will help me structure the class going forward."
  },
  {
    "objectID": "slides/00-slides.html#next-week",
    "href": "slides/00-slides.html#next-week",
    "title": "Welcome to POLS 1600",
    "section": "Next Week:",
    "text": "Next Week:\n\nComplete the class survey\nDownload and Install R and R studio\nRead Chapters 1 (Friday) and start Chapter 3 in QSS\nTuesday: Lecture: Describing Data in R\nThursday: Lab: Exploring COVID-19 data in the US\nFriday: Submit Tutorials: “00-intro” & “01-measurement”\n\nOnly time you’ll have two tutorials due (Ok to submit late)\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/00-slides-template.html#class-plan",
    "href": "slides/00-slides-template.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/00-slides-template.html#annoucements",
    "href": "slides/00-slides-template.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/00-slides-template.html#feedback",
    "href": "slides/00-slides-template.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "slides/00-slides-template.html#concepts",
    "href": "slides/00-slides-template.html#concepts",
    "title": "POLS 1600",
    "section": " Concepts",
    "text": "Concepts"
  },
  {
    "objectID": "slides/00-slides-template.html#code",
    "href": "slides/00-slides-template.html#code",
    "title": "POLS 1600",
    "section": " Code",
    "text": "Code"
  },
  {
    "objectID": "slides/00-slides-template.html#review-1",
    "href": "slides/00-slides-template.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "slides/00-slides-template.html#concept-1",
    "href": "slides/00-slides-template.html#concept-1",
    "title": "POLS 1600",
    "section": "Concept",
    "text": "Concept"
  },
  {
    "objectID": "slides/00-slides-template.html#code-2",
    "href": "slides/00-slides-template.html#code-2",
    "title": "POLS 1600",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "slides/00-slides-template.html#summary-1",
    "href": "slides/00-slides-template.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/00-slides-template.html#references",
    "href": "slides/00-slides-template.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "POLS 1600",
    "section": "",
    "text": "This course provides a foundation in the principles and practice of quantitative social science, with a focus on building tools to make descriptive, causal, and predictive inferences.\n\nLogistics\nWe meet twice a week, alternating lectures on Tuesdays and labs on Thursday. Both sessions require laptops that run R and RStudio\n\n\n\n\nLecture\nTuesdays 9-10:20 am\n\n\n\n\n\nLabs\nThursdays 9-10:20 am\n\n\n\n\n\nAssignments\nDue on Canvas\n\n\n\n\n\nLocation\nBarus & Holley 751\n\n\n\n\n\nZoom\n\n\n\n\n\nOffice Hours\n111 Thayer St Rm 339\n\n\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\n\n\n\nLecture\nLabs\nSolutions\nAssignments\n\n\n\n\nWeek 0\n\n\n\n\n\n\nIntroductions\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nData & Measurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\nCausation -- Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\n\nCausation -- Observational Studies\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\nBivariate Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\n\nMultiple Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\nRegression Extensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\n\nProbability - Random Variables & Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n\n\nProbability - Limit Theorems\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\n\n\nInference -- Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\n\n\nInference -- Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\n\n\nWorkshop\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\n\n\nPresentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructors\n\n\nPaul Testa\nAssistant Professor\nDepartment of Political Science\nBrown University &lt;paul_testa@brown.edu&gt;\n\n\n\n\n\nManuel Moscoso Rojas\nTeaching Assistant\nDepartment of Political Science\nBrown University &lt;Manuel_Moscoso_Rojas@Brown.edu&gt;"
  },
  {
    "objectID": "resources/04-packages.html",
    "href": "resources/04-packages.html",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "",
    "text": "This document provides instructions for installing the following packages:\n\ndataverse a package to download and read files from dataverses like Harvard’s Dataverse\nDeclareDesign a set of packages useful for describing the properities of experimetnal and observational design\ntidycensus set of functions of that allow us to download data from the US Census’ API\neasystats a set of packages like the tidyverse but for statistics.\n\nThese are useful packages, but in the past, I’ve found they don’t play nicely with the simple the_packages ipak(the_packages) approach we take in class.\nAdditionally, for tidycensus, each of you will need to request an API key from the Census and install it locally to your computers."
  },
  {
    "objectID": "resources/04-packages.html#request-an-api-key-from-the-census",
    "href": "resources/04-packages.html#request-an-api-key-from-the-census",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.1 Request an API key from the Census",
    "text": "3.1 Request an API key from the Census\nClick on this link: https://api.census.gov/data/key_signup.html\nAnd fill in the following information\n\nOrganization: “Brown University”\nEmail: firstname_lastname@brown.edu\nAgree to terms of service\nSubmit request\n\n\nknitr::include_graphics(\"images/census1.png\")"
  },
  {
    "objectID": "resources/04-packages.html#check-email",
    "href": "resources/04-packages.html#check-email",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.2 Check Email",
    "text": "3.2 Check Email\nYou should receive an email like this:\n\n\n\n\n\n\n\n\n\n\nClick on the link to activate your API key"
  },
  {
    "objectID": "resources/04-packages.html#activate-api-key",
    "href": "resources/04-packages.html#activate-api-key",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.3 Activate API key",
    "text": "3.3 Activate API key\n\nClicking the link should take you to a page that looks like this"
  },
  {
    "objectID": "resources/04-packages.html#save-api-key-in-r",
    "href": "resources/04-packages.html#save-api-key-in-r",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.4 Save API key in R",
    "text": "3.4 Save API key in R\nGo back to the email from the census\n\nCopy the string of letters and digits from the email (blocked out in red in the image above)\nThis is your unique census API key\nPaste that string in between the quotation marks below and run census_api_key()\n\n\ncensus_api_key(\"YOUR API KEY GOES HERE\")"
  },
  {
    "objectID": "resources/04-packages.html#check-that-everything-worked",
    "href": "resources/04-packages.html#check-that-everything-worked",
    "title": "Installing the tidycensus, DeclareDesign, and dataverse packages",
    "section": "3.5 Check that everything worked",
    "text": "3.5 Check that everything worked\n\ncensus_api_key() should save your unique API to your .Renviron file which tidycensus will use whenever you make ask the Census to Download data.\nIf everything worked as planned, running Sys.getenv(\"CENSUS_API_KEY\") should display your long API key\n\n\nSys.getenv(\"CENSUS_API_KEY\")\n\n[1] \"cad56d0c712406cfe825878e3bd0de256d19f2aa\"\n\n\nAnd you should be able to use functions from tidycensus to download census data:\n\nage10 &lt;- tidycensus::get_decennial(geography = \"state\", \n                       variables = \"P013001\", \n                       year = 2010)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\nhead(age10)\n\n# A tibble: 6 × 4\n  GEOID NAME       variable value\n  &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;\n1 01    Alabama    P013001   37.9\n2 02    Alaska     P013001   33.8\n3 04    Arizona    P013001   35.9\n4 05    Arkansas   P013001   37.4\n5 06    California P013001   35.2\n6 22    Louisiana  P013001   35.8\n\n\nYou can read more about what tidycensus can do here"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "You have three types of assignments in this course",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#labs",
    "href": "assignments/index.html#labs",
    "title": "Assignments",
    "section": "Labs",
    "text": "Labs\n\nEach Thursday we will work in groups to complete an in-class lab\nThe labs are designed to reinforce and extend concepts from lecture using real world data.",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#tutorials",
    "href": "assignments/index.html#tutorials",
    "title": "Assignments",
    "section": "Tutorials",
    "text": "Tutorials\nCoding tutorials to reinforce concepts from lecture and textbook.\nAccessed by running in the console of R Studio\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nComplete the tutorial. Save output as “LASTNAME_TutorialNumber.html”\nUpload output to Canvas by Friday by 11:59 pm",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#final-project",
    "href": "assignments/index.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\nFinally, throughout the semester you will be have periodic assignments to ensure that you’re making progress on your final projects.\n\nWeek 4:  Research Topics\nWeek 6:  Potential Data\nWeek 8:  Explortatory Analysis\nWeek 11: Draft\nWeek 12: Presentations\nWeek 13: Final Paper",
    "crumbs": [
      "Assignments",
      "Overview",
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/a4.html",
    "href": "assignments/a4.html",
    "title": "A4: Project Drafts",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A4: Draft"
    ]
  },
  {
    "objectID": "assignments/a1.html",
    "href": "assignments/a1.html",
    "title": "POLS 1600: Research Topics",
    "section": "",
    "text": "Asking a good research question is one of the most important skills you will develop in your academic careers. It’s also one of the hardest.\nWe often think we’re asking one question, when in fact the study we conduct really addresses a related but distinct question. When a priest asked Willie Sutton why he robbed banks, he replied the “Well, that’s where the money is”. The priest’s question was about why rob at all, while Sutton answered the different question “Given one robs, why rob banks?” Similarly, Medieval philosophers might ask why objects stay in motion, while Newton suggests what really need is not an explanation of motion itself but of changes in motion.\nThe object of our question shapes the form of our explanation.\nIn this assignment, I would like your group to craft three potential research questions that we might explore in our research project for this class. Each question, should be a single sentence, with a few sentences answering the following questions (More details below):\n\nWhy do we care about the answer to this research question?\nWhat’s would a hypothetical “ideal experiment” to answer this question look like?\nWhat would a study with observational data look like?\nA published study that relates to this question\nHow feasible would it be to do a study like this for the course\n\nYou may use this Rmd file as a template (click here to download) or create your own file. Please submit your responses to Canvas.\nYou might start by writing down several questions of different forms about the same topic:\n\nWhy do people vote?\nWhy do people not vote?\nWhy do the rich vote at higher rates than the poor?\nWhen might people who don’t vote, be motivated to vote?\nWhat is the effect of encouraging someone to vote via a phone call?\nAre phone calls more or less effective than in-person contact for get-out the vote efforts?\n\nEach of these questions addresses a general topic that political scientists seem to think is important. Each carries some suppositions and assumptions that in turn influence the type of explanation we might find convincing. Why do people vote feels a bit broad to me. People probably vote for many reasons. How can we hope to adjudicate between all the possible reasons for voting? (Further are these the same reasons for not voting or do we need another set of explanations altogether?)\nWhether phone calls are more or less effective than in-person contacts for GOTV efforts seems more tractable, but also perhaps to narrow. Do we really care? If we’re confident we can identify an effect or difference in one study, are we sure we’ll see similar effects in a different study conducted under different circumstances?\nIn crafting your research questions, you want to strike a balance between things we actually care about (why do people vote) and things we can actually assess (what’s effect of a particularly type of encouragement to vote). A few thoughts on this process:\n\n“Why” questions tend to be more compelling than “What” or “How” or “Do” questions, I think in part because “why” questions often imply a theory and suggest a counterfactual (why this and not that), while other ways of asking questions feel more descriptive. For example, why do the rich vote at higher rates than the poor. Well, one explanation may be that their relative social and economic status means they are more likely to be targets of mobilization efforts by campaigns (among many things). So a natural follow up to this larger question might be, what’s the effect of providing similar mobilization efforts to the poor. Would they vote at similar rates to the rich? If so, then we’ve learned something about how mobilization explains class differences in participation.\nThinking about questions in terms of puzzles is another useful trick. Why do parties exist when politicians’ ideological preferences can explain the vast majority of their legislative behavior? Note this type of question contains a lot of presuppositions (how do we measure ideological preferences? Do they really explain legislative behavior? Is that what we care about?), but as point of departure for a study these type arguments can be useful\nTry to be simple and clear. Don’t worry about asking the perfect question right away. Your questions can and should evolve over time, and I suspect some of you will write a paper that has nothing to do with the questions you posed here.\n\nFor each question, please discuss the following:\n\nWhy do we care? Why we should care about the answer to this question. A strong justification is often that existing theories yield conflicting predictions and so your study will offer some insight into how to adjudicate betweeen these theories. A less strong justification is that no one has ever studied this before. Even if this is true (and it’s often not) it may be true for good reason. No need for formal citations, but if there are specific theories or claims your addressing feel free to name names.\nThe ideal experiment Please describe an “ideal” experiment that you could run that would give you some purchase on your question. Note the key feature of an experiment, is that you the researcher are able to manipulate (through random assignment) some facet of the world. Assume money, resources, physics, and even ethics are not an object. If you could randomly assign anything, what would you manipulate. At what level of analysis would your manipulation occur (i.e. are your units of analysis individuals or countries or something else). How would you measure your outcome, again assuming you were all power and all-seeing. If that manipulation isn’t feasible, what does that say about the ability to make a causal claim about your question?\nThe observational study Finally, considering some of the potential limitations that might prevent you from implementing your ideal experiment (it’s hard to randomly assign democratic government), what is one way you might address your research question with observational data. Would your study use cross-sectional or longitudinal data. What are some of the concerns (selection on observables) that arise in this setting. Is there a natural experiment or some sort of discontinuity you might leverage to approximate this experimental ideal.\n\nAgain, each paragraph should be brief and to the point. No need to specify a full research design–just give me the broad strokes. You’re writing for each question should not exceed a page.\nAfter you’ve thought through how you might go about answering your question, please find\n\nA published study that relates to this question. It need not be exactly your question as posed, but it should be in a similar area. Include a full citation, and link to the study. Then in a paragraph sentences try to summarize:\n\n\nThe study’s research question\nEmprical design\nCore findings.\n\nFinally on a scale of 1 (least feasible) to 10 (most feasible), please evaluate how likely you think it is you could write an empirical paper on this question for this course.\nDon’t worry about getting everything right. Your final projects can, will, and probably should change. The point of this exercise is to get some practice thinking about questions that interest you in the language of causal inference and potential outcomes.",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care",
    "href": "assignments/a1.html#why-do-we-care",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment",
    "href": "assignments/a1.html#the-ideal-experiment",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study",
    "href": "assignments/a1.html#the-observational-study",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study",
    "href": "assignments/a1.html#a-published-study",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10",
    "href": "assignments/a1.html#feasibility-x10",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care-1",
    "href": "assignments/a1.html#why-do-we-care-1",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment-1",
    "href": "assignments/a1.html#the-ideal-experiment-1",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study-1",
    "href": "assignments/a1.html#the-observational-study-1",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study-1",
    "href": "assignments/a1.html#a-published-study-1",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10-1",
    "href": "assignments/a1.html#feasibility-x10-1",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#why-do-we-care-2",
    "href": "assignments/a1.html#why-do-we-care-2",
    "title": "POLS 1600: Research Topics",
    "section": "Why do we care:",
    "text": "Why do we care:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-ideal-experiment-2",
    "href": "assignments/a1.html#the-ideal-experiment-2",
    "title": "POLS 1600: Research Topics",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#the-observational-study-2",
    "href": "assignments/a1.html#the-observational-study-2",
    "title": "POLS 1600: Research Topics",
    "section": "The observational study:",
    "text": "The observational study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#a-published-study-2",
    "href": "assignments/a1.html#a-published-study-2",
    "title": "POLS 1600: Research Topics",
    "section": "A published study:",
    "text": "A published study:",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a1.html#feasibility-x10-2",
    "href": "assignments/a1.html#feasibility-x10-2",
    "title": "POLS 1600: Research Topics",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A1: Introductions"
    ]
  },
  {
    "objectID": "assignments/a2.html",
    "href": "assignments/a2.html",
    "title": "POLS 1600: Research Topics",
    "section": "",
    "text": "For this assignment, please upload to Canvas an html file named a2_group0X_data.html produced from an Quarto Markdown file called a2_group0X_data.qmd (changing group0X to your group’s number group0X) that contains the following:\n\nA revised description of your group’s research project\nA description of a linear model implied by your question\nR code that loads some potentially relevant data to your question and at least one descriptive summary of that data.\nSome information about your group such as:\n\nA group name1\nA group color or color scheme\nA group motto, mascot, crest, etc.\nYour group’s theme song\nYour group’s astrological sign\nAnything else that you think well help you form strong ingroup bounds that facilitate collaboration\n\n\nYour files (.qmd file, .html file, and maybe a data file2) should be saved in the shared Google drive folder Group_XX for your group.\n\n\n\n\n\n\nNote\n\n\n\nGoogle is great for sharing files but not so great for version control. When working on this assignment, please Coordinate with your group so only ONE PERSON is editing the file at a given time",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#gathering-data-from-existing-data-sets",
    "href": "assignments/a2.html#gathering-data-from-existing-data-sets",
    "title": "POLS 1600: Research Topics",
    "section": "Gathering Data from Existing Data Sets",
    "text": "Gathering Data from Existing Data Sets\nBelow are some common data sets in American, Comparative and International Relations. Again your question doesn’t have to be political and you can look elsewhere for data – (the U.S. Census, Data.gov).",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#american-politics",
    "href": "assignments/a2.html#american-politics",
    "title": "POLS 1600: Research Topics",
    "section": "American Politics",
    "text": "American Politics\n\nThe American National Elections Studies (NES)\n\nhttp://www.electionstudies.org/\n\nCCES\n\nhttp://projects.iq.harvard.edu/cces/home\n\nGeneral Social Survey\n\nhttp://www3.norc.org/GSS+Website/\n\nRoper Center\n\nhttps://ropercenter.cornell.edu//\n\nPew\n\nhttp://www.pewresearch.org/data/",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#comparative-politicsinternational-relations",
    "href": "assignments/a2.html#comparative-politicsinternational-relations",
    "title": "POLS 1600: Research Topics",
    "section": "Comparative Politics/International Relations",
    "text": "Comparative Politics/International Relations\n\nWorld Values Survey\n\nhttp://www.worldvaluessurvey.org/wvs.jsp\n\nEurobarometer\n\nhttp://ec.europa.eu/public_opinion/index_en.htm\n\nLatinobarometer\n\nhttp://www.latinobarometro.org/lat.jsp\n\nOECD\n\nhttps://data.oecd.org/\n\nQuality of Government Data (I love this)\n\nhttp://qog.pol.gu.se/data\n\nUppsala Conflict Data (I hate this8)\n\nhttp://www.pcr.uu.se/research/UCDP/\n\nThreat and imposition of sanctions\n\nhttp://www.unc.edu/~bapat/TIES.htm\n\nCorrelates of War (Also hate this)\n\nhttp://www.correlatesofwar.org/",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#general-research",
    "href": "assignments/a2.html#general-research",
    "title": "POLS 1600: Research Topics",
    "section": "General Research",
    "text": "General Research\n\nHarvard Dataverse (Good for finding replication data from published studies)\n\nhttps://dataverse.harvard.edu/\n\nICPSR (Same)\n\nhttps://www.icpsr.umich.edu/icpsrweb/landing.jsp",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "assignments/a2.html#footnotes",
    "href": "assignments/a2.html#footnotes",
    "title": "POLS 1600: Research Topics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re Group 01 don’t change your name to Group 4↩︎\nPlease don’t upload terabytes of data though.↩︎\nThat is, don’t worry about whether your question is causally identified.↩︎\nThis is a common explanation for socio-economic differences in political participation.↩︎\nAlso known as a dummy variable, or binary variable because it only takes values of 0 or 1↩︎\nYou’d probably also want to some clarity about how you’re treating Independents and people who say they’re Independents but when asked if they lean toward one party or the other. Typically, these partisan leaners tend to behave like partisans.↩︎\nAssuming for simplicity that we’re excluding independents from our analysis…↩︎\nBasically conflict data is real pain to work with, and it’s not always clear what statitistical inference means for these type of questions. But if your group is desperate to study these types of questions we can talk. I’d recommend trying to replicate and extend someone else’s analysis of these data, rather than starting from scratch↩︎",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A2: Data"
    ]
  },
  {
    "objectID": "labs/07-lab.html",
    "href": "labs/07-lab.html",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we will begin the process of replicating Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages and set the working directory to where this file is saved. (5 minutes)\nSummarize the study in terms of it’s research question, theory, design, and results. (10 minutes)\nDownload the replication files and save them in the same folder as this lab (5 minutes)\nLoad the data from your computers into R (5 minutes)\nGet a quick HLO of the data (10 minutes)\nMerge data on election policy into data on voting (5 minutes, together),\nRecode the covariates, key predictors, and outcome for the study (10 minutes, partly together)\nRecreate Figure 1 (15 minutes)\nRecreate Figure 2 (15 minutes)\n\nFinally, we’ll take the weekly survey which should be a fun one\nOne of these 8 tasks will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/07-lab.html#please-render-this-.qmd-file",
    "href": "labs/07-lab.html#please-render-this-.qmd-file",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/07-lab.html#load-packages",
    "href": "labs/07-lab.html#load-packages",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nWe will also want to set our working directory to where your lab is saved."
  },
  {
    "objectID": "labs/07-lab.html#important-set-your-working-directory",
    "href": "labs/07-lab.html#important-set-your-working-directory",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "IMPORTANT: Set your working directory",
    "text": "IMPORTANT: Set your working directory\n\nOn the top panel of R Studio click:\n\n\nSession Session &gt; Set working directory &gt; Source file location\n\n\nPaste the output that shows up in your console into the code chunk below\n\n\n# Set working directory\n# Session &gt; Set working directory &gt; Source file location\n# paste output here:\n\nAll right, now let’s summarize the study"
  },
  {
    "objectID": "labs/07-lab.html#recode-covariates",
    "href": "labs/07-lab.html#recode-covariates",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.1 Recode covariates",
    "text": "6.1 Recode covariates\nThe CPS are messy data. Please run the code below to recode the covariates in the spirit of (i.e. with minor changes) what Grumbach and Hill did. 2\n\n\n\n\n\n\nNote\n\n\n\nThe file cps_00021.cbk.txt contains the codebook for the data, telling us what numeric values of each variable correspond to substantively. So if you’re wondering how I know what should be recoded to what specific values, it comes from reading the codebook, looking at Grumbach and Hill’s code, looking at the raw variable with a table, and the using case_when() to judiciously code the data. You’ll get practice doing this in your final projects, but I don’t want to spend too much time on this this lab, which is why you’re only recoding the outcome voted\n\n\n\n# # Recode covariates\n# cps %&gt;% \n#   mutate(\n#     # Useful for plotting figure 2\n#     SDR = ifelse(sdr == 1, \"SDR\", \"non-SDR\"),\n#     education = case_when(\n#       educ == 1 ~ NA, #Blank\n#       educ &lt; 40 ~ 1, # No high school\n#       educ &gt;= 40 & educ &lt; 73 ~ 2, # Some high school\n#       educ == 73 ~ 3, # High school degree\n#       educ &gt;= 80 & educ &lt;= 110 ~ 4, # Some college\n#       educ &gt;= 111 & educ &lt;123 ~ 5, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n#       educ &gt;= 123 & educ &lt;=125 ~ 6, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n#       educ == 999 ~ NA # Missing/unknown\n#     ),\n#     race_f = case_when(\n#       race == 999 ~ NA,\n#       T ~ factor(race)\n#     ),\n#     is_white = case_when(\n#       race == 100 ~ 1,\n#       race == 999 ~ NA,\n#       T ~ 0\n#     ),\n#     is_black = case_when(\n#       race == 200 ~ 1,\n#       race == 999 ~ NA,\n#       T ~ 0\n#     ),\n#     is_aapi = case_when(\n#       race == 650 ~ 1,\n#       race == 651 ~ 1,\n#       race == 652 ~ 1,\n#       race == 999 ~ NA,\n#       T ~ 0\n#     ),\n#     is_other = case_when(\n#       is_white == 1 ~ 0,\n#       is_black == 1 ~ 0,\n#       is_aapi ==  1 ~ 0,\n#       race == 999 ~ NA,\n#       T ~ 1\n#     ),\n#     income = case_when(\n#       faminc &gt; 843 ~ NA, # Remove Missing/Refused\n#       T ~ as.numeric(factor(faminc))\n#     ),\n#     is_female = case_when(\n#       sex == 2 ~ 1,\n#       sex == 1 ~ 0,\n#       T ~ NA # recode Not in Universe as NA\n#     )\n#     \n#   ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab.html#create-age_group-and-age_group_xx_xx-indicators",
    "href": "labs/07-lab.html#create-age_group-and-age_group_xx_xx-indicators",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.2 Create age_group and age_group_XX_XX indicators",
    "text": "6.2 Create age_group and age_group_XX_XX indicators\nNext we’ll create an age_group variable and binary indicators for each age cohort of the form age_group_XX_XX.\nPlease uncomment and run the code below\n\n# # Create age variables\n# cps %&gt;% \n#   mutate(\n#     age_group = case_when(\n#       age &gt; 18 & age &lt;= 24 ~ \"18-24\",\n#       age &gt; 24 & age &lt;= 34  ~ \"25-34\",\n#       age &gt; 34 & age &lt;= 44  ~ \"35-44\",\n#       age &gt; 44 & age &lt;= 54  ~ \"45-54\",\n#       age &gt; 54 & age &lt;= 64  ~ \"55-64\",\n#       age &gt; 64 ~ \"65+\",\n#       T ~ NA\n# \n#     ),\n#     age_group_18_24 = ifelse(age_group == \"18-24\", 1, 0),\n#     age_group_25_34 = ifelse(age_group == \"25-34\", 1, 0),\n#     age_group_35_44 = ifelse(age_group == \"35-24\", 1, 0),\n#     age_group_45_54 = ifelse(age_group == \"45-24\", 1, 0),\n#     age_group_55_64 = ifelse(age_group == \"55-24\", 1, 0),\n#     age_group_65plus = ifelse(age_group == \"65+\", 1, 0)\n#   ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab.html#check-age-recodes",
    "href": "labs/07-lab.html#check-age-recodes",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.3 Check age recodes",
    "text": "6.3 Check age recodes\nIt’s good practice when recoding, to check the output. Please use the table() to create a cross-tab of age_group and age_group_18_24.\n\n#|label: checkage\n\n# Compare age_group to age_group_18_24 using table()\n\nExplain in words how the variable age_group_18_24 relates to the variable age_group"
  },
  {
    "objectID": "labs/07-lab.html#recode-the-outcome",
    "href": "labs/07-lab.html#recode-the-outcome",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.4 Recode the outcome",
    "text": "6.4 Recode the outcome\nNow it’s your turn. Please do the following:\n\nLook at the variable voted using the table() function\n\n1 corresponds to Did not vote\n`2 corresponds to Voted\n96,97,98 to people who didn’t provide and answer, or didn’t remember\n99 corresponds to people who shouldn’t be in the sample (“Not in universe”)\n\nCreate a new variable called dv_voted using case_when() inside of mutate() that is:\n\n1 when voted == 2\n0 when voted == 1,\n0 when voted &gt; 2 & voted &lt;99\nNA when voted == 99\n\n\n\n# Look at distribution of voted using table()\n\n\n# Create variable dv_voted using mutate(), case_when(), and voted variable"
  },
  {
    "objectID": "labs/07-lab.html#save-the-recoded-data",
    "href": "labs/07-lab.html#save-the-recoded-data",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.5 Save the recoded data",
    "text": "6.5 Save the recoded data\nFinally, let’s save our recoded data to file called cps_clean.rda that we can use for next week’s lab\nUncomment and run the following:\n\n# save(cps, file = \"cps_clean.rda\")"
  },
  {
    "objectID": "labs/07-lab.html#write-down-aesthetic-mappings-from-the-figure",
    "href": "labs/07-lab.html#write-down-aesthetic-mappings-from-the-figure",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.1 Write down aesthetic mappings from the figure:",
    "text": "7.1 Write down aesthetic mappings from the figure:\nBefore we create this figure, think about the information conveyed by the figure’s aesthetics (the x axis, the y axis, the color of the squares), and the corresponding columns from policy_data that contain this information.\n\nx-axis:\ny-axis:\ncol:"
  },
  {
    "objectID": "labs/07-lab.html#create-a-variable-called-sdr",
    "href": "labs/07-lab.html#create-a-variable-called-sdr",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.2 Create a variable called SDR",
    "text": "7.2 Create a variable called SDR\nIt will be helpful to have a variable called SDR in policy_data that takes the value of “SDR” when sdr == 1 and “non-SDR” when sdr == 0\nPlease use case_when() or ifelse() to create SDR in policy_data\n\n# Create a variable called SDR in policy_data"
  },
  {
    "objectID": "labs/07-lab.html#recreate-figure-1-1",
    "href": "labs/07-lab.html#recreate-figure-1-1",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.3 Recreate Figure 1",
    "text": "7.3 Recreate Figure 1\nRecall, we need three things to make a figure:\n\ndata\naesthetics\ngeometries\n\nUsing data from policy_data starting in 1978 (hint add a filter()) and the aesthetic mappings identified above use ggplot() with the geom_point() geometry to make a version of Figure 1 from paper.\n\n# Recreate Figure 1"
  },
  {
    "objectID": "labs/07-lab.html#interpret-figure-1.",
    "href": "labs/07-lab.html#interpret-figure-1.",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.4 Interpret Figure 1.",
    "text": "7.4 Interpret Figure 1.\nPlease answer the following questions:\n\nHow many states had Same Day Registration at some point in time? XX states\nHow many states had Same Day Registration in 2018? XX states had SDR in 2018\nDid any states get rid of Same Day Registration? When did they get rid of this policy?\nWhat’s up with North Dakota?\n\nUse this code chunk to write any code that might help you answer these questions\n\n# Write code to help you answer the questions above (if needed)"
  },
  {
    "objectID": "labs/07-lab.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "href": "labs/07-lab.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.1 Calculate the proption voting by age group and SDR",
    "text": "8.1 Calculate the proption voting by age group and SDR\nWith the cps data, use group_by() and summarize to calculate the proportion of people voting by age group in states that did and did not have same day registration in the code chunk below.\nSave the results to a new object called fig2_df\n\n#  Calculate the proportion of voting by age group and SDR"
  },
  {
    "objectID": "labs/07-lab.html#recreate-figure-2-1",
    "href": "labs/07-lab.html#recreate-figure-2-1",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.2 Recreate Figure 2",
    "text": "8.2 Recreate Figure 2\nUsing fig2_df recreate a Figure 2 from the paper:\n\nfilter out values of age_group that are NA\nset the appropriate aesthetic mappings in ggplot()\nuse geom_bar(stat = \"identity\", position = \"dodge\")\n\n\n#Recreate Figure 2"
  },
  {
    "objectID": "labs/07-lab.html#interpret-figure-2",
    "href": "labs/07-lab.html#interpret-figure-2",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.3 Interpret Figure 2",
    "text": "8.3 Interpret Figure 2\nWhat does Figure 2 tell us? Figure 2 provides …"
  },
  {
    "objectID": "labs/07-lab.html#footnotes",
    "href": "labs/07-lab.html#footnotes",
    "title": "Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe CPS coding on this is not great and there’s no measure of ethnicity in these data. Forgive the crude indicators, but their necessary to recreate some of Grumbach and Hill’s analysis next week. ↩︎\nNote the way the recoding is described in the appendix to the paper is not how it is actually implemented in the replication code in rock_the_reg_replication_code.R. For example, the appendix describes income as ranging from 1 (Under $10k) to 16 ($500k and above), when their code, implemented above produces 32 unique values, in part because the way the CPS asked and coded the income question changed overtime. We’re going to roll with it for now…↩︎"
  },
  {
    "objectID": "labs/06-lab.html",
    "href": "labs/06-lab.html",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "",
    "text": "Today we will explore the critiques and alternative explanations for the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nRecall the core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\n\nA skeptic of this claim might argue that relationship between electoral and epidemelogical outcomes is spurious, saying somthing like:\n\nThere are lots of ways that Red States differ from Blue States — demographics, economics, geography, culture, and so on – and it is these differences that explain the phenomena of Red Covid. If we were to control for these omitted variables the relationship between a state’s partisan leanings and Covid-19 would go away.\n\nIn this lab, we will see how we can explore these claims using multiple regression to control for competing explanations.\nTo accomplish this we will:\n\nGet set up to work (10 minutes)\n\nThen we will estimate and interpret a series of regression models:\n\nA baseline Red Covid model using simple bivariate regression using the Republican vote share of states to predict the 14-day average of per capita Covid-19 deaths on September 23, 2021 (10 Minutes)\nA multiple regression model controlling for Republican vote share the median age (15 minutes)\nA model controlling for Republican vote share, the median age and median income (15 minutes)\nA model controlling for Republican vote share, the median age median income and vaccination rates (15 minutes)\nA model using Republican vote share, the median age median income to predict vaccination rates (15 minutes)\n\nFinally, we’ll take the weekly survey which will serve as a mid semester check in.\nOne of these 6 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/06-lab.html#load-packages",
    "href": "labs/06-lab.html#load-packages",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\nThere’s one new package, htmltools which we’ll use to display regression tables while we work.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "labs/06-lab.html#load-the-data",
    "href": "labs/06-lab.html#load-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the data that we created in class on Tuesday which provides a snapshot of the state of Covid-19 on September 23, 2021 in the U.S.\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nAfter running this code, the data frame covid_lab should appear in your environment pane in R Studio"
  },
  {
    "objectID": "labs/06-lab.html#describe-the-data",
    "href": "labs/06-lab.html#describe-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nIn the code chunk below, please write some code get an high level overview of the data:\n\n# High level overview\n# Number of observations and variables\n\n\n# Names of variables\n\n\n# Glimpse of data\n\n\n# Summary of data\n\n\n# Calculate standard deviations\n\nPlease use this HLO to answer the following questions:\n\nHow many observations are there:\nWhat is an observation (i.e. what is the unit of analysis):\nWhat is the primary outcome variable for today:\nWhat are the four main predictors we’ll be using:\nWill we be using the the raw values of these predictors or their standardized values?\nWhat is the standard deviation of our outcome and predictor variables:\n\nCovid-19 deaths:\nRepublican vote share:\nMedian age:\nMedian income:\nVaccination Rate:"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model",
    "href": "labs/06-lab.html#fit-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.1 Fit the model",
    "text": "2.1 Fit the model\n\nm1 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results",
    "href": "labs/06-lab.html#summarize-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.2 Summarize the results",
    "text": "2.2 Summarize the results\nNow we apply the summary() function to our model m1\n\nsummary(m1)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63271 -0.22488 -0.03769  0.13746  1.00634 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04817  11.741 7.51e-16 ***\nrep_voteshare_std  0.22682    0.04865   4.662 2.44e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.344 on 49 degrees of freedom\nMultiple R-squared:  0.3073,    Adjusted R-squared:  0.2931 \nF-statistic: 21.73 on 1 and 49 DF,  p-value: 2.44e-05\n\n\nWe see that m1 returns two coefficients, which define a line of best fit predicting Covid-19 deaths with the Republican vote share of the 2020 Presidential election:\n\n\\(\\beta_0\\) corresponds to the intercept. This is model’s prediction for a state where Trump got 0 percent of the vote. This is typically not something we care about.\n\\(\\beta_1\\) corresponds to the slope. Because we used a standardized measure of vote share, we would say that a 1-standard deviation (about 10 percentage points) increase in Republican vote share is associated with a 0.23 increase the average number of new Covid-19 deaths. Given that this per-capita measure has a standard deviation of 0.4, this is a fairly sizable association.\nFinally, note that last column of summary(m1) Pr(&gt;|t|) both the coefficients for the intercept \\((\\beta_0)\\) and rep_voteshare_std (\\((\\beta_1)\\)) are statistically significant (ie have an * next to them)."
  },
  {
    "objectID": "labs/06-lab.html#display-the-model-as-a-regression-table",
    "href": "labs/06-lab.html#display-the-model-as-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.3 Display the model as a regression table",
    "text": "2.3 Display the model as a regression table\nNext we’ll format the results of summary(m1) into a regression table using the htmlreg() function.\nRegression tables are a the standard way of concisely presenting the results of regression models.\n\nEach named row corresponds to the coefficients form the model\nIf there is an asterisks next to a coefficient, that coefficient is statistically significant with a p value below a certain threshold.\nThe numbers in parentheses below each coefficient correspond to the standard error of the coefficient (more on that later)2\nThe bottom of the table contains summary statistics of of our model, which we’ll ignore for today.\n\nThe code after htmlreg(m1) allows you to see what output of the table will look like in the html document while you’re working in the qmd file.\n\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n0.57***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab.html#visualize-the-model",
    "href": "labs/06-lab.html#visualize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.4 Visualize the model",
    "text": "2.4 Visualize the model\nNow let’s visualize the results of our m1 with a scatter plot.\nIn the code chunk below, I’ve written some comments to help you get started. You can also refer to last week’s lab for help\n\n# 1. Tell ggplot what data to use\n\n# 2. Set the aesthetic mappings of our figure\n \n# 3. Draw points with x values corresponding to Rep vote share and y values corresponding to Covid deaths. \n\n# 4. Add labels using `label=state_po` aesthetic (set in aes()) above\n\n# 5. Plot the regression model using geom_smooth(method = \"lm\")\n\n# 6. Change the axis labels"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results",
    "href": "labs/06-lab.html#interpret-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.5 Interpret the results",
    "text": "2.5 Interpret the results\nIn a sentence our two, summarize the results of your analysis in this section\nYou words here!"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-1",
    "href": "labs/06-lab.html#fit-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.1 Fit the model",
    "text": "3.1 Fit the model\nNow let’s test our skeptics’ claims by fitting a model m2 that controls for Age (med_age_std).\n\nRemember the first argument in lm() is formula of the form outcome variable ~ predictor1 + predictor2 + ...\n\n\n# m2"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-model",
    "href": "labs/06-lab.html#summarize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.2 Summarize the model",
    "text": "3.2 Summarize the model\nNow let’s print out a statistical summary of m2\n\n# summary of m2"
  },
  {
    "objectID": "labs/06-lab.html#display-the-model-as-a-regression-table-1",
    "href": "labs/06-lab.html#display-the-model-as-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.3 Display the model as a regression table",
    "text": "3.3 Display the model as a regression table\nNext, let’s create a regression table that displays m1 in the first column and m2 in the second column.\n\nTo do this, change list(m1) from the code above to list(m1, m2)"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results-1",
    "href": "labs/06-lab.html#interpret-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.4 Interpret the results",
    "text": "3.4 Interpret the results\nIn a few sentences, explain whether the results from m2 support the skeptics criticisms or not?"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-2",
    "href": "labs/06-lab.html#fit-the-model-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.1 Fit the Model",
    "text": "4.1 Fit the Model\nPlease fit a model called m3 implied by the skeptic’s revised claims\n\n# m3"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-model-1",
    "href": "labs/06-lab.html#summarize-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.2 Summarize the model",
    "text": "4.2 Summarize the model\nSummarize the model m3 using summary()\n\n# summary m3"
  },
  {
    "objectID": "labs/06-lab.html#display-the-models-in-a-regression-table",
    "href": "labs/06-lab.html#display-the-models-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.3 Display the models in a regression table",
    "text": "4.3 Display the models in a regression table\nAnd then display the results of models m1, m2, and m3.\n\n# regression table of m1, m2, m3"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-skeptics-claims",
    "href": "labs/06-lab.html#interpret-the-skeptics-claims",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.4 Interpret the skeptic’s claims",
    "text": "4.4 Interpret the skeptic’s claims\nIn a few sentences, explain whether the results from m3 support the skeptics criticisms or not?\nControlling for median age and income, the coefficient on Republican sote share decreases in size by more than half and is no longer statistically significant. The coefficient on median income is statistically significant and substantively suggests that states with higher median incomes tended to have fewer Covid-19 deaths on September 23, 2021."
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-3",
    "href": "labs/06-lab.html#fit-the-model-3",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.1 Fit the model",
    "text": "5.1 Fit the model\nYou know the drill.\n\n# m4"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results-1",
    "href": "labs/06-lab.html#summarize-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.2 Summarize the results",
    "text": "5.2 Summarize the results\nAgain, let’s get a quick summary of our results\n\n# summary of m4"
  },
  {
    "objectID": "labs/06-lab.html#display-the-models-in-a-regression-table-1",
    "href": "labs/06-lab.html#display-the-models-in-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.3 Display the models in a regression table",
    "text": "5.3 Display the models in a regression table\nAnd add m4 to list of models in our regression table"
  },
  {
    "objectID": "labs/06-lab.html#interpet-the-results",
    "href": "labs/06-lab.html#interpet-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.4 Interpet the results",
    "text": "5.4 Interpet the results\nBriefly interpret the results of m4"
  },
  {
    "objectID": "labs/06-lab.html#fit-the-model-4",
    "href": "labs/06-lab.html#fit-the-model-4",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.1 Fit the model",
    "text": "6.1 Fit the model\nNow let’s fit the model. For ease of interpretation, let’s use the unstandardized measure of vaccination rates, percent_vaccinated as our outcome variable.\n\n# m5"
  },
  {
    "objectID": "labs/06-lab.html#summarize-the-results-2",
    "href": "labs/06-lab.html#summarize-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.2 Summarize the results",
    "text": "6.2 Summarize the results\nAnd summarize the results\n\n# summary of m5"
  },
  {
    "objectID": "labs/06-lab.html#display-the-results-in-a-regression-table",
    "href": "labs/06-lab.html#display-the-results-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.3 Display the results in a regression table",
    "text": "6.3 Display the results in a regression table\nDisplay them in a regression table"
  },
  {
    "objectID": "labs/06-lab.html#interpret-the-results-2",
    "href": "labs/06-lab.html#interpret-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.4 Interpret the results",
    "text": "6.4 Interpret the results\nSummarize the results of m5 and offer some broader discussion of what we’ve learned today"
  },
  {
    "objectID": "labs/06-lab.html#footnotes",
    "href": "labs/06-lab.html#footnotes",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn short, these * correspond to \\(p-values\\) below different thresholds. One * typically means \\(p &lt; 0.05\\). A p-value is a conditional probability that arises from a hypothesis test summarizing the likelihood of observing a particular test statistic (here a regression coefficient, or more specifically, a t-statistic which is the regression coefficient divided by its standard error) given a paritcular hypothesis (typically, but not allows a null hypothesis that the true coefficient is 0). In sum, a p-value assess the likelihood of seeing what we did, if in fact, there was no relationship. If that likelihood is small (p&lt;0.05), we reject the claim of no relationship. We remain uncertain about the true value of the coefficient, but we are pretty confident it’s not 0.↩︎\nA standard error is another one of those things that in the cart we’re putting before horse today. Briefly, it is an estimate of the standard deviation of the sampling distribution of a coefficient and describes how much our coefficient might vary had we had a different sample…↩︎"
  },
  {
    "objectID": "labs/04-lab.html",
    "href": "labs/04-lab.html",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "In this assignment, we’ll walk through the logic and design of Ferwerda and Miller (2014).\n\n\nConceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/04-lab.html#goals",
    "href": "labs/04-lab.html#goals",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "Conceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/02-lab-comments.html",
    "href": "labs/02-lab-comments.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to first, reproduce this figure:\n\n\n\n\n\n\n\n\n\nAnd then adapt and improve this figure (or other figures) to explore questions we have about the data\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides and the comments to last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nHow do I produce a figure that shows the average rate of new cases pe month for states with a particular type of face mask policy?\n\nWell first, I’ll need to load some packages to work with and visualize data.\nThen, I’ll need to get the data. And then…\n\n\nThink and write programmatically\n\nIn this .qmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll be working through this collectively\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nYou shouldn’t have to write much new code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!\n.html file{.unnumbered}\nFor every lab:\n\nDownload the file\nSave it in your course folder\nRender the document\nOpen the html file in your browser (Easier to read)\nCheck Render on Save and render the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "href": "labs/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.1 Create an object listing all the packages I will use today",
    "text": "2.1 Create an object listing all the packages I will use today\nThis code creates a object called the_packages which contains a vector of character strings corresponding to the names of the packages I want to use today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)"
  },
  {
    "objectID": "labs/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "href": "labs/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.2 Define a function to install and load packages",
    "text": "2.2 Define a function to install and load packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "labs/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "href": "labs/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.3 Use the ipak function to load the necessary packages",
    "text": "2.3 Use the ipak function to load the necessary packages\nNow I run the ipak() giving it the object the_packages as an input. It sorts through the packages, checks to see if they’re installed, if not installs them, and then loads all of the packages so I can use them.\n\nipak(the_packages)\n\n   tinytex kableExtra  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally    COVID19 \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n      maps    mapdata         DT \n      TRUE       TRUE       TRUE"
  },
  {
    "objectID": "labs/02-lab-comments.html#filter-out-u.s.-territories",
    "href": "labs/02-lab-comments.html#filter-out-u.s.-territories",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.1 Filter out U.S. Territories",
    "text": "5.1 Filter out U.S. Territories\nFor simplicity, (and practice filtering observations), I’ve asked us to remove observations from U.S. territories.\nThe code below\n\nCreates an object called us_territories.\nUse this object to filter out observations that are US territories\nCreates a new data frame that is just observations from the 50 U.S. states. and D.C.\nChecks that this recoding seems to have worked\n\n\n# U.S. Territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out U.S. Territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\n# Check to make sure covid_us contains only 50 states and D.C.\ndim(covid)\n\n[1] 58809    47\n\ndim(covid_us)\n\n[1] 53678    47\n\nlength(unique(covid$administrative_area_level_2)) \n\n[1] 56\n\nlength(unique(covid_us$administrative_area_level_2)) == 51\n\n[1] TRUE"
  },
  {
    "objectID": "labs/02-lab-comments.html#create-a-state-variable",
    "href": "labs/02-lab-comments.html#create-a-state-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.2 Create a state variable",
    "text": "5.2 Create a state variable\nThis is purely for convenience, because typing administrative_area_level_2 is annoying. The code copies the values of this variable into a new variable called state using the mutate() function.\nMutate returns the original data frame plus the new column. We have to save this output for our our changes to persist (i.e. we have to assign the output of mutate() back into covid_us)\nIn last week’s lab, I just piped the output to the next command, did some more recoding with mutate, and then finally saved the output back into covid_us. In this lab, I’ll save the output after each step.\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) -&gt; covid_us"
  },
  {
    "objectID": "labs/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "href": "labs/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.3 Group by the state variable to calculate new Covid-19 cases",
    "text": "5.3 Group by the state variable to calculate new Covid-19 cases\nNow I use this shorter variable state to calculate the number of new cases (new_cases) in a given state on a given date, and rescale this variable so that it’s expressed in terms of new cases per 100,000 residents.\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population * 100000\n    ) -&gt; covid_us\n\nThe slides from Tuesday, helped demonstrate what this code was doing, and why we wanted to group by state.\nHere’s an example for a subset of the data from April 1, 2020 to April 7, 2020\nWe see that the lag() function simply moves the observation of a variable “up” one row so that we can take the difference between the total number of cases in a state on one date and the total number of cases on the date before, to calculate the number of new cases\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  group_by(state) %&gt;%\n  select(state, date, confirmed) %&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\n# A tibble: 306 × 5\n# Groups:   state [51]\n   state      date       confirmed confirmed_lag1 new_cases\n   &lt;chr&gt;      &lt;date&gt;         &lt;int&gt;          &lt;int&gt;     &lt;int&gt;\n 1 Minnesota  2020-04-01       689             NA        NA\n 2 Minnesota  2020-04-02       742            689        53\n 3 Minnesota  2020-04-03       789            742        47\n 4 Minnesota  2020-04-04       865            789        76\n 5 Minnesota  2020-04-05       935            865        70\n 6 Minnesota  2020-04-06       986            935        51\n 7 California 2020-04-01      9857             NA        NA\n 8 California 2020-04-02     11190           9857      1333\n 9 California 2020-04-03     12569          11190      1379\n10 California 2020-04-04     13796          12569      1227\n# ℹ 296 more rows\n\n\nIf we hadn’t grouped by state, then when we lagged the confirmed variable, R thinks the number of confirmed cases in California before April 1, 2020, is 986 which is actually the number of cases in Minnesota on April 7, 2020\n\n# No group_by would create errors where the last observation from\n# one state becomes the first lagged observation for the next state\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;%\n  ungroup() %&gt;%\n  select(state, date, confirmed) %&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\n# A tibble: 306 × 5\n   state      date       confirmed confirmed_lag1 new_cases\n   &lt;chr&gt;      &lt;date&gt;         &lt;int&gt;          &lt;int&gt;     &lt;int&gt;\n 1 Minnesota  2020-04-01       689             NA        NA\n 2 Minnesota  2020-04-02       742            689        53\n 3 Minnesota  2020-04-03       789            742        47\n 4 Minnesota  2020-04-04       865            789        76\n 5 Minnesota  2020-04-05       935            865        70\n 6 Minnesota  2020-04-06       986            935        51\n 7 California 2020-04-01      9857            986      8871\n 8 California 2020-04-02     11190           9857      1333\n 9 California 2020-04-03     12569          11190      1379\n10 California 2020-04-04     13796          12569      1227\n# ℹ 296 more rows"
  },
  {
    "objectID": "labs/02-lab-comments.html#recode-the-facial_coverings-variable",
    "href": "labs/02-lab-comments.html#recode-the-facial_coverings-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.4 Recode the facial_coverings variable",
    "text": "5.4 Recode the facial_coverings variable\nNext we use the case_when() function inside the mutate() function to create a variable called face_masks based on the values of the facial_coverings variable in the data.\ncase_when() when uses R’s ability to make logical comparisons. When the variable facial_coverings equals 0, R will input the character string \"No policy\" into the face_masks variable.\nWhen the absolute value of facial_coverings equals 1 (i.e.facial_coverings equals 1 or -1 ), R will input the character string \"Recommended\" into the face_masks variable. And so on.\nWe use the abs() function to take the absolute value of the facial_coverings variable because codebook for these data implied:\n\nIn short: positive integers identify policies applied to the entire administrative area. Negative integers are used to identify policies that represent a best guess of the policy in force, but may not represent the real status of the given area. The negative sign is used solely to distinguish the two cases, it should not be treated as a real negative value.\n\nWe know from last weeks lab, that negative values in the U.S. typically seem to be cases where a city had a more stringent policy than the state (e.g. Chicago adopts more stringent face mask policies than Illinois).\nFinally, we put a %&gt;% after the output of case_when() and pass it’s output to the factor() function.\nThe . acts as sort of placeholder, factor() expects some input here (like a variable from a data frame), . tells R to use the output of case_when().\nThe levels = then transforms the character data produced by case_when() into a factor with an implicit ordering of levels (i.e. “No policy” &lt; “Recommended”&lt; “Some requirements” &lt;“Required shared places” &lt;“Required all times”) which turns out to be useful trick for organizing how data are plotted and visualized.\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "href": "labs/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.5 Create a variable capturing the year and month of the observation",
    "text": "5.5 Create a variable capturing the year and month of the observation\nFinally we create some variables that extract components of an observation’s date:\n\nyear = year(date) returns just the year from a variable of class Date\nmonth = month(date) returns just the month from a variable of class Date\nyear_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\") pastes these to variables together.\nstr_pad(month, width = 2, pad=0) adds a leading 0 to any month with only 1 digit, to ensure that all the months have 2 characters.\n\nThe code from your lab also calculates the percent of a states population that is vaccinated, which isn’t strictly needed for today.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nCreating separte year and month variables aren’t strictly necessary,\nWe could have written something like:\n\ncovid_us %&gt;%\n  mutate(\n    year_month = paste(year(date), str_pad(month(date), width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nBut that year_month line was already feeling kind of clunky, and maybe we’ll want the year and month variables later."
  },
  {
    "objectID": "labs/02-lab-comments.html#adding-meaningful-labels-and-title",
    "href": "labs/02-lab-comments.html#adding-meaningful-labels-and-title",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.1 Adding meaningful labels and title",
    "text": "8.1 Adding meaningful labels and title\nBecause we saved the output of our ggplot to an object called fig1 we can add additional commands to this object using the + without having to rewrite all the code.\nFirst let’s add better labels to the graph.\n\nNote that even though we flipped the coordinates, the aes aesthetic mappings stay the same. So to change the label of the figures y-axis to “Date” we change the label of x = \"Date\"\nggplot automatically generates a legend for aesthetic mappings like color We can add a line break using the the special character \\n in our code\n\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\n Policy\"\n  )\n\n\n\n\n\n\n\n\nNote the code above didn’t update fig1\n\nfig1\n\n\n\n\n\n\n\n\nWe have to save the output (if we like it) for our changes to persist.\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\nPolicy\"\n  ) -&gt; fig1\n\nfig1"
  },
  {
    "objectID": "labs/02-lab-comments.html#changing-the-theme-of-the-plot",
    "href": "labs/02-lab-comments.html#changing-the-theme-of-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.2 Changing the theme of the plot",
    "text": "8.2 Changing the theme of the plot\nHere’s an example of some different themes\n\n# Black and white\nfig1 +\n  theme_bw()\n\n\n\n\n\n\n\n# Minimal\nfig1 +\n  theme_minimal()\n\n\n\n\n\n\n\n# Classic\nfig1 +\n  theme_classic()\n\n\n\n\n\n\n\n\nThis is pretty personal, and depends of the figure itself. I like a white background and some guide lines:\n\nfig1 +\n  theme_bw() -&gt; fig1\n\nfig1"
  },
  {
    "objectID": "labs/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "href": "labs/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.3 Make the size of the dots reflect the number of states with this policy",
    "text": "8.3 Make the size of the dots reflect the number of states with this policy\nIn the cases_by_month_and_policy we have a column called n which is the number of states which had a given policy in a given month.\nWe can add an aesthetic to our plot that varies the size of the points by the number of states.\n\nfig1 +\n  aes(size = n) -&gt; fig1\n\nWe call this type of plot a bubble plot{target=“_blank”\nI have mixed feelings about multiple legends. We can remove the legend for size using the scale_size() function. I had to Google how to do this for the millionth time.\n\nfig1 +\n  scale_size(guide = \"none\") -&gt; fig1"
  },
  {
    "objectID": "labs/02-lab-comments.html#facet-the-plot",
    "href": "labs/02-lab-comments.html#facet-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.4 Facet the plot",
    "text": "8.4 Facet the plot\nVarying the size of the dots by the number of states conveys more information. But makes the chart a little harder to read. Dots overlap.\nThe facet_wrap command will produce separate bubble plots for each level of the “facetting” variable, in this case `face_masks\n\nfig1 +\n  facet_wrap(~face_masks) -&gt; fig1\nfig1\n\n\n\n\n\n\n\n\nNow I think also want a second legend for the number of states\n\nfig1 +\n  scale_size(guide = \"legend\")+\n  labs(\n    size = \"# of States\\nwith Policy\"\n  )-&gt; fig1\nfig1\n\n\n\n\n\n\n\n\nThis seems pretty good if our goal was to show in general terms\n\nIt shows the average number new cases for states with a given face mask policy over time.\nIt shows how the mix of types of face mask policies states have adopted has changed over time\n\nIf our goal was to make comparisons across face mask policies over a given time period, I’m might still prefer something closer to our original graph."
  },
  {
    "objectID": "labs/10-lab.html",
    "href": "labs/10-lab.html",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "",
    "text": "In our final lab, you will apply concepts and skills from this course to explore data from the 2020 American National Election Study. Specifically you will\n\nIdentify an outcome of interest (5-10 minutes)\nIdentify key predictors and covariates (5-10 minutes)\nRecode your data (20 minutes)\nDescribe your data (20 minutes)\nDescribe your question, expectations, and models (10 minutes)\nEstimate, present, and interpret your models (20 minutes, Graded Question)\n\nIdeally, each group will pursue a question that interests them. I will also complete these tasks live, so, if you’re not feeling confident, you can follow along with me and submit the code I demo in class as your lab for a grade of 85.."
  },
  {
    "objectID": "labs/10-lab.html#summary-statistics",
    "href": "labs/10-lab.html#summary-statistics",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "4.1 Summary statistics:",
    "text": "4.1 Summary statistics:\nProducing a table of summary statistics requires a little foresight.\nEssentially you want to make a data frame where each row is a (numeric) variable, and each column is a statistic (minimum, 25th percentile, median, mean, 75th percentile, max, Number of missing).\nTo do this, I would:\n\ncreate a object called the_vars which contains the names (in quotation marks) of the variables you want to summarize.\nSelect these variables from your data set. using df%&gt;%select(all_of(the_vars))\nUse %&gt;%pivot_longer() specifying cols=select(all_of(the_vars)), and names_to equals \"Variable\" and values_to = \"value\" to transform this wide dataset into a long dataset\nThen use %&gt;%group_by(Variable)%&gt;% and summarise() to calculate the statistics for each variable of interest (e.g. %&gt;%summarise(Mean = mean(value, na.rm=T))))\nSave the output to an object called something like sum_df\nIn a new chunk use knitr::kable(sum_df) %&gt;% kableExtra::kable_styling() to format your table. Set echo=F in the code chunk head\n\n\n# Summarise data\n\n# Display results"
  },
  {
    "objectID": "labs/10-lab.html#descriptive-figures",
    "href": "labs/10-lab.html#descriptive-figures",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "4.2 Descriptive Figures",
    "text": "4.2 Descriptive Figures\nTo create a figure, you’ll need to specificy the following\n\ndata (e.g. df %&gt;%)\naesthetic mappings, ggplot(aes(x = predictor, y = outcome))\ngeometries\n\nUnivariate: geom_density(), geom_boxplot() geom_histogram()\nBivariate: geom_point() (for a scatterplot), geom_line() for a trend.\n\n\nOnce you have a minimal working example, play around with other grammars of graphics:\n\nlabs() for custom labels\ntheme_XXX for custom themes\nfacet_wrap(~group) to produce the same plot facetted by some categorical grouping variable\n\nWhen you’re happy with your figure, save it as object in R (e.g. fig1 &lt;- df %&gt;% ggplot(aes(predictor, outcome))+geom_point()). Put that object in its own chunk to display it in your document.\nDon’t let the perfect be the enemy of the good.\n\n# Descriptive figures"
  },
  {
    "objectID": "labs/10-lab.html#descrptive-interpretation",
    "href": "labs/10-lab.html#descrptive-interpretation",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "4.3 Descrptive Interpretation:",
    "text": "4.3 Descrptive Interpretation:\nPlease provide an overview of the data (source, number of observations, unit of analysis).\nDescribe a typical observation, making reference to the statistics in your summary table.\nOffer a substantive interpretation of your descriptive figure(s). What do they tell us about the distribution of a key variable, or the relationship between two variables."
  },
  {
    "objectID": "labs/10-lab.html#fit-the-models",
    "href": "labs/10-lab.html#fit-the-models",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "6.1 Fit the models",
    "text": "6.1 Fit the models\n\n# Model 1: Bivariate Model\n\n# Model 2: Multiple Regression"
  },
  {
    "objectID": "labs/10-lab.html#display-the-models-in-a-regression-table",
    "href": "labs/10-lab.html#display-the-models-in-a-regression-table",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "6.2 Display the models in a regression table",
    "text": "6.2 Display the models in a regression table\n\n# Regression table"
  },
  {
    "objectID": "labs/10-lab.html#interpet-your-models",
    "href": "labs/10-lab.html#interpet-your-models",
    "title": "Lab 10 / Assignment 3 - Final Project Data Setup and Explorations",
    "section": "6.3 Interpet your models",
    "text": "6.3 Interpet your models\nPlease write a 1 paragraph summary interpreting your results in terms of both their statistical and substantive significance. Assume your audience is smart, but has never taken POLS 1600. Explain to them what a regression model is, what a standard error, p-value, and/or confidence interval is. How should they interepret the substantive findings of your model. How should they assess the statistical uncertainty around these results?\nPerhaps you might reade create a plot of predicted values from a model to help facilitate the substantive interpretation of your results. If so, here’s a code chunk for you:\n\n# Additional code chunk to facilitate interpretation of models"
  },
  {
    "objectID": "labs/09-lab.html",
    "href": "labs/09-lab.html",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "",
    "text": "Today, we will return to exploring Russians’ support the war in Ukraine using a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project.\nThe survey was conducted by phone using a random sample of mobile phone numbers to produce a sample of respondents representative of the population in terms of age, sex, and geography. It was in the field from February 28 to March 2.\nFirst, we will explore how support for the war varies with the demographic predictors age, sex and education. We will compare the results of modeling this relationship using Ordinary Least Squares regression and Logisitic Regression\nWe’ll talk more about the technical aspects of logistic regression next week. For today we’ll simply compare the results from these two approaches.\nNext, we will gain insight into how our estimates from these models might vary using the statistical process of bootstrapping. Specifically, we will simulate the idea of repeated sampling that is the foundation of frequentist interpretations of probability, by sampling from our sample with replacement.\nWe’ll walk through the mechanics of simulation together. Then you’ll quantify the uncertainty described by these bootstrapped sampling distributions.\nFinally, we’ll see what other questions we might ask of these data and practice various skills we’ve developed through out the course.\nPlan to spend the following amount of time on each section\n\nGet set up to work (5 minutes)\nModel the relationship between demographic predictors and war support using OLS and Logistic regression (20 minutes)\nAssess the uncertainty around your estimated coefficients (15 minutes)\nQuantify the uncertainy described by your sampling distributions (10 minutes)\nExplore other relationships in the data. (30 minutes)\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file produced by knitting the lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nYou can find your assigned groups in previous labs"
  },
  {
    "objectID": "labs/09-lab.html#load-packages",
    "href": "labs/09-lab.html#load-packages",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\", \"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"boot\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled        modelr \n         TRUE          TRUE          TRUE          TRUE          TRUE \n        purrr         ggmap       ggrepel      ggridges      ggthemes \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       ggpubr        GGally        scales       dagitty         ggdag \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggforce       COVID19          maps       mapdata           qss \n         TRUE          TRUE          TRUE          TRUE          TRUE \n   tidycensus     dataverse DeclareDesign          boot \n         TRUE          TRUE          TRUE          TRUE \n\n\nThere are three packages in particular that we’ll need to maker sure are installed and loaded\n\nmodelr\npurrr\nbroom\n\nIf ipak didn’t return TRUE for each of these, please uncomment and run:\n\n# install.packages(\"modelr\")\n# install.packages(\"purrr\")\n# install.packages(\"broom\")"
  },
  {
    "objectID": "labs/09-lab.html#load-the-data",
    "href": "labs/09-lab.html#load-the-data",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the recoded data for the lab\nOur primary outcome of interest (dependent variable) for today is a binary measure of support for war:\n\nsupport_war01 “Please tell me, do you support or do not support Russia’s military actions on the territory of Ukraine?” (1=yes, 0 = no)\n\nOur key predictors (independent variables) are the following demographic variables:\n\nage “How old are you?”\nsex “Gender of respondent” (As assessed by the interviewer)\neducation_n “What is your highest level of education (confirmed by a diploma, certificate)?” (1 = Primary school, 2 = “High School”, 3 = “Vocational School” 4 = “College”, 5 = Graduate Degree)1\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))"
  },
  {
    "objectID": "labs/09-lab.html#fit-the-models",
    "href": "labs/09-lab.html#fit-the-models",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.1 Fit the models",
    "text": "2.1 Fit the models\nPlease estimate the following models:\n\nAn OLS regression model called m1 using lm()\nA Logistic regression model called m2 using glm() with family=binomial\n\n\n# OLS\n# m1 &lt;- ???\n\n# Logisitic \n# m2 &lt;- ???"
  },
  {
    "objectID": "labs/09-lab.html#display-the-results-in-a-regression-table",
    "href": "labs/09-lab.html#display-the-results-in-a-regression-table",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.2 Display the results in a regression table",
    "text": "2.2 Display the results in a regression table\nNext, please display the results of your regressions in a table using htmlreg()\n# Regression Table"
  },
  {
    "objectID": "labs/09-lab.html#produce-predicted-values-from-the-model",
    "href": "labs/09-lab.html#produce-predicted-values-from-the-model",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.3 Produce predicted values from the model",
    "text": "2.3 Produce predicted values from the model\nThe coefficients from a logistic regression aren’t easy to directly interpret.\nInstead, we will produce predicted values for each model\nTo do this, we will need to create a prediction dataframe called pred_df Every variable in your model, needs to be represented in your prediction data frame.\n\nUse expand_grid() to create a data frame where\n\nage varies from 18 to 99\nsex is held constant at “Female”\neducation_n is held constant at its mean\n\n\n\n## Create prediction data frame\n# pred_df &lt;- expand_grid(\n#   age = ??? : ???,\n#   ??? = ???,\n#   ???\n# )\n\nThen you use the predict() function to produce predicted values from each model.\nSave the output of predict() for m1 to a new column in pred_df called pred_ols.\nFor m2 you need to tell are to transform the predictions from m2 back into the units of the response (outcome) variable, by setting the argument type = \"response\". Save the output of predict() for m1 to a new column in pred_df called pred_logit.\n\n# #Predicted values for m1\n# pred_df$pred_ols &lt;- predict(???,\n#                             newdata = ???)\n# #Predicted values for m2\n# #Remember to add type = \"response\"\n# pred_df$pred_logit &lt;- ???"
  },
  {
    "objectID": "labs/09-lab.html#plot-the-predicted-values-and-interpet-the-results",
    "href": "labs/09-lab.html#plot-the-predicted-values-and-interpet-the-results",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.4 Plot the predicted values and interpet the results",
    "text": "2.4 Plot the predicted values and interpet the results\nNow we can compare the predictions of OLS and Logistic regression by plotting the predicted values of support for the war from each model.\nTo produce this plot you’ll need to\n\nspecify data (you want to use the values from pred_df)\nmap values from your data aesthetics in ggplot\n\nput age on the x axis and and pred_ols on the y-axis\n\nspecify the geometries to plot\n\nadd two geom_line() to the plot\nleave the first one empty (e.g. geom_line())\nfor the second, specify a new aes of y=pred_logit\n\n\n\n# data %&gt;%\n\n# aesthetics with ggplot()\n\n# geometries geom_line()\n\nHow do the predictions of the two models compare\nSo the predictions from OLS produce impossible values (levels of support above 100 percent) at for very old respondents, while the predictions from logistic regression are constrained to be between 0 and 1.\nIf we think that logistic regression provides a more credible way of modeling support for the war, then our OLS regression appears to overstate the level of support among young and old, while possibly understating the level of support among the middle age. The differences aren’t huge – a few percentage points – but for a binary outcome we will often prefer to model it with logistic regression.\nAlso note that marginal effect for age in the logistic regression is not constant. An increase in age from 25 to 26 is associated with a larger increase in support, than an increase in age from 75 to 76."
  },
  {
    "objectID": "labs/09-lab.html#take-1000-bootstrap-samples-from-df_drww",
    "href": "labs/09-lab.html#take-1000-bootstrap-samples-from-df_drww",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.1 Take 1,000 bootstrap samples from df_drww",
    "text": "3.1 Take 1,000 bootstrap samples from df_drww\nBelow we create 1,000 boostrapped samples\n\n# Make sure these packages are loaded\nlibrary(modelr)\nlibrary(purrr)\nlibrary(broom)\n# Set random seed for reproducability\n\nset.seed(1234)\n\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n\nLet’s take a moment to understand what boot is and why we’re sampling with replacement.\nThe object boot contains 1,000 bootstrapped samples from df_drww.\nIf we look at the first bootstrap we see:\n\nboot$strap[[1]]\n\n&lt;resample [1,807 x 42]&gt; 1308, 1018, 1125, 1004, 623, 905, 645, 934, 400, 900, ...\n\n\nThe numbers 1308, 1018, 1125, 1004, 623, 905, ... correspond to rows in df_drww. So person 1308 is the first observation in this boot strap sample, then person 1018 and so on.\nBecause we are sampling with replacement observations from df_drww can appear multiple times. In our first bootstrap sample:\n\n666 observations appeared once\n342 appeared twice\n105 appeared three times\n27 appeared four times\n5 appeared five times.\n2 appeared six times\n\n\ntable(table(boot$strap[[1]]$idx))\n\n\n  1   2   3   4   5   6 \n666 342 104  27   5   2 \n\n\nWhy would we want to sample with replacement?\nWell, what we’d really love are 1,000 separate random surveys all drawn from the same population in the same way.\nSince that’s not feasible, we instead use the one sample we do have to learn things like how much our estimate might vary in repeated sampling. Efron (1979) called this procedure “bootstrapping” after the idiom “to pull oneself up by one’s own bootstraps”\nWe do this by sampling from our sample with replacement.\nWhen we sample with replacement, we are sampling from our sample, as our sample was sampled from the population.\nWith replacement means that some observations will appear multiple times in our bootstrapped sample (while others will not be included at all).\nWhen an observation appears multiple times in a bootstrap sample, conceptually, we’re using that original observation to represent the other people like that observation in the population who – had we taken a different sample – might have been included in our data.\nNote each bootstrap sample is a different random sample with replacement. In our second bootstrap sample, one observation (person 1496) appeared five times.\n\ntable(table(boot$strap[[2]]$idx))\n\n\n  1   2   3   4   5   6 \n661 342 105  31   1   3 \n\n# Person 406\nsum(boot$strap[[2]]$idx == 1496)\n\n[1] 5\n\n# Person 1 is not in boostrap 2\nsum(boot$strap[[2]]$idx == 1)\n\n[1] 0"
  },
  {
    "objectID": "labs/09-lab.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "href": "labs/09-lab.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples",
    "text": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples\nNow let’s estimate our model for each bootstrapped sample, using the map function.\n\nIn the code below, for every sample in boot, map estimates the model lm(support_war01 ~ age + sex + education_n) plugging in the bootstrap sample into the data=..\n\n\n# bootstrap simulations\nbs_ols &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + sex + education_n, data =.))\n\nThe end result is a large list with 1,000 separate linear regression models estimated on each bootstrapped sample.\nThe coefficients from each bootstrap vary from one simulation\n\n# First bootstrap\nbs_ols[[1]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.305019     0.009746     0.081039    -0.024142  \n\n\nto the next:\n\n# Second boostrap\nbs_ols[[2]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.245000     0.009142     0.095631    -0.009285  \n\n\nBecause they’re estimated off of different bootstrapped samples.\nWe will visualize and quantify that variation to describe the uncertainty associated with our estimates.\nBut first, we need to transform our large list of linear models, into a more tidy data frame that’s easier to manipulate."
  },
  {
    "objectID": "labs/09-lab.html#tidy-the-results-of-our-bootstrapping",
    "href": "labs/09-lab.html#tidy-the-results-of-our-bootstrapping",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.3 Tidy the results of our bootstrapping",
    "text": "3.3 Tidy the results of our bootstrapping\nIn the code below we transform this large list of models into a tidy data frame of coefficients.\n\n# Tidy bootstrapp sims\nbs_ols_df &lt;- map_df(bs_ols, tidy, .id = \"id\")\n\nIn the resulting data frame the id variable identifies the bootstrap simulation (1 to 1,000), the term variable indentifies the specific coefficient from the model estimated for that simulation.\n\nhead(bs_ols_df)\n\n# A tibble: 6 × 6\n  id    term        estimate std.error statistic  p.value\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 1     (Intercept)  0.305    0.0522        5.84 6.29e- 9\n2 1     age          0.00975  0.000702     13.9  3.31e-41\n3 1     sexMale      0.0810   0.0222        3.65 2.73e- 4\n4 1     education_n -0.0241   0.0107       -2.26 2.39e- 2\n5 2     (Intercept)  0.245    0.0533        4.60 4.61e- 6\n6 2     age          0.00914  0.000731     12.5  3.36e-34"
  },
  {
    "objectID": "labs/09-lab.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "href": "labs/09-lab.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age",
    "text": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age\nFinally, let’s get a sense of how our coefficients could vary.\nSpecifically, let’s compare the the observed coefficient from m1 for age, to the bootstrapped sampling distribution of coefficients in bs_ols_df\n\nFirst, we’ll create a basic plot called p_ols_age that shows the distribution of the coefficients for age from our simulation\n\n\np_ols_age &lt;- bs_ols_df %&gt;%\n  filter(term == \"age\") %&gt;%\n  ggplot(aes(estimate))+\n    geom_density()\n\np_ols_age\n\n\n\n\n\n\n\n\nNext we’ll add some additional geometries and labels to our figure\n\nFirst we’ll put a rug to show the individual coefficients\n\n\np_ols_age +\n  geom_rug() -&gt; p_ols_age\n\np_ols_age\n\n\n\n\n\n\n\n\n\nThen we’ll add a vertical line where our observed coefficient on age\n\n\np_ols_age +\n  geom_vline(xintercept =  coef(m1)[2],\n             linetype = 2) -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'm1' not found\n\np_ols_age\n\n\n\n\n\n\n\n\n\nFinally, let’s add some nice labels\n\n\np_ols_age +\n  theme_bw()+\n  labs(\n    x = \"Age\",\n    y = \"\",\n    title = \"Bootstrapped Sampling Distribution of Age Coefficient\"\n  ) -&gt; p_ols_age\n\np_ols_age\n\n\n\n\n\n\n\n\nCongratulations you’ve just produced and visualized your first bootstrapped sampling distribution!\nConceptually, this distribution describes *how much we would expect the coefficient on age in model to vary** from sample to sample.\nJust from eyeballing the figure above, it looks like the observed the relationship between age and support for war or 0.009 could be about as high as 0.011, and as low as 0.007.\nOf course, as the budding quantitative social scientists that we are, we can do better than just eyeballing the data."
  },
  {
    "objectID": "labs/09-lab.html#footnotes",
    "href": "labs/09-lab.html#footnotes",
    "title": "Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think, google translate was a bit unclear. But higher numbers equal more education.↩︎"
  },
  {
    "objectID": "labs/01-lab.html",
    "href": "labs/01-lab.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/01-lab.html#uncomment-and-run-the-following-code",
    "href": "labs/01-lab.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\n# covid_us %&gt;%\n#   mutate(\n#     year = year(date),\n#     month = month(date),\n#     year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n#     percent_vaccinated = people_fully_vaccinated/population*100  \n#     ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/01-lab.html#uncomment-and-run-the-code-below",
    "href": "labs/01-lab.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\n# covid_us %&gt;%\n#   filter(facial_coverings == -4) %&gt;%\n#   select(date, state) %&gt;%\n#   group_by(state) %&gt;%\n#   summarize(\n#     n = n(),\n#     earliest_date = min(date),\n#     latest_date = max(date),\n#   )%&gt;%\n#   arrange(earliest_date)"
  },
  {
    "objectID": "labs/01-lab.html#please-explainwhat-each-line-of-code-is-doing",
    "href": "labs/01-lab.html#please-explainwhat-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explainwhat each line of code is doing:",
    "text": "6.2 Please explainwhat each line of code is doing:\n\ncovid_us %&gt;% Write your explanation here\nfilter(facial_coverings == -4) %&gt;%\nselect(date, state) %&gt;%\ngroup_by(state) %&gt;%\nsummarize(\nn = n(),\nearliest_date = min(date),\nlatest_date = max(date),\n)%&gt;%\narrange(earliest_date)\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\n\n6.2.1 Substantively, what does the previous chunk of code tell us?\n\n\n\n\n\n\nNote\n\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/01-lab.html#please-run-the-following-code",
    "href": "labs/01-lab.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  ) %&gt;%\n  filter(\n    RegionName == \"Illinois\",\n    date &gt; \"2020-08-01\", \n    date &lt; \"2021-01-01\",\n    !is.na(H6_Notes)\n    ) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\n\nError in oxford_us %&gt;% mutate(date = ymd(Date)) %&gt;% filter(RegionName == : could not find function \"%&gt;%\"\n\nil_facemasks\n\nError in eval(expr, envir, enclos): object 'il_facemasks' not found"
  },
  {
    "objectID": "labs/01-lab.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/01-lab.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;%\nmutate(date = ymd(Date))%&gt;%\nfilter(RegionName == \"Illinois\",\ndate &gt; \"2020-08-01\",\ndate &lt; \"2021-01-01\",\n!is.na(H6_Notes)) %&gt;%\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\nError in eval(expr, envir, enclos): object 'il_facemasks' not found\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\n# il_facemasks$H6_Notes[???]"
  },
  {
    "objectID": "labs/01-lab.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/01-lab.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-central-tendency",
    "href": "labs/01-lab.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nLater in the course we’ll talk about how we can use something like a mean to estimate an Expected Value: Something like\n\\[ E[Y|X=x] \\]\nOr to make it more concrete:\n\\[ E[\\text{New Cases} | \\text{Policy = \"recommended\"}] \\]\nIn code, we could accomplish this manually, using the index operator:\n\n# mean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommended”\nBy using group_by() with summarise() we can accomplish this more quickly:\n\n# covid_us%&gt;%\n#   group_by(face_masks)%&gt;%\n#   summarise(\n#     new_cases_pc = mean(new_cases_pc, na.rm=T)\n#   )\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n\\[\nM_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx=\\int_{x_i}^\\infty f_x(X)dx=1/2\n\\]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\n\n\n\n\n\nNote\n\n\n\n\nMedians are less influenced by outliers than means\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\n# sort(table(covid_us$face_masks))\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-dispersion",
    "href": "labs/01-lab.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n\\[\np_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx= p; \\int_{x_i}^\\infty f_x(X)dx=1-p\n\\]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n\\[\n\\sigma^2_x=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2\n\\]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\n\n8.2.3.1 Use the var() function to calculate the variance of the new_cases_pc variable.\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n\\[\n\\sigma_x=\\sqrt{\\sigma^2_x}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nStandard deviations are easier to interpret because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\n\n8.2.4.1 let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable"
  },
  {
    "objectID": "labs/01-lab.html#measures-of-association",
    "href": "labs/01-lab.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n\\[\ncov(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n\\[\n\\rho_{x,y} = \\frac{cov(x_y)}{\\sigma_x,\\sigma_y}\n\\]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\n# covid_us %&gt;%\n#   filter(year &gt; 2020)%&gt;%\n#   ungroup() %&gt;%\n#   group_by(year,month)%&gt;%\n#   summarise(\n#     mn_per_vax = mean(percent_vaccinated, na.rm=T),\n#     cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n#   )"
  },
  {
    "objectID": "labs/01-lab.html#what-do-these-averages-really-tell-us",
    "href": "labs/01-lab.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?"
  },
  {
    "objectID": "labs/01-lab.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/01-lab.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\n# DT::datatable(cases_by_month_and_policy,\n#               filter = \"top\")\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\n# cases_by_month_and_policy %&gt;%\n#   ggplot(aes(\n#     x= year_month,\n#     y = new_cases_pc,\n#     col=face_masks))+\n#   geom_point()+coord_flip()\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/01-lab.html#footnotes",
    "href": "labs/01-lab.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/03-lab.html",
    "href": "labs/03-lab.html",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "",
    "text": "Today we will explore the logic and design of Broockman and Kalla’s 2016 study, “Durably reducing transphobia: A field experiment on door-to-door canvassing”, from the recruitment of subjects for the study to the delivery of their interventions. Then we will explore whether the intervention had any effect on respondents’ feelings toward transgender individuals.\nTo accomplish this we will:\n\nSummarize the study (5 Minutes)\nSet up our work space (2-3 Minutes)\nLoad a portion of the replication data (1-2 Minutes)\nGet a high level overview of the data (5 minutes)\nDescribe the distribution of covariates in the full dataset (5 minutes)\nExamine the difference in covariates between those who did and did not complete the survey (10 minutes)\nExamine the difference in covariates between those assigned to each treatment condition in the study. (10 minutes)\nEstimate the average treatment effect of the intervention (10 minutes)\nPlot the results and comment on the study (10 minutes)\nTake the weekly survey (3-5 minutes)\n\nOne of these 9 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/03-lab.html#footnotes",
    "href": "labs/03-lab.html#footnotes",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find the full set of replication files here↩︎\nThe actual study contains a number of measures about transgender attitudes and policies which are scaled together to produce a single measure of subjects latent tolerance. For simplicity, we’ll focus on this single survey item.↩︎\nRecall that only some people who completed the baseline and were assigned to receive the treatment actually answered the door when canvassers came knocking.↩︎"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html",
    "href": "labs/comments/02-lab-comments.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to reproduce this figure:\n\n\n\n\n\n\n\n\n\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides (week 1 here, week 2 here) and last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nLearn how to take a complex problem (“How do I produce a figure that shows the average rate of new cases per month for states with a particular type of face mask policy”) which you may think you have no idea how to do and break this challenge down into concrete tasks which you do know how do (“Well first, I’ll need to load some packages to work with and visualize data. Then, I’ll need to get the data. And then…”)\n\nThink and write programmatically\n\nIn this .Rmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll produce a collective outline of what we need to do before getting to the actual coding\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nAgain you shouldn’t have to write much code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\nBut shouldn’t we be writing our own code?! Yes. Sure. Eventually.\nThe tutorials give you practice writing single commands, and by the end of the class you should be able write this code like this for to accomplish similar tasks\nBut even then, you will not be writing code from memory. I still have to Google functions, and often search my old code to find a clever solution to task.\nEveryone starts learning to code by copying and pasting other people’s code.\nThis will help minimize (but not eliminate) syntactic errors, while over time we get better writing code from scratch and fixing errors as the develop.\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nFor fun, let’s say that the first group that successfully recreates this figure gets to choose one of the following non-monetary prizes:\n\nI’ll tell them a joke\nOne AMA I will answer truthfully\nOne question to be asked on the weekly class survey\n0.00001% extra credit added to their final grade for the course.\n\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "href": "labs/comments/02-lab-comments.html#create-an-object-listing-all-the-packages-i-will-use-today",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.1 Create an object listing all the packages I will use today",
    "text": "2.1 Create an object listing all the packages I will use today\nThis code creates a object called the_packages which contains a vector of character strings corresponding to the names of the packages I want to use today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "href": "labs/comments/02-lab-comments.html#define-a-function-to-install-and-load-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.2 Define a function to install and load packages",
    "text": "2.2 Define a function to install and load packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "href": "labs/comments/02-lab-comments.html#use-the-ipak-function-to-load-the-necessary-packages",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "2.3 Use the ipak function to load the necessary packages",
    "text": "2.3 Use the ipak function to load the necessary packages\nNow I run the ipak() giving it the object the_packages as an input. It sorts through the packages, checks to see if they’re installed, if not installs them, and then loads all of the packages so I can use them.\n\nipak(the_packages)\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#filter-out-u.s.-territories",
    "href": "labs/comments/02-lab-comments.html#filter-out-u.s.-territories",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.1 Filter out U.S. Territories",
    "text": "5.1 Filter out U.S. Territories\nFor simplicity, (and practice filtering observations), I’ve asked us to remove observations from U.S. territories.\nThe code below\n\nCreates an object called us_territories.\nUse this object to filter out observations that are US territories\nCreates a new data frame that is just observations from the 50 U.S. states. and D.C.\nChecks that this recoding seems to have worked\n\n\n# U.S. Territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out U.S. Territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\nError in covid %&gt;% filter(!administrative_area_level_2 %in% territories): could not find function \"%&gt;%\"\n\n# Check to make sure covid_us contains only 50 states and D.C.\ndim(covid)\n\n[1] 80156    47\n\ndim(covid_us)\n\nError in eval(expr, envir, enclos): object 'covid_us' not found\n\nlength(unique(covid$administrative_area_level_2)) \n\n[1] 56\n\nlength(unique(covid_us$administrative_area_level_2)) == 51\n\nError in eval(expr, envir, enclos): object 'covid_us' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-a-state-variable",
    "href": "labs/comments/02-lab-comments.html#create-a-state-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.2 Create a state variable",
    "text": "5.2 Create a state variable\nThis is purely for convenience, because typing administrative_area_level_2 is annoying. The code copies the values of this variable into a new variable called state using the mutate() function.\nMutate returns the original data frame plus the new column. We have to save this output for our our changes to persist (i.e. we have to assign the output of mutate() back into covid_us)\nIn last week’s lab, I just piped the output to the next command, did some more recoding with mutate, and then finally saved the output back into covid_us. In this lab, I’ll save the output after each step.\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(state = administrative_area_level_2, ): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "href": "labs/comments/02-lab-comments.html#group-by-the-state-variable-to-calculate-new-covid-19-cases",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.3 Group by the state variable to calculate new Covid-19 cases",
    "text": "5.3 Group by the state variable to calculate new Covid-19 cases\nNow I use this shorter variable state to calculate the number of new cases (new_cases) in a given state on a given date, and rescale this variable so that it’s expressed in terms of new cases per 100,000 residents.\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000\n    ) -&gt; covid_us\n\nError in covid_us %&gt;% dplyr::group_by(state) %&gt;% mutate(new_cases = confirmed - : could not find function \"%&gt;%\"\n\n\nThe slides from Tuesday, helped demonstrate what this code was doing, and why we wanted to group by state.\nHere’s an example for a subset of the data from April 1, 2020 to April 7, 2020\nWe see that the lag() function simply moves the observation of a variable “up” one row so that we can take the difference between the total number of cases in a state on one date and the total number of cases on the date before, to calculate the number of new cases\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  group_by(state) %&gt;%\n  select(state, date, confirmed)%&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\nError in covid_us %&gt;% filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;% : could not find function \"%&gt;%\"\n\n\nIf we hadn’t grouped by state, then when we lagged the confirmed variable, R thinks the number of confirmed cases in California before April 1, 2020, is 986 which is actually the number of cases in Minnesota on April 7, 2020\n\ncovid_us %&gt;%\n  filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\")%&gt;%\n  ungroup() %&gt;%\n  select(state, date, confirmed)%&gt;%\n  mutate(\n    confirmed_lag1 = lag(confirmed),\n    new_cases = confirmed - lag(confirmed)\n  )\n\nError in covid_us %&gt;% filter(date &gt;= \"2020-04-01\" & date &lt; \"2020-04-07\") %&gt;% : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#recode-the-facial_coverings-variable",
    "href": "labs/comments/02-lab-comments.html#recode-the-facial_coverings-variable",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.4 Recode the facial_coverings variable",
    "text": "5.4 Recode the facial_coverings variable\nNext we use the case_when() function inside the mutate() function to create a variable called face_masks based on the values of the facial_coverings variable in the data.\ncase_when() when uses R’s ability to make logical comparisons. When the variable facial_coverings equals 0, R will input the character string \"No policy\" into the face_masks variable.\nWhen the absolute value of facial_coverings equals 1 (i.e.facial_coverings equals 1 or -1 ), R will input the character string \"Recommended\" into the face_masks variable. And so on.\nWe use the abs() function to take the absolute value of the facial_coverings variable because codebook for these data implied:\n\nIn short: positive integers identify policies applied to the entire administrative area. Negative integers are used to identify policies that represent a best guess of the policy in force, but may not represent the real status of the given area. The negative sign is used solely to distinguish the two cases, it should not be treated as a real negative value.\n\nWe know from last weeks lab, that negative values in the U.S. typically seem to be cases where a city had a more stringent policy than the state (e.g. Chicago adopts more stringent face mask policies than Illinois).\nFinally, we put a %&gt;% after the output of case_when() and pass it’s output to the factor() function.\nThe . acts as sort of placeholder, factor() expects some input here (like a variable from a data frame), . tells R to use the output of case_when().\nThe levels = then transforms the character data produced by case_when() into a factor with an implicit ordering of levels (i.e. “No policy” &lt; “Recommended”&lt; “Some requirements” &lt;“Required shared places” &lt;“Required all times”) which turns out to be useful trick for organizing how data are plotted and visualized.\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(face_masks = case_when(facial_coverings == : could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "href": "labs/comments/02-lab-comments.html#create-a-variable-capturing-the-year-and-month-of-the-observation",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "5.5 Create a variable capturing the year and month of the observation",
    "text": "5.5 Create a variable capturing the year and month of the observation\nFinally we create some variables that extract components of an observation’s date:\n\nyear = year(date) returns just the year from a variable of class Date\nmonth = month(date) returns just the month from a variable of class Date\nyear_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\") pastes these to variables together.\nstr_pad(month, width = 2, pad=0) adds a leading 0 to any month with only 1 digit, to ensure that all the months have 2 characters.\n\nThe code from your lab also calculates the percent of a states population that is vaccinated, which isn’t strictly needed for today.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nError in covid_us %&gt;% mutate(year = year(date), month = month(date), year_month = paste(year, : could not find function \"%&gt;%\"\n\n\nCreating separte year and month variables aren’t strictly necessary,\nWe could have written something like:\n\ncovid_us %&gt;%\n  mutate(\n    year_month = paste(year(date), str_pad(month(date), width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\nBut that year_month line was already feeling kind of clunky, and maybe we’ll want the year and month variables later."
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#adding-meaningful-labels-and-title",
    "href": "labs/comments/02-lab-comments.html#adding-meaningful-labels-and-title",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.1 Adding meaningful labels and title",
    "text": "8.1 Adding meaningful labels and title\nBecause we saved the output of our ggplot to an object called fig1 we can add additional commands to this object using the + without having to rewrite all the code.\nFirst let’s add better labels to the graph.\n\nNote that even though we flipped the coordinates, the aes aesthetic mappings stay the same. So to change the label of the figures y-axis to “Date” we change the label of x = \"Date\"\nggplot automatically generates a legend for aesthetic mappings like color We can add a line break using the the special character \\n in our code\n\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\n Policy\"\n  )\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nNote the code above didn’t update fig1\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nWe have to save the output (if we like it) for our changes to persist.\n\nfig1 +\n  labs(\n    x = \"Date\",\n    y = \"Average number of new cases (per 100k)\",\n    col = \"Face Mask\\nPolicy\"\n  ) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#changing-the-theme-of-the-plot",
    "href": "labs/comments/02-lab-comments.html#changing-the-theme-of-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.2 Changing the theme of the plot",
    "text": "8.2 Changing the theme of the plot\nHere’s an example of some different themes\n\n# Black and white\nfig1 +\n  theme_bw()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n# Minimal\nfig1 +\n  theme_minimal()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n# Classic\nfig1 +\n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nThis is pretty personal, and depends of the figure itself. I like a white background and some guide lines:\n\nfig1 +\n  theme_bw() -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "href": "labs/comments/02-lab-comments.html#make-the-size-of-the-dots-reflect-the-number-of-states-with-this-policy",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.3 Make the size of the dots reflect the number of states with this policy",
    "text": "8.3 Make the size of the dots reflect the number of states with this policy\nIn the cases_by_month_and_policy we have a column called n which is the number of states which had a given policy in a given month.\nWe can add an aesthetic to our plot that varies the size of the points by the number of states.\n\nfig1 +\n  aes(size = n) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nWe call this type of plot a bubble plot{target=“_blank”\nI have mixed feelings about multiple legends. We can remove the legend for size using the scale_size() function. I had to Google how to do this for the millionth time.\n\nfig1 +\n  scale_size(guide = \"none\") -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found"
  },
  {
    "objectID": "labs/comments/02-lab-comments.html#facet-the-plot",
    "href": "labs/comments/02-lab-comments.html#facet-the-plot",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "8.4 Facet the plot",
    "text": "8.4 Facet the plot\nVarying the size of the dots by the number of states conveys more information. But makes the chart a little harder to read. Dots overlap.\nThe facet_wrap command will produce separate bubble plots for each level of the “facetting” variable, in this case `face_masks\n\nfig1 +\n  facet_wrap(~face_masks) -&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nNow I think also want a second legend for the number of states\n\nfig1 +\n  scale_size(guide = \"legend\")+\n  labs(\n    size = \"# of States\\nwith Policy\"\n  )-&gt; fig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\nfig1\n\nError in eval(expr, envir, enclos): object 'fig1' not found\n\n\nThis seems pretty good if our goal was to show in general terms\n\nIt shows the average number new cases for states with a given face mask policy over time.\nIt shows how the mix of types of face mask policies states have adopted has changed over time\n\nIf our goal was to make comparisons across face mask policies over a given time period, I’m might still prefer something closer to our original graph."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html",
    "href": "labs/comments/05-lab-comments.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3032022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\n\nGrading Questin 9: Basically, if you made any changes to fig_m5 100 percent. If you simply recreated fig_m5 80 percent. If you didn’t create figure fig_m5 0 percent. Sorry! But don’t fret, remember your 3 lowest lab scores are dropped from your lab grade.\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, 1, 2, 3, 4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#load-covid-19-data",
    "href": "labs/comments/05-lab-comments.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid data\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  level = 2,\n  verbose = F\n)"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#load-election-data",
    "href": "labs/comments/05-lab-comments.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\n# This joyously stopped working last night...\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# Backup\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#recode-the-covid-19-data",
    "href": "labs/comments/05-lab-comments.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide, starting here and ending here\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/comments/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, change the new_deaths_pc_7da to new_deaths_pc_14da and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7da = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14da = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/comments/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-all-the-observations",
    "href": "labs/comments/05-lab-comments.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7da)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7da\n  &lt;fct&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 Trump        18.4             0.324\n2 Biden        21.1             0.276"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/comments/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7da\n  &lt;fct&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 Trump        22.8             0.400\n2 Biden        30.6             0.380"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/comments/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7da = mean(new_deaths_pc_7da, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7da\n  &lt;fct&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 Trump        15.9             0.281\n2 Biden        15.5             0.216\n\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that states which Biden won had about 27 new deaths compared to 23.8 new deaths in states which Trump one.\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that rates tend to be higher in Red States (0.415 deaths per 100k) than Blue States (0.349 deaths per 100k). This difference reflects the fact that Biden tended to win more populous states than trump, so simply looking at the average number of new deaths is bit misleading. Comparing 7-day averages per 100,000 residents adjusts for differences in population between Red and Blue States.\nWhen we limit our analysis, to just observations before April 19, 2021, the difference in the 7-day average rate of new Covid-19 deaths per 100,000 residents is relatively small (0.02 more deaths per 100,000 residents in Red States)\nWhen we look at observations after the vaccine became widely available the difference is more than 6 times as big (0.125 more deaths per 100,000 residents in Red States)"
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#recreating-the-nyt-figures",
    "href": "labs/comments/05-lab-comments.html#recreating-the-nyt-figures",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "9.1 Recreating the NYT Figures",
    "text": "9.1 Recreating the NYT Figures\nThis turned out to be more annoying than I thought, but if you really wanted to recreate the figures from the articles, this was as close as I could get:\n\n# Vector containing labeled states\nthe_labs &lt;- c(\"WV\",\"WY\",\"MS\",\"KY\",\"TX\",\"FL\",\"GA\",\"IL\",\"NY\",\"VT\",\"MD\",\"CA\")\n\ncovid_us %&gt;%\n  # Only include labels for states in the the_labs\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  # Subset data\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics, flipping vax to % unvaxxed\n  ggplot(aes(x = rep_voteshare,\n             y = (100-percent_vaccinated),\n             label = nyt_labs\n             ))+\n  # points coloreded by vote share\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # color gradient\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # add simple regression line\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  # add labels\n  geom_text_repel()+\n  # futz with limits\n  ylim(15,60)+\n  # add grid lines by hand\n  geom_hline(yintercept = 60, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 40, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 20, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=20, yend = 60), col = \"lightgrey\", size = .25)+\n  # Add arrows\n  geom_segment(aes(x = 34, xend = 32, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add biden text\n  annotate(\"text\",x = 34.5, y=18.5 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=17.1 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.7, y=17.1 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  # Add trump arrow\n  geom_segment(aes(x = 70, xend = 72, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add trump text\n  annotate(\"text\",x = 69.5, y=18.5 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.1, y=17.1 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=17.1 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  # Label y-axis\n  annotate(\"text\",x = 30, y=20 ,label = \"20%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=40 ,label = \"40%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=60 ,label = \"60% of residents not fully vaccinated\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  # get rid of default theme\n  theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values or values outside the scale range\n(`geom_text_repel()`).\n\n\n\n\n\n\n\n\n\n\n# Same as above, but now modeling deaths with rep vote share\n\ncovid_us %&gt;%\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14da,\n             label = nyt_labs\n             ))+\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  geom_text_repel()+\n  # theme_void()+\n  ylim(-.2,2.2)+\n  geom_hline(yintercept = 0, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = .5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1.5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 2, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=0, yend = 2), col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 34, xend = 32, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.1 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.2 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.82, y=-.2 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  geom_segment(aes(x = 70, xend = 72, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.1 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.09, y=-.2 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.2 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  annotate(\"text\",x = 30, y=0.5 ,label = \"0.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1 ,label = \"1\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1.5 ,label = \"1.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=2 ,label = \"2 deaths per 100,000 residents\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  theme_void()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values or values outside the scale range\n(`geom_text_repel()`)."
  },
  {
    "objectID": "labs/comments/05-lab-comments.html#footnotes",
    "href": "labs/comments/05-lab-comments.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/comments/04-lab-comments.html",
    "href": "labs/comments/04-lab-comments.html",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "",
    "text": "set.seed(20231005)\n  the.questions&lt;-1:12\n  graded&lt;-sample(the.questions,1)\n  graded\n\n[1] 3\nQuestion 3 is the graded question for this assignment"
  },
  {
    "objectID": "labs/comments/04-lab-comments.html#goals",
    "href": "labs/comments/04-lab-comments.html#goals",
    "title": "Lab 4 Comments: Causation in Observational Studies",
    "section": "0.1 Goals",
    "text": "0.1 Goals\nConceptually, our goal in this lab is to see how scholars might use historical knowledge to make causal claims with observational data.\nSpecifically, we will see how F&M leverage a claim about how borders are drawn to assess the effects of different types of governing strategies.\nPractically, we will continue to develop our statistical skills, introducing some core concepts from base R.\nSpefically we will see how we can use:\n\nfor() loops to repeat a process like calculating a mean, over multiple variables\nself-defined functions to abstract and generalize repeated tasks\nthe with() function to avoid having to write out df$variable\ndifferent types of apply() functions (namely sapply() and tapply()) to apply functions to a sets of variables (sapply()) and to subgroups within a set of variables (tapply())\n\nThese are useful skills that broadly help you write your code more efficiently. Things like for() loops, functions() and apply() can reduce the amount of copying, pasting and replacing you have to do, which in turn can reduce the amount of errors induced by forgetting to change a variable name, or mistyping a command.\nBut the first time you see a for loop, or define your own function, it will likely seem a bit abstract, and obtuse.That’s ok. The goal is that you have a better, if not perfect, understanding of these concepts which we will use throughout the course."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html",
    "href": "labs/comments/10-lab-comments.html",
    "title": "Lab 10 The final lab!!",
    "section": "",
    "text": "In our final lab, you will apply concepts and skills from this course to explore data from the 2020 American National Election Study. Specifically you will\n\nIdentify an outcome of interest (5-10 minutes)\nIdentify key predictors and covariates (5-10 minutes)\nRecode your data (20 minutes)\nDescribe your data (20 minutes)\nDescribe your question, expectations, and models (10 minutes)\nEstimate, present, and interpret your models (20 minutes, Graded Question)\n\nIdeally, each group will pursue a question that interests them. I will also complete these tasks live, so, if you’re not feeling confident, you can follow along with me and submit the code I demo in class as your lab for a grade of 85.."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#summary-statistics",
    "href": "labs/comments/10-lab-comments.html#summary-statistics",
    "title": "Lab 10 The final lab!!",
    "section": "4.1 Summary statistics:",
    "text": "4.1 Summary statistics:\nProducing a table of summary statistics requires a little foresight.\nEssentially you want to make a data frame where each row is a (numeric) variable, and each column is a statistic (minimum, 25th percentile, median, mean, 75th percentile, max, Number of missing).\nTo do this, I would:\n\ncreate a object called the_vars which contains the names (in quotation marks) of the variables you want to summarize.\nSelect these variables from your data set. using df%&gt;%select(all_of(the_vars))\nUse %&gt;%pivot_longer() specifying cols=select(all_of(the_vars)), and names_to equals \"Variable\" and values_to = \"value\" to transform this wide dataset into a long dataset\nThen use %&gt;%group_by(Variable)%&gt;% and summarise() to calculate the statistics for each variable of interest (e.g. %&gt;%summarise(Mean = mean(value, na.rm=T))))\nSave the output to an object called something like sum_df\nIn a new chunk use knitr::kable(sum_df) %&gt;% kableExtra::kable_styling() to format your table. Set echo=F in the code chunk head\n\n\n# Summarise data\n\nthe_vars &lt;- c(\"ft_police\",\n              \"is_white\",\n              \"been_arrested\",\n              \"income\")\n\ndf%&gt;%\n  select(all_of(the_vars)) %&gt;%\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\",\n    values_to = \"value\"\n  )%&gt;%group_by(Variable)%&gt;%\n  summarise(\n    Min = min(value, na.rm=T),\n    p25 = quantile(value, probs = .25,na.rm=T),\n    Median = median(value, na.rm=T),\n    Mean = mean(value, na.rm=T),\n    p75 = quantile(value, probs = .75,na.rm=T),\n    Max = max(value, na.rm=T)\n\n            ) %&gt;% \n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  ) %&gt;% \n  arrange(Variable) -&gt; sum_df\n\nError in UseMethod(\"select\"): no applicable method for 'select' applied to an object of class \"function\"\n\n\n# Display results\nknitr::kable(sum_df, digits = 2) %&gt;% kableExtra::kable_styling()\nError in eval(expr, envir, enclos): object 'sum_df' not found"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#descriptive-figures",
    "href": "labs/comments/10-lab-comments.html#descriptive-figures",
    "title": "Lab 10 The final lab!!",
    "section": "4.2 Descriptive Figures",
    "text": "4.2 Descriptive Figures\nTo create a figure, you’ll need to specificy the following\n\ndata (e.g. df %&gt;%)\naesthetic mappings, ggplot(aes(x = predictor, y = outcome))\ngeometries\n\nUnivariate: geom_density(), geom_boxplot() geom_histogram()\nBivariate: geom_point() (for a scatterplot), geom_line() for a trend.\n\n\nOnce you have a minimal working example, play around with other grammars of graphics:\n\nlabs() for custom labels\ntheme_XXX for custom themes\nfacet_wrap(~group) to produce the same plot facetted by some categorical grouping variable\n\nWhen you’re happy with your figure, save it as object in R (e.g. fig1 &lt;- df %&gt;% ggplot(aes(predictor, outcome))+geom_point()). Put that object in its own chunk to display it in your document.\nDon’t let the perfect be the enemy of the good.\n\n# Descriptive figures"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#descrptive-interpretation",
    "href": "labs/comments/10-lab-comments.html#descrptive-interpretation",
    "title": "Lab 10 The final lab!!",
    "section": "4.3 Descrptive Interpretation:",
    "text": "4.3 Descrptive Interpretation:\nPlease provide an overview of the data (source, number of observations, unit of analysis).\nDescribe a typical observation, making reference to the statistics in your summary table.\nOffer a substantive interpretation of your descriptive figure(s). What do they tell us about the distribution of a key variable, or the relationship between two variables."
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#fit-the-models",
    "href": "labs/comments/10-lab-comments.html#fit-the-models",
    "title": "Lab 10 The final lab!!",
    "section": "6.1 Fit the models",
    "text": "6.1 Fit the models\n\n# Model 1: Bivariate Model\n\n# Model 2: Multiple Regression"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#display-the-models-in-a-regression-table",
    "href": "labs/comments/10-lab-comments.html#display-the-models-in-a-regression-table",
    "title": "Lab 10 The final lab!!",
    "section": "6.2 Display the models in a regression table",
    "text": "6.2 Display the models in a regression table\n\n# Regression table"
  },
  {
    "objectID": "labs/comments/10-lab-comments.html#interpet-your-models",
    "href": "labs/comments/10-lab-comments.html#interpet-your-models",
    "title": "Lab 10 The final lab!!",
    "section": "6.3 Interpet your models",
    "text": "6.3 Interpet your models\nPlease write a 1 paragraph summary interpreting your results in terms of both their statistical and substantive significance. Assume your audience is smart, but has never taken POLS 1600. Explain to them what a regression model is, what a standard error, p-value, and/or confidence interval is. How should they interepret the substantive findings of your model. How should they assess the statistical uncertainty around these results?\nPerhaps you might reade create a plot of predicted values from a model to help facilitate the substantive interpretation of your results. If so, here’s a code chunk for you:\n\n# Additional code chunk to facilitate interpretation of models"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html",
    "href": "labs/comments/01-lab-comments.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#uncomment-and-run-the-following-code",
    "href": "labs/comments/01-lab-comments.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#uncomment-and-run-the-code-below",
    "href": "labs/comments/01-lab-comments.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\ncovid_us %&gt;%\n  filter(facial_coverings == -4) %&gt;%\n  select(date, state) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    n = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n  )%&gt;%\n  arrange(earliest_date)\n\n# A tibble: 4 × 4\n  state              n earliest_date latest_date\n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;        &lt;date&gt;     \n1 Illinois         156 2020-10-01    2021-05-15 \n2 Massachusetts     35 2020-10-02    2020-11-05 \n3 South Carolina    61 2020-10-13    2020-12-12 \n4 Maryland         158 2020-11-06    2021-04-12"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "href": "labs/comments/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explain in words your best understanding of what each line of code is doing:",
    "text": "6.2 Please explain in words your best understanding of what each line of code is doing:\n\ncovid_us %&gt;% tells R that every line of code after will use covid_us dataframe\nfilter(facial_coverings == -4) %&gt;% tells R to filter out only the rows where the facial coverings variable equals -4\nselect(date, state) %&gt;% tells R to select the columns named date and state\ngroup_by(state) %&gt;% tells R that subsequent commands should be done separately for each unique value of state\nsummarize( tells R we want to summarize the output of susequent commands\nn = n(), tells R to count the number of observations (state-dates) for each state that had a value of -4 on the facial_coverings variable\nearliest_date = min(date), tells R to report the earliest date that each state had a value of -4\nlatest_date = max(date), tells R to report the last date that each state had a value of -4\n)%&gt;% tells R we’re finished with the summarize() function\narrange(earliest_date) arranges the data in asscending order from earliest to latest start date\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\nSubstantively, what does the previous chunk of code tell us?\n\nSo there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. Illinois was the first state where this code appears, and it appears present in 156 observations while Montana was the last adopting a policy code -4 on March 25, 2021\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#please-run-the-following-code",
    "href": "labs/comments/01-lab-comments.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  )%&gt;%\n  filter(RegionName == \"Illinois\", \n         date &gt; \"2020-08-01\", \n         date &lt; \"2021-01-01\",\n         !is.na(H6_Notes)) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\n# A tibble: 8 × 4\n  date       `H6_Facial Coverings` H6_Flag H6_Notes                             \n  &lt;date&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                \n1 2020-08-21                     2       1 \"In Executive Order 2020-52, Executi…\n2 2020-08-26                     2       1 \"Effective from 26 August 2020, the …\n3 2020-09-18                     2       1 \"On 18 September, in Executive Order…\n4 2020-10-01                     4       0 \"Originally coded a 3T, but looking …\n5 2020-10-16                     4       0 \"In Executive Order (EO) 2020-59, Go…\n6 2020-11-13                     4       0 \"Noting that Executive Order 2020-71…\n7 2020-11-20                     4       0 \"Executive Order 2020-73 requires pe…\n8 2020-12-01                     3       1 \"Chicago seems to have changed its g…"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/comments/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;% Tells R to use the Oxford policy data\nmutate(date = ymd(Date))%&gt;% Creates a date variable of class date from the original Date variable (which was class numeric)\nfilter(RegionName == \"Illinois\", subsets the data to just Illinois\ndate &gt; \"2020-08-01\", filters out dates before August 1, 2020\ndate &lt; \"2021-01-01\", filters out observations with dates after January 1,2021\n!is.na(H6_Notes)) %&gt;% filters out observations without notes (which appear in the data when policy changes)\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks Selects just the date and notes variables and saves them to an object called il_facemasks\nil_facemasks prints the obejct in the console\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\n[1] \"On 18 September, in Executive Order 2020-55, the Governor reissued most executive orders, extending a majority of the provisions through 17 October 2020. This includes mask requirements.      https://web.archive.org/web/20200922144918/https://www2.illinois.gov/Pages/Executive-Orders/ExecutiveOrder2020-55.aspx\"\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\n# il_facemasks$H6_Notes[???]"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/comments/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy\nIn Illinois, the -4’s seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of facial_coverings to construct our face_mask variable, we’re probably over stating the extent the extensiveness of policies in effect.\nSo we should be cautious in how we interpret our collapsed variable, face_mask. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-central-tendency",
    "href": "labs/comments/01-lab-comments.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n[ {x}=_{i=1}^n x_i ]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\n\nmean(covid_us$new_cases_pc, na.rm=T)\n\n[1] 28.11486\n\n\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nFormally, you’ll often see this written in terms of Expected Values: Something like\n[ E[Y|X=x] ]\nOr to make it more concrete:\n[ E[ | ] ]\nIn code, we could accomplish this manually, using the index operator:\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n[1] 10.26168\n\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommend”\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"Recommended\"], na.rm=T)\n\n[1] 16.61408\n\n\nBy using group_by() with summarise() we can accomplish this more quickly:\n\ncovid_us%&gt;%\n  group_by(face_masks)%&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  )\n\n# A tibble: 6 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2\n6 &lt;NA&gt;                           11.8\n\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n[ M_x = X_i : ^{x_i} f_x(X)dx=^f_x(X)dx=1/2 ]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\nmedian(covid_us$new_cases_pc, na.rm=T)\n\n[1] 10.52355\n\n\nInteresting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the new_cases_pc has a “long tail” or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.\n\n\nMedians are less influenced by outliers than means\n\n\n\nhist(covid_us$new_cases_pc, breaks = 100)\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\nsort(table(covid_us$face_masks))\n\n\n    Required all times              No policy            Recommended \n                  1032                   3893                   8879 \nRequired shared places      Some requirements \n                 15088                  24786 \n\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:\n\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Visualisation_mode_median_mean.svg/240px-Visualisation_mode_median_mean.svg.png\")"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-dispersion",
    "href": "labs/comments/01-lab-comments.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\nrange(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916 1531.8669\n\nmin(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916\n\nmax(covid_us$new_cases_pc,na.rm = T)\n\n[1] 1531.867\n\n\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\ncovid_us %&gt;%\n  filter(\n    new_cases_pc &lt; -188 |\n    new_cases_pc &gt; 1500\n  )%&gt;%\n  select(state, date,new_cases_pc)\n\n# A tibble: 5 × 3\n# Groups:   state [5]\n  state        date       new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;            &lt;dbl&gt;\n1 Florida      2021-06-04        -189.\n2 Rhode Island 2022-01-04        1532.\n3 Tennessee    2023-01-01        -267.\n4 Nebraska     2022-10-28        -276.\n5 Kentucky     2022-10-11        -198.\n\n\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n[ p_x = X_i : ^{x_i} f_x(X)dx= p; ^f_x(X)dx=1-p ]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\n\nquantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)\n\n     25%      75% \n 0.00000 32.14152 \n\n\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n[ ^2_x=_{i=1}n(x_i-{x})2 ]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\nUse the var() function to calculate the variance of the new_cases_pc variable.\n\nvar(covid_us$new_cases_pc,na.rm=T)\n\n[1] 3402.718\n\n# Calculate by hand\n\nsum(\n  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, \n  na.rm=T\n  )/(sum(!is.na(covid_us$new_cases_pc))-1)\n\n[1] 3402.718\n\n\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n[ _x== ]\nStandard deviations are easier to interpet because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\nAgain, let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable\n\nsd(covid_us$new_cases_pc,na.rm=T)\n\n[1] 58.33282"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#measures-of-association",
    "href": "labs/comments/01-lab-comments.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n[ cov(x,y)=_{i=1}^n(x_i-{x})(y_i-{y}) ]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\nvar(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)\n\n[1] -19.96569\n\n\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n[ _{x,y} = ]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\n\ncor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = \"complete.obs\")\n\n[1] -0.01369243\n\n\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\ncovid_us %&gt;%\n  filter(year &gt; 2020)%&gt;%\n  ungroup() %&gt;%\n  group_by(year,month)%&gt;%\n  summarise(\n    mn_per_vax = mean(percent_vaccinated, na.rm=T),\n    cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n  )\n\nError in `summarise()`:\nℹ In argument: `cor = cor(new_cases_pc, percent_vaccinated, use =\n  \"complete.obs\")`.\nℹ In group 28: `year = 2023` and `month = 4`.\nCaused by error in `cor()`:\n! no complete element pairs"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "href": "labs/comments/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?\nProbably not that much. Different Face mask policies are implemented at different times in the pandemic. For example, by 2021, almost all states have some requirements. Comparing the average for new cases in states with no policy to states with full requirements, is comparing the state of world in early 2020 to the state of the world in late 2020 to mid 2021. But lots of things differ between these periods. Other policies are also going into effect, new variants are emerging.\nIn short, those simple conditional means across the full data don’t really provide an apples to apples comparison.\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,n,fill=face_masks))+\n  geom_bar(stat=\"identity\")\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    new_cases = sum(new_cases)\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,new_cases))+\n  geom_smooth()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 322 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/comments/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\nSo when we compare states in September 2020, we see that rates of new cases tend to be much higher in states with only recommend face mask policies compared to states with at least some requirements.\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  filter(year_month == \"2020-09\")%&gt;%\n  group_by(face_masks) %&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc,na.rm=T)\n  )\n\n# A tibble: 4 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Recommended                    43.9\n2 Some requirements              13.5\n3 Required shared places         13.0\n4 Required all times             10.1\n\n\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(year_month,face_masks) %&gt;%\n  summarise(\n    n = length(unique(state)),\n    new_cases_pc = round(mean(new_cases_pc,na.rm=T)),\n    total_cases = round(mean(confirmed,na.rm=T))\n  ) -&gt; cases_by_month_and_policy\n\n`summarise()` has grouped output by 'year_month'. You can override using the\n`.groups` argument.\n\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\nDT::datatable(cases_by_month_and_policy,\n              filter = \"top\")\n\n\n\n\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\ncases_by_month_and_policy %&gt;%\n  ggplot(aes(\n    x= year_month,\n    y = new_cases_pc,\n    col=face_masks))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/comments/01-lab-comments.html#footnotes",
    "href": "labs/comments/01-lab-comments.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html",
    "href": "labs/comments/09-lab-comments.html",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "",
    "text": "Today, we will return to exploring Russians’ support the war in Ukraine using a public opinion survey from Russia conducted by Alexei Miniailo’s “Do Russians Want War” project.\nThe survey was conducted by phone using a random sample of mobile phone numbers to produce a sample of respondents representative of the population in terms of age, sex, and geography. It was in the field from February 28 to March 2.\nFirst, we will explore how support for the war varies with the demographic predictors age, sex and education. We will compare the results of modeling this relationship using Ordinary Least Squares regression and Logisitic Regression\nWe’ll talk more about the technical aspects of logistic regression next week. For today we’ll simply compare the results from these two approaches.\nNext, we will gain insight into how our estimates from these models might vary using the statistical process of bootstrapping. Specifically, we will simulate the idea of repeated sampling that is the foundation of frequentist interpretations of probability, by sampling from our sample with replacement.\nWe’ll walk through the mechanics of simulation together. Then you’ll quantify the uncertainty described by these bootstrapped sampling distributions.\nFinally, we’ll see what other questions we might ask of these data and practice various skills we’ve developed through out the course.\nPlan to spend the following amount of time on each section\n\nGet set up to work (5 minutes)\nModel the relationship between demographic predictors and war support using OLS and Logistic regression (20 minutes)\nAssess the uncertainty around your estimated coefficients (15 minutes)\nQuantify the uncertainty described by your sampling distributions (10 minutes)\nExplore other relationships in the data. (30 minutes)\n\nThe graded question for today is:\n\nset.seed(472022)\ngraded_question &lt;- sample(1:5,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 5 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file produced by knitting the lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nYou can find your assigned groups in previous labs"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#load-packages",
    "href": "labs/comments/09-lab-comments.html#load-packages",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  \"flair\", # Comments only\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  \"modelr\", \"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"boot\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\n\nThere are three packages in particular that we’ll need to maker sure are installed and loaded\n\nmodelr\npurrr\nbroom\n\nIf ipak didn’t return TRUE for each of these, please uncomment and run:\n\n# install.packages(\"modelr\")\n# install.packages(\"purrr\")\n# install.packages(\"broom\")"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#load-the-data",
    "href": "labs/comments/09-lab-comments.html#load-the-data",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the recoded data for the lab\nOur primary outcome of interest (dependent variable) for today is a binary measure of support for war:\n\nsupport_war01 “Please tell me, do you support or do not support Russia’s military actions on the territory of Ukraine?” (1=yes, 0 = no)\n\nOur key predictors (independent variables) are the following demographic variables:\n\nage “How old are you?”\nsex “Gender of respondent” (As assessed by the interviewer)\neducation_n “What is your highest level of education (confirmed by a diploma, certificate)?” (1 = Primary school, 2 = “High School”, 3 = “Vocational School” 4 = “College”, 5 = Graduate Degree)1\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\ndf_drww %&gt;%\n  mutate(\n    person_id = 1:n()\n  ) -&gt; df_drww\n\nError in df_drww %&gt;% mutate(person_id = 1:n()): could not find function \"%&gt;%\""
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#fit-the-models",
    "href": "labs/comments/09-lab-comments.html#fit-the-models",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.1 Fit the models",
    "text": "2.1 Fit the models\nPlease estimate the following models:\n\nAn OLS regression model called m1 using lm()\nA Logistic regression model called m2 using glm() with family=binomial\n\n\n# OLS\nm1 &lt;- lm(support_war01 ~ age + sex + education_n, df_drww)\n\n# Logisitic \nm2 &lt;- glm(support_war01 ~ age + sex + education_n, df_drww,\n          family = binomial)"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#display-the-results-in-a-regression-table",
    "href": "labs/comments/09-lab-comments.html#display-the-results-in-a-regression-table",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.2 Display the results in a regression table",
    "text": "2.2 Display the results in a regression table\nNext, please display the results of your regressions in a table using htmlreg()\n# Regression Table\nhtmlreg(list(m1,m2))\nError in htmlreg(list(m1, m2)): could not find function \"htmlreg\""
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#produce-predicted-values-from-the-model",
    "href": "labs/comments/09-lab-comments.html#produce-predicted-values-from-the-model",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.3 Produce predicted values from the model",
    "text": "2.3 Produce predicted values from the model\nThe coefficients from a logistic regression aren’t easy to directly interpret.\nInstead, we will produce predicted values for each model\nTo do this, we will need to create a prediction dataframe called pred_df Every variable in your model, needs to be represented in your prediction data frame.\n\nUse expand_grid() to create a data frame where\n\nage varies from 18 to 99\nsex is held constant at “Female”\neducation_n is held constant at it’s mean\n\n\n\n## Create prediction data frame\npred_df &lt;- expand_grid(\n  age = 18 : 99,\n  sex = \"Female\",\n  education_n = mean(df_drww$education_n, na.rm = T)\n)\n\nError in expand_grid(age = 18:99, sex = \"Female\", education_n = mean(df_drww$education_n, : could not find function \"expand_grid\"\n\n\nThen you use the predict() function to produce predicted values from each model.\nSave the output of predict() for m1 to a new column in pred_df called pred_ols.\nFor m2 you need to tell are to transform the predictions from m2 back into the units of the response (outcome) variable, by setting the argument type = \"response\". Save the output of predict() for m1 to a new column in pred_df called pred_logit.\n\n# #Predicted values for m1\npred_df$pred_ols &lt;- predict(m1,\n                            newdata = pred_df)\n\nError in eval(expr, envir, enclos): object 'pred_df' not found\n\n# Predicted values for m2\n# Remember to add type = \"response\"\npred_df$pred_logit &lt;- predict(m2,\n                            newdata = pred_df,\n                            type = \"response\")\n\nError in eval(expr, envir, enclos): object 'pred_df' not found"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#plot-the-predicted-values-and-interpet-the-results",
    "href": "labs/comments/09-lab-comments.html#plot-the-predicted-values-and-interpet-the-results",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "2.4 Plot the predicted values and interpet the results",
    "text": "2.4 Plot the predicted values and interpet the results\nNow we can compare the predictions of OLS and Logistic regression by plotting the predicted values of support for the war from each model.\nTo produce this plot you’ll need to\n\nspecify data (you want to use the values from pred_df)\nmap values from your data aesthetics in ggplot\n\nput age on the x axis and and pred_ols on the y-axis\n\nspecify the geometries to plot\n\nadd two geom_line() to the plot\nleave the first one empty (e.g. geom_line())\nfor the second, specify a new aes of y=pred_logit\n\n\n\n# data\npred_df%&gt;%\n  # aesthetics\n  ggplot(aes(age, pred_ols, col = \"OLS\"))+\n  # geometries\n  geom_line()+\n  geom_line(aes(y = pred_logit, col = \"Logistic\"))+\n  geom_jitter(data=df_drww, aes(age, support_war01),\n              col = \"black\",\n              height = .05,\n              size = .5,\n              alpha = .5)+\n  labs(\n    col = \"Model\",\n    x = \"Age\",\n    y = \"Predicted Values\"\n  )\n\nError in pred_df %&gt;% ggplot(aes(age, pred_ols, col = \"OLS\")): could not find function \"%&gt;%\"\n\n\nHow do the predictions of the two models compare\nSo the predictions from OLS produce impossible values (levels of support above 100 percent) at for very old respondents, while the predictions from logistic regression are constrained to be between 0 and 1.\nIf we think that logistic regression provides a more credible way of modeling support for the war, then our OLS regression appears to overstate the level of support among young and old, while possibly understating the level of support among the middle age. The differences aren’t huge – a few percentage points – but for a binary outcome we will often prefer to model it with logistic regression.\nAlso note that marginal effect for age in the logistic regression is not constant. An increase in age from 25 to 26 is associated with a larger increase in support, than an increase in age from 75 to 76."
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#take-1000-bootstrap-samples-from-df_drww",
    "href": "labs/comments/09-lab-comments.html#take-1000-bootstrap-samples-from-df_drww",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.1 Take 1,000 bootstrap samples from df_drww",
    "text": "3.1 Take 1,000 bootstrap samples from df_drww\nBelow we create 1,000 boostrapped samples\n\n# Make sure these packages are loaded\nlibrary(modelr)\nlibrary(purrr)\nlibrary(broom)\n# Set random seed for reproducability\n\nset.seed(1234)\n\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n\nLet’s take a moment to understand what boot is and why we’re sampling with replacement.\nThe object boot contains 1,000 bootstrapped samples from df_drww.\nIf we look at the first bootstrap we see:\n\nboot$strap[[1]]\n\n&lt;resample [1,807 x 42]&gt; 1308, 1018, 1125, 1004, 623, 905, 645, 934, 400, 900, ...\n\n\nThe numbers 1308, 1018, 1125, 1004, 623, 905, ... correspond to rows in df_drww. So person 1308 is the first observation in this boot strap sample, then person 1018 and so on.\nBecause we are sampling with replacement observations from df_drww can appear multiple times. In our first bootstrap sample:\n\n666 observations appeared once\n342 appeared twice\n105 appeared three times\n27 appeared four times\n5 appeared five times.\n2 appeared six times\n\n\ntable(table(boot$strap[[1]]$idx))\n\n\n  1   2   3   4   5   6 \n666 342 104  27   5   2 \n\n\nWhy would we want to sample with replacement?\nWell, what we’d really love are 1,000 separate random surveys all drawn from the same population in the same way.\nSince that’s not feasible, we instead use the one sample we do have to learn things like how much our estimate might vary in repeated sampling. Efron (1979) called this procedure “bootstrapping” after the idiom “to pull oneself up by one’s own bootstraps”\nWe do this by sampling from our sample with replacement.\nWhen we sample with replacement, we are sampling from our sample, as our sample was sampled from the population.\nWith replacement means that some observations will appear multiple times in our bootstrapped sample (while others will not be included at all).\nWhen an observation appears multiple times in a bootstrap sample, conceptually, we’re using that original observation to represent the other people like that observation in the population who – had we taken a different sample – might have been included in our data.\nNote each bootstrap sample is a different random sample with replacement. In our second bootstrap sample, one observation (person 1496) appeared five times.\n\ntable(table(boot$strap[[2]]$idx))\n\n\n  1   2   3   4   5   6 \n661 342 105  31   1   3 \n\n# Person 406\nsum(boot$strap[[2]]$idx == 1496)\n\n[1] 5\n\n# Person 1 is not in boostrap 2\nsum(boot$strap[[2]]$idx == 1)\n\n[1] 0"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "href": "labs/comments/09-lab-comments.html#estimate-1000-models-from-the-1000-bootstrapped-samples",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples",
    "text": "3.2 Estimate 1,000 models from the 1,000 bootstrapped samples\nNow let’s estimate our model for each bootstrapped sample, using the map function.\n\nIn the code below, for every sample in boot, map estimates the model lm(support_war01 ~ age + sex + education_n) plugging in the bootstrap sample into the data=..\n\n\n# bootstrap simulations\nbs_ols &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + sex + education_n, data =.))\n\nThe end result is a large list with 1,000 separate linear regression models estimated on each bootstrapped sample.\nThe coefficients from each bootstrap vary from one simulation\n\n# First bootstrap\nbs_ols[[1]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.305019     0.009746     0.081039    -0.024142  \n\n\nto the next:\n\n# Second boostrap\nbs_ols[[2]]\n\n\nCall:\nlm(formula = support_war01 ~ age + sex + education_n, data = .)\n\nCoefficients:\n(Intercept)          age      sexMale  education_n  \n   0.245000     0.009142     0.095631    -0.009285  \n\n\nBecause they’re estimated off of different bootstrapped samples.\nWe will visualize and quantify that variation to describe the uncertainty associated with our estimates.\nBut first, we need to transform our large list of linear models, into a more tidy data frame that’s easier to manipulate."
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#tidy-the-results-of-our-bootstrapping",
    "href": "labs/comments/09-lab-comments.html#tidy-the-results-of-our-bootstrapping",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.3 Tidy the results of our bootstrapping",
    "text": "3.3 Tidy the results of our bootstrapping\nIn the code below we transform this large list of models into a tidy data frame of coefficients.\n\n# Tidy bootstrapp sims\nbs_ols_df &lt;- map_df(bs_ols, tidy, .id = \"id\")\n\nIn the resulting data frame the id variable identifies the bootstrap simulation (1 to 1,000), the term variable indentifies the specific coefficient from the model estimated for that simulation.\n\nhead(bs_ols_df)\n\n# A tibble: 6 × 6\n  id    term        estimate std.error statistic  p.value\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 1     (Intercept)  0.305    0.0522        5.84 6.29e- 9\n2 1     age          0.00975  0.000702     13.9  3.31e-41\n3 1     sexMale      0.0810   0.0222        3.65 2.73e- 4\n4 1     education_n -0.0241   0.0107       -2.26 2.39e- 2\n5 2     (Intercept)  0.245    0.0533        4.60 4.61e- 6\n6 2     age          0.00914  0.000731     12.5  3.36e-34"
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "href": "labs/comments/09-lab-comments.html#plot-the-bootstrapped-sampling-distribution-of-the-coefficient-for-age",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age",
    "text": "3.4 Plot the bootstrapped sampling distribution of the coefficient for age\nFinally, let’s get a sense of how our coefficients could vary.\nSpecifically, let’s compare the the observed coefficient from m1 for age, to the bootstrapped sampling distribution of coefficients in bs_ols_df\n\nFirst, we’ll create a basic plot called p_ols_age that shows the distribution of the coefficients for age from our simulation\n\n\np_ols_age &lt;- bs_ols_df %&gt;%\n  filter(term == \"age\") %&gt;%\n  ggplot(aes(estimate))+\n    geom_density()\n\nError in ggplot(., aes(estimate)): could not find function \"ggplot\"\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\nNext we’ll add some additional geometries and labels to our figure\n\nFirst we’ll put a rug to show the individual coefficients\n\n\np_ols_age +\n  geom_rug() -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\n\nThen we’ll add a vertical line where our observed coefficient on age\n\n\np_ols_age +\n  geom_vline(xintercept =  coef(m1)[2],\n             linetype = 2) -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\n\nFinally, let’s add some nice labels\n\n\np_ols_age +\n  theme_bw()+\n  labs(\n    x = \"Age\",\n    y = \"\",\n    title = \"Bootstrapped Sampling Distribution of Age Coefficient\"\n  ) -&gt; p_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\np_ols_age\n\nError in eval(expr, envir, enclos): object 'p_ols_age' not found\n\n\nCongratulations you’ve just produced and visualized your first bootstrapped sampling distribution!\nConceptually, this distribution describes *how much we would expect the coefficient on age in model to vary** from sample to sample.\nJust from eyeballing the figure above, it looks like the observed the relationship between age and support for war or 0.009 could be about as high as 0.011, and as low as 0.007.\nOf course, as the budding quantitative social scientists that we are, we can do better than just eyeballing the data."
  },
  {
    "objectID": "labs/comments/09-lab-comments.html#footnotes",
    "href": "labs/comments/09-lab-comments.html#footnotes",
    "title": "Comments for Lab 09 - Exploring Russians’ Attitudes About the War in Ukraine with OLS and Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think, google translate was a bit unclear. But higher numbers equal more education.↩︎"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html",
    "href": "labs/comments/08-lab-comments.html",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Today we’ll work through exercise 6.6.2 A Probability Model for Betting Market Election Prediction from QSS (pp. 309-310). We will use the daily data from the Intrade betting market to derive probabilities from degenerate gamblers about the likelihood that Obama would win the 2008 presidential election. We will use these probabilities to simulate possible elections and the summarize outcomes of these simulations graphically.\nPlan to spend the following amount of time\n\nGet set up to work (5 minutes)\nCalculate Obama’s expected electoral vote share on November 3, 2008 (the day before the election) (10 minutes)\nSimulate a 1000 elections for November 3, 2008 using the betting market prices a measure of the probability that Obama wins or loses a state (10 minutes)\nDisplay the results of your simulation with a histogram. (5 minutes)\nTransform these probabilities to reduce the likelihood that Obama wins states like Alabama and increase the likelihood that Obama wins states like California (5 minutes)\nSimulate another 1000 elections using these transformed probabilities. Compare the results to your initial simulation. (10)\nCalculate Obama’s expected total number of votes for each day in the 120 days before the 2008 election (15 minutes)\nSimulate 100 elections for each of the 120 days before the election, plot the results of your simulation. (20 minutes)\nTake the class survey for Donuts.\n\n\n\nConceptually, the main goal of this lab is to give you some practice working with probabilities in an application to the real world (predicting elections)\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)\n\n\n\n\n\n\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#goals",
    "href": "labs/comments/08-lab-comments.html#goals",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Conceptually, the main goal of this lab is to give you some practice working with probabilities in an application to the real world (predicting elections)\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#workflow",
    "href": "labs/comments/08-lab-comments.html#workflow",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "As with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#load-packages",
    "href": "labs/comments/08-lab-comments.html#load-packages",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst we’ll load the pacakges we need for today:\n\nlibrary(tidyverse)\nlibrary(qss)\n\nNow we’ll load two data sets from the qss package: pres08 and intrade08\n\npres08 contains the 2008 US presidential election outcomes by state.\nintrade08 contains from Intrade, an online prediction market, in days leading up to the 2008 United States Presidential Election.\n\n\n# Results from 2008 election\ndata(\"pres08\")\n\n# Daily betting market data\ndata(\"intrade08\")"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#provide-a-high-level-overview-each-data-set",
    "href": "labs/comments/08-lab-comments.html#provide-a-high-level-overview-each-data-set",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.2 Provide a high level overview each data set",
    "text": "1.2 Provide a high level overview each data set\nIn the code chunk below, please write some code to provide a high level overview of each data set.\nWe will primarily work with the EV variable from pres08 which contains the electoral votes for each state, and the PriceD variable from intrade08 from which we will construct a probability that Obama wins that state.\n\n# HLO\n# pres08\nglimpse(pres08)\n\nRows: 51\nColumns: 5\n$ state.name &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"…\n$ state      &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\",…\n$ Obama      &lt;int&gt; 39, 38, 45, 39, 61, 54, 61, 92, 62, 51, 47, 72, 36, 62, 50,…\n$ McCain     &lt;int&gt; 60, 59, 54, 59, 37, 45, 38, 7, 37, 48, 52, 27, 62, 37, 49, …\n$ EV         &lt;int&gt; 9, 3, 10, 6, 55, 9, 7, 3, 3, 27, 15, 4, 4, 21, 11, 7, 6, 8,…\n\nsummary(pres08)\n\n  state.name           state               Obama           McCain     \n Length:51          Length:51          Min.   :33.00   Min.   : 7.00  \n Class :character   Class :character   1st Qu.:43.00   1st Qu.:40.00  \n Mode  :character   Mode  :character   Median :51.00   Median :47.00  \n                                       Mean   :51.37   Mean   :47.06  \n                                       3rd Qu.:57.50   3rd Qu.:56.00  \n                                       Max.   :92.00   Max.   :66.00  \n       EV       \n Min.   : 3.00  \n 1st Qu.: 4.50  \n Median : 8.00  \n Mean   :10.55  \n 3rd Qu.:11.50  \n Max.   :55.00  \n\n# intrade08\nglimpse(intrade08)\n\nRows: 36,891\nColumns: 10\n$ .row      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ day       &lt;date&gt; 2006-11-12, 2006-11-12, 2006-11-12, 2006-11-12, 2006-11-12,…\n$ statename &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"C…\n$ MarketD   &lt;chr&gt; \"Democratic Party Nominee to win Alabama's Electoral College…\n$ PriceD    &lt;dbl&gt; 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 33.3, 40.0, …\n$ VolumeD   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MarketR   &lt;chr&gt; \"Republican Party Nominee to win Alabama's Electoral College…\n$ PriceR    &lt;dbl&gt; 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 40.0, 33.3, 40.0, …\n$ VolumeR   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state     &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"GA\", …\n\nsummary(intrade08)\n\n      .row            day              statename           MarketD         \n Min.   :    1   Min.   :2006-11-12   Length:36891       Length:36891      \n 1st Qu.: 9224   1st Qu.:2007-05-12   Class :character   Class :character  \n Median :18446   Median :2007-11-09   Mode  :character   Mode  :character  \n Mean   :18446   Mean   :2007-11-09                                        \n 3rd Qu.:27668   3rd Qu.:2008-05-08                                        \n Max.   :36891   Max.   :2008-11-19                                        \n     PriceD         VolumeD          MarketR              PriceR     \n Min.   : 0.00   Min.   :    0.0   Length:36891       Min.   : 0.00  \n 1st Qu.:12.50   1st Qu.:    0.0   Class :character   1st Qu.:15.20  \n Median :43.00   Median :    0.0   Mode  :character   Median :51.00  \n Mean   :46.78   Mean   :    6.4                      Mean   :51.27  \n 3rd Qu.:82.50   3rd Qu.:    0.0                      3rd Qu.:85.00  \n Max.   :99.90   Max.   :17353.0                      Max.   :99.50  \n    VolumeR             state          \n Min.   :    0.000   Length:36891      \n 1st Qu.:    0.000   Class :character  \n Median :    0.000   Mode  :character  \n Mean   :    5.583                     \n 3rd Qu.:    0.000                     \n Max.   :14885.000                     \n\nsum(table(intrade08$state)&gt;723)\n\n[1] 3\n\nwhich(table(intrade08$state)&gt;723)\n\nIN MO NC \n16 25 28 \n\n\nBriefly describe each data set\n\nThe pres08 data set contains the 51 observations corresponding the 50 states plus the District of Columbia. Two variables describe the state name (state.name) and postal abbreviation (state). The Obama and McCain variables correspond to the vote share of each candidate in the each state in 2008 and the EV variable corresponds the Electoral College votes for each state.\nThe intrade08 data contain 36,891 observations. Each observation corresponds to a “state-day” and in total there are about just under two years of observations (Most states have 723 observations, Indiana, Missouri, and North Carolina have more than 723 observations, for some reason). That data contain information on the price of “bets” that the Democratic or Republican candidates will win that state in the election (PriceD and PriceR), as well was descriptions of the volume of trading (VolumeD and VolumeR) and the specific market."
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#re-arrange-the-intrade",
    "href": "labs/comments/08-lab-comments.html#re-arrange-the-intrade",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.3 Re-Arrange the Intrade",
    "text": "1.3 Re-Arrange the Intrade\nIf you look closely at the data, you’d see that both are arranged alphabetically by state name, but in the pres08 data, the District of Columbia is named “D.C.”, while in the intrade08 data it is called “District of Columbia”.\n\npres08$state.name[8]\n\n[1] \"D.C.\"\n\nintrade08$statename[9]\n\n[1] \"District of Columbia\"\n\n\nAs a result, D.C. comes before Delaware in pres08 but after Delaware in intrade08. It will be useful below, for the states to be in the same order in both data sets.\n\n# DC and DE are reversed\npres08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\"\n\nintrade08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" \"DC\"\n\n# Same abbrevs\nsum(pres08$state %in% intrade08$state)\n\n[1] 51\n\n# Different namings of DC\nsum(pres08$state.name %in% intrade08$statename)\n\n[1] 50\n\n\nPlease use the arrange() function to re-arrange both data sets using the state variable which is the postal abbreviation code for each state and is the same in both data sets.\nFor the pres08 data arrange() by state and intrade08 by day and then by state\n\nRemember to save the output of arrange() back into each respective data frame\n\n\n# arrange pres08\npres08 &lt;- pres08 %&gt;%\n  arrange(state)\n\n# arrange intrade08\nintrade08 &lt;- intrade08 %&gt;%\n  arrange(day,state)\n\nIf your code was correct, DC should now come before DE in intrade08\n\n# pres08$state[1:9]\n# intrade08$state[1:9]\n\nAnd the following code should return TRUE\n\n# all.equal(pres08$state[1:51], intrade08$state[intrade08$day==\"2008-11-03\"])"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "href": "labs/comments/08-lab-comments.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.4 Convert the PriceD variable in intrade08 to a probability",
    "text": "1.4 Convert the PriceD variable in intrade08 to a probability\nThe intrade08 data contain a variable called PriceD which we will treat as the probability that Obama will win the presidential election.\nRecall that probabilities must be between 0 and 1. Please create a variable called prob_obama_win in intrade08 by dividing PriceD by 100.\n\n# Create variable prob_obama_win from PriceD in intrade08\nintrade08 %&gt;%\n  mutate(\n    prob_obama_win = PriceD/100\n  ) -&gt; intrade08"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#create-a-subset-of-intrade08-called-df_nov3",
    "href": "labs/comments/08-lab-comments.html#create-a-subset-of-intrade08-called-df_nov3",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.5 Create a subset of intrade08, called df_nov3",
    "text": "1.5 Create a subset of intrade08, called df_nov3\nNext we’ll create a subset of the data, called df_nov3 that contains just the data from the day before the election (that is Monday, November 3, 2008)\n\nHint: try filter()\n\n\ndf_nov3 &lt;- intrade08 %&gt;%\n  filter(day == \"2008-11-03\")"
  },
  {
    "objectID": "labs/comments/08-lab-comments.html#footnotes",
    "href": "labs/comments/08-lab-comments.html#footnotes",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we’re assuming these probabilities are independent. Actual forecasting models like 538 or the NYT generally assume the probabilities of winning some states correlate with the probability of winning other neighboring states.↩︎"
  },
  {
    "objectID": "labs/02-lab.html",
    "href": "labs/02-lab.html",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "",
    "text": "Our goal for today is to first, reproduce this figure:\n\n\n\n\n\n\n\n\n\nAnd then adapt and improve this figure (or other figures) to explore questions we have about the data\nI don’t expect anyone to be able to recite from memory the exact code, functions, and syntax to accomplish this task.\nThat said, you’ve already seen the code you need.\nIt’s available to you in multiple places like the slides and the comments to last week’s labs\nMy hope is that this lab will help you do the following:\n\nChunk big tasks into smaller concrete steps\n\nHow do I produce a figure that shows the average rate of new cases pe month for states with a particular type of face mask policy?\n\nWell first, I’ll need to load some packages to work with and visualize data.\nThen, I’ll need to get the data. And then…\n\n\nThink and write programmatically\n\nIn this .qmd file, I’ll first ask you to outline, conceptually, all the steps you’ll need to do to produce this figure.\nDon’t worry if you can’t think of all the necessary steps or aren’t sure of the order. We’ll be working through this collectively\nWhen we do code, I’ll ask you to organize your code as outlined below:\n\nSeparate your steps into sections using the # headers in Markdown\nWrite a brief overview in words that a normal human can understand, what the code in that section is doing\nPaste the code for that section into a code chunk\nAdd brief comments to this code to help your reader understand what’s happening\nKnit your document after completing each section.\n\n\nMapping concepts to code\n\nYou shouldn’t have to write much new code. Just copy and paste from the labs and slides.\nYour goal for today is to interpret that code and develop a mental map that allows you to say when I want to do this type of task (say “recode data”), I need to use some combination of these functions (%&gt;%, mutate(), maybe group_by() or case_when())\n\nPractice wrangling data\n\nHow do you load data?\nHow do you look at data?\nHow do you transform data?\n\nPractice visualizing data\n\nUsing the grammar of graphics to translate raw data into visual graphics\nUnderstanding the components of this grammar:\n\ndata\naesthetics\ngeometries\nfacets\nstatistics\ncoordinates\nthemes\n\nExploring what happens when we change these components\n\n\nWe’ll work in pairs and periodically check in as a class to check our progress, review concepts, and share insights.\nIf we finish early, you’re free to go. If you want, we can take some time to explore some additional figures we might produce like maps or lollipop plots.\nOk, let’s begin!"
  },
  {
    "objectID": "labs/02-lab.html#step-2.1",
    "href": "labs/02-lab.html#step-2.1",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "3.1 Step 2.1",
    "text": "3.1 Step 2.1\nDescribe briefly (in a sentence or two or a couple of bullet points) what this section does\n\n# Write the code for Step 2.1 here"
  },
  {
    "objectID": "labs/02-lab.html#step-2.2",
    "href": "labs/02-lab.html#step-2.2",
    "title": "Lab 02 - Visualizing data on COVID-19 in the U.S.",
    "section": "3.2 Step 2.2",
    "text": "3.2 Step 2.2\nDescribe briefly (in a sentence or two or a couple of bullet points) what this section does\n\n# Write the code for Step 2.2 here"
  },
  {
    "objectID": "labs/01-lab-comments.html",
    "href": "labs/01-lab-comments.html",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "",
    "text": "Today, we’ll continuing exploring the COVID-19 data for the U.S.\nWe covered a lot of ground in our last lecture. Conceptually, talked about how to\n\nWrite and code in R Markdown\nInstall and load packages\nDownload and inspect data\nClean and recode data\nCalculate simple descriptive statistics with that data\n\nTo do this, we copied and pasted a lot of code. Today, we’ll get practice writing our own code. Specifically we will\n\nRepeat some steps from lecture to get our workspace and data set up\nRecode some additional variables\nInvestigate what negative values mean for face mask policy\nExplore, in greater depth, tools for descriptive inference\nRevisit the question of face masks and new cases, conditioning on time."
  },
  {
    "objectID": "labs/01-lab-comments.html#uncomment-and-run-the-following-code",
    "href": "labs/01-lab-comments.html#uncomment-and-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "5.1 Uncomment and run the following code",
    "text": "5.1 Uncomment and run the following code\nHighlight the commented code below from # covid_us %&gt;% to #   ) -&gt; covid_us and press shift + cmd + C on a mac or shift + ctrl + C on PC to uncomment the code.\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, str_pad(month, width = 2, pad=0), sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\nThe year(date) extracts the year from our date variable and saves it in new column called year\nSimilarly, the month(date) extracts the month from our date variable and saves it in a new column called month\nFinally the paste() command pastes these two variables together, with the str_pad() adding a leading 0 to single digit months.\nTo calculate the percent of states population that is fully vaccinated on a given date we divide the total number of fully vaccinated by the state’s population and multiply by 100 to make it a percent."
  },
  {
    "objectID": "labs/01-lab-comments.html#uncomment-and-run-the-code-below",
    "href": "labs/01-lab-comments.html#uncomment-and-run-the-code-below",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.1 Uncomment and run the code below,",
    "text": "6.1 Uncomment and run the code below,\n\ncovid_us %&gt;%\n  filter(facial_coverings == -4) %&gt;%\n  select(date, state) %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    n = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n  )%&gt;%\n  arrange(earliest_date)\n\n# A tibble: 4 × 4\n  state              n earliest_date latest_date\n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;        &lt;date&gt;     \n1 Illinois         156 2020-10-01    2021-05-15 \n2 Massachusetts     35 2020-10-02    2020-11-05 \n3 South Carolina    61 2020-10-13    2020-12-12 \n4 Maryland         158 2020-11-06    2021-04-12"
  },
  {
    "objectID": "labs/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "href": "labs/01-lab-comments.html#please-explain-in-words-your-best-understanding-of-what-each-line-of-code-is-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "6.2 Please explain in words your best understanding of what each line of code is doing:",
    "text": "6.2 Please explain in words your best understanding of what each line of code is doing:\n\ncovid_us %&gt;% tells R that every line of code after will use covid_us dataframe\nfilter(facial_coverings == -4) %&gt;% tells R to filter out only the rows where the facial coverings variable equals -4\nselect(date, state) %&gt;% tells R to select the columns named date and state\ngroup_by(state) %&gt;% tells R that subsequent commands should be done separately for each unique value of state\nsummarize( tells R we want to summarize the output of susequent commands\nn = n(), tells R to count the number of observations (state-dates) for each state that had a value of -4 on the facial_coverings variable\nearliest_date = min(date), tells R to report the earliest date that each state had a value of -4\nlatest_date = max(date), tells R to report the last date that each state had a value of -4\n)%&gt;% tells R we’re finished with the summarize() function\narrange(earliest_date) arranges the data in asscending order from earliest to latest start date\n\nYou may find this cheatsheet useful and you can find a more detailed discussion here\nSubstantively, what does the previous chunk of code tell us?\n\nSo there are five states that had -4 on the facial covering variable: Illinois, Maryland, Massachusetts, Montana, and South Carolina. Illinois was the first state where this code appears, and it appears present in 156 observations while Montana was the last adopting a policy code -4 on March 25, 2021\n\n\n\nFiltering data, selecting specific variables, and summarizing variables are important skills that let us “know our data”"
  },
  {
    "objectID": "labs/01-lab-comments.html#please-run-the-following-code",
    "href": "labs/01-lab-comments.html#please-run-the-following-code",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.1 Please run the following code:",
    "text": "7.1 Please run the following code:\n\noxford_us %&gt;%\n  mutate(\n    date = ymd(Date)\n  )%&gt;%\n  filter(RegionName == \"Illinois\", \n         date &gt; \"2020-08-01\", \n         date &lt; \"2021-01-01\",\n         !is.na(H6_Notes)) %&gt;%\n  select(date,starts_with(\"H6_\")) -&gt; il_facemasks\nil_facemasks\n\n# A tibble: 8 × 4\n  date       `H6_Facial Coverings` H6_Flag H6_Notes                             \n  &lt;date&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                \n1 2020-08-21                     2       1 \"In Executive Order 2020-52, Executi…\n2 2020-08-26                     2       1 \"Effective from 26 August 2020, the …\n3 2020-09-18                     2       1 \"On 18 September, in Executive Order…\n4 2020-10-01                     4       0 \"Originally coded a 3T, but looking …\n5 2020-10-16                     4       0 \"In Executive Order (EO) 2020-59, Go…\n6 2020-11-13                     4       0 \"Noting that Executive Order 2020-71…\n7 2020-11-20                     4       0 \"Executive Order 2020-73 requires pe…\n8 2020-12-01                     3       1 \"Chicago seems to have changed its g…"
  },
  {
    "objectID": "labs/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "href": "labs/01-lab-comments.html#again-explain-in-words-what-the-components-of-this-code-are-doing",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.2 Again, explain in words, what the components of this code are doing:",
    "text": "7.2 Again, explain in words, what the components of this code are doing:\n\noxford_us %&gt;% Tells R to use the Oxford policy data\nmutate(date = ymd(Date))%&gt;% Creates a date variable of class date from the original Date variable (which was class numeric)\nfilter(RegionName == \"Illinois\", subsets the data to just Illinois\ndate &gt; \"2020-08-01\", filters out dates before August 1, 2020\ndate &lt; \"2021-01-01\", filters out observations with dates after January 1,2021\n!is.na(H6_Notes)) %&gt;% filters out observations without notes (which appear in the data when policy changes)\nselect(date,starts_with(\"H6_\")) -&gt; il_facemasks Selects just the date and notes variables and saves them to an object called il_facemasks\nil_facemasks prints the obejct in the console\n\nLet’s take a look at the H6_Notes variable for 2020-09-18\n\nil_facemasks$H6_Notes[3]\n\n[1] \"On 18 September, in Executive Order 2020-55, the Governor reissued most executive orders, extending a majority of the provisions through 17 October 2020. This includes mask requirements.      https://web.archive.org/web/20200922144918/https://www2.illinois.gov/Pages/Executive-Orders/ExecutiveOrder2020-55.aspx\"\n\n\nNow update the code to select H6_Notes variable for 2020-10-01\n\nil_facemasks$H6_Notes[il_facemasks$date == \"2020-10-01\"]\n\n[1] \"Originally coded a 3T, but looking at the below description, which includes even residential buildings, it is hard to conceive of a time outside the home when a Chicago resident would not be required to wear a mask. The Phase IV \\\"Gradually Resume\\\" guidelines seem not to provide any significant exemption (https://archive.fo/dOyY9). Hence code moves up to 4T.    Effective October 1, 2020, residents of Chicago are required to wear masks in all public places.     “Any individual who is over age two and able to medically tolerate a mask shall be required to wear a mask when in a public place, which for purposes of this Order includes any common or shared space in: (1) a residential multi-unit building or (2) any non-residential building, unless otherwise provided for in the Phase IV: Gradually Resume guidelines promulgated by the Office of the Mayor (\\\"Gradually Resume Guidelines\\\")”    Additionally, but separately, “Individuals must, at all times and as much as reasonably possible, maintain Social Distancing from any other person who does not live with them.”    https://web.archive.org/web/20201116163255/https://www.chicago.gov/content/dam/city/sites/covid/health-orders/CDPH%20Order%202020-9%20-%205th%20Amended%20FINAL%209.30.20_AAsigned.pdf\""
  },
  {
    "objectID": "labs/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "href": "labs/01-lab-comments.html#what-have-we-learned-about-our-variables-measuring-face_mask-policy",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "7.3 What have we learned about our variables measuring face_mask policy",
    "text": "7.3 What have we learned about our variables measuring face_mask policy\nIn Illinois, the -4’s seem to correspond to more stringent mask policies implemented in Chicago relative to the rest of the state. So by collapsing negative and positive values of facial_coverings to construct our face_mask variable, we’re probably over stating the extent the extensiveness of policies in effect.\nSo we should be cautious in how we interpret our collapsed variable, face_mask. Perhaps we could construct another variable that distinguished state-level policies from more localized policies, or we could only look at cases where there was a uniform state policy."
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-central-tendency",
    "href": "labs/01-lab-comments.html#measures-of-central-tendency",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.1 Measures of Central Tendency",
    "text": "8.1 Measures of Central Tendency\nMeasures of central tendency describe what a typical value of some variable. In this course, we’ll use three measures of what’s typical:\n\nmean\nmedian\nmode\n\n\n8.1.1 Mean\nOne of the most frequent measures of central tendency we’ll use in this course is a mean or average.\nSuppose we have \\(n\\) observations of some variable \\(x\\). We can calculate the mean of \\(\\bar{x}\\) (“x bar), by adding up all the values of x\n[ {x}=_{i=1}^n x_i ]\nWe’ll see later in the course that means are closely related to the concept of expected values in probability and that conditional means (which we’ll calculate below) are central to thinking about linear regression.\nFor now, please calculate the mean (average) number of new cases per 100,000 residents in our data:\n\nmean(covid_us$new_cases_pc, na.rm=T)\n\n[1] 28.11486\n\n\nLast class, when we calculated the the average number of new cases under each type of face mask policy, we were calculating a conditional mean the mean of some variable, conditional on some other variable taking a specific value.\nFormally, you’ll often see this written in terms of Expected Values: Something like\n[ E[Y|X=x] ]\nOr to make it more concrete:\n[ E[ | ] ]\nIn code, we could accomplish this manually, using the index operator:\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"No policy\"], na.rm=T)\n\n[1] 10.26168\n\n\n\n8.1.1.1 How would we calculate the conditional mean of new_cases_pc when face_masks equals “Recommend”\n\nmean(covid_us$new_cases_pc[covid_us$face_masks == \"Recommended\"], na.rm=T)\n\n[1] 16.61408\n\n\nBy using group_by() with summarise() we can accomplish this more quickly:\n\ncovid_us%&gt;%\n  group_by(face_masks)%&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  )\n\n# A tibble: 6 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2\n6 &lt;NA&gt;                           11.8\n\n\n\n\n\n8.1.2 Median\nThe median is another measure of what’s typical for variables that take numeric values\nImagine, we took our data new Covid-19 cases and arranged them in ascending order, from the smallest value to highest value\nThe median would be the value in the exact middle of that sequence, also known as the 50th percentile.1\nFormally, we can define that median as:\n[ M_x = X_i : ^{x_i} f_x(X)dx=^f_x(X)dx=1/2 ]\nWhich might look like Greek to you, which is fine. Just think of it as the middle value.\n\n8.1.2.1 Please calculate the median number of new Covid-19 cases per 100,000 using the median() function. How does it compare to the mean?\n\nmedian(covid_us$new_cases_pc, na.rm=T)\n\n[1] 10.52355\n\n\nInteresting the median is much lower than the mean. If we were to look at a histogram of our data (more on that next week; think of it as a graphical representation of a frequency table), we see that the new_cases_pc has a “long tail” or is skewed to the right. Most of the values are close to 0, but there are few cases that are extreme outliers.\n\n\nMedians are less influenced by outliers than means\n\n\n\nhist(covid_us$new_cases_pc, breaks = 100)\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Modes\nConceptually, a mode describes the most frequent outcome.\nModes are useful for describing what’s typical of “nominal” or categorical data like our measure of face mask policy.\nTo calculate the mode of our face_masks variable, wrap the output of table() with the sort() function\n\nsort(table(covid_us$face_masks))\n\n\n    Required all times              No policy            Recommended \n                  1032                   3893                   8879 \nRequired shared places      Some requirements \n                 15088                  24786 \n\n\nFor numeric data, modes correspond to the peak of a variable’s density function (more on this later in the class).\nYou can get a sense of the relationship between, means, median’s and modes from this helpful figure from Wikipedia:"
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-dispersion",
    "href": "labs/01-lab-comments.html#measures-of-dispersion",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.2 Measures of Dispersion",
    "text": "8.2 Measures of Dispersion\nMeasures of dispersion describe how much the data “vary.” Let’s discuss the following ways we can summarize how our data vary:\n\nrange\npercentile range\nvariance\nstandard deviation\n\n\n8.2.1 Range\nThe range of a variable is simply it’s minimum and maximum value\n\n8.2.1.1 Please calculate the range of our new_cases_pc using the range() function\n\nrange(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916 1531.8669\n\nmin(covid_us$new_cases_pc,na.rm = T)\n\n[1] -275.6916\n\nmax(covid_us$new_cases_pc,na.rm = T)\n\n[1] 1531.867\n\n\n\n\n8.2.1.2 What states on what dates observed these minimum and maximum values?\n\ncovid_us %&gt;%\n  filter(\n    new_cases_pc &lt; -188 |\n    new_cases_pc &gt; 1500\n  )%&gt;%\n  select(state, date,new_cases_pc)\n\n# A tibble: 5 × 3\n# Groups:   state [5]\n  state        date       new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;            &lt;dbl&gt;\n1 Florida      2021-06-04        -189.\n2 Rhode Island 2022-01-04        1532.\n3 Tennessee    2023-01-01        -267.\n4 Nebraska     2022-10-28        -276.\n5 Kentucky     2022-10-11        -198.\n\n\n\n\n\n8.2.2 Percentiles Ranges\nThe \\(p\\)-th percentile is the value of the observation such that 100*p percent of the data are to the left and 100-100*p are two the right.\n[ p_x = X_i : ^{x_i} f_x(X)dx= p; ^f_x(X)dx=1-p ]\nThe median is just the 50th percentile\nIn R we calculate the \\(p\\)-th percentile using the quantile() setting the probs argument to the \\(p/100\\) percentile that we we want.\n\n8.2.2.1 Please use the quantile() function to calculate the 25th and 75th percentiles of the new_cases_pc variable.\n\nquantile(covid_us$new_cases_pc, probs = c(.25,.75), na.rm=T)\n\n     25%      75% \n 0.00000 32.14152 \n\n\nThe 25th and 75th percentile define the “Interquartile Range” where 50 percent of the observations lie within this range, and 50 percent lie outside the range.\n\n\n\n8.2.3 Variance\nVariance describes how much observations of a given measure vary around that measure’s mean.\nThe variance in a given sample is calculated by taking the average of the sum of squared deviations (i.e. differences) around a measure’s mean.\n[ ^2_x=_{i=1}n(x_i-{x})2 ]\n\n\\(x_i-\\bar{x}\\) is the deviation of each observation from the overall mean\n\\((x_i-\\bar{x})^2}\\) squaring this ensures that we treat positive and negative deviations the same when calculating the overall variance\n\\(\\sum_{i=1}\\) sums up all the differences\n\\(\\frac{1}{n-1}\\) is like taking the average of these differences (we divide by \\(n-1\\) instead of \\(n\\) for statistical reasons that we’ll return two when we talk about estimation)\n\nUse the var() function to calculate the variance of the new_cases_pc variable.\n\nvar(covid_us$new_cases_pc,na.rm=T)\n\n[1] 3402.718\n\n# Calculate by hand\n\nsum(\n  (covid_us$new_cases_pc - mean(covid_us$new_cases_pc,na.rm=T))^2, \n  na.rm=T\n  )/(sum(!is.na(covid_us$new_cases_pc))-1)\n\n[1] 3402.718\n\n\nVariance will be important for thinking about uncertainty and inference (e.g. how might our estimate have been different)\n\n\n8.2.4 Standard Deviations\nA standard deviation is simply the square root of variable’s variance.\n[ _x== ]\nStandard deviations are easier to interpet because their units are the same as variable.\nThink of them as a measure of the typical amount of variation for variable.\nAgain, let’s use the sd() function to calculate the standard deviation of the new_cases_pc variable\n\nsd(covid_us$new_cases_pc,na.rm=T)\n\n[1] 58.33282"
  },
  {
    "objectID": "labs/01-lab-comments.html#measures-of-association",
    "href": "labs/01-lab-comments.html#measures-of-association",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "8.3 Measures of Association",
    "text": "8.3 Measures of Association\nMeasures of association describe how variables relate to each other.\n\n8.3.1 Covariance\nCovariance describes how two variables “co-vary”.\nWhen \\(x\\) is above its mean, \\(y\\) also tends to be above it’s mean, these variables have a positive covariance.\nIf when \\(x\\) tends to be high, \\(y\\) tends to be low, these variables have a negative variance\nFormally, the sample2 covariance of two variables can written as follows:\n[ cov(x,y)=_{i=1}^n(x_i-{x})(y_i-{y}) ]\n\n8.3.1.1 Please calculate the covariance between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the var() function\n\nvar(covid_us$new_cases_pc,covid_us$percent_vaccinated,na.rm = T)\n\n[1] -19.96569\n\n\n\n\n\n8.3.2 Correlation\nLike variances, covariances don’t really have intrinsic meaning, since x and y can be measured on very different scales.\nThe correlation between two variables takes their covariance and scales this by the standard deviation of each variable, creating a measure that can range from -1 (perfect negative correlation) to 1 perfect positive correlation.\nAgain, we can write this formally\n[ _{x,y} = ]\nBut don’t sweat the formulas too much. I’m just contractually obligated to show you math.\n\n8.3.2.1 Calculate the correlation between the percent of state’s population that is fully vaccinated (percent_vaccinated) and new_cases_pc using the cor() function.\nYou’ll need to set the argument use=\"complete.obs\n\ncor(covid_us$percent_vaccinated, covid_us$new_cases_pc, use = \"complete.obs\")\n\n[1] -0.01369243\n\n\nHmm… That seems a little strange. What if we calculated the correlation between vaccination rates and new cases separately for each month in 2021\n\n\n8.3.2.2 Uncomment and interpret the output of the code below\n\ncovid_us %&gt;%\n  filter(year &gt; 2020)%&gt;%\n  ungroup() %&gt;%\n  group_by(year,month)%&gt;%\n  summarise(\n    mn_per_vax = mean(percent_vaccinated, na.rm=T),\n    cor = cor(new_cases_pc, percent_vaccinated, use = \"complete.obs\")\n  )\n\nError in `summarise()`:\nℹ In argument: `cor = cor(new_cases_pc, percent_vaccinated, use =\n  \"complete.obs\")`.\nℹ In group 28: `year = 2023` and `month = 4`.\nCaused by error in `cor()`:\n! no complete element pairs"
  },
  {
    "objectID": "labs/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "href": "labs/01-lab-comments.html#what-do-these-averages-really-tell-us",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.1 What do these averages really tell us?",
    "text": "9.1 What do these averages really tell us?\nProbably not that much. Different Face mask policies are implemented at different times in the pandemic. For example, by 2021, almost all states have some requirements. Comparing the average for new cases in states with no policy to states with full requirements, is comparing the state of world in early 2020 to the state of the world in late 2020 to mid 2021. But lots of things differ between these periods. Other policies are also going into effect, new variants are emerging.\nIn short, those simple conditional means across the full data don’t really provide an apples to apples comparison.\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    n = n()\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,n,fill=face_masks))+\n  geom_bar(stat=\"identity\")\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\ncovid_us%&gt;%\n  group_by(date,face_masks)%&gt;%\n  summarise(\n    new_cases = sum(new_cases)\n  )%&gt;%\n  filter(date &gt;= \"2020-03-01\")%&gt;%\n  ggplot(aes(date,new_cases))+\n  geom_smooth()\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 328 rows containing non-finite values (`stat_smooth()`)."
  },
  {
    "objectID": "labs/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "href": "labs/01-lab-comments.html#add-another-filter-to-the-code-above-to-caluclate-the-conditional-means-for-just-1-month-in-a-particular-year-in-the-data",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data",
    "text": "9.2 Add another filter to the code above to caluclate the conditional means for just 1 month in a particular year in the data\nIf we limit our comparison to a more narrow time period, say one month in one year, we’re making a fairer comparison between states that are likely facing more similar conditions/challenges.\nSo when we compare states in September 2020, we see that rates of new cases tend to be much higher in states with only recommend face mask policies compared to states with at least some requirements.\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  filter(year_month == \"2020-09\")%&gt;%\n  group_by(face_masks) %&gt;%\n  summarise(\n    new_cases_pc = mean(new_cases_pc,na.rm=T)\n  )\n\n# A tibble: 4 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Recommended                    43.9\n2 Some requirements              13.5\n3 Required shared places         13.0\n4 Required all times             10.1\n\n\n\n9.2.1 Add another arguement to the group_by() command from the original code to calcutate the conditional means by face mask policy for each month in each year of the data\n\nSave the output of summarize into an object called cases_by_month_and_policy\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(year_month,face_masks) %&gt;%\n  summarise(\n    n = length(unique(state)),\n    new_cases_pc = round(mean(new_cases_pc,na.rm=T)),\n    total_cases = round(mean(confirmed,na.rm=T))\n  ) -&gt; cases_by_month_and_policy\n\n`summarise()` has grouped output by 'year_month'. You can override using the\n`.groups` argument.\n\n\n\n\n9.2.2 Uncomment the code below to display cases_by_month_and_policy in a searchable table\n\nDT::datatable(cases_by_month_and_policy,\n              filter = \"top\")\n\n\n\n\n\n\n\n9.2.3 Uncomment the code below to visualize this cases_by_month_and_policy\nWhat does this figure tell us?\n\ncases_by_month_and_policy %&gt;%\n  ggplot(aes(\n    x= year_month,\n    y = new_cases_pc,\n    col=face_masks))+\n  geom_point()+\n  coord_flip()\n\n\n\n\n\n\n\n\nSo this figure graphically displays the data cases_by_month_and_policy\nFrom about August 2020 to October 2020 states with facemask requirements saw much lower rates of new cases than states that only recommended face masks.\nAfter October 2020, every state has at least some requirement, and the differences between the stringency of requirements is a little harder to see.\nAgain this stuff is complicated. Lots of things are changing and these month comparisons are by no means perfect. Lot’s of things differ between states with different mask policies. What we’d really like to know is a sort of counterfactual comparison between the number new cases in a state with a given policy and what those new cases would have been had that state had a different policy.\nThe problem is, we don’t get to see that counterfactual outcome. So how can we make causal claims about the effects of facemasks, or any other policy that interests us? Finding creative ways to answer these questions is the key to making credible causal claims.\nNext week, we’ll explore how to make this figure and many more from our data"
  },
  {
    "objectID": "labs/01-lab-comments.html#footnotes",
    "href": "labs/01-lab-comments.html#footnotes",
    "title": "Lab 01 - Exploring data on COVID-19 in the U.S.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a little more complicated as we need to decide how to handle situations where their are ties, or an even number of cases. For now we’ll just accept the default rules R uses.↩︎\nAstute readers might ask, why are you talking about samples? We’ll come back to this later in the course when we talk about probability, estimation and statistical inference.↩︎"
  },
  {
    "objectID": "labs/08-lab.html",
    "href": "labs/08-lab.html",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Today we’ll work through exercise 6.6.2 A Probability Model for Betting Market Election Prediction from QSS (pp. 309-310). We will use the daily data from the Intrade betting market to derive probabilities from degenerate gamblers about the likelihood that Obama would win the 2008 presidential election. We will use these probabilities to simulate possible elections and the summarize outcomes of these simulations graphically.\nPlan to spend the following amount of time\n\nGet set up to work (5 minutes)\nCalculate Obama’s expected electoral vote share on November 3, 2008 (the day before the election) (10 minutes)\nSimulate a 1000 elections for November 3, 2008 using the betting market prices a measure of the probability that Obama wins or loses a state (10 minutes)\nDisplay the results of your simulation with a histogram. (5 minutes)\nTransform these probabilities to reduce the likelihood that Obama wins states like Alabama and increase the likelihood that Obama wins states like California (5 minutes)\nSimulate another 1000 elections using these transformed probabilities. Compare the results to your initial simulation. (10)\nCalculate Obama’s expected total number of votes for each day in the 120 days before the 2008 election (15 minutes)\nSimulate 100 elections for each of the 120 days before the election, plot the results of your simulation. (20 minutes)\n\n\n\nConceptually, the main goals of this lab are to\n\ngive you some practice working with probabilities in an application to the real world (predicting elections)\nintroduce the idea of simulations as a tool for understanding probability and describing our uncertainty about what could have happened\n\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)\n\n\n\n\n\n\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab.html#goals",
    "href": "labs/08-lab.html#goals",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "Conceptually, the main goals of this lab are to\n\ngive you some practice working with probabilities in an application to the real world (predicting elections)\nintroduce the idea of simulations as a tool for understanding probability and describing our uncertainty about what could have happened\n\nTechnically, you learn how to\n\nuse for() loops, which a useful programming skill when you need to do something repeatedly\nwork with R’s built in probability functions. Specifically, we’ll use the:\n\nrbernoulli() function to simulate “coin flips” for each state and on each date, will the betting market data will define the probability that Obama wins that state.\npnorm() and qnorm() functions to transform these probabilities, giving more probability to places where Obama is likely to win, and less probability to states Obama is unlikely to win.\n\nYou’ll also get practice wrangling and visualizing data (today using R’s base graphics functions)"
  },
  {
    "objectID": "labs/08-lab.html#workflow",
    "href": "labs/08-lab.html#workflow",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "",
    "text": "As with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nKnit the document\nOpen the html file in your browser (Easier to read)\nWrite yourcode in the chunks provided\nComment out or delete any test code you do not need\nKnit the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab.html#load-packages",
    "href": "labs/08-lab.html#load-packages",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst we’ll load the pacakges we need for today:\n\nlibrary(tidyverse)\nlibrary(qss)\n\nNow we’ll load two data sets from the qss package: pres08 and intrade08\n\npres08 contains the 2008 US presidential election outcomes by state.\nintrade08 contains from Intrade, an online prediction market, in days leading up to the 2008 United States Presidential Election.\n\n\n# Results from 2008 election\ndata(\"pres08\")\n\n# Daily betting market data\ndata(\"intrade08\")"
  },
  {
    "objectID": "labs/08-lab.html#provide-a-high-level-overview-each-data-set",
    "href": "labs/08-lab.html#provide-a-high-level-overview-each-data-set",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.2 Provide a high level overview each data set",
    "text": "1.2 Provide a high level overview each data set\nIn the code chunk below, please write some code to provide a high level overview of each data set.\nWe will primarily work with the EV variable from pres08 which contains the electoral votes for each state, and the PriceD variable from intrade08 from which we will construct a probability that Obama wins that state.\n\n# HLO\n# pres08\n\n# intrade08\n\nBriefly describe each data set\n\nThe pres08 data set contain …\nThe intrade08 data contain …"
  },
  {
    "objectID": "labs/08-lab.html#re-arrange-the-intrade",
    "href": "labs/08-lab.html#re-arrange-the-intrade",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.3 Re-Arrange the Intrade",
    "text": "1.3 Re-Arrange the Intrade\nIf you look closely at the data, you’d see that both are arranged alphabetically by state name, but in the pres08 data, the District of Columbia is named “D.C.”, while in the intrade08 data it is called “District of Columbia”.\n\npres08$state.name[8]\n\n[1] \"D.C.\"\n\nintrade08$statename[9]\n\n[1] \"District of Columbia\"\n\n\nAs a result, D.C. comes before Delaware in pres08 but after Delaware in intrade08. It will be useful below, for the states to be in the same order in both data sets.\n\n# DC and DE are reversed\npres08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\"\n\nintrade08$state[1:9]\n\n[1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" \"DC\"\n\n# Same abbrevs\nsum(pres08$state %in% intrade08$state)\n\n[1] 51\n\n# Different namings of DC\nsum(pres08$state.name %in% intrade08$statename)\n\n[1] 50\n\n\nPlease use the arrange() function to re-arrange both data sets using the state variable which is the postal abbreviation code for each state and is the same in both data sets.\nFor the pres08 data arrange() by state and intrade08 by day and then by state\n\nRemember to save the output of arrange() back into each respective data frame\n\n\n# arrange pres08\n\n\n# arrange intrade08\n\nIf your code was correct, DC should now come before DE in intrade08\n\n# pres08$state[1:9]\n# intrade08$state[1:9]\n\nAnd the following code should return TRUE\n\n# all.equal(pres08$state[1:51], intrade08$state[intrade08$day==\"2008-11-03\"])"
  },
  {
    "objectID": "labs/08-lab.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "href": "labs/08-lab.html#convert-the-priced-variable-in-intrade08-to-a-probability",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.4 Convert the PriceD variable in intrade08 to a probability",
    "text": "1.4 Convert the PriceD variable in intrade08 to a probability\nThe intrade08 data contain a variable called PriceD which we will treat as the probability that Obama will win the presidential election.\nRecall that probabilities must be between 0 and 1. Please create a variable called prob_obama_win in intrade08 by dividing PriceD by 100.\n\n# Create variable prob_obama_win from PriceD in intrade08"
  },
  {
    "objectID": "labs/08-lab.html#create-a-subset-of-intrade08-called-df_nov3",
    "href": "labs/08-lab.html#create-a-subset-of-intrade08-called-df_nov3",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "1.5 Create a subset of intrade08, called df_nov3",
    "text": "1.5 Create a subset of intrade08, called df_nov3\nNext we’ll create a subset of the data, called df_nov3 that contains just the data from the day before the election (that is Monday, November 3, 2008)\n\nHint: try filter()\n\n\n# create df_nov3"
  },
  {
    "objectID": "labs/08-lab.html#footnotes",
    "href": "labs/08-lab.html#footnotes",
    "title": "Lab 08: Predicting Election Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we’re assuming these probabilities are independent. Actual forecasting models like 538 or the NYT generally assume the probabilities of winning some states correlate with the probability of winning other neighboring states.↩︎"
  },
  {
    "objectID": "labs/05-lab-comments.html",
    "href": "labs/05-lab-comments.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3032022)\ngraded_question &lt;- sample(1:10,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\n\nGrading Questin 9: Basically, if you made any changes to fig_m5 100 percent. If you simply recreated fig_m5 80 percent. If you didn’t create figure fig_m5 0 percent. Sorry! But don’t fret, remember your 3 lowest lab scores are dropped from your lab grade.\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, 1, 2, 3, 4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/05-lab-comments.html#load-covid-19-data",
    "href": "labs/05-lab-comments.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid data\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "labs/05-lab-comments.html#load-election-data",
    "href": "labs/05-lab-comments.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- dataverse::get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# Backup\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/05-lab-comments.html#recode-the-covid-19-data",
    "href": "labs/05-lab-comments.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide: here\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/05-lab-comments.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, change the new_deaths_pc_7day to new_deaths_pc_14day and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "labs/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/05-lab-comments.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes*100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df"
  },
  {
    "objectID": "labs/05-lab-comments.html#for-all-the-observations",
    "href": "labs/05-lab-comments.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7day)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\ncovid_us %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7day\n  &lt;fct&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 Trump        19.3              0.340\n2 Biden        22.0              0.287"
  },
  {
    "objectID": "labs/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/05-lab-comments.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &lt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7day\n  &lt;fct&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 Trump        22.8              0.400\n2 Biden        30.6              0.380"
  },
  {
    "objectID": "labs/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/05-lab-comments.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2021-04-19\") %&gt;%\n  group_by(winner)%&gt;%\n  summarise(\n    new_deaths = mean(new_deaths, na.rm=T),\n    new_deaths_pc_7day = mean(new_deaths_pc_7day, na.rm=T),\n  )\n\n# A tibble: 2 × 3\n  winner new_deaths new_deaths_pc_7day\n  &lt;fct&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n1 Trump        17.1              0.302\n2 Biden        16.3              0.226\n\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that states which Biden won had about 27 new deaths compared to 23.8 new deaths in states which Trump one.\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that rates tend to be higher in Red States (0.415 deaths per 100k) than Blue States (0.349 deaths per 100k). This difference reflects the fact that Biden tended to win more populous states than trump, so simply looking at the average number of new deaths is bit misleading. Comparing 7-day averages per 100,000 residents adjusts for differences in population between Red and Blue States.\nWhen we limit our analysis, to just observations before April 19, 2021, the difference in the 7-day average rate of new Covid-19 deaths per 100,000 residents is relatively small (0.02 more deaths per 100,000 residents in Red States)\nWhen we look at observations after the vaccine became widely available the difference is more than 6 times as big (0.125 more deaths per 100,000 residents in Red States)"
  },
  {
    "objectID": "labs/05-lab-comments.html#recreating-the-nyt-figures",
    "href": "labs/05-lab-comments.html#recreating-the-nyt-figures",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "9.1 Recreating the NYT Figures",
    "text": "9.1 Recreating the NYT Figures\nThis turned out to be more annoying than I thought, but if you really wanted to recreate the figures from the articles, this was as close as I could get:\n\n# Vector containing labeled states\nthe_labs &lt;- c(\"WV\",\"WY\",\"MS\",\"KY\",\"TX\",\"FL\",\"GA\",\"IL\",\"NY\",\"VT\",\"MD\",\"CA\")\n\ncovid_us %&gt;%\n  # Only include labels for states in the the_labs\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  # Subset data\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  # Set aesthetics, flipping vax to % unvaxxed\n  ggplot(aes(x = rep_voteshare,\n             y = (100-percent_vaccinated),\n             label = nyt_labs\n             ))+\n  # points coloreded by vote share\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  # color gradient\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  # add simple regression line\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  # add labels\n  geom_text_repel()+\n  # futz with limits\n  ylim(15,60)+\n  # add grid lines by hand\n  geom_hline(yintercept = 60, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 40, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 20, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=20, yend = 60), col = \"lightgrey\", size = .25)+\n  # Add arrows\n  geom_segment(aes(x = 34, xend = 32, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add biden text\n  annotate(\"text\",x = 34.5, y=18.5 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=17.1 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.7, y=17.1 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  # Add trump arrow\n  geom_segment(aes(x = 70, xend = 72, y=18.5, yend = 18.5),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  # Add trump text\n  annotate(\"text\",x = 69.5, y=18.5 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.1, y=17.1 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=17.1 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  # Label y-axis\n  annotate(\"text\",x = 30, y=20 ,label = \"20%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=40 ,label = \"40%\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=60 ,label = \"60% of residents not fully vaccinated\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  # get rid of default theme\n  theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values (`geom_text_repel()`).\n\n\n\n\n\n\n\n\n\n\n# Same as above, but now modeling deaths with rep vote share\n\ncovid_us %&gt;%\n  mutate(\n    nyt_labs = ifelse(state_po %in%the_labs, state_po, NA)\n  )%&gt;%\n  filter(date == \"2021-09-23\") %&gt;%\n  filter(state != \"District of Columbia\") %&gt;%\n  ggplot(aes(x = rep_voteshare,\n             y = new_deaths_pc_14day,\n             label = nyt_labs\n             ))+\n  geom_point(aes(col=rep_voteshare),size=2,alpha=.5)+\n  scale_color_gradient2(\n    midpoint = 50,\n    low = \"blue\", mid = \"grey\", high = \"red\",\n    guide = \"none\")+\n  geom_smooth(method = \"lm\", \n              se=F,\n              linetype = 2,\n              col =\"grey\")+\n  geom_text_repel()+\n  # theme_void()+\n  ylim(-.2,2.2)+\n  geom_hline(yintercept = 0, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = .5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 1.5, col = \"lightgrey\", size = .25)+\n  geom_hline(yintercept = 2, col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 50, xend = 50, y=0, yend = 2), col = \"lightgrey\", size = .25)+\n  geom_segment(aes(x = 34, xend = 32, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.1 ,label = \"Larger vote\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 34.5, y=-.2 ,label = \"share for\",\n           hjust=0,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 38.82, y=-.2 ,label = \"Biden\",\n           colour = \"#494ca6\", \n           fontface =2,\n           hjust=0,vjust=0)+\n  geom_segment(aes(x = 70, xend = 72, y=-.12, yend = -.12),\n               arrow = arrow(length = unit(0.2, \"cm\")),\n               col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.1 ,label = \"Larger vote\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 66.09, y=-.2 ,label = \"share for\",\n           hjust=1,vjust=0, col = \"#95959c\")+\n  annotate(\"text\",x = 69.5, y=-.2 ,label = \"Trump\",\n           colour = \"#991a38\", \n           fontface =2,\n           hjust=1,vjust=0)+\n  annotate(\"text\",x = 30, y=0.5 ,label = \"0.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1 ,label = \"1\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=1.5 ,label = \"1.5\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  annotate(\"text\",x = 30, y=2 ,label = \"2 deaths per 100,000 residents\",col = \"#95959c\",\n           vjust = -0.5, hjust = 0)+\n  theme_void()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Removed 38 rows containing missing values (`geom_text_repel()`)."
  },
  {
    "objectID": "labs/05-lab-comments.html#footnotes",
    "href": "labs/05-lab-comments.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/05-lab.html",
    "href": "labs/05-lab.html",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "",
    "text": "Today we will explore the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nThe core thesis of Red Covid is something like the following:\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\nIn this lab, we’ll reproduce some basic evidence of this phenomena, using bivariate linear regression as a tool to summarize and describe relationships.\nNext week, we’ll see how multiple regression (linear regression with multiple predictors) can be used to assess alternative explanations for the patterns we see.\nTo accomplish this we will:\n\nSet up our work space (2-3 Minutes)\nLoad data on Covid-19 and the 2020 Election. (5 Minutes)\nDescribe the structure of these two datasets (5 Minutes)\nTransform the datasets so we can analyze them (10 minutes)\nMerge the election data into our Covid-19 data (5 minues)\nCalculate the average number new Covid-19 deaths in Red and Blue States (5 minutes)\nCalculate the average number new Covid-19 deaths in Red and B Blue States using linear regression (10 minutes)\nExplore the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (10 minutes)\nVisualize the relationships between Republican vote share, vaccination rates, and deaths from Covid-19 on September 23, 2021 (15-20 minutes)\nDiscuss some alternative explanations for these relationships (5-10 minutes)\nTake the weekly survey (2-3 minutes)\n\nOne of these 10 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester.\n\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, 1, 2, 3, 4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/05-lab.html#load-covid-19-data",
    "href": "labs/05-lab.html#load-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.1 Load Covid-19 data",
    "text": "2.1 Load Covid-19 data\nFirst we’ll need data on Covid-19 cases and deaths that we’ve worked with throughout the course.\nIn the chunk below, please write code to load data on Covid-19 in the states using the covid19() function from the COVID19 package. (slides)\n\n# Load covid-19 data"
  },
  {
    "objectID": "labs/05-lab.html#load-election-data",
    "href": "labs/05-lab.html#load-election-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "2.2 Load Election Data",
    "text": "2.2 Load Election Data\nNext we need data on the 2020 presidential election.\nIn the code chunk below, write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse.\nThe code you’ll need is here\n\n# Load election data\n\n\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\") sets a parameter in your R enivornment that tells the dataverse package to use Harvard’s dataverse\nget_dataframe_by_name() downloads the \"1976-2020-president.tab\" file from the U.S. President 1976–2020 dataverse using its digital object identifier (DOI): doi:10.7910/DVN/42MVDX\nIf this doesn’t work, you can use load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\")) instead"
  },
  {
    "objectID": "labs/05-lab.html#recode-the-covid-19-data",
    "href": "labs/05-lab.html#recode-the-covid-19-data",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.1 Recode the Covid-19 data",
    "text": "4.1 Recode the Covid-19 data\nIn the chunk below, please recode the covid data to create a covid_us data set, again using code from the slides as your guide: here\n\n# Create a vector containing of US territories\n\n\n# Filter out Territories and create state variable\n\n\n# Calculate new cases, new cases per capita, and 7-day average\n\n\n\n# Recode facemask policy (Not strictly necessary so feel free to skip)\n\n\n# Create year-month and percent vaccinated variables"
  },
  {
    "objectID": "labs/05-lab.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "href": "labs/05-lab.html#create-new-measures-of-the-7-day-and-14-day-averages-of-new-deaths-from-covid-19-per-100000-residents",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents",
    "text": "4.2 Create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nUsing the code from this slide as a guide:\n\nAnywhere you see new_cases write new_deaths\nAnywhere you see confirmed write deaths\nFor the 14-day average, copy the code for new_deaths_pc_7day change the new_deaths_pc_7day to new_deaths_pc_14day and set k=14 in the zoo::rollmean()\nRemember to save the output of mutate() back into covid_us\n\n\n# Create the following variables:\n# new_deaths\n# new_deaths_pc\n# new_deaths_pc_7day\n# new_deaths_pc_14day"
  },
  {
    "objectID": "labs/05-lab.html#reshape-and-recode-the-presidential-election-data.",
    "href": "labs/05-lab.html#reshape-and-recode-the-presidential-election-data.",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "4.3 Reshape and recode the presidential election data.",
    "text": "4.3 Reshape and recode the presidential election data.\nWe want to add election data to our Covid-19 data. To do this, we need to transform our election data, which is structured by candidate-state-election, into a data set that contains the election results by state for 2020.\nUsing the code from this slide transform pres_df to create a new data frame called pres2020_df by\n\nCreating a copy of the year variable called year_election\n\nThis is a stupid technical thing for merging later…\n\nTaking the state variable which was ALLCAPS and turning into Title Case using the str_to_title() function\nChanging the observations of state which are now \"District Of Columbia\" to \"District Of Columbia\"\nFiltering the data to include only candidates from the Democratic and Republican Parties\nFiltering the data to inlcude only the results from the 2020 election.\nSelecting the state, state_po, year_election, party_simplified, candidatevotes and totalvotes columns from pres_df\nPivoting the candidatevotes into two new columns with names from the party_simplified column\nCreating measures of the Democratic (dem_voteshare)and Republican (rep_voteshare) canditdates’ vote shares in each state by dividing the new DEMOCRAT and REPUBLICAN columns by the values from the totalvotes column\nCreating a variable called winner which takes a value of \"Trump\" if the rep_voteshare variable for a state is greater than the dem_voteshare for a state.\nMaking the winner variable a factor, with Trump as the first level and Biden as the second level\n\nThis is a trick for ggplot so that if we want to use winner to color points on a scatter plot, the points for Trump observations will show up as red and the points for Biden observations will show as blue.\n\nSaving the output of these transformations to an data frame called pres2020_df\n\nWhich, I know sounds like a lot, but…\nAll you need to do is copy and paste the code from this slide.\n\n# Transform Presidential Election data"
  },
  {
    "objectID": "labs/05-lab.html#for-all-the-observations",
    "href": "labs/05-lab.html#for-all-the-observations",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.1 For all the observations",
    "text": "6.1 For all the observations\nWith the covid_us data set:\n\nuse the group_by() command to have summarise() calculate values separately by the winner of each state.\nuse the summarise() command with mean() function to calculate the average number of new deaths (new_deaths) and the average of the 7-day rolling average of new deaths per 100,000 citizens (new_deaths_pc_7day)\n\nRemember to tell mean() what to do with NAs using the na.rm argument.\n\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7day"
  },
  {
    "objectID": "labs/05-lab.html#for-the-all-the-observations-before-april-19-2021",
    "href": "labs/05-lab.html#for-the-all-the-observations-before-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.2 For the all the observations before April 19, 2021",
    "text": "6.2 For the all the observations before April 19, 2021\nNow let’s compare one of the empirical implications of Leonhardt’s claims, specifically that “Red Covid” emerged as a phenomena because Republicans were less willing to take the vaccine.\nIf that’s true, then the differences between Red and Blue states in terms of new deaths and new deaths per 100,000 residents should be smaller or reversed (i.e. more deaths in Blue states compared to Red States)\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date less than \"2021-04-19\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7day before April 19, 2021"
  },
  {
    "objectID": "labs/05-lab.html#for-the-all-the-observations-after-april-19-2021",
    "href": "labs/05-lab.html#for-the-all-the-observations-after-april-19-2021",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "6.3 For the all the observations after April 19, 2021",
    "text": "6.3 For the all the observations after April 19, 2021\nSimilarly, if Leonhardt’s claim is true, then the differences between Red and Blue states should be more evident in the period after the vaccine became widely available.\n\nIn the code chunk below, take the your code from the previous chunk, and use the filter() command to subset the data to include only obsevations with a value of date greater than \"2021-04-19\n\n\n# Calculate the mean number of new_deaths and new_deaths_pc_7day after April 19, 2021\n\n\nPlease interpret the results of this analysis here\nWhen we look at the difference in the average number of new deaths between Red and Blue States in the full dataset, we see that …\nHowever, when we consider differences in the 7-day average of new deaths per 100,000 residents, we see that …\nWhen we limit our analysis, to just observations before April 19, 2021 …\nWhen we look at observations after the vaccine became widely available …"
  },
  {
    "objectID": "labs/05-lab.html#footnotes",
    "href": "labs/05-lab.html#footnotes",
    "title": "Lab 05 - Examining the Phenomena of Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is why so much of the start of this course has been focused on developing our coding skills↩︎"
  },
  {
    "objectID": "labs/06-lab-comments.html",
    "href": "labs/06-lab-comments.html",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "",
    "text": "Today we will explore the critiques and alternative explanations for the phenomena of “Red Covid” discussed by the NYT’s David Leonhardt in articles last fall here and more recently here.\nRecall the core thesis of Red Covid is something like the following:\n\nSince Covid-19 vaccines became widely available to the general public in the spring of 2021, Republicans have been less likely to get the vaccine. Lower rates of vaccination among Republicans have in turn led to higher rates of death from Covid-19 in Red States compared to Blue States.\n\nA skeptic of this claim might argue that relationship between electoral and epidemelogical outcomes is spurious, saying somthing like:\n\nThere are lots of ways that Red States differ from Blue States — demographics, economics, geography, culture, and so on – and it is these differences that explain the phenomena of Red Covid. If we were to control for these omitted variables the relationship between a state’s partisan leanings and Covid-19 would go away.\n\nIn this lab, we will see how we can explore these claims using multiple regression to control for competing explanations.\nTo accomplish this we will:\n\nGet set up to work (10 minutes)\n\nThen we will estimate and interpret a series of regression models:\n\nA baseline Red Covid model using simple bivariate regression using the Republican vote share of states to predict the 14-day average of per capita Covid-19 deaths on September 23, 2021 (10 Minutes)\nA multiple regression model controlling for Republican vote share the median age (15 minutes)\nA model controlling for Republican vote share, the median age and median income (15 minutes)\nA model controlling for Republican vote share, the median age median income and vaccination rates (15 minutes)\nA model using Republican vote share, the median age median income to predict vaccination rates (15 minutes)\n\nFinally, we’ll take the weekly survey which will serve as a mid semester check in.\nOne of these 6 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(3052024)\ngraded_question &lt;- sample(1:6,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 4 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/06-lab-comments.html#load-packages",
    "href": "labs/06-lab-comments.html#load-packages",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.1 Load Packages",
    "text": "1.1 Load Packages\nFirst lets load the libraries we’ll need for today.\nThere’s one new package, htmltools which we’ll use to display regression tables while we work.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE"
  },
  {
    "objectID": "labs/06-lab-comments.html#load-the-data",
    "href": "labs/06-lab-comments.html#load-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.2 Load the data",
    "text": "1.2 Load the data\nNext we’ll load the data that we created in class on Tuesday which provides a snapshot of the state of Covid-19 on September 23, 2021 in the U.S.\n\nload(url(\"https://pols1600.paultesta.org/files/data/06_lab.rda\"))\n\nAfter running this code, the data frame covid_lab should appear in your environment pane in R Studio"
  },
  {
    "objectID": "labs/06-lab-comments.html#describe-the-data",
    "href": "labs/06-lab-comments.html#describe-the-data",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "1.3 Describe the data",
    "text": "1.3 Describe the data\nIn the code chunk below, please write some code get an high level overview of the data:\n\n# High level overview\n# Number of observations and variables\ndim(covid_lab)\n\n[1] 51 14\n\n# Names of variables\nnames(covid_lab)\n\n [1] \"state\"                  \"state_po\"               \"date\"                  \n [4] \"new_deaths_pc_14day\"    \"percent_vaccinated\"     \"winner\"                \n [7] \"rep_voteshare\"          \"med_age\"                \"med_income\"            \n[10] \"population\"             \"rep_voteshare_std\"      \"med_age_std\"           \n[13] \"med_income_std\"         \"percent_vaccinated_std\"\n\n# Glimpse of data\nglimpse(covid_lab)\n\nRows: 51\nColumns: 14\n$ state                  &lt;chr&gt; \"Minnesota\", \"California\", \"Florida\", \"Wyoming\"…\n$ state_po               &lt;I&lt;chr&gt;&gt; MN, CA, FL, WY, SD, KS, NV, VA, WA, OR, WI, …\n$ date                   &lt;date&gt; 2021-09-23, 2021-09-23, 2021-09-23, 2021-09-23…\n$ new_deaths_pc_14day    &lt;dbl&gt; 0.2216457, 0.2953878, 1.6069796, 0.9379675, 0.2…\n$ percent_vaccinated     &lt;dbl&gt; 60.97676, 60.75909, 58.42173, 43.87526, 52.3523…\n$ winner                 &lt;fct&gt; Biden, Biden, Trump, Trump, Trump, Trump, Biden…\n$ rep_voteshare          &lt;dbl&gt; 45.28494, 34.32072, 51.21982, 69.49979, 61.7693…\n$ med_age                &lt;dbl&gt; 38.0, 36.5, 42.0, 37.7, 37.0, 36.7, 38.0, 38.2,…\n$ med_income             &lt;dbl&gt; 71306, 75235, 55660, 64049, 58275, 59597, 60365…\n$ population             &lt;int&gt; 5639632, 39512223, 21477737, 578759, 884659, 29…\n$ rep_voteshare_std      &lt;dbl&gt; -0.322498636, -1.235628147, 0.171773837, 1.6941…\n$ med_age_std            &lt;dbl&gt; -0.16122563, -0.78101262, 1.49153966, -0.285183…\n$ med_income_std         &lt;dbl&gt; 0.76603213, 1.13270974, -0.69414553, 0.08876578…\n$ percent_vaccinated_std &lt;dbl&gt; 0.5889289, 0.5623160, 0.2765518, -1.5018949, -0…\n\n# Summary of data\nsummary(covid_lab)\n\n    state             state_po              date            new_deaths_pc_14day\n Length:51          Length:51          Min.   :2021-09-23   Min.   :0.08097    \n Class :character   Class :AsIs        1st Qu.:2021-09-23   1st Qu.:0.25590    \n Mode  :character   Mode  :character   Median :2021-09-23   Median :0.37909    \n                                       Mean   :2021-09-23   Mean   :0.56561    \n                                       3rd Qu.:2021-09-23   3rd Qu.:0.85135    \n                                       Max.   :2021-09-23   Max.   :1.81515    \n percent_vaccinated   winner   rep_voteshare       med_age        med_income   \n Min.   :43.58      Trump:25   Min.   : 5.397   Min.   :30.80   Min.   :45081  \n 1st Qu.:49.40      Biden:26   1st Qu.:40.975   1st Qu.:36.95   1st Qu.:55560  \n Median :54.65                 Median :49.237   Median :38.20   Median :61439  \n Mean   :56.16                 Mean   :49.157   Mean   :38.39   Mean   :63098  \n 3rd Qu.:62.07                 3rd Qu.:57.866   3rd Qu.:39.50   3rd Qu.:71464  \n Max.   :72.39                 Max.   :69.500   Max.   :44.70   Max.   :86420  \n   population       rep_voteshare_std    med_age_std       med_income_std   \n Min.   :  578759   Min.   :-3.644447   Min.   :-3.13620   Min.   :-1.6814  \n 1st Qu.: 1789606   1st Qu.:-0.681443   1st Qu.:-0.59508   1st Qu.:-0.7034  \n Median : 4467673   Median : 0.006679   Median :-0.07859   Median :-0.1548  \n Mean   : 6445656   Mean   : 0.000000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 7446805   3rd Qu.: 0.725319   3rd Qu.: 0.45856   3rd Qu.: 0.7807  \n Max.   :39512223   Max.   : 1.694179   Max.   : 2.60716   Max.   : 2.1766  \n percent_vaccinated_std\n Min.   :-1.5381       \n 1st Qu.:-0.8265       \n Median :-0.1841       \n Mean   : 0.0000       \n 3rd Qu.: 0.7230       \n Max.   : 1.9845       \n\n# Variables I want to calculate sd for\nthe_vars &lt;- c(\"new_deaths_pc_14day\",\"rep_voteshare\",\"med_age\",\"med_income\",\"percent_vaccinated\")\n\n# Calculate standard deviations\ncovid_lab %&gt;%\n  select(all_of(the_vars))%&gt;%\n  summarise_all(sd)\n\n# A tibble: 1 × 5\n  new_deaths_pc_14day rep_voteshare med_age med_income percent_vaccinated\n                &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n1               0.409          12.0    2.42     10715.               8.18\n\n\nPlease use this HLO to answer the following questions:\n\nHow many observations are there: 50\nWhat is an observation (i.e. what is the unit of analysis): A U.S. State (on September 23, 2021)\nWhat is the primary outcome variable for today: The 14-day average of new Covid-19 deaths\nWhat are the four main predictors we’ll be using: We’ll be predicting Covid-19 deaths with measures of Republican Vote Share (rep_voteshare), Median Age (med_age), Median Income (med_income), and Percent Vaccintated (percent_vaccinated).\nWill we be using the the raw values of these predictors or their standardized values? We’ll be using the standardized version of these variables which all have the suffix _std\nWhat are the standard deviations of our outcome and predictor variables:\n\nCovid-19 deaths: 0.40 deaths\nRepublican vote share: 10.4 percentage points\nMedian age: 2.36 years\nMedian income: $ 10,288\nVaccination Rate: 7.98 percentage points"
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model",
    "href": "labs/06-lab-comments.html#fit-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.1 Fit the model",
    "text": "2.1 Fit the model\n\nm1 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-results",
    "href": "labs/06-lab-comments.html#summarize-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.2 Summarize the results",
    "text": "2.2 Summarize the results\nNow we apply the summary() function to our model m1\n\nsummary(m1)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63271 -0.22488 -0.03769  0.13746  1.00634 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04817  11.741 7.51e-16 ***\nrep_voteshare_std  0.22682    0.04865   4.662 2.44e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.344 on 49 degrees of freedom\nMultiple R-squared:  0.3073,    Adjusted R-squared:  0.2931 \nF-statistic: 21.73 on 1 and 49 DF,  p-value: 2.44e-05\n\n\nWe see that m1 returns two coefficients, which define a line of best fit predicting Covid-19 deaths with the Republican vote share of the 2020 Presidential election:\n\n\\(\\beta_0\\) corresponds to the intercept. This is model’s prediction for a state where Trump got 0 percent of the vote. This is typically not something we care about.\n\\(\\beta_1\\) corresponds to the slope. Because we used a standardized measure of vote share, we would say that a 1-standard deviation (about 10 percentage points) increase in Republican vote share is associated with a 0.23 increase the average number of new Covid-19 deaths. Given that this per-capita measure has a standard deviation of 0.4, this is a fairly sizable association.\nFinally, note that last column of summary(m1) Pr(&gt;|t|) both the coefficients for the intercept \\((\\beta_0)\\) and rep_voteshare_std (\\((\\beta_1)\\)) are statistically significant (ie have an * next to them)."
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-model-as-a-regression-table",
    "href": "labs/06-lab-comments.html#display-the-model-as-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.3 Display the model as a regression table",
    "text": "2.3 Display the model as a regression table\nNext we’ll format the results of summary(m1) into a regression table using the htmlreg() function.\nRegression tables are a the standard way of concisely presenting the results of regression models.\n\nEach named row corresponds to the coefficients form the model\nIf there is an asterisks next to a coefficient, that coefficient is statistically significant with a p value below a certain threshold.\nThe numbers in parentheses below each coefficient correspond to the standard error of the coefficient (more on that later)2\nThe bottom of the table contains summary statistics of of our model, which we’ll ignore for today.\n\nThe code after htmlreg(m1) allows you to see what output of the table will look like in the html document while you’re working in the Rmd file.\n\n\n\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n0.57***\n\n\n \n(0.05)\n\n\nrep_voteshare_std\n0.23***\n\n\n \n(0.05)\n\n\nR2\n0.31\n\n\nAdj. R2\n0.29\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#visualize-the-model",
    "href": "labs/06-lab-comments.html#visualize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.4 Visualize the model",
    "text": "2.4 Visualize the model\nNow let’s visualize the results of our m1 with a scatter plot.\nIn the code chunk below, I’ve written some comments to help you get started. You can also refer to last week’s lab for help\n\n# 1. Tell ggplot what data to use\ncovid_lab %&gt;%\n# 2. Set the aesthetic mappings of our figure\n  ggplot(aes(x = rep_voteshare_std,\n             y = new_deaths_pc_14day,\n             label = state_po))+\n# 3. Draw points with x values corresponding to Rep vote share and y values corresponding to Covid deaths. \n  geom_point(\n# 4. Make the points smaller in size\n    size = .5\n    )+\n# 5. Add labels using `label=state_po` aesthetic \n  geom_text_repel(\n# 6. Make the label size smaller    \n    size = 2)+\n# 7. Plot the regression model\n  geom_smooth(method = \"lm\")+\n# 8. Change the axis labels\n  labs(\n    x = \"Republican Vote Share (std)\",\n    y = \"New Covid-19 Deaths\\n(14-day ave)\"\n  )"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-results",
    "href": "labs/06-lab-comments.html#interpret-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "2.5 Interpret the results",
    "text": "2.5 Interpret the results\nIn a sentence our two, summarize the results of your analysis in this section\nOur model in this section provides results consistent with the phenomena of Red Covid. State’s with higher Republican vote shares tended to have higher per capita rates of death from Covid-19 on September 23, 2022."
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-1",
    "href": "labs/06-lab-comments.html#fit-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.1 Fit the model",
    "text": "3.1 Fit the model\nNow let’s test our skeptics’ claims by fitting a model m2 that controls for Age (med_age_std).\n\nRemember the first argument in lm() is formula of the form outcome variable ~ predictor1 + predictor2 + ...\n\n\nm2 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-model",
    "href": "labs/06-lab-comments.html#summarize-the-model",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.2 Summarize the model",
    "text": "3.2 Summarize the model\nNow let’s print out a statistical summary of m2\n\nsummary(m2)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, \n    data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59508 -0.24816 -0.05547  0.13825  0.99419 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04847  11.670 1.27e-15 ***\nrep_voteshare_std  0.23074    0.04933   4.677 2.40e-05 ***\nmed_age_std        0.03152    0.04933   0.639    0.526    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3461 on 48 degrees of freedom\nMultiple R-squared:  0.3131,    Adjusted R-squared:  0.2845 \nF-statistic: 10.94 on 2 and 48 DF,  p-value: 0.0001218"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-model-as-a-regression-table-1",
    "href": "labs/06-lab-comments.html#display-the-model-as-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.3 Display the model as a regression table",
    "text": "3.3 Display the model as a regression table\nNext, let’s create a regression table that displays m1 in the first column and m2 in the second column.\n\nTo do this, change list(m1) from the code above to list(m1, m2)\n\n\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n\n\n \n(0.05)\n(0.05)\n\n\nmed_age_std\n \n0.03\n\n\n \n \n(0.05)\n\n\nR2\n0.31\n0.31\n\n\nAdj. R2\n0.29\n0.28\n\n\nNum. obs.\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-results-1",
    "href": "labs/06-lab-comments.html#interpret-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "3.4 Interpret the results",
    "text": "3.4 Interpret the results\nIn a few sentences, explain whether the results from m2 support the skeptics criticisms or not?\n\nThey do not. Controlling for differences in the median age of state’s population does little to change the relationship between partisanship and Covid-19 that we saw in m1. If anything the the relationship is slightly stronger, while the coefficient on age is substantively small and statistically non-significant.\n\nPart of what’s at play here, is that relationship between age and Covid-19 outcomes at the state level is pretty weak, perhaps reflecting the early focus on vaccinating the eldery.\nSimilarly, the idea that Red States tend to be older doesn’t appear to be empirically true, perhaps reflecting differences between the general population and voting population.\n\n# Fit models\nm2_death_age &lt;- lm(new_deaths_pc_14day ~ med_age_std, covid_lab)\nm2_rep_age &lt;- lm(rep_voteshare_std ~ med_age_std, covid_lab)\n\n# Summarize models\nsummary(m2_death_age)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ med_age_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4796 -0.3095 -0.1863  0.2853  1.2488 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.565611   0.057879   9.772  4.3e-13 ***\nmed_age_std 0.002763   0.058454   0.047    0.962    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4133 on 49 degrees of freedom\nMultiple R-squared:  4.56e-05,  Adjusted R-squared:  -0.02036 \nF-statistic: 0.002234 on 1 and 49 DF,  p-value: 0.9625\n\nsummary(m2_rep_age)\n\n\nCall:\nlm(formula = rep_voteshare_std ~ med_age_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8705 -0.6764  0.0464  0.6577  1.8335 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.883e-16  1.403e-01   0.000    1.000\nmed_age_std -1.246e-01  1.417e-01  -0.879    0.384\n\nResidual standard error: 1.002 on 49 degrees of freedom\nMultiple R-squared:  0.01553,   Adjusted R-squared:  -0.004562 \nF-statistic: 0.7729 on 1 and 49 DF,  p-value: 0.3836\n\n# Display as formatted regression table\nhtmlreg(list(m2_rep_age,\n             m2_death_age),\n        custom.model.names = c(\"Deaths\", \"Rep Vote\"),\n        custom.coef.names = c(\"(Intercept)\",\"Median Age (std)\"),\n        custom.header = list(\"DV\"=1:2)  )%&gt;% HTML() %&gt;% browsable()\n\n\n\nStatistical models\n\n\n \nDV\n\n\n \nDeaths\nRep Vote\n\n\n\n\n(Intercept)\n-0.00\n0.57***\n\n\n \n(0.14)\n(0.06)\n\n\nMedian Age (std)\n-0.12\n0.00\n\n\n \n(0.14)\n(0.06)\n\n\nR2\n0.02\n0.00\n\n\nAdj. R2\n-0.00\n-0.02\n\n\nNum. obs.\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\n\n\n# Visualize the models\ncovid_lab %&gt;%\n  ggplot(aes(x = med_age_std,\n             y = rep_voteshare_std,\n             label = state_po))+\n  geom_point(size = .5)+\n  geom_text_repel(size = 2)+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\ncovid_lab %&gt;%\n  ggplot(aes(x = med_age_std,\n             y = new_deaths_pc_14day,\n             label = state_po))+\n  geom_point(size = .5)+\n  geom_text_repel(size = 2)+\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-2",
    "href": "labs/06-lab-comments.html#fit-the-model-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.1 Fit the Model",
    "text": "4.1 Fit the Model\nPlease fit a model called m3 implied by the skeptic’s revised claims\n\nm3 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-model-1",
    "href": "labs/06-lab-comments.html#summarize-the-model-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.2 Summarize the model",
    "text": "4.2 Summarize the model\nSummarize the model m3 using summary()\n\nsummary(m3)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50751 -0.19703 -0.06278  0.20024  0.92320 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.56561    0.04425  12.782  &lt; 2e-16 ***\nrep_voteshare_std  0.07140    0.06654   1.073  0.28869    \nmed_age_std       -0.01692    0.04744  -0.357  0.72296    \nmed_income_std    -0.21669    0.06660  -3.254  0.00211 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.316 on 47 degrees of freedom\nMultiple R-squared:  0.4394,    Adjusted R-squared:  0.4036 \nF-statistic: 12.28 on 3 and 47 DF,  p-value: 4.689e-06"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-models-in-a-regression-table",
    "href": "labs/06-lab-comments.html#display-the-models-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.3 Display the models in a regression table",
    "text": "4.3 Display the models in a regression table\nAnd then display the results of models m1, m2, and m3.\n\nhtmlreg(list(m1,m2,m3)) %&gt;% HTML() %&gt;% browsable()\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n0.07\n\n\n \n(0.05)\n(0.05)\n(0.07)\n\n\nmed_age_std\n \n0.03\n-0.02\n\n\n \n \n(0.05)\n(0.05)\n\n\nmed_income_std\n \n \n-0.22**\n\n\n \n \n \n(0.07)\n\n\nR2\n0.31\n0.31\n0.44\n\n\nAdj. R2\n0.29\n0.28\n0.40\n\n\nNum. obs.\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-skeptics-claims",
    "href": "labs/06-lab-comments.html#interpret-the-skeptics-claims",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "4.4 Interpret the skeptic’s claims",
    "text": "4.4 Interpret the skeptic’s claims\nIn a few sentences, explain whether the results from m3 support the skeptics criticisms or not?\nControlling for median age and income, the coefficient on Republican sote share decreases in size by more than half and is no longer statistically significant. The coefficient on median income is statistically significant and substantively suggests that states with higher median incomes tended to have fewer Covid-19 deaths on September 23, 2021."
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-3",
    "href": "labs/06-lab-comments.html#fit-the-model-3",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.1 Fit the model",
    "text": "5.1 Fit the model\nYou know the drill.\n\nm4 &lt;- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std + percent_vaccinated_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-results-1",
    "href": "labs/06-lab-comments.html#summarize-the-results-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.2 Summarize the results",
    "text": "5.2 Summarize the results\nAgain, let’s get a quick summary of our results\n\nsummary(m4)\n\n\nCall:\nlm(formula = new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + \n    med_income_std + percent_vaccinated_std, data = covid_lab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52997 -0.16350 -0.02941  0.12926  0.94733 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.56561    0.04091  13.825  &lt; 2e-16 ***\nrep_voteshare_std      -0.08933    0.08161  -1.095  0.27940    \nmed_age_std             0.07110    0.05278   1.347  0.18458    \nmed_income_std         -0.11888    0.06969  -1.706  0.09478 .  \npercent_vaccinated_std -0.28633    0.09554  -2.997  0.00438 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2922 on 46 degrees of freedom\nMultiple R-squared:  0.531, Adjusted R-squared:  0.4902 \nF-statistic: 13.02 on 4 and 46 DF,  p-value: 3.621e-07"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-models-in-a-regression-table-1",
    "href": "labs/06-lab-comments.html#display-the-models-in-a-regression-table-1",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.3 Display the models in a regression table",
    "text": "5.3 Display the models in a regression table\nAnd add m4 to list of models in our regression table\n\n\n\n\nStatistical models\n\n\n \nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n0.57***\n0.57***\n0.57***\n0.57***\n\n\n \n(0.05)\n(0.05)\n(0.04)\n(0.04)\n\n\nrep_voteshare_std\n0.23***\n0.23***\n0.07\n-0.09\n\n\n \n(0.05)\n(0.05)\n(0.07)\n(0.08)\n\n\nmed_age_std\n \n0.03\n-0.02\n0.07\n\n\n \n \n(0.05)\n(0.05)\n(0.05)\n\n\nmed_income_std\n \n \n-0.22**\n-0.12\n\n\n \n \n \n(0.07)\n(0.07)\n\n\npercent_vaccinated_std\n \n \n \n-0.29**\n\n\n \n \n \n \n(0.10)\n\n\nR2\n0.31\n0.31\n0.44\n0.53\n\n\nAdj. R2\n0.29\n0.28\n0.40\n0.49\n\n\nNum. obs.\n51\n51\n51\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpet-the-results",
    "href": "labs/06-lab-comments.html#interpet-the-results",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "5.4 Interpet the results",
    "text": "5.4 Interpet the results\nBriefly interpret the results of m4\nControlling for vaccination rates, none of the other variables in m4 are statistically significant predictors of Covid-19 deaths."
  },
  {
    "objectID": "labs/06-lab-comments.html#fit-the-model-4",
    "href": "labs/06-lab-comments.html#fit-the-model-4",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.1 Fit the model",
    "text": "6.1 Fit the model\nNow let’s fit the model. For ease of interpretation, let’s use the unstandardized measure of vaccination rates, percent_vaccinated as our outcome variable.\n\nm5 &lt;- lm(percent_vaccinated ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)"
  },
  {
    "objectID": "labs/06-lab-comments.html#summarize-the-results-2",
    "href": "labs/06-lab-comments.html#summarize-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.2 Summarize the results",
    "text": "6.2 Summarize the results\nAnd summarize the results\n\nsummary(m5)\n\n\nCall:\nlm(formula = percent_vaccinated ~ rep_voteshare_std + med_age_std + \n    med_income_std, data = covid_lab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7719 -2.7849  0.2386  1.7743  7.6442 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        56.1597     0.5109 109.919  &lt; 2e-16 ***\nrep_voteshare_std  -4.5916     0.7683  -5.977 2.92e-07 ***\nmed_age_std         2.5143     0.5477   4.590 3.31e-05 ***\nmed_income_std      2.7939     0.7690   3.633 0.000691 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.649 on 47 degrees of freedom\nMultiple R-squared:  0.8129,    Adjusted R-squared:  0.801 \nF-statistic: 68.09 on 3 and 47 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/06-lab-comments.html#display-the-results-in-a-regression-table",
    "href": "labs/06-lab-comments.html#display-the-results-in-a-regression-table",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.3 Display the results in a regression table",
    "text": "6.3 Display the results in a regression table\nDisplay them in a regression table\n\n\n\n\nStatistical models\n\n\n \nModel 1\n\n\n\n\n(Intercept)\n56.16***\n\n\n \n(0.51)\n\n\nrep_voteshare_std\n-4.59***\n\n\n \n(0.77)\n\n\nmed_age_std\n2.51***\n\n\n \n(0.55)\n\n\nmed_income_std\n2.79***\n\n\n \n(0.77)\n\n\nR2\n0.81\n\n\nAdj. R2\n0.80\n\n\nNum. obs.\n51\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "labs/06-lab-comments.html#interpret-the-results-2",
    "href": "labs/06-lab-comments.html#interpret-the-results-2",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "6.4 Interpret the results",
    "text": "6.4 Interpret the results\nSummarize the results of m5 and offer some broader discussion of what we’ve learned today\nIn m5 all three predictors have strong statistically significant relationships with vaccination rates in the expected direction. States where more people voted for Trump in 2020 tend to have lower rates of vaccination. States with an older population, and a richer population tend to have higher rates of vaccination.\nThinking back to the previous models we’ve estimated, we might argue that the effects these predictors have on Covid-19 death rates is mediated through their relationship with vaccination rates.\nMore broadly, what does this analysis mean for arguments about Red Covid. I guess, I’d say it’s complicated. Vaccines are clearly effective at reducing Covid-19 deaths. In both aggregate and invidual level data, Republicans appear to be less willing to get vaccinated. But lots of other factors influence vaccination rates and public health more broadly.\nRegression is a tool for trying to explore these competing explanations, but without strong theory and clever design, it’s unlikely to resolve debates. There’s almost always a skeptic waiting to say “Yes, but have you controlled for …” We can try to address there concerns by controlling for more and more variables. But a better strategy is often to say, I don’t need to control for X because the logic of my design already accounts for X.\nStill it’s hard to think what that kind of design would be for something like for debates about Red Covid. Maybe we need different (individual data) or maybe we’d want to reframe the question or draw out further testable implications of the claim. If your group is looking for a final project, there are number of directions you could take this kind of analysis:\n\nLooking at different periods overtime\nLooking at smaller units of aggregation (counties)\nControlling for alternative factors\nLeveraging variation in the availability of vaccines over time and place."
  },
  {
    "objectID": "labs/06-lab-comments.html#footnotes",
    "href": "labs/06-lab-comments.html#footnotes",
    "title": "Lab 06 - Examining Alternative Explanations for Red Covid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn short, these * correspond to \\(p-values\\) below different thresholds. One * typically means \\(p &lt; 0.05\\). A p-value is a conditional probability that arises from a hypothesis test summarizing the likelihood of observing a particular test statistic (here a regression coefficient, or more specifically, a t-statistic which is the regression coefficient divided by its standard error) given a paritcular hypothesis (typically, but not allows a null hypothesis that the true coefficient is 0). In sum, a p-value assess the likelihood of seeing what we did, if in fact, there was no relationship. If that likelihood is small (p&lt;0.05), we reject the claim of no relationship. We remain uncertain about the true value of the coefficient, but we are pretty confident it’s not 0.↩︎\nA standard error is another one of those things that in the cart we’re putting before horse today. Briefly, it is an estimate of the standard deviation of the sampling distribution of a coefficient and describes how much our coefficient might vary had we had a different sample…↩︎"
  },
  {
    "objectID": "labs/08-lab-comments.html",
    "href": "labs/08-lab-comments.html",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we continue our replication of Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages and set the working directory to where this file is saved. (5 minutes)\nSummarize the study in terms of it’s research question, theory, design, and results. (10 minutes)\nDownload the replication files and save them in the same folder as this lab (5 minutes)\nLoad the data from your computers into R (5 minutes)\nGet a quick HLO of the data (10 minutes)\nMerge data on election policy into data on voting (5 minutes, together),\nRecode the covariates, key predictors, and outcome for the study (10 minutes, partly together)\nRecreate Figure 1 (15 minutes)\nRecreate Figure 2 (15 minutes)\n\nFinally, we’ll take the weekly survey which should be a fun one\nOne of these 8 tasks will be randomly selected as the graded question for the lab.\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/08-lab-comments.html#please-render-this-.qmd-file",
    "href": "labs/08-lab-comments.html#please-render-this-.qmd-file",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/08-lab-comments.html#load-packages",
    "href": "labs/08-lab-comments.html#load-packages",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\",\"margins\",\n  \"modelsummary\", \"ggeffects\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo       margins \n         TRUE          TRUE          TRUE          TRUE          TRUE \n modelsummary     ggeffects \n         TRUE          TRUE"
  },
  {
    "objectID": "labs/08-lab-comments.html#state",
    "href": "labs/08-lab-comments.html#state",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.1 State",
    "text": "3.1 State\n\n# Create dataframe of average voting rates by state\ncps %&gt;% \n  group_by(st) %&gt;% \n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) %&gt;% \n  mutate(\n    st = fct_reorder(st, turnout)\n    ) -&gt; df_state\n\ndf_state %&gt;% \n  ggplot(aes(turnout,st))+\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "labs/08-lab-comments.html#variation-over-time",
    "href": "labs/08-lab-comments.html#variation-over-time",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.2 Variation over time",
    "text": "3.2 Variation over time\n\ncps %&gt;% \n  group_by(year, election_type ) %&gt;%\n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) -&gt; year_df\n\nyear_df %&gt;% \n  ggplot(aes(year, turnout)) +\n  geom_line() +\n  facet_grid(~election_type)"
  },
  {
    "objectID": "labs/08-lab-comments.html#variation-over-state-and-time",
    "href": "labs/08-lab-comments.html#variation-over-state-and-time",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "3.3 Variation over state and time",
    "text": "3.3 Variation over state and time\n\ncps %&gt;% \n  group_by(year, st, election_type ) %&gt;%\n  summarise(\n    turnout = mean(dv_voted, na.rm=T)\n  ) -&gt; year_state_df\n\nyear_state_df %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(year, turnout, group=st)) +\n  geom_line()+\n  stat_smooth(geom = \"line\", col = \"red\",aes(group = NULL))+\n  facet_grid(~election_type)"
  },
  {
    "objectID": "labs/08-lab-comments.html#estimate-a-simple-model",
    "href": "labs/08-lab-comments.html#estimate-a-simple-model",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "4.1 Estimate a simple model",
    "text": "4.1 Estimate a simple model\n\nm1 &lt;- lm_robust(dv_voted ~ sdr, \n                data = cps,\n                se_type = \"classical\")\nm2 &lt;- lm_robust(dv_voted ~ sdr, \n                data = cps,\n                se_type = \"stata\",\n                fixed_effects = ~ st + year,\n                try_cholesky = T)\nm3 &lt;- lm_robust(dv_voted ~ sdr,\n                data = cps,\n                fixed_effects = ~ st + year,\n                se_type = \"stata\",\n                clusters = st,\n                try_cholesky = T)\n\n\nmodelsummary(list(m1,m2,m3))\n\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n0.548\n\n\n\n\n\n(0.000)\n\n\n\n\nsdr\n0.062\n0.007\n0.007\n\n\n\n(0.001)\n(0.002)\n(0.014)\n\n\nNum.Obs.\n1988501\n1988501\n1988501\n\n\nR2\n0.002\n0.028\n0.028\n\n\nR2 Adj.\n0.002\n0.028\n0.028\n\n\nAIC\n2859151.1\n2804888.5\n2804888.5\n\n\nBIC\n2859188.6\n2804913.5\n2804913.5\n\n\nRMSE\n0.50\n0.49\n0.49\n\n\nStd.Errors\n\n\nby: st"
  },
  {
    "objectID": "labs/08-lab-comments.html#marginal-effects-of-interactions",
    "href": "labs/08-lab-comments.html#marginal-effects-of-interactions",
    "title": "Lab 08 - Replicating Grumbach and Hill (2022):",
    "section": "5.1 Marginal Effects of Interactions",
    "text": "5.1 Marginal Effects of Interactions\n\nme_fn &lt;- function(mod, cohort, ci=0.95){\n  # Confidence Level for CI\n  alpha &lt;- 1-ci\n  z &lt;- qnorm(1-alpha/2)\n  \n  # Age (Always one for indicator of specific cohort)\n  age &lt;- 1\n  \n  # Variance Covariance Matrix from Model\n  cov &lt;- vcov(mod)\n  \n  # coefficient for SDR (Marginal Effect for reference category: 65+)\n  b1 &lt;- coef(mod)[\"sdr\"]\n  \n  # If age is one of the interactions\n  if(cohort %in% c(\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\")){\n    # get the name of the specific interaction\n    the_int &lt;- paste(\"sdr:age_group\",cohort,sep=\"\")\n    # the coefficient on the interaction\n    b2 &lt;- coef(mod)[the_int]\n    # Calculate marginal effect for age cohort\n    me &lt;- b1 + b2*age\n    me_se &lt;- sqrt(cov[\"sdr\",\"sdr\"] + age^2*cov[the_int,the_int] + 2*age*cov[\"sdr\",the_int])\n    ll &lt;- me - z*me_se\n    ul &lt;- me + z*me_se\n  }\n  if(!cohort %in% c(\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\")){\n    me &lt;- b1 \n    me_se &lt;- mod$std.error[\"sdr\"]\n    ll &lt;- mod$conf.low[\"sdr\"]\n    ul &lt;- mod$conf.high[\"sdr\"]\n  }\n\n  res &lt;- tibble(\n    Age = cohort,\n    Effect = me,\n    SE = me_se,\n    ll = ll,\n    ul = ul\n  )\n  return(res)\n\n\n}\n\n\nthe_age_groups &lt;- levels(cps$age_group)\n\nthe_age_groups %&gt;% \n  purrr::map_df(~me_fn(m1gh, cohort=.)) %&gt;% \n  mutate(\n    Age = factor(Age),\n    Model = \"No controls\"\n  ) -&gt; fig3_no_controls\n\nthe_age_groups %&gt;% \n  purrr::map_df(~me_fn(m2gh, cohort=.)) %&gt;% \n  mutate(\n    Age = factor(Age),\n    Model = \"With Controls\"\n  ) -&gt; fig3_controls"
  },
  {
    "objectID": "labs/03-lab-comments.html",
    "href": "labs/03-lab-comments.html",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "",
    "text": "Today we will explore the logic and design of Broockman and Kalla’s 2016 study, “Durably reducing transphobia: A field experiment on door-to-door canvassing”, from the recruitment of subjects for the study to the delivery of their interventions. Then we will explore whether the intervention had any effect on respondents’ feelings toward transgender individuals.\nTo accomplish this we will:\n\nSummarize the study (5 Minutes)\nSet up our work space (2-3 Minutes)\nLoad a portion of the replication data (1-2 Minutes)\nGet a high level overview of the data (5 minutes)\nDescribe the distribution of covariates in the full dataset (5 minutes)\nExamine the difference in covariates between those who did and did not complete the survey (10 minutes)\nExamine the difference in covariates between those assigned to each treatment condition in the study. (10 minutes)\nEstimate the average treatment effect of the intervention (10 minutes)\nPlot the results and comment on the study (10 minutes)\nTake the weekly survey (3-5 minutes)\n\nOne of these 9 tasks (excluding the weekly survey) will be randomly selected as the graded question for the lab.\n\nset.seed(22022024)\ngraded_question &lt;- sample(1:9,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 9 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/03-lab-comments.html#footnotes",
    "href": "labs/03-lab-comments.html#footnotes",
    "title": "Lab 03 - Replicating Broockman and Kalla (2016)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find the full set of replication files here↩︎\nThe actual study contains a number of measures about transgender attitudes and policies which are scaled together to produce a single measure of subjects latent tolerance. For simplicity, we’ll focus on this single survey item.↩︎\nRecall that only some people who completed the baseline and were assigned to receive the treatment actually answered the door when canvassers came knocking.↩︎"
  },
  {
    "objectID": "labs/07-lab-comments.html",
    "href": "labs/07-lab-comments.html",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "",
    "text": "In this lab, we will begin the process of replicating Grumbach and Hill (2021) “Rock the Registration: Same Day Registration Increases Turnout of Young Voters.”\nTo accomplish this we will:\n\nLoad packages and set the working directory to where this file is saved. (5 minutes)\nSummarize the study in terms of it’s research question, theory, design, and results. (10 minutes)\nDownload the replication files and save them in the same folder as this lab (5 minutes)\nLoad the data from your computers into R (5 minutes)\nGet a quick HLO of the data (10 minutes)\nMerge data on election policy into data on voting (5 minutes, together),\nRecode the covariates, key predictors, and outcome for the study (10 minutes, partly together)\nRecreate Figure 1 (15 minutes)\nRecreate Figure 2 (15 minutes)\n\nFinally, we’ll take the weekly survey which should be a fun one\nOne of these 8 tasks will be randomly selected as the graded question for the lab.\n\nset.seed(3142024)\ngraded_question &lt;- sample(1:8,size = 1)\npaste(\"Question\",graded_question,\"is the graded question for this week\")\n\n[1] \"Question 6 is the graded question for this week\"\n\n\nYou will work in your assigned groups. Only one member of each group needs to submit the html file of lab.\nThis lab must contain the names of the group members in attendance.\nIf you are attending remotely, you will submit your labs individually.\nHere are your assigned groups for the semester."
  },
  {
    "objectID": "labs/07-lab-comments.html#please-render-this-.qmd-file",
    "href": "labs/07-lab-comments.html#please-render-this-.qmd-file",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Please render this .qmd file",
    "text": "Please render this .qmd file\nAs with every lab, you should:\n\nDownload the file\nSave it in your course folder\nUpdate the author: section of the YAML header to include the names of your group members in attendance.\nRender the document\nOpen the html file in your browser (Easier to read)\nWrite your code in the chunks provided under each section\nComment out or delete any test code you do not need\nRender the document again after completing a section or chunk (Error checking)\nUpload the final lab to Canvas."
  },
  {
    "objectID": "labs/07-lab-comments.html#load-packages",
    "href": "labs/07-lab-comments.html#load-packages",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Load packages",
    "text": "Load packages\nAs always, let’s load the packages we’ll need for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\"htmltools\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\",\n  \"janitor\",\n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n# Define function to load packages\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     htmltools     tidyverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    lubridate       forcats         haven      labelled         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      janitor DeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nWe will also want to set our working directory to where your lab is saved."
  },
  {
    "objectID": "labs/07-lab-comments.html#important-set-your-working-directory",
    "href": "labs/07-lab-comments.html#important-set-your-working-directory",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "IMPORTANT: Set your working directory",
    "text": "IMPORTANT: Set your working directory\n\nOn the top panel of R Studio click:\n\n\nSession Session &gt; Set working directory &gt; Source file location\n\n\nPaste the output that shows up in your console into the code chunk below\n\n\n# Set working directory\n# Session &gt; Set working directory &gt; Source file location\n# paste output here:\n\nAll right, now let’s summarize the study"
  },
  {
    "objectID": "labs/07-lab-comments.html#recode-covariates",
    "href": "labs/07-lab-comments.html#recode-covariates",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.1 Recode covariates",
    "text": "6.1 Recode covariates\nThe CPS are messy data. Please run the code below to recode the covariates in the spirit of (i.e. with minor changes) what Grumbach and Hill did. 3\n\n\n\n\n\n\nNote\n\n\n\nThe file cps_00021.cbk.txt contains the codebook for the data, telling us what numeric values of each variable correspond to substantively. So if you’re wondering how I know what should be recoded to what specific values, it comes from reading the codebook, looking at Grumbach and Hill’s code, looking at the raw variable with a table, and the using case_when() to judiciously code the data. You’ll get practice doing this in your final projects, but I don’t want to spend too much time on this this lab, which is why you’re only recoding the outcome voted\n\n\n\n# Recode covariates\ncps %&gt;% \n  mutate(\n    # Useful for plotting figure 2\n    SDR = ifelse(sdr == 1, \"SDR\", \"non-SDR\"),\n    education = case_when(\n      educ == 1 ~ NA, #Blank\n      educ &lt; 40 ~ 1, # No high school\n      educ &gt;= 40 & educ &lt; 73 ~ 2, # Some high school\n      educ == 73 ~ 3, # High school degree\n      educ &gt;= 80 & educ &lt;= 110 ~ 4, # Some college\n      educ &gt;= 111 & educ &lt;123 ~ 5, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n      educ &gt;= 123 & educ &lt;=125 ~ 6, # BA degree (And weirdly people who completed 5, 5+ and 6+ years of college)\n      educ == 999 ~ NA # Missing/unknown\n    ),\n    race_f = case_when(\n      race == 999 ~ NA,\n      T ~ factor(race)\n    ),\n    is_white = case_when(\n      race == 100 ~ 1,\n      race == 999 ~ NA,\n      T ~ 0\n    ),\n    is_black = case_when(\n      race == 200 ~ 1,\n      race == 999 ~ NA,\n      T ~ 0\n    ),\n    is_aapi = case_when(\n      race == 650 ~ 1,\n      race == 651 ~ 1,\n      race == 652 ~ 1,\n      race == 999 ~ NA,\n      T ~ 0\n    ),\n    is_other = case_when(\n      is_white == 1 ~ 0,\n      is_black == 1 ~ 0,\n      is_aapi ==  1 ~ 0,\n      race == 999 ~ NA,\n      T ~ 1\n    ),\n    income = case_when(\n      faminc &gt; 843 ~ NA, # Remove Missing/Refused\n      T ~ as.numeric(factor(faminc))\n    ),\n    is_female = case_when(\n      sex == 2 ~ 1,\n      sex == 1 ~ 0,\n      T ~ NA # recode Not in Universe as NA\n    )\n    \n  ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab-comments.html#create-age_group-and-age_group_xx_xx-indicators",
    "href": "labs/07-lab-comments.html#create-age_group-and-age_group_xx_xx-indicators",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.2 Create age_group and age_group_XX_XX indicators",
    "text": "6.2 Create age_group and age_group_XX_XX indicators\nNext we’ll create an age_group variable and binary indicators for each age cohort of the form age_group_XX_XX.\nPlease uncomment and run the code below\n\n# Create age variables \ncps %&gt;% \n  mutate(\n    age_group = case_when(\n      age &gt;= 18 & age &lt;= 24 ~ \"18-24\",\n      age &gt; 24 & age &lt;= 34  ~ \"25-34\",\n      age &gt; 34 & age &lt;= 44  ~ \"35-44\",\n      age &gt; 44 & age &lt;= 54  ~ \"45-54\",\n      age &gt; 54 & age &lt;= 64  ~ \"55-64\",\n      age &gt; 64 ~ \"65+\",\n      T ~ NA\n\n    ),\n    age_group_18_24 = ifelse(age_group == \"18-24\", 1, 0),\n    age_group_25_34 = ifelse(age_group == \"25-34\", 1, 0),\n    age_group_35_44 = ifelse(age_group == \"35-24\", 1, 0),\n    age_group_45_54 = ifelse(age_group == \"45-24\", 1, 0),\n    age_group_55_64 = ifelse(age_group == \"55-24\", 1, 0),\n    age_group_65plus = ifelse(age_group == \"65+\", 1, 0)\n  ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab-comments.html#check-age-recodes",
    "href": "labs/07-lab-comments.html#check-age-recodes",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.3 Check age recodes",
    "text": "6.3 Check age recodes\nIt’s good practice when recoding, to check the output. Please use the table() to create a crosstab of age_group and age_group_18_24.\n\n#|label: checkage\n\n# Compare age_group to age_group_18_24 using table()\ntable(cps$age_group, cps$age_group_18_24)\n\n       \n             0      1\n  18-24      0 274383\n  25-34 416613      0\n  35-44 399527      0\n  45-54 353672      0\n  55-64 296342      0\n  65+   372517      0\n\n\nExplain in words how the variable age_group_18_24 relates to the variable age_group age_group is a categorical variable which describes the age cohort that respondent to the CPS belongs to. age_group_18_24 is a binary indicator variable that takes a value 1 for respondents who belong to the cohort of people 18-24 years old and 0 otherwise."
  },
  {
    "objectID": "labs/07-lab-comments.html#recode-the-outcome",
    "href": "labs/07-lab-comments.html#recode-the-outcome",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.4 Recode the outcome",
    "text": "6.4 Recode the outcome\nNow it’s your turn. Please do the following:\n\nLook at the variable voted using the table() function\n\n1 corresponds to Did not vote\n`2 corresponds to Voted\n96,97,98 to people who didn’t provide and answer, or didn’t remember\n99 corresponds to people who shouldn’t be in the sample (“Not in universe”)\n\nCreate a new variable called dv_voted using case_when() inside of mutate() that is:\n\n1 when voted == 2\n0 when voted == 1,\n0 when voted &gt; 2 & voted &lt;99\nNA when voted == 99\n\n\n\n# Look at distribution of voted using table()\ntable(cps$voted)\n\n\n      1       2      96      97      98      99 \n 704510 1103606   14257   38897  127231  881162 \n\n# Create variable dv_voted using mutate(), case_when(), and voted variable\ncps %&gt;% \n  mutate(\n    dv_voted = case_when(\n      voted == 2 ~ 1,\n      voted == 1 ~ 0,\n      voted &gt; 2 & voted &lt; 99 ~ 0,\n      voted == 99 ~ NA\n    )\n  ) -&gt; cps"
  },
  {
    "objectID": "labs/07-lab-comments.html#save-the-recoded-data",
    "href": "labs/07-lab-comments.html#save-the-recoded-data",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "6.5 Save the recoded data",
    "text": "6.5 Save the recoded data\nFinally, let’s save our recoded data to file called cps_clean.rda that we can use for next week’s lab\nUncomment and run the following:\n\n# save(cps, file = \"cps_clean.rda\")"
  },
  {
    "objectID": "labs/07-lab-comments.html#write-down-aesthetic-mappings-from-the-figure",
    "href": "labs/07-lab-comments.html#write-down-aesthetic-mappings-from-the-figure",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.1 Write down aesthetic mappings from the figure:",
    "text": "7.1 Write down aesthetic mappings from the figure:\nBefore we create this figure, think about the information conveyed by the figure’s aesthetics (the x axis, the y axis, the color of the squares), and the corresponding columns from policy_data that contain this information.\n\nx-axis: year\ny-axis: st\ncol: sdr"
  },
  {
    "objectID": "labs/07-lab-comments.html#create-a-variable-called-sdr",
    "href": "labs/07-lab-comments.html#create-a-variable-called-sdr",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.2 Create a variable called SDR",
    "text": "7.2 Create a variable called SDR\nIt will be helpful to have a variable called SDR in policy_data that takes the value of “SDR” when sdr == 1 and “non-SDR” when sdr == 0\nPlease use case_when() or ifelse() to create SDR in policy_data\n\n# Create a variable called SDR in policy_data\npolicy_data %&gt;% \n  mutate(\n    SDR = ifelse(sdr == 1, \"SDR\", \"non-SDR\")\n  ) -&gt; policy_data"
  },
  {
    "objectID": "labs/07-lab-comments.html#recreate-figure-1-1",
    "href": "labs/07-lab-comments.html#recreate-figure-1-1",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.3 Recreate Figure 1",
    "text": "7.3 Recreate Figure 1\nRecall, we need three things to make a figure:\n\ndata\naesthetics\ngeometries\n\nUsing data from policy_data starting in 1978 (hint add a filter()) and the aesthetic mappings identified above use ggplot() with the geom_point() geometry to make a version of Figure 1 from paper.\n\n# Recreate Figure 1\npolicy_data %&gt;% \n  filter(year &gt;= 1978) %&gt;% \n  ggplot(aes(year, st,\n             col = SDR)) +\n  geom_point() -&gt; fig1\n\nfig1"
  },
  {
    "objectID": "labs/07-lab-comments.html#interpret-figure-1.",
    "href": "labs/07-lab-comments.html#interpret-figure-1.",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "7.4 Interpret Figure 1.",
    "text": "7.4 Interpret Figure 1.\nPlease answer the following questions:\n\nHow many states had Same Day Registration at some point in time? 19 states\nHow many states had Same Day Registration in 2018? 18 states had SDR in 2018\nDid any states get rid of Same Day Registration? When did they get rid of this policy? Yes, Ohio in 2014\nWhat’s up with North Dakota? North Dakota doesn’t require registration to vote. As per footnote 5, they’re excluded from the main analysis.\n\nUse this code chunk to write any code that might help you answer these questions\n\n# Write code to help you answer the questions above (if needed)\n# 19 states had SDR at some point\nlength(unique(policy_data$state[policy_data$sdr==1]))\n\n[1] 19\n\nlength(unique(policy_data$state[policy_data$sdr==1 & policy_data$ year == 2018])) \n\n[1] 18\n\nwith(policy_data %&gt;% \n       filter(state==\"Ohio\"),\n     table(year,sdr))\n\n      sdr\nyear   0 1\n  1970 1 0\n  1971 1 0\n  1972 1 0\n  1973 1 0\n  1974 1 0\n  1975 1 0\n  1976 1 0\n  1977 1 0\n  1978 1 0\n  1979 1 0\n  1980 1 0\n  1981 1 0\n  1982 1 0\n  1983 1 0\n  1984 1 0\n  1985 1 0\n  1986 1 0\n  1987 1 0\n  1988 1 0\n  1989 1 0\n  1990 1 0\n  1991 1 0\n  1992 1 0\n  1993 1 0\n  1994 1 0\n  1995 1 0\n  1996 1 0\n  1997 1 0\n  1998 1 0\n  1999 1 0\n  2000 1 0\n  2001 1 0\n  2002 1 0\n  2003 1 0\n  2004 1 0\n  2005 0 1\n  2006 0 1\n  2007 0 1\n  2008 0 1\n  2009 0 1\n  2010 0 1\n  2011 0 1\n  2012 0 1\n  2013 0 1\n  2014 1 0\n  2015 1 0\n  2016 1 0\n  2017 1 0\n  2018 1 0"
  },
  {
    "objectID": "labs/07-lab-comments.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "href": "labs/07-lab-comments.html#calculate-the-proption-voting-by-age-group-and-sdr",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.1 Calculate the proption voting by age group and SDR",
    "text": "8.1 Calculate the proption voting by age group and SDR\nWith the cps data, use group_by() and summarize() to calculate the proportion of people voting by age group in states that did and did not have same day registration in the code chunk below.\nSave the results to a new object called fig2_df\n\n#  Calculate the proportion of voting by age group and SDR\ncps %&gt;% \n  group_by(age_group, SDR) %&gt;% \n  summarise(\n    prop_vote = mean(dv_voted, na.rm=T)\n  ) -&gt; fig2_df\nfig2_df\n\n# A tibble: 14 × 3\n# Groups:   age_group [7]\n   age_group SDR     prop_vote\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1 18-24     SDR         0.386\n 2 18-24     non-SDR     0.315\n 3 25-34     SDR         0.509\n 4 25-34     non-SDR     0.452\n 5 35-44     SDR         0.604\n 6 35-44     non-SDR     0.561\n 7 45-54     SDR         0.658\n 8 45-54     non-SDR     0.617\n 9 55-64     SDR         0.705\n10 55-64     non-SDR     0.666\n11 65+       SDR         0.715\n12 65+       non-SDR     0.660\n13 &lt;NA&gt;      SDR         0    \n14 &lt;NA&gt;      non-SDR     0"
  },
  {
    "objectID": "labs/07-lab-comments.html#recreate-figure-2-1",
    "href": "labs/07-lab-comments.html#recreate-figure-2-1",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.2 Recreate Figure 2",
    "text": "8.2 Recreate Figure 2\nUsing fig2_df recreate a Figure 2 from the paper:\n\nfilter out values of age_group that are NA\nset the appropriate aesthetic mappings in ggplot()\nuse geom_bar(stat = “identity”, position = “dodge”)\n\n\n#Recreate Figure 2\nfig2_df %&gt;% \n  filter(!is.na(age_group)) %&gt;% \n  ggplot(aes(age_group,prop_vote,fill = SDR))+\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") -&gt; fig2\n\nfig2"
  },
  {
    "objectID": "labs/07-lab-comments.html#interpret-figure-2",
    "href": "labs/07-lab-comments.html#interpret-figure-2",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "8.3 Interpret Figure 2",
    "text": "8.3 Interpret Figure 2\nWhat does Figure 2 tell us? Figure 2 provides initial support for Grumbach and Hill’s general argument. The proportion of people voting in states with SDR is higher than states with out SDR, and this gap is larger among younger age cohorts."
  },
  {
    "objectID": "labs/07-lab-comments.html#footnotes",
    "href": "labs/07-lab-comments.html#footnotes",
    "title": "Comments Lab 07 - Replicating Grumbach and Hill (2022):",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey also estimate models using both individual and aggregate data, for reasons a bit beyond the scope of this course↩︎\nThe CPS coding on this is not great and there’s no measure of ethnicity in these data. Forgive the crude indicators, but their necessary to recreate some of Grumbach and Hill’s analysis next week. ↩︎\nNote the way the recoding is described in the appendix to the paper is not how it is actually implemented in the replication code in rock_the_reg_replication_code.R. For example, the appendix describes income as ranging from 1 (Under $10k) to 16 ($500k and above), when their code, implemented above produces 32 unique values, in part because the way the CPS asked and coded the income question changed overtime. We’re going to roll with it for now…↩︎"
  },
  {
    "objectID": "assignments/a3.html",
    "href": "assignments/a3.html",
    "title": "A3: Initial Analyses",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A3: Exploratory Analysis"
    ]
  },
  {
    "objectID": "assignments/final.html",
    "href": "assignments/final.html",
    "title": "A5: Presentation",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "Final Paper"
    ]
  },
  {
    "objectID": "assignments/a5.html",
    "href": "assignments/a5.html",
    "title": "A5: Presentation",
    "section": "",
    "text": "Check back soon",
    "crumbs": [
      "Assignments",
      "Assignemnts",
      "A5: Presentations"
    ]
  },
  {
    "objectID": "assignments/A1_Research_Questions.html",
    "href": "assignments/A1_Research_Questions.html",
    "title": "POLS 2580 Assignment 1",
    "section": "",
    "text": "Asking a good research question is one of the most important skills you will develop in your academic careers. It’s also one of the hardest.\nWe often think we’re asking one question, when in fact the study we conduct really addresses a related but distinct question. When a priest asked Willie Sutton why he robbed banks, he replied the “Well, that’s where the money is”. The priest’s question was about why rob at all, while Sutton answered the different question “Given one robs, why rob banks?” Similarly, Medieval philosophers might ask why objects stay in motion, while Newton suggests what really need is not an explanation of motion itself but of changes in motion.\nThe object of our question shapes the form of our explanation.\nIn this assignment, I would like your group to craft three potential research questions that we might explore in our research project for this class. Each question, should be a single sentence, with a few sentences answering the following questions (More details below):\n\nWhy do we care about the answer to this research question?\nWhat’s would a hypothetical “ideal experiment” to answer this question look like?\nWhat would a study with observational data look like?\nA published study that relates to this question\nHow feasible would it be to do a study like this for the course\n\nYou may use this Rmd file as a template (click here to download) or create your own file. Please submit your responses to Canvas.\nYou might start by writing down several questions of different forms about the same topic:\n\nWhy do people vote?\nWhy do people not vote?\nWhy do the rich vote at higher rates than the poor?\nWhen might people who don’t vote, be motivated to vote?\nWhat is the effect of encouraging someone to vote via a phone call?\nAre phone calls more or less effective than in-person contact for get-out the vote efforts?\n\nEach of these questions addresses a general topic that political scientists seem to think is important. Each carries some suppositions and assumptions that in turn influence the type of explanation we might find convincing. Why do people vote feels a bit broad to me. People probably vote for many reasons. How can we hope to adjudicate between all the possible reasons for voting? (Further are these the same reasons for not voting or do we need another set of explanations altogether?)\nWhether phone calls are more or less effective than in-person contacts for GOTV efforts seems more tractable, but also perhaps to narrow. Do we really care? If we’re confident we can identify an effect or difference in one study, are we sure we’ll see similar effects in a different study conducted under different circumstances?\nIn crafting your research questions, you want to strike a balance between things we actually care about (why do people vote) and things we can actually assess (what’s effect of a particularly type of encouragement to vote). A few thoughts on this process:\n\n“Why” questions tend to be more compelling than “What” or “How” or “Do” questions, I think in part because “why” questions often imply a theory and suggest a counterfactual (why this and not that), while other ways of asking questions feel more descriptive. For example, why do the rich vote at higher rates than the poor. Well, one explanation may be that their relative social and economic status means they are more likely to be targets of mobilization efforts by campaigns (among many things). So a natural follow up to this larger question might be, what’s the effect of providing similar mobilization efforts to the poor. Would they vote at similar rates to the rich? If so, then we’ve learned something about how mobilization explains class differences in participation.\nThinking about questions in terms of puzzles is another useful trick. Why do parties exist when politicians’ ideological preferences can explain the vast majority of their legislative behavior? Note this type of question contains a lot of presuppositions (how do we measure ideological preferences? Do they really explain legislative behavior? Is that what we care about?), but as point of departure for a study these type arguments can be useful\nTry to be simple and clear. Don’t worry about asking the perfect question right away. Your questions can and should evolve over time, and I suspect some of you will write a paper that has nothing to do with the questions you posed here.\n\nFor each question, please discuss the following:\n\nWhy do we care? Why we should care about the answer to this question. A strong justification is often that existing theories yield conflicting predictions and so your study will offer some insight into how to adjudicate betweeen these theories. A less strong justification is that no one has ever studied this before. Even if this is true (and it’s often not) it may be true for good reason. No need for formal citations, but if there are specific theories or claims your addressing feel free to name names.\nThe ideal experiment Please describe an “ideal” experiment that you could run that would give you some purchase on your question. Note the key feature of an experiment, is that you the researcher are able to manipulate (through random assignment) some facet of the world. Assume money, resources, physics, and even ethics are not an object. If you could randomly assign anything, what would you manipulate. At what level of analysis would your manipulation occur (i.e. are your units of analysis individuals or countries or something else). How would you measure your outcome, again assuming you were all power and all-seeing. If that manipulation isn’t feasible, what does that say about the ability to make a causal claim about your question?\nThe observational study Finally, considering some of the potential limitations that might prevent you from implementing your ideal experiment (it’s hard to randomly assign democratic government), what is one way you might address your research question with observational data. Would your study use cross-sectional or longitudinal data. What are some of the concerns (selection on observables) that arise in this setting. Is there a natural experiment or some sort of discontinuity you might leverage to approximate this experimental ideal.\n\nAgain, each paragraph should be brief and to the point. No need to specify a full research design–just give me the broad strokes. You’re writing for each question should not exceed a page.\nAfter you’ve thought through how you might go about answering your question, please find\n\nA published study that relates to this question. It need not be exactly your question as posed, but it should be in a similar area. Include a full citation, and link to the study. Then in a paragraph sentences try to summarize:\n\n\nThe study’s research question\nEmprical design\nCore findings.\n\nFinally on a scale of 1 (least feasible) to 10 (most feasible), please evaluate how likely you think it is you could write an empirical paper on this question for this course.\nDon’t worry about getting everything right. Your final projects can, will, and probably should change. The point of this exercise is to get some practice thinking about questions that interest you in the language of causal inference and potential outcomes."
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study",
    "href": "assignments/A1_Research_Questions.html#the-observational-study",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study",
    "href": "assignments/A1_Research_Questions.html#a-published-study",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care-1",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care-1",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment-1",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment-1",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study-1",
    "href": "assignments/A1_Research_Questions.html#the-observational-study-1",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study-1",
    "href": "assignments/A1_Research_Questions.html#a-published-study-1",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10-1",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10-1",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#why-do-we-care-2",
    "href": "assignments/A1_Research_Questions.html#why-do-we-care-2",
    "title": "POLS 2580 Assignment 1",
    "section": "Why do we care:",
    "text": "Why do we care:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-ideal-experiment-2",
    "href": "assignments/A1_Research_Questions.html#the-ideal-experiment-2",
    "title": "POLS 2580 Assignment 1",
    "section": "The ideal experiment:",
    "text": "The ideal experiment:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#the-observational-study-2",
    "href": "assignments/A1_Research_Questions.html#the-observational-study-2",
    "title": "POLS 2580 Assignment 1",
    "section": "The observational study:",
    "text": "The observational study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#a-published-study-2",
    "href": "assignments/A1_Research_Questions.html#a-published-study-2",
    "title": "POLS 2580 Assignment 1",
    "section": "A published study:",
    "text": "A published study:"
  },
  {
    "objectID": "assignments/A1_Research_Questions.html#feasibility-x10-2",
    "href": "assignments/A1_Research_Questions.html#feasibility-x10-2",
    "title": "POLS 2580 Assignment 1",
    "section": "Feasibility: (X/10)",
    "text": "Feasibility: (X/10)"
  },
  {
    "objectID": "resources/00-software-setup.html#tldr",
    "href": "resources/00-software-setup.html#tldr",
    "title": "Getting started with R",
    "section": "TLDR",
    "text": "TLDR\nIf you’re pressed for time here’s the short version of what you should do before next class.\n\nUpdate your operating system\nDownload R\nDownload R Studio\nOpen R Studio on your computer\nCreate an Quarto Markdown Document (.qmd) in R Studio\n\n\nFile &gt; New File &gt; Quarto Document\nInstall any requested packages\n\n\nRender this .Qmd into an html file.\n\n\nClick the render arrow with the needle OR:\nUse the hot keys: Mac: cmd + shift + k PC: crtl + shift + k\n\n\nInstall some additional packages for the course\n\nSpecifically copy and paste the following into the console in R Studio and hit enter:\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nYou’ll need to have the package rmarkdown installed for\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n\nTo work, so make sure you’ve copied and pasted the code above into your console\n\nIf you have a question or somethings not working, don’t hesitate to ask. Please email me at paul_testa@brown.edu or come to my office at 111 Thayer St Rm 339."
  },
  {
    "objectID": "resources/00-software-setup.html#install-the-devtools-and-remotes-packages",
    "href": "resources/00-software-setup.html#install-the-devtools-and-remotes-packages",
    "title": "Getting started with R",
    "section": "7.1 Install the devtools and remotes packages",
    "text": "7.1 Install the devtools and remotes packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers.\nTo get the most out of R, you’ll need to install add-on packages, which are user-written to extend/expand your R capabilities.\nPackages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE).\nAlternatively, they may be available via the software sharing platform GitHub.\n\nTo download these packages, you first need to install the devtools and remotes packages.\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\n\nPlace your cursor in the console (lower left panel), and copy and paste each line of code above. After you’ve pasted a line, hit Enter/Return and R will execute (run) that line of code. So type:\n\ninstall.packages(“devtools”)\n\nHit enter.\nThen type\n\ninstall.packages(“remotes”)\n\nAnd hit enter again.\nEach time, R will likely spit out some cryptic red text as it installs the packages.\nWhen it’s done, R will you should see a line with a single &gt; in the console.\nYou should be able to see the newly installed packages by scrolling through or searching the Packages pane on the bottom left"
  },
  {
    "objectID": "resources/00-software-setup.html#install-packages-for-course",
    "href": "resources/00-software-setup.html#install-packages-for-course",
    "title": "Getting started with R",
    "section": "7.2 Install Packages for Course",
    "text": "7.2 Install Packages for Course\nNow we’ll use the intall_github() function from the remotes package, to install some packages we’ll use for this course.\nAgain, copy and paste each line of code into your console, and hit Enter/Return to run that code.\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nWe’ll go over this during our next meeting so don’t worry if this doesn’t work\nYou will likely be asked to update some existing packages\nType 1 in the console and hit enter\n\nIn particular, we’ll be using a version of Matthew Blackwell’s qsslearnr as problem sets for this course.\nYou can see the available problem sets by running the following code in your console:\n\nlearnr::run_tutorial(package = \"qsslearnr\")\n\nAnd start a tutorial by running:\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nTo try and explain in words what this code is doing:\n\nlearnr::run_tutorial( Says use the run_tutorial() from the learnr package\n\"00-intro\" tells run_tutorial() to run the \"00-intro\" tutorial\npackage = \"qsslearnr\" tells run_tutorial() to look for this tutorial in the qsslearnr package.\n\nIf you run this code, you should see the following tutorial show up in the upper right panel:"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-adding-a-tutorial-panel",
    "href": "resources/00-software-setup.html#optional-adding-a-tutorial-panel",
    "title": "Getting started with R",
    "section": "7.3 Optional: Adding a Tutorial Panel",
    "text": "7.3 Optional: Adding a Tutorial Panel\nYou can also add a “Tutorial” panel to R Studio.\n\nClick on “Tools &gt; Global Options”\n\nAlternatively you can use the hotkey combination cmd + , on a Mac cntrl + , … No Shortcut for PC :(\n\n\n\n\nSelect the Pain Layout tab. In the upper right of the four pains, check the box next to Tutorial\nYou may need to close and re-open R Studio. When you do, in the upper right tab you should see:"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-adjusting-quarto-r-markdown-display-options",
    "href": "resources/00-software-setup.html#optional-adjusting-quarto-r-markdown-display-options",
    "title": "Getting started with R",
    "section": "7.4 Optional: Adjusting Quarto / R Markdown Display Options",
    "text": "7.4 Optional: Adjusting Quarto / R Markdown Display Options\n\n2+2\n\n[1] 4\n\n\nThis is just personal perference, but while we’re changin some global options, I’d recommend\n\nClick on `RMarkdown\nSet “Show In Document Outline” to `All Sections and Chunks\nUncheck the box that says “Show Output in line for all R Markdown Documents”"
  },
  {
    "objectID": "resources/00-software-setup.html#optional-dont-save-r-history",
    "href": "resources/00-software-setup.html#optional-dont-save-r-history",
    "title": "Getting started with R",
    "section": "7.5 Optional: Don’t Save R History",
    "text": "7.5 Optional: Don’t Save R History\nFinally, in the R General tab, I’d strongly recommend unchecking the box that says “Always save R History”\n\n** Be sure to click OK** when you’re done updating these settings."
  },
  {
    "objectID": "slides/08-slides.html#class-plan",
    "href": "slides/08-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncements\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/08-slides.html#annoucements",
    "href": "slides/08-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements"
  },
  {
    "objectID": "slides/08-slides.html#packages-for-today",
    "href": "slides/08-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/08-slides.html#feedback",
    "href": "slides/08-slides.html#feedback",
    "title": "POLS 1600",
    "section": "Feedback",
    "text": "Feedback\n\nload(url(\"https://pols1600.paultesta.org/files/data/cps_clean.rda\"))\n#load(\"../files/data/cps_clean.rda\")"
  },
  {
    "objectID": "slides/08-slides.html#goals",
    "href": "slides/08-slides.html#goals",
    "title": "POLS 1600",
    "section": "Goals",
    "text": "Goals"
  },
  {
    "objectID": "slides/08-slides.html#variation-in-turnout-overtime",
    "href": "slides/08-slides.html#variation-in-turnout-overtime",
    "title": "POLS 1600",
    "section": "Variation in Turnout overtime",
    "text": "Variation in Turnout overtime"
  },
  {
    "objectID": "slides/08-slides.html#review-1",
    "href": "slides/08-slides.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "slides/08-slides.html#concept-1",
    "href": "slides/08-slides.html#concept-1",
    "title": "POLS 1600",
    "section": "Concept",
    "text": "Concept"
  },
  {
    "objectID": "slides/08-slides.html#code-1",
    "href": "slides/08-slides.html#code-1",
    "title": "POLS 1600",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "slides/08-slides.html#summary-1",
    "href": "slides/08-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "slides/08-slides.html#references",
    "href": "slides/08-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/05-slides.html#overview-1",
    "href": "slides/05-slides.html#overview-1",
    "title": "POLS 1600",
    "section": "Overview",
    "text": "Overview\n\nAnnouncements\nSetup\nFeedback\nReview\nClass plan"
  },
  {
    "objectID": "slides/05-slides.html#learing-goals",
    "href": "slides/05-slides.html#learing-goals",
    "title": "POLS 1600",
    "section": "Learing goals",
    "text": "Learing goals\n\nIntroduce the concept of Directed Acyclic Graphs to describe causal relationships and illustrate potential bias from confounders and colliders\nDiscuss three approaches to covariate adjustment\n\nSubclassification\nMatching\nLinear Regression\n\nBegin discussing three research designs to make causal claims with observational data\n\nDifferences-in-Differences (If there’s time)\nRegression Discontinuity Designs\nInstrumental Variables"
  },
  {
    "objectID": "slides/05-slides.html#annoucements",
    "href": "slides/05-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nSit with your groups (for now)\nUpdated timeline for final projects next week"
  },
  {
    "objectID": "slides/05-slides.html#group-assignments",
    "href": "slides/05-slides.html#group-assignments",
    "title": "POLS 1600",
    "section": "Group Assignments",
    "text": "Group Assignments"
  },
  {
    "objectID": "slides/05-slides.html#what-did-we-like",
    "href": "slides/05-slides.html#what-did-we-like",
    "title": "POLS 1600",
    "section": "What did we like",
    "text": "What did we like"
  },
  {
    "objectID": "slides/05-slides.html#what-did-we-dislike",
    "href": "slides/05-slides.html#what-did-we-dislike",
    "title": "POLS 1600",
    "section": "What did we dislike",
    "text": "What did we dislike"
  },
  {
    "objectID": "slides/05-slides.html#grinding-an-iron-pestle-into-a-needle",
    "href": "slides/05-slides.html#grinding-an-iron-pestle-into-a-needle",
    "title": "POLS 1600",
    "section": "Grinding an Iron Pestle into a Needle",
    "text": "Grinding an Iron Pestle into a Needle\n\n\n\nMe\n\nLess is more\nGo slow\nProvide labs/code earlier\nAdapt assignments/policies\n\n\n\nYou\n\nActive reading\nDo tutorials\nReview labs before class\nReview comments after class\nAsk for help\nDon’t give up!"
  },
  {
    "objectID": "slides/05-slides.html#review-1",
    "href": "slides/05-slides.html#review-1",
    "title": "POLS 1600",
    "section": "Review",
    "text": "Review\n\nData wrangling\nDescriptive Statistics\nLevels of understanding\nData visualization"
  },
  {
    "objectID": "slides/05-slides.html#data-wrangling-1",
    "href": "slides/05-slides.html#data-wrangling-1",
    "title": "POLS 1600",
    "section": "Data wrangling",
    "text": "Data wrangling\n\n\n\n\nYou're learning how to map conceptual tasks to commands in R\n\n\nSkill\nCommon Commands\n\n\n\n\nSetup R\nlibrary(), ipak()\n\n\nLoad data\nread_csv(), load()\n\n\nGet HLO of data\ndf$x, glimpse(), table(), summary()\n\n\nTransform data\n&lt;-, mutate(), ifelse(), case_when()\n\n\nReshape data\npivot_longer(), left_join()\n\n\nSummarize data numerically\nmean(), median(), summarise(), group_by()\n\n\nSummarize data graphically\nggplot(), aes(), geom_"
  },
  {
    "objectID": "slides/05-slides.html#mapping-concepts-to-code",
    "href": "slides/05-slides.html#mapping-concepts-to-code",
    "title": "POLS 1600",
    "section": "Mapping Concepts to Code",
    "text": "Mapping Concepts to Code\n\nTakes time and practice\nDon’t be afraid to FAAFO\nDon’t worry about memorizing everything.\nStatistical programming is necessary to actually do empirical research\nLearning to code will help us understand statistical concepts.\nLearning to think programmatically and algorithmically will help us tackle complex problems"
  },
  {
    "objectID": "slides/05-slides.html#descriptive-statistics",
    "href": "slides/05-slides.html#descriptive-statistics",
    "title": "POLS 1600",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\nDescriptive statistics help us describe what’s typical of our data\nWhat’s a typical value in our data\n\nMean\nMedian\nMode\n\nHow much do our data vary?\n\nVariance\nStandard deviation\n\nAs one variable changes how does another change?\n\nCovariance\nCorrelation\n\nDescriptive statistics are:\n\nDiagnostic\nGenerative"
  },
  {
    "objectID": "slides/05-slides.html#levels-of-understanding-in-pols-1600",
    "href": "slides/05-slides.html#levels-of-understanding-in-pols-1600",
    "title": "POLS 1600",
    "section": "Levels of understanding in POLS 1600",
    "text": "Levels of understanding in POLS 1600\n\nConceptual\nPractical\nDefinitional\nTheoretical\n\n\nLet’s illustrate these different levels of understanding about our old friend the mean"
  },
  {
    "objectID": "slides/05-slides.html#mean-conceptual-understanding",
    "href": "slides/05-slides.html#mean-conceptual-understanding",
    "title": "POLS 1600",
    "section": "Mean: Conceptual Understanding",
    "text": "Mean: Conceptual Understanding\nA mean is:\n\nA common and important measure of central tendency (what’s typical)\nIt’s the arithmetic average you learned in school\nWe can think of it as the balancing point of a distribution\nA conditional mean is the average of one variable \\(X\\), when some other variable, \\(Z\\) takes a value \\(z\\)\n\nThink about the average height in our class (unconditional mean) vs the average height among men and women ([conditional means].{blue})"
  },
  {
    "objectID": "slides/05-slides.html#mean-as-a-balancing-point",
    "href": "slides/05-slides.html#mean-as-a-balancing-point",
    "title": "POLS 1600",
    "section": "Mean as a balancing point",
    "text": "Mean as a balancing point\n\nSource"
  },
  {
    "objectID": "slides/05-slides.html#mean-practical",
    "href": "slides/05-slides.html#mean-practical",
    "title": "POLS 1600",
    "section": "Mean: Practical",
    "text": "Mean: Practical\nThere are lots of ways to calculate means in R\n\nThe simplest is to use the mean() function\n\nIf our data have missing values, we need to to tell R to remove them\n\n\n\nmean(df$x, na.rm=T)"
  },
  {
    "objectID": "slides/05-slides.html#conditional-means-practical",
    "href": "slides/05-slides.html#conditional-means-practical",
    "title": "POLS 1600",
    "section": "Conditional Means: Practical",
    "text": "Conditional Means: Practical\n\nTo calculate a conditional mean we could us a logical index [df$z == 1]\n\n\nmean(df$x[df$z == 1], na.rm=T)\n\n\nIf we wanted to a calculate a lot of conditional means we could use the mean() in combination with group_by() and summarise()\n\n\ndf %&gt;% \n  group_by(z)%&gt;%\n  summarise(\n    x = mean(x, na.rm=T)\n  )"
  },
  {
    "objectID": "slides/05-slides.html#mean-definitional",
    "href": "slides/05-slides.html#mean-definitional",
    "title": "POLS 1600",
    "section": "Mean: Definitional",
    "text": "Mean: Definitional\nFormally, we define the arithmetic mean of \\(x\\) as \\(\\bar{x}\\):\n\\[\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}\n\\]\nIn words, this formula says, to calculate the average of x, we sum up all the values of \\(x_i\\) from observation \\(i=1\\) to \\(i=n\\) and then divide by the total number of observations \\(n\\)"
  },
  {
    "objectID": "slides/05-slides.html#mean-definitional-1",
    "href": "slides/05-slides.html#mean-definitional-1",
    "title": "POLS 1600",
    "section": "Mean: Definitional",
    "text": "Mean: Definitional\n\nIn this class, I don’t put a lot of weight on memorizing definitions (that’s what Google’s for).\nBut being comfortable with “the math” is important and useful\nDefinitional knowledge is a prerequisite for understanding more theoretical claims."
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical",
    "href": "slides/05-slides.html#mean-theoretical",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nSuppose I asked you to show that the sum of deviations from a mean equals 0?\n\\[\n\\text{Claim:} \\sum_{i=1}^n (x_i -\\bar{x}) = 0\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical-1",
    "href": "slides/05-slides.html#mean-theoretical-1",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nKnowing the definition of an arithmetic mean, we could write:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (x_i -\\bar{x}) &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n\\bar{x} & \\text{Distribute Summation}\\\\\n              &= \\sum_{i=1}^n x_i - n\\bar{x} & \\text{Summing a constant, } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - n\\times \\left ( \\frac{1}{n} \\sum_{i=1}^n{x_i}\\right ) & \\text{Definition of } \\bar{x}\\\\\n              &= \\sum_{i=1}^n x_i - \\sum_{i=1}^n{x_i} & n \\times \\frac{1}{n}=1\\\\\n              &= 0             \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical-2",
    "href": "slides/05-slides.html#mean-theoretical-2",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nWhy do we care?\n\nShowing the deviations sum to 0 is another way of saying the mean is a balancing point.\nThis turns out to be a useful property of means that will reappear throughout the course\nIf I asked you to make a prediction, \\(\\hat{x}\\) of a random person’s height in this class, the mean would have the lowest mean squared error (MSE \\(=\\frac{1}{n}\\sum (x_i - \\hat{x_i})^2)\\)"
  },
  {
    "objectID": "slides/05-slides.html#mean-theoretical-3",
    "href": "slides/05-slides.html#mean-theoretical-3",
    "title": "POLS 1600",
    "section": "Mean: Theoretical",
    "text": "Mean: Theoretical\nOccasionally, you’ll read or here me say say things like:\n\nThe sample mean is an unbiased estimator of the population mean\n\nIn a statistics class, we would take time to prove this."
  },
  {
    "objectID": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean",
    "href": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean",
    "title": "POLS 1600",
    "section": "The sample mean is an unbiased estimator of the population mean",
    "text": "The sample mean is an unbiased estimator of the population mean\nClaim:\nLet \\(x_1, x_2, \\dots x_n\\) from a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\nThen:\n\\[\n\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n x_i\\right )\n\\]\nis an unbiased estimator of \\(\\mu\\)\n\\[\nE[\\bar{x}] = \\mu\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean-1",
    "href": "slides/05-slides.html#the-sample-mean-is-an-unbiased-estimator-of-the-population-mean-1",
    "title": "POLS 1600",
    "section": "The sample mean is an unbiased estimator of the population mean",
    "text": "The sample mean is an unbiased estimator of the population mean\nProof:\n\\[\n\\begin{aligned}\nE\\left [\\bar{x} \\right] &= E\\left [\\frac{1}{n}\\left (\\sum_{i=1}^n x_i \\right) \\right] & \\text{Definition of } \\bar{x} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^nE\\left [ x_i \\right]  & \\text{Linearity of Expectations} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu  & E[x_i] = \\mu \\\\\n&= \\frac{n}{n}  \\mu  & \\sum_{i=1}^n \\mu = n\\mu \\\\\n&= \\mu  & \\blacksquare \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#levels-of-understanding-1",
    "href": "slides/05-slides.html#levels-of-understanding-1",
    "title": "POLS 1600",
    "section": "Levels of understanding",
    "text": "Levels of understanding\n\nIn this course, we tend to emphasize the\n\nConceptual\nPractical\n\nOver\n\nDefinitional\nTheoretical\n\nIn an intro statistics class, the ordering might be reversed.\nTrade offs:\n\n\nPro: We actually get to work with data and do empirical research much sooner\nCons: We substitute intuitive understandings for more rigorous proofs"
  },
  {
    "objectID": "slides/05-slides.html#data-visualization-1",
    "href": "slides/05-slides.html#data-visualization-1",
    "title": "POLS 1600",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nThe grammar of graphics\nAt minimum you need:\n\ndata\naesthetic mappings\ngeometries\n\nTake a sad plot and make it better by:\n\nlabels\nthemes\nstatistics\ncooridnates\nfacets\ntransforming your data before plotting"
  },
  {
    "objectID": "slides/05-slides.html#you-are-about-to-be-reincarnated-hlo",
    "href": "slides/05-slides.html#you-are-about-to-be-reincarnated-hlo",
    "title": "POLS 1600",
    "section": "You are about to be reincarnated: HLO",
    "text": "You are about to be reincarnated: HLO\n\ndf$reincarnation\n\n&lt;labelled&lt;double&gt;[10]&gt;: You're about to be re-incarnated. Do you want to come back as a:\n [1] 1 1 1 7 6 1 6 6 7 1\n\nLabels:\n value                  label\n     1    Animal/land dweller\n     6       Bird/air dweller\n     7     Fish/water dweller\n     2                 Insect\n     9                  Plant\n    10 Single-celled organism\n\ntable(df$reincarnation)\n\n\n1 6 7 \n5 3 2"
  },
  {
    "objectID": "slides/05-slides.html#basic-plot",
    "href": "slides/05-slides.html#basic-plot",
    "title": "POLS 1600",
    "section": "Basic Plot",
    "text": "Basic Plot\n\nCodeFigure\n\n\n\ndf %&gt;%\n  ggplot(aes(x = reincarnation, \n             fill = reincarnation))+\n  geom_bar(\n    stat = \"count\"\n  )"
  },
  {
    "objectID": "slides/05-slides.html#use-a-factor-to-label-and-order-responses",
    "href": "slides/05-slides.html#use-a-factor-to-label-and-order-responses",
    "title": "POLS 1600",
    "section": "Use a factor to label and order responses",
    "text": "Use a factor to label and order responses\n\nRecodeCheck recoding\n\n\n\ndf %&gt;%\n  mutate(\n    # Turn numeric values into factor labels \n    Reincarnation = forcats::as_factor(reincarnation),\n    # Order factor in decreasing frequency of levels\n    Reincarnation = forcats::fct_infreq(Reincarnation),\n    # Reverse order so levels are increasing in frequency\n    Reincarnation = forcats::fct_rev(Reincarnation),\n    # Rename explanations\n    Why = reincarnation_why\n  ) -&gt; df\n\n\n\n\ntable(recode= df$Reincarnation, original = df$reincarnation)\n\n                        original\nrecode                   1 6 7\n  Single-celled organism 0 0 0\n  Plant                  0 0 0\n  Insect                 0 0 0\n  Fish/water dweller     0 0 2\n  Bird/air dweller       0 3 0\n  Animal/land dweller    5 0 0"
  },
  {
    "objectID": "slides/05-slides.html#revised-figure",
    "href": "slides/05-slides.html#revised-figure",
    "title": "POLS 1600",
    "section": "Revised figure",
    "text": "Revised figure\n\nCodeRevised Figure\n\n\n\ndf %&gt;% # Data\n  # Aesthetics\n  ggplot(aes(x = Reincarnation, \n             fill = Reincarnation))+\n  # Geometry\n  geom_bar(stat = \"count\")+ # Statistic\n  ## Include levels of Reincarnation w/ no values\n  scale_x_discrete(drop=FALSE)+\n  # Don't include a legend\n  scale_fill_discrete(drop=FALSE, guide=\"none\")+\n  # Flip x and y\n  coord_flip()+\n  # Remove lines\n  theme_classic() -&gt; fig1"
  },
  {
    "objectID": "slides/05-slides.html#section",
    "href": "slides/05-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "What creature and why?"
  },
  {
    "objectID": "slides/05-slides.html#adding-labelled-values",
    "href": "slides/05-slides.html#adding-labelled-values",
    "title": "POLS 1600",
    "section": "Adding labelled values",
    "text": "Adding labelled values\n\nRecodesRecode OutputAggregate Data\n\n\n\ndf %&gt;%\n  mutate(\n    # Create numeric id\n    id = 1:n(),\n    # Create a label with 3 answers and NA elsewhere\n    Label = case_when(\n      id == 10 ~ str_wrap(reincarnation_why[10],30),\n      id == 4 ~ str_wrap(reincarnation_why[4],30),\n      id == 7 ~ str_wrap(reincarnation_why[7],30),\n      TRUE ~ NA_character_\n\n    )\n\n  ) -&gt; df\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate totals before calling ggplot\nplot_df &lt;- df %&gt;%\n  group_by(Reincarnation)%&gt;% \n  summarise( \n    Count = n(), \n    Why = na.omit(unique(Label)) \n  )"
  },
  {
    "objectID": "slides/05-slides.html#youre-about-to-be-reincarnated",
    "href": "slides/05-slides.html#youre-about-to-be-reincarnated",
    "title": "POLS 1600",
    "section": "You’re about to be reincarnated:",
    "text": "You’re about to be reincarnated:\n\nAggregate dfRevised Figure CodeLabelled Figure\n\n\n\n\n\n\n\n\n\n\n\nplot_df %&gt;%\n  ggplot(aes(x = Reincarnation, \n             y = Count,\n             fill = Reincarnation, \n             label=Why))+\n  geom_bar(stat = \"identity\")+ #&lt;&lt;\n  ## Include levels of Reincarnation w/ no values\n  scale_x_discrete(drop=FALSE)+\n  # Don't include a legend\n  scale_fill_discrete(drop=FALSE, guide=\"none\")+\n  coord_flip()+\n  labs(x = \"\",y=\"\",title=\"You're about to be reincarnated.\\nWhat do you want to come back as?\")+\n  theme_classic()+\n  ggrepel::geom_label_repel(\n    fill=\"white\",\n    nudge_y = 1, \n    hjust = \"left\",\n    size=3,\n    arrow = arrow(length = unit(0.015, \"npc\"))\n    )+ \n  scale_y_continuous(\n    breaks = c(0,2,4,6,8,10,12),\n    expand = expansion(add =c(0,6))\n    ) -&gt; fig1"
  },
  {
    "objectID": "slides/05-slides.html#data-visualization-is-an-iterative-process",
    "href": "slides/05-slides.html#data-visualization-is-an-iterative-process",
    "title": "POLS 1600",
    "section": "Data visualization is an iterative process",
    "text": "Data visualization is an iterative process\n\nData visualization is an iterative process\nGood data viz requires lots of data transformations\nStart with a minimum working example and build from there\nDon’t let the perfect be the enemy of the good enough."
  },
  {
    "objectID": "slides/05-slides.html#new-packages",
    "href": "slides/05-slides.html#new-packages",
    "title": "POLS 1600",
    "section": "New packages",
    "text": "New packages\nThis week’s lab we’ll be using the dataverse package to download data on presidential elections\nNext week’s lab, we’ll be using the tidycensus package to download census data.\nWe’ll also need to install a census API to get the data.\nHere’s a detailed guide of what we’ll do in class right now."
  },
  {
    "objectID": "slides/05-slides.html#install-new-packages",
    "href": "slides/05-slides.html#install-new-packages",
    "title": "POLS 1600",
    "section": "Install new packages",
    "text": "Install new packages\nThese packages are easier to install live:\n\ninstall.packages(\"dataverse\")\ninstall.packages(\"tidycensus\")\ninstall.packages(\"easystats\")\ninstall.packages(\"DeclareDesign\")"
  },
  {
    "objectID": "slides/05-slides.html#census-api",
    "href": "slides/05-slides.html#census-api",
    "title": "POLS 1600",
    "section": "Census API",
    "text": "Census API\nPlease follow these steps so you can download data directly from the U.S. Census here:\n\nInstall the tidycensus package\nLoad the installed package\nRequest an API key from the Census\nCheck your email\nActivate your key\nInstall your API key in R\nCheck that everything worked"
  },
  {
    "objectID": "slides/05-slides.html#packages-for-today",
    "href": "slides/05-slides.html#packages-for-today",
    "title": "POLS 1600",
    "section": "Packages for today",
    "text": "Packages for today\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    easystats           zoo \n         TRUE          TRUE"
  },
  {
    "objectID": "slides/05-slides.html#red-covid",
    "href": "slides/05-slides.html#red-covid",
    "title": "POLS 1600",
    "section": "Red Covid",
    "text": "Red Covid\n\n\n\nRed Covid New York Times, 27 September, 2021\n\n Red Covid, an Update New York Times, 18 February, 2022"
  },
  {
    "objectID": "slides/05-slides.html#preview-of-the-lab",
    "href": "slides/05-slides.html#preview-of-the-lab",
    "title": "POLS 1600",
    "section": "Preview of the Lab",
    "text": "Preview of the Lab\nPlease download Thursday’s lab here\n\nConceptually, this lab is designed to help reinforce the relationship between linear models like \\(y=\\beta_0 + \\beta_1x\\) and the conditional expectation function \\(E[Y|X]\\).\nSubstantively, we will explore whether David Leonhardt’s claims about Red Covid the political polarization of vaccines and its consequences"
  },
  {
    "objectID": "slides/05-slides.html#lab-questions-1-5-review",
    "href": "slides/05-slides.html#lab-questions-1-5-review",
    "title": "POLS 1600",
    "section": "Lab: Questions 1-5: Review",
    "text": "Lab: Questions 1-5: Review\nQuestions 1-5 are designed to reinforce your data wrangling skills. In particular, you will get practice:\n\nCreating and recoding variables using mutate()\nCalculating a moving average or rolling mean using the rollmean() function from the zoo package\nTransforming the data on presidential elections so that it can be merged with the data on Covid-19 using the pivot_wider() function.\nMerging data together using the left_join() function."
  },
  {
    "objectID": "slides/05-slides.html#lab-questions-6-10-simple-linear-regression",
    "href": "slides/05-slides.html#lab-questions-6-10-simple-linear-regression",
    "title": "POLS 1600",
    "section": "Lab: Questions 6-10: Simple Linear Regression",
    "text": "Lab: Questions 6-10: Simple Linear Regression\n\nIn question 6, you will see how calculating conditional means provides a simple test of “Red Covid” claim.\nIn question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)\nIn question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)\nIn question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.\nQuestion 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week’s lab."
  },
  {
    "objectID": "slides/05-slides.html#before-thursday",
    "href": "slides/05-slides.html#before-thursday",
    "title": "POLS 1600",
    "section": "Before Thursday",
    "text": "Before Thursday\nThe following slides provide detailed explanations of all the code you’ll need for each question.\n\nPlease run this code before class on Thursday\nWe will review this material together at the start of class, but you will spend most of our time on the Questions 6-10"
  },
  {
    "objectID": "slides/05-slides.html#q1-setup-your-workspace",
    "href": "slides/05-slides.html#q1-setup-your-workspace",
    "title": "POLS 1600",
    "section": "Q1: Setup your workspace",
    "text": "Q1: Setup your workspace\n\nTaskCode for Q1\n\n\nQ1 asks you to setup your workspace\nThis means loading and, if needed, installing the packages you will use.\n\n\n\n## Pacakges for today\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)\n\n## Define a function to load (and if needed install) packages\n\n#| label = \"ipak\"\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\n## Install (if needed) and load libraries in the_packages\nipak(the_packages)"
  },
  {
    "objectID": "slides/05-slides.html#q2-load-the-data",
    "href": "slides/05-slides.html#q2-load-the-data",
    "title": "POLS 1600",
    "section": "Q2 Load the data",
    "text": "Q2 Load the data\nTo explore Leonhardt’s claims about Red Covid, we’ll need data on:\n\nCovid-19\nThe 2020 Presidential Election"
  },
  {
    "objectID": "slides/05-slides.html#q2.1-load-the-covid-19-data",
    "href": "slides/05-slides.html#q2.1-load-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Q2.1 Load the Covid-19 Data",
    "text": "Q2.1 Load the Covid-19 Data\nTo load data on Covid-19 just run this\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/05-slides.html#q2.2-load-election-data",
    "href": "slides/05-slides.html#q2.2-load-election-data",
    "title": "POLS 1600",
    "section": "Q2.2 Load Election Data",
    "text": "Q2.2 Load Election Data\n\nTaskCode for Q2.2\n\n\nQ2.2. asks you to write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab’s dataverse\n\nOnce you’ve installed the dataverse package you should be able to do this:\n\n\n\n\n# Try this code first\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\n\npres_df &lt;- dataverse::get_dataframe_by_name(\n  \"1976-2020-president.tab\",\n  \"doi:10.7910/DVN/42MVDX\"\n)\n\n# If the code above fails, comment out and uncomment the code below:\n\n# load(url(\"https://pols1600.paultesta.org/files/data/pres_df.rda\"))"
  },
  {
    "objectID": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset",
    "href": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset",
    "title": "POLS 1600",
    "section": "Q3 Describe the structure of each dataset",
    "text": "Q3 Describe the structure of each dataset\nQuestion 3 asks you to describe the structure of each dataset.\n\nSpecifically, it asks you to get a high level overview of covid and pres_df and describe the unit of analysis in each dataset:\n\nDescribe substantively what specific, observation each row in the dataset corresponds to\nIn covid covid dataset, the unit of analysis is a state-date"
  },
  {
    "objectID": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset-1",
    "href": "slides/05-slides.html#q3-describe-the-structure-of-each-dataset-1",
    "title": "POLS 1600",
    "section": "Q3 Describe the structure of each dataset",
    "text": "Q3 Describe the structure of each dataset\nHere’s some possible code you could use to get a quick HLO of each dataset:\n\nHLO covidHLO pres_df\n\n\n\n# check names in `covid`\nnames(covid)\n\n# take a quick look values of each variable\n\nglimpse(covid)\n\n# Look at first few observations for:\n# date, administrative_area_level_2, \n\ncovid %&gt;% \n  select(date, administrative_area_level_2) %&gt;%\n  head()\n\n# Summarize data to get a better sense of the unit of observastion\n\ncovid %&gt;% \n  group_by(administrative_area_level_2) %&gt;%\n  summarise(\n    n = n(), # Number of observations for each state\n    start_date = min(date, na.rm = T),\n    end_date = max(date, na.rm=T)\n  ) -&gt; hlo_covid_df\n\nhlo_covid_df\n\n\n# How many unique values of date and state are their:\n\nn_dates &lt;- length(unique(covid$date))\nn_states &lt;- length(unique(covid$administrative_area_level_2))\nn_dates\nn_states\n\n# If we had observations for every state on every date then the number of rows \n# in the data \ndim(covid)[1]\n# Should equal\ndim(covid)[1] == n_dates * n_states\n\n# This is what economists would call an unbalanced panel\n\n\n\n\n# check names in `pres_df`\nnames(pres_df)\n\n# take a quick look values of each variable\n\nglimpse(pres_df)\n\n# Unit of analysis is a year-state-candidate\npres_df %&gt;% \n  select(year, state_po, candidate) %&gt;%\n  head()\n\n# How many states?\nlength(unique(pres_df$state_po))\n\n\n\n# How many candidates and parties on the ballot in a given election year\npres_df %&gt;% \n  group_by(year) %&gt;%\n  summarise(\n    n_candidates = length(unique(candidate)),\n    # Look at both party_detailed and party_simplified\n    n_parties_detailed = length(unique(party_detailed)),\n    n_parties_simplified = length(unique(party_simplified))\n  ) -&gt; hlo_pres_df\nhlo_pres_df\n\n# Look at 2020\n# pres_df$candidate[pres_df$year == \"2020\"]"
  },
  {
    "objectID": "slides/05-slides.html#q4-recode-the-data-for-analysis",
    "href": "slides/05-slides.html#q4-recode-the-data-for-analysis",
    "title": "POLS 1600",
    "section": "Q4 Recode the data for analysis",
    "text": "Q4 Recode the data for analysis\nUsing our understanding of the structure of the data, Q4 asks you to:\n\nRecode the Covid-19 data like we’ve done before plus\nCalculate rolling means, 7 and 14 day averages\nReshape, recode, and filter the presidential election data"
  },
  {
    "objectID": "slides/05-slides.html#q4.1-recode-the-covid-19",
    "href": "slides/05-slides.html#q4.1-recode-the-covid-19",
    "title": "POLS 1600",
    "section": "Q4.1 Recode the Covid-19",
    "text": "Q4.1 Recode the Covid-19\n\nTaskCode for Q4.1Template Code for Q4.2\n\n\nThis is the same code we’ve used before to create covid_us from covid with the addition of code to calculate a rolling mean or moving average of the number of new cases\n\n\n\n# Create a vector containing of US territories\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n# Filter out Territories and create state variable\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)%&gt;%\n  mutate(\n    state = administrative_area_level_2\n  )\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7da = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us\n\n# Recode facemask policy\n\ncovid_us %&gt;%\nmutate(\n  # Recode facial_coverings to create face_masks\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ),\n    # Turn face_masks into a factor with ordered policy levels\n    face_masks = factor(face_masks,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n# Create year-month and percent vaccinated variables\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(year, \n                       str_pad(month, width = 2, pad=0), \n                       sep = \"-\"),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us\n\n\n\n\n# Calculate new cases, new cases per capita, and 7-day average\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000,\n    new_cases_pc_7day = zoo::rollmean(new_cases_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/05-slides.html#q4.2-calculate-rolling-means-of-covid-deaths",
    "href": "slides/05-slides.html#q4.2-calculate-rolling-means-of-covid-deaths",
    "title": "POLS 1600",
    "section": "Q4.2 Calculate Rolling Means of Covid Deaths",
    "text": "Q4.2 Calculate Rolling Means of Covid Deaths\n\nTaskCode for Q4.2\n\n\nQ4.2 asks you to create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents\nIt encourages you to use the code new_cases_pc_7da as a template\nTo build your coding skills, try writing this yourself, then comparing it to the code in the next tab:\n\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = new_deaths / population *100000,\n    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, \n                                     k = 7, \n                                     align = \"right\",\n                                     fill=NA ),\n    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, \n                                     k = 14, \n                                     align = \"right\",\n                                     fill=NA )\n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/05-slides.html#rolling-averages",
    "href": "slides/05-slides.html#rolling-averages",
    "title": "POLS 1600",
    "section": "Rolling Averages",
    "text": "Rolling Averages\nThe next slides aren’t necessary for the lab but are designed to illustrate:\n\nthe concept of a rolling mean\nwhat the code does\nwhy might prefer rolling averages over daily values"
  },
  {
    "objectID": "slides/05-slides.html#look-at-the-output-of-zoorollmean",
    "href": "slides/05-slides.html#look-at-the-output-of-zoorollmean",
    "title": "POLS 1600",
    "section": "Look at the output of zoo::rollmean()",
    "text": "Look at the output of zoo::rollmean()\n\ncovid_us %&gt;%\n  filter(date &gt; \"2020-03-05\") %&gt;%\n  select(date,new_cases_pc,new_cases_pc_7day)\n\n# A tibble: 52,580 × 4\n# Groups:   state [51]\n   state     date       new_cases_pc new_cases_pc_7day\n   &lt;chr&gt;     &lt;date&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Minnesota 2020-03-06      NA                NA     \n 2 Minnesota 2020-03-07       0                NA     \n 3 Minnesota 2020-03-08       0.0177           NA     \n 4 Minnesota 2020-03-09       0                NA     \n 5 Minnesota 2020-03-10       0.0177           NA     \n 6 Minnesota 2020-03-11       0.0355           NA     \n 7 Minnesota 2020-03-12       0.0709           NA     \n 8 Minnesota 2020-03-13       0.0887            0.0329\n 9 Minnesota 2020-03-14       0.124             0.0507\n10 Minnesota 2020-03-15       0.248             0.0836\n# ℹ 52,570 more rows"
  },
  {
    "objectID": "slides/05-slides.html#comparing-daily-cases-to-rolling-average",
    "href": "slides/05-slides.html#comparing-daily-cases-to-rolling-average",
    "title": "POLS 1600",
    "section": "Comparing Daily Cases to Rolling Average",
    "text": "Comparing Daily Cases to Rolling Average\nThe following code illustrates how a 7-day rolling mean smooths (new_cases_pc_7da) over the noisiness of the daily measure\n\nCodeFigure\n\n\n\ncovid_us %&gt;%\n  filter(date &gt; \"2020-03-05\", \n         state == \"Minnesota\") %&gt;%\n  select(date,\n         new_cases_pc,\n         new_cases_pc_7day)%&gt;%\n  ggplot(aes(date,new_cases_pc ))+\n  geom_line(aes(col=\"Daily\"))+\n  # set y aesthetic for second line of rolling average\n  geom_line(aes(y = new_cases_pc_7day,\n                col = \"7-day average\")\n            ) +\n  theme(legend.position=\"bottom\")+\n    labs( col = \"Measure\",\n    y = \"New Cases Per 100k\", x = \"\",\n    title = \"Minnesota\"\n  ) -&gt; fig_covid_mn"
  },
  {
    "objectID": "slides/05-slides.html#q4.3-recode-presidential-data",
    "href": "slides/05-slides.html#q4.3-recode-presidential-data",
    "title": "POLS 1600",
    "section": "Q4.3 Recode Presidential data",
    "text": "Q4.3 Recode Presidential data\n\nTaskCode for Q4.3\n\n\nQ4.3 Gives you a long list of steps to recode, reshape, and filter pres_df to produce pres_df2020\nMost of this is review but it can seem like a lot.\nWalk through the provided code and see if you can map each conceptual step in Q4.3 to its implementation in the code\n\n\n\npres_df %&gt;%\n  mutate(\n    year_election = year,\n    state = str_to_title(state),\n    # Fix DC\n    state = ifelse(state == \"District Of Columbia\", \"District of Columbia\", state)\n  ) %&gt;%\n  filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))%&gt;%\n  filter(year == 2020) %&gt;%\n  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes\n         ) %&gt;%\n  pivot_wider(names_from = party_simplified,\n              values_from = candidatevotes) %&gt;%\n  mutate(\n    dem_voteshare = DEMOCRAT/totalvotes *100,\n    rep_voteshare = REPUBLICAN/totalvotes*100,\n    winner = forcats::fct_rev(factor(ifelse(rep_voteshare &gt; dem_voteshare,\"Trump\",\"Biden\")))\n  ) -&gt; pres2020_df\n\n# Check Output:\n\nglimpse(pres2020_df)\n\nRows: 51\nColumns: 9\n$ state         &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\"…\n$ state_po      &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"F…\n$ year_election &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 20…\n$ totalvotes    &lt;dbl&gt; 2323282, 359530, 3387326, 1219069, 17500881, 3279980, 18…\n$ DEMOCRAT      &lt;dbl&gt; 849624, 153778, 1672143, 423932, 11110250, 1804352, 1080…\n$ REPUBLICAN    &lt;dbl&gt; 1441170, 189951, 1661686, 760647, 6006429, 1364607, 7147…\n$ dem_voteshare &lt;dbl&gt; 36.56999, 42.77195, 49.36469, 34.77506, 63.48395, 55.011…\n$ rep_voteshare &lt;dbl&gt; 62.031643, 52.833143, 49.055981, 62.395730, 34.320724, 4…\n$ winner        &lt;fct&gt; Trump, Trump, Biden, Trump, Biden, Biden, Biden, Biden, …"
  },
  {
    "objectID": "slides/05-slides.html#q5-merging-data",
    "href": "slides/05-slides.html#q5-merging-data",
    "title": "POLS 1600",
    "section": "Q5 merging data",
    "text": "Q5 merging data\n\nTaskMerge election data into Covid data\n\n\nQ5 asks you to merge the 2020 election data from pres2020_df into covid_us using the common state variable in each dataset using the function left_join()\n\n\n\ndim(covid_us)\n\n[1] 53678    61\n\ndim(pres2020_df)\n\n[1] 51  9\n\ncovid_us &lt;- covid_us %&gt;% left_join(\n  pres2020_df,\n  by = c(\"state\" = \"state\")\n)\ndim(covid_us) \n\n[1] 53678    69"
  },
  {
    "objectID": "slides/05-slides.html#advice-for-merging",
    "href": "slides/05-slides.html#advice-for-merging",
    "title": "POLS 1600",
    "section": "Advice for merging",
    "text": "Advice for merging\n\nAdviceIllustration\n\n\nWhen merging datasets:\n\nCheck the matches in your joining variables\n\nMake sure the values state are the same in each dataset\nCheck for differences in spelling, punctuation, etc.\n\nCheck the dimensions of output of your left_join()\n\nIf there is a 1-1 match the number of rows should be the same before after\n\n\n\n\n\n# Should be 51 states and DC in each\nsum(unique(pres_df$state) %in% covid_us$state)\n\n[1] 0\n\n# Look at each state variable\n## With [] index\npres_df$state[1:5]\n\n[1] \"ALABAMA\" \"ALABAMA\" \"ALABAMA\" \"ALABAMA\" \"ALABAMA\"\n\ncovid_us$state[1:5]\n\n[1] \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\"\n\n# Matching is case sensitive \n\n# make pres_df$state title case\n\n## Base R:\npres_df$state &lt;- str_to_title(pres_df$state )\n## Tidy R:\npres_df %&gt;% \n  mutate(\n    state = str_to_title(state )\n  ) -&gt; pres_df\n\n# Should be 51\nsum(unique(pres_df$state) %in% covid_us$state)\n\n[1] 50\n\n# Find the mismatch:\nunique(pres_df$state[!pres_df$state %in% covid_us$state])\n\n[1] \"District Of Columbia\"\n\n# Two equivalent ways to fix this mismatch\n## Base R: Quick fix to change spelling of DC\npres_df$state[pres2020_df$state == \"District Of Columbia\"] &lt;- \"District of Columbia\"\n\n## Tidy R: Quick fix to change spelling of DC\n\npres_df %&gt;% \n  mutate(\n    state = ifelse(test = state == \"District Of Columbia\",\n                   yes = \"District of Columbia\",\n                   no = state\n                   )\n  ) -&gt; pres_df\n\n\n# Problem Solved\nsum(unique(pres2020_df$state) %in% covid_us$state)\n\n[1] 51"
  },
  {
    "objectID": "slides/05-slides.html#causal-inference-is-about-counterfactual-comparisons",
    "href": "slides/05-slides.html#causal-inference-is-about-counterfactual-comparisons",
    "title": "POLS 1600",
    "section": "Causal inference is about counterfactual comparisons",
    "text": "Causal inference is about counterfactual comparisons\n\nCausal inference is about counterfactual comparisons\n\nWhat would have happened if some aspect of the world either had or had not been present"
  },
  {
    "objectID": "slides/05-slides.html#causal-identification",
    "href": "slides/05-slides.html#causal-identification",
    "title": "POLS 1600",
    "section": "Causal Identification",
    "text": "Causal Identification\n\nCasual Identification refers to “the assumptions needed for statistical estimates to be given a causal interpretation” Keele (2015)]\n\nWhat do we need to assume to make our claims about cause and effect credible\n\nExperimental Designs rely on randomization of treatment to justify their causal claims\nObservational Designs require additional assumptions and knowledge to make causal claims"
  },
  {
    "objectID": "slides/05-slides.html#experimental-designs",
    "href": "slides/05-slides.html#experimental-designs",
    "title": "POLS 1600",
    "section": "Experimental Designs",
    "text": "Experimental Designs\n\nExperimental designs are studies in which a causal variable of interest, the treatement, is manipulated by the researcher to examine its causal effects on some outcome of interest\nRandom assignment is the key to causal identification in experiments because it creates statistical independence between treatment and potential outcomes any potential confounding factors\n\n\n\\[\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#randomization-creates-credible-counterfactual-comparisons",
    "href": "slides/05-slides.html#randomization-creates-credible-counterfactual-comparisons",
    "title": "POLS 1600",
    "section": "Randomization creates credible counterfactual comparisons",
    "text": "Randomization creates credible counterfactual comparisons\nIf treatment has been randomly assigned, then:\n\nThe only thing that differs between treatment and control is that one group got the treatment, and another did not.\nWe can estimate the Average Treatment Effect (ATE) using the difference of sample means\n\n\n\\[\n\\begin{aligned}\nE \\left[ \\frac{\\sum_1^m Y_i}{m}-\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]&=\\overbrace{E \\left[ \\frac{\\sum_1^m Y_i}{m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among treated}\\\\ \\text{units}}}\n-\\overbrace{E \\left[\\frac{\\sum_{m+1}^N Y_i}{N-m}\\right]}^{\\substack{\\text{Average outcome}\\\\\n\\text{among control}\\\\ \\text{units}}}\\\\\n&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#observational-designs",
    "href": "slides/05-slides.html#observational-designs",
    "title": "POLS 1600",
    "section": "Observational Designs",
    "text": "Observational Designs\n\nObservational designs are studies in which a causal variable of interest is determined by someone/thing other than the researcher (nature, governments, people, etc.)\nSince treatment has not been randomly assigned, observational studies typically require stronger assumptions to make causal claims.\nGenerally speaking, these assumptions amount to a claim about conditional independence\n\n\n\\[\nY_i(1),Y_i(0),\\mathbf{X_i},\\mathbf{U_i} \\unicode{x2AEB} D_i | K_i\n\\]\n\n\nWhere after conditioning on \\(K_i\\), some knowledge about the world and how the data were generated, our treatment is as good as (as-if) randomly assigned (hence conditionally independent)\n\nEconomists often call this assumption of selection on observables"
  },
  {
    "objectID": "slides/05-slides.html#causal-inference-in-observational-studies",
    "href": "slides/05-slides.html#causal-inference-in-observational-studies",
    "title": "POLS 1600",
    "section": "Causal Inference in Observational Studies",
    "text": "Causal Inference in Observational Studies\nTo understand how to make causal claims in observational studies we will:\n\nIntroduce the concept of Directed Acyclic Graphs to describe causal relationships\nDiscuss three approaches to covariate adjustment\n\nSubclassification\nMatching\nLinear Regression\n\nThree research designs for observational data\n\nDifferences-in-Differences\nRegression Discontinuity Designs\nInstrumental Variables"
  },
  {
    "objectID": "slides/05-slides.html#two-ways-to-describe-causal-claims",
    "href": "slides/05-slides.html#two-ways-to-describe-causal-claims",
    "title": "POLS 1600",
    "section": "Two Ways to Describe Causal Claims",
    "text": "Two Ways to Describe Causal Claims\nIn this course, we will use two forms of notation to describe our causal claims.\n\nPotential Outcomes Notation (last lecture)\n\nIllustrates the fundamental problem of causal inference\n\nDirected Acyclic Graphs (DAGs)\n\nIllustrates potential bias from confounders and colliders"
  },
  {
    "objectID": "slides/05-slides.html#directed-acyclic-graphs-1",
    "href": "slides/05-slides.html#directed-acyclic-graphs-1",
    "title": "POLS 1600",
    "section": "Directed Acyclic Graphs",
    "text": "Directed Acyclic Graphs\n\nDirected Acyclic Graphs provide a way of encoding assumptions about casual relationships\n\nDirected Arrows \\(\\to\\) describe a direct causal effect\nArrow from \\(D\\to Y\\) means \\(Y_i(d) \\neq Y_i(d^\\prime)\\) “The outcome ( \\(Y\\)) for person \\(i\\) when D happens ( \\(Y_i(d)\\) ) is different than the the outcome when \\(D\\) doesn’t happen ( \\(Y_i(d^\\prime)\\) )\nNo arrow = no effect ( \\(Y_i(d) = Y_i(d^\\prime)\\) )\nAcyclic: No cycles. A variable can’t cause itself"
  },
  {
    "objectID": "slides/05-slides.html#types-of-variables-in-a-dag",
    "href": "slides/05-slides.html#types-of-variables-in-a-dag",
    "title": "POLS 1600",
    "section": "Types of variables in a DAG",
    "text": "Types of variables in a DAG\n\nDAGVariables\n\n\n\nBlair, Coppock, and Humphreys (2023) (Chap. 6.2)\n\n\n\nCausal Explanations Involve:\n\nY our outcome\nD A possible cause of Y\nM A mediator or mechanism through which D effects Y\nZ An instrument that can help us isolate the effects of D on `Y\nX2 a covariate that may moderate the effect of D on Y\n\nThreats to causal claims/Sources of bias:\n\nX1 an observed confounder that is a common cause of both D & Y\nU an unobserved confounder a common cause of both D & Y\nK a collider that is a common consequence of both D & Y"
  },
  {
    "objectID": "slides/05-slides.html#dags-illustrate-two-sources-of-bias",
    "href": "slides/05-slides.html#dags-illustrate-two-sources-of-bias",
    "title": "POLS 1600",
    "section": "DAGs illustrate two sources of bias:",
    "text": "DAGs illustrate two sources of bias:\n\n\nConfounder bias: Failing to control for a common cause of D and Y (aka Omitted Variable Bias)\nCollider bias: Controlling for a common consequence (aka Selection Bias1)\n\n\nNote in practice there’s some slippage/debate/disagreement around this nomenclature"
  },
  {
    "objectID": "slides/05-slides.html#section-1",
    "href": "slides/05-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Confounding Bias: The Coffee Example\n\nConfounding BiasCoffee and CancerAdjusting for Smoking\n\n\n\n\nDrinking coffee doesn’t cause lung cancer we might find correlation between them because they share a common cause: smoking.\nSmoking is a [confounding] variable, that if omitted will bias our results producing a spurious relationsip\n[Adjusting] for [confounders] removes this source of bias"
  },
  {
    "objectID": "slides/05-slides.html#section-2",
    "href": "slides/05-slides.html#section-2",
    "title": "POLS 1600",
    "section": "",
    "text": "Collider Bias: The Dating Example\n\nCollider biasSelection biasNo relationship in population\n\n\n\n\nWhy are attractive people such jerks?\nSuppose dating is a function of looks and personality\nDating is a common consequences of looks and personality\nBasing our claim off of who we date is an example of selection bias created by controlling for collider"
  },
  {
    "objectID": "slides/05-slides.html#when-to-control-for-a-variable",
    "href": "slides/05-slides.html#when-to-control-for-a-variable",
    "title": "POLS 1600",
    "section": "When to control for a variable:",
    "text": "When to control for a variable:\n\n(Blair, Coppock, and Humphreys 2023) (Chap. 6.2)"
  },
  {
    "objectID": "slides/05-slides.html#covariate-adjustment-1",
    "href": "slides/05-slides.html#covariate-adjustment-1",
    "title": "POLS 1600",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment\nCovariate adjustment refers a broad class of procedures that try to make a comparison more credible or meaningful by adjusting for some other potentially confounding factor."
  },
  {
    "objectID": "slides/05-slides.html#covariate-adjustment-2",
    "href": "slides/05-slides.html#covariate-adjustment-2",
    "title": "POLS 1600",
    "section": "Covariate Adjustment",
    "text": "Covariate Adjustment\nWhen you hear people talk about\n\nControlling for age\nConditional on income\nHolding age and income constant\nCeteris paribus (All else equal)\n\nThey are typically talking about some sort of covariate adjustment."
  },
  {
    "objectID": "slides/05-slides.html#three-approaches-to-covariate-adjustment",
    "href": "slides/05-slides.html#three-approaches-to-covariate-adjustment",
    "title": "POLS 1600",
    "section": "Three approaches to covariate adjustment",
    "text": "Three approaches to covariate adjustment\n\nSubclassification\n\n👍: Easy to implement and interpret\n👎: Curse of dimensionality, Selection on observables\n\nMatching\n\n👍: Balance on multiple covariates, Mirrors logic of experimental design\n👎: Selection on observables, Only provides balance on observed variables, Lot’s of technical details…\n\nRegression\n\n👍: Easy to implement, control for many factors (good and bad)\n👎: Selection on observables, easy to fit “bad” models"
  },
  {
    "objectID": "slides/05-slides.html#understanding-linear-regression",
    "href": "slides/05-slides.html#understanding-linear-regression",
    "title": "POLS 1600",
    "section": "Understanding Linear Regression",
    "text": "Understanding Linear Regression\n\n\nConceptual\n\nSimple linear regression estimates “a line of best fit” that summarizes relationships between two variables\n\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n\\]\n\nPractical\n\nWe estimate linear models in R using the lm() function\n\n\n\nlm(y ~ x, data = df)"
  },
  {
    "objectID": "slides/05-slides.html#understanding-linear-regression-1",
    "href": "slides/05-slides.html#understanding-linear-regression-1",
    "title": "POLS 1600",
    "section": "Understanding Linear Regression",
    "text": "Understanding Linear Regression\n\n\nTechnical/Definitional\n\nLinear regression chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]\n\nTheoretical\n\nLinear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\)"
  },
  {
    "objectID": "slides/05-slides.html#conceptual-linear-regression-1",
    "href": "slides/05-slides.html#conceptual-linear-regression-1",
    "title": "POLS 1600",
    "section": "Conceptual: Linear Regression",
    "text": "Conceptual: Linear Regression\n\nRegression is a tool for describing relationships.\n\nHow does some outcome we’re interested in tend to change as some predictor of that outcome changes?\nHow does economic development vary with democracy?\nHow does economic development vary with democracy, adjusting for natural resources like oil and gas"
  },
  {
    "objectID": "slides/05-slides.html#conceptual-linear-regression-2",
    "href": "slides/05-slides.html#conceptual-linear-regression-2",
    "title": "POLS 1600",
    "section": "Conceptual: Linear Regression",
    "text": "Conceptual: Linear Regression\n\nMore formally:\n\\[\ny_i = f(x_i) + \\epsilon\n\\]\n\nY is a function of X plus some error, \\(\\epsilon\\)\nLinear regression assumes that relationship between an outcome and a predictor can be by a linear function\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-and-the-line-of-best-fit",
    "href": "slides/05-slides.html#linear-regression-and-the-line-of-best-fit",
    "title": "POLS 1600",
    "section": "Linear Regression and the Line of Best Fit",
    "text": "Linear Regression and the Line of Best Fit\n\n\nThe goal of linear regression is to choose coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to summarizes the relationship between \\(y\\) and \\(x\\)\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\n\nTo accomplish this we need some sort of criteria.\nFor linear regression, that criteria is minimizing the error between what our model predicts \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\) and what we actually observed \\((y_i)\\)\nMore on this to come. But first…"
  },
  {
    "objectID": "slides/05-slides.html#regression-notation",
    "href": "slides/05-slides.html#regression-notation",
    "title": "POLS 1600",
    "section": "Regression Notation",
    "text": "Regression Notation\n\n\\(y_i\\) an outcome variable or thing we’re trying to explain\n\nAKA: The dependent variable, The response Variable, The left hand side of the model\n\n\\(x_i\\) a predictor variables or things we think explain variation in our outcome\n\nAKA: The independent variable, covariates, the right hand side of the model.\nCap or No Cap: I’ll use \\(X\\) (should be \\(\\mathbf{X}\\)) to denote a set (matrix) of predictor variables. \\(y\\) vs \\(Y\\) can also have technical distinctions (Sample vs Population, observed value vs Random Variable, …)\n\n\\(\\beta\\) a set of unknown parameters that describe the relationship between our outcome \\(y_i\\) and our predictors \\(x_i\\)\n\\(\\epsilon\\) the error term representing variation in \\(y_i\\) not explained by our model."
  },
  {
    "objectID": "slides/05-slides.html#linear-regression",
    "href": "slides/05-slides.html#linear-regression",
    "title": "POLS 1600",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\nWe call this a bivariate regression, because there are only two variables\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\n\nWe call this a linear regression, because \\(y_i = \\beta_0 + \\beta_1 x_i\\) is the equation for a line, where:\n\n\\(\\beta_0\\) corresponds to the \\(y\\) intercept, or the model’s prediction when \\(x = 0\\).\n\\(\\beta_1\\) corresponds to the slope, or how \\(y\\) is predicted to change as \\(x\\) changes."
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-1",
    "href": "slides/05-slides.html#linear-regression-1",
    "title": "POLS 1600",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\nIf you find this notation confusing, try plugging in substantive concepts for what \\(y\\) and \\(x\\) represent\nSay we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.\n\nThe generic linear model\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon\\]\nReflects:\n\\[\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\]"
  },
  {
    "objectID": "slides/05-slides.html#practical-estimating-a-linear-regression-1",
    "href": "slides/05-slides.html#practical-estimating-a-linear-regression-1",
    "title": "POLS 1600",
    "section": "Practical: Estimating a Linear Regression",
    "text": "Practical: Estimating a Linear Regression\n\n\nWe estimate linear regressions in R using the lm() function.\nlm() requires two arguments:\n\na formula argument of the general form y ~ x read as “Y modeled by X” or below “Transgender Feeling Thermometer (y) modeled by (~) Age (x)\na data argument telling R where to find the variables in the formula\n\n\n\nload(url(\"https://pols1600.paultesta.org/files/data/03_lab.rda\"))\nm1 &lt;- lm(therm_trans_t0 ~ vf_age, data = df)\nm1\n\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031"
  },
  {
    "objectID": "slides/05-slides.html#the-lm-function",
    "href": "slides/05-slides.html#the-lm-function",
    "title": "POLS 1600",
    "section": "The lm() function",
    "text": "The lm() function\n\nThe coefficients from lm() are saved in object called m1\n\nm1\n\n\nCall:\nlm(formula = therm_trans_t0 ~ vf_age, data = df)\n\nCoefficients:\n(Intercept)       vf_age  \n    62.8196      -0.2031  \n\n\nm1 actually contains a lot of information\n\nnames(m1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\nm1$coefficients\n\n(Intercept)      vf_age \n 62.8195994  -0.2030711"
  },
  {
    "objectID": "slides/05-slides.html#practical-interpreting-a-linear-regression",
    "href": "slides/05-slides.html#practical-interpreting-a-linear-regression",
    "title": "POLS 1600",
    "section": "Practical: Interpreting a Linear Regression",
    "text": "Practical: Interpreting a Linear Regression\nWe can extract the intercept and slope from this simple bivariate model, using the coef() function\n\n# All the coefficients\ncoef(m1)\n\n(Intercept)      vf_age \n 62.8195994  -0.2030711 \n\n# Just the intercept\ncoef(m1)[1]\n\n(Intercept) \n    62.8196 \n\n# Just the slope\ncoef(m1)[2]\n\n    vf_age \n-0.2030711"
  },
  {
    "objectID": "slides/05-slides.html#practical-interpreting-a-linear-regression-1",
    "href": "slides/05-slides.html#practical-interpreting-a-linear-regression-1",
    "title": "POLS 1600",
    "section": "Practical: Interpreting a Linear Regression",
    "text": "Practical: Interpreting a Linear Regression\nThe two coefficients from m1 define a line of best fit, summarizing how feelings toward transgender individuals change with age\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon\\]\n\\[\\text{Transgender Feeling Thermometer}_i = \\beta_0 + \\beta_1\\text{Age}_i + \\epsilon_i\\]\n\\[\\text{Transgender Feeling Thermometer}_i = 62.82 + -0.2 \\text{Age}_i + \\epsilon_i\\]"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\n\n\nOften it’s useful for interpretation to obtain predicted values from a regression.\nTo obtain predicted vales \\((\\hat{y})\\), we simply plug in a value for \\(x\\) (In this case, \\(Age\\)) and evaluate our equation.\nFor example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?\n\n\\[\\hat{FT}_{x=18} = 62.82 + -0.2 \\times 18  = 59.16\\] \\[\\hat{FT}_{x=65} = 62.82 + -0.2 \\times 68  = 49.01\\]"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-1",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-1",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\nWe could do this by hand\n\ncoef(m1)[1] + coef(m1)[2] * 18\n\n(Intercept) \n   59.16432 \n\ncoef(m1)[1] + coef(m1)[2] * 68\n\n(Intercept) \n   49.01076"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-2",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-2",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\nMore often we will:\n\nMake a prediction data frame (called pred_df below) with the values of interests\nUse the predict() function with our linear model (m1) and pred_df\nSave the predicted values to our new column in our prediction data frame"
  },
  {
    "objectID": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-3",
    "href": "slides/05-slides.html#practical-predicted-values-from-a-linear-regression-3",
    "title": "POLS 1600",
    "section": "Practical: Predicted values from a Linear Regression",
    "text": "Practical: Predicted values from a Linear Regression\n\n# Make prediction data frame\npred_df &lt;- data.frame(\n  vf_age = c(18, 68)\n)\n# Predict FT for 18 and 68 year-olds\npredict(m1, newdata = pred_df)\n\n       1        2 \n59.16432 49.01076 \n\n# Save predictions to data frame\npred_df$ft_trans_hat &lt;- predict(m1, newdata = pred_df)\npred_df\n\n  vf_age ft_trans_hat\n1     18     59.16432\n2     68     49.01076"
  },
  {
    "objectID": "slides/05-slides.html#section-3",
    "href": "slides/05-slides.html#section-3",
    "title": "POLS 1600",
    "section": "",
    "text": "Practical: Visualizing Linear Regression\n\nConceptCodeInterceptSlopeErrors\n\n\nWe can visualize simple regression by:\n\nplotting a scatter plot of the outcome (y-axis) and predictors (x-axis)\noverlaying the line defined by lm()\n\n\n\n\nfig_lm &lt;- df %&gt;%\n  ggplot(aes(vf_age,therm_trans_t0))+\n  geom_point(size=.5, alpha=.5)+\n  geom_abline(intercept = coef(m1)[1],\n              slope = coef(m1)[2],\n              col = \"blue\"\n              )+\n  geom_vline(xintercept = 0,linetype = 2)+\n  xlim(0,100)+\n  annotate(\"point\",\n           x = 0, y = coef(m1)[1],\n           col= \"red\",\n           )+\n  annotate(\"text\",\n           label = expression(paste(beta[0],\"= 62.81\" )),\n           x = 1, y = coef(m1)[1]+5,\n           hjust = \"left\",\n           )+\n  labs(\n    x = \"Age\",\n    y = \"Feeling Thermometer toward\\nTransgender People\"\n  )+\n  theme_classic() -&gt; fig_lm"
  },
  {
    "objectID": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1",
    "href": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1",
    "title": "POLS 1600",
    "section": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\nP: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression\nQ: Ok, that’s not really that helpful…\n\nWhat’s an error?\nWhy would we square and sum them\nHow do we minimize them.\n\n\nP: Good questions!"
  },
  {
    "objectID": "slides/05-slides.html#whats-an-error",
    "href": "slides/05-slides.html#whats-an-error",
    "title": "POLS 1600",
    "section": "What’s an error?",
    "text": "What’s an error?\nAn error, \\(\\epsilon_i\\) is simply the difference between the observed value of \\(y_i\\) and what our model would predict, \\(\\hat{y_i}\\) given some value of \\(x_i\\). So for a model:\n\\[y_i=\\beta_0+\\beta_1 x_{i} + \\epsilon_i\\]\nWe simply subtract our model’s prediction \\(\\beta_0+\\beta_1 x_{i}\\) from the the observed value, \\(y_i\\)\n\\[\\hat{\\epsilon_i}=y_i-\\hat{y_i}=(Y_i-(\\beta_0+\\beta_1 x_{i}))\\]\nTo get \\(\\epsilon_i\\)"
  },
  {
    "objectID": "slides/05-slides.html#why-are-we-squaring-and-summing-epsilon",
    "href": "slides/05-slides.html#why-are-we-squaring-and-summing-epsilon",
    "title": "POLS 1600",
    "section": "Why are we squaring and summing \\(\\epsilon\\)",
    "text": "Why are we squaring and summing \\(\\epsilon\\)\n\nThere are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)\n\nSquaring \\(\\epsilon\\) treats positive and negative residuals equally.\nSumming produces single value summarizing our models overall performance.\n\nThere are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties"
  },
  {
    "objectID": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2",
    "href": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2",
    "title": "POLS 1600",
    "section": "How do we minimize \\(\\sum \\epsilon^2\\)",
    "text": "How do we minimize \\(\\sum \\epsilon^2\\)\nOLS chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize \\(\\sum \\epsilon^2\\), the Sum of Squared Residuals (SSR)\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]"
  },
  {
    "objectID": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1-1",
    "href": "slides/05-slides.html#how-did-lm-choose-beta_0-and-beta_1-1",
    "title": "POLS 1600",
    "section": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "How did lm() choose \\(\\beta_0\\) and \\(\\beta_1\\)\nIn an intro stats course, we would walk through the process of finding\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\] Which involves a little bit of calculus. The big payoff is that\n\\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\\] And\n\\[ \\beta_1 = \\frac{Cov(x,y)}{Var(x)}\\] Which is never quite the epiphany, I think we think it is…\nThe following slides walk you through the mechanics of this exercise. We’re gonna skip through them in class, but they’re there for your reference"
  },
  {
    "objectID": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2-1",
    "href": "slides/05-slides.html#how-do-we-minimize-sum-epsilon2-1",
    "title": "POLS 1600",
    "section": "How do we minimize \\(\\sum \\epsilon^2\\)",
    "text": "How do we minimize \\(\\sum \\epsilon^2\\)\nTo understand what’s going on under the hood, you need a broad understanding of some basic calculus.\nThe next few slides provide a brief review of derivatives and differential calculus."
  },
  {
    "objectID": "slides/05-slides.html#derivatives",
    "href": "slides/05-slides.html#derivatives",
    "title": "POLS 1600",
    "section": "Derivatives",
    "text": "Derivatives\n\nThe derivative of \\(f\\) at \\(x\\) is its rate of change at \\(x\\)\n\nFor a line: the slope\nFor a curve: the slope of a line tangent to the curve\n\nYou’ll see two notations for derivatives:\n\nLeibniz notation:\n\n\\[\n\\frac{df}{dx}(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{(x+h)-x}\n\\]\n\nLagrange: \\(f^{\\prime}(x)\\)"
  },
  {
    "objectID": "slides/05-slides.html#some-useful-facts-about-derivatives",
    "href": "slides/05-slides.html#some-useful-facts-about-derivatives",
    "title": "POLS 1600",
    "section": "Some useful Facts about Derivatives",
    "text": "Some useful Facts about Derivatives\nDerivative of a constant\n\\[\nf^{\\prime}(c)=0\n\\]\nDerivative of a line f(x)=2x\n\\[\nf^{\\prime}(2x)=2\n\\]\nDerivative of \\(f(x)=x^2\\)\n\\[\nf^{\\prime}(x^2)=2x\n\\]\nChain rule: y= f(g(x)). The derivative of y with respect to x is\n\\[\n\\frac{d}{dx}(f(g(x)))=f^{\\prime}(g(x))g^{\\prime}(x)\n\\]\nThe derivative of the “outside” times the derivative of the “inside,” remembering that the derivative of the outside function is evaluated at the value of the inside function."
  },
  {
    "objectID": "slides/05-slides.html#finding-a-local-minimums",
    "href": "slides/05-slides.html#finding-a-local-minimums",
    "title": "POLS 1600",
    "section": "Finding a Local Minimums",
    "text": "Finding a Local Minimums\nLocal minimum:\n\\[\nf^{\\prime}(x)=0 \\text{ and } f^{\\prime\\prime}(x)&gt;0\n\\]\n\nSource"
  },
  {
    "objectID": "slides/05-slides.html#partial-derivatives",
    "href": "slides/05-slides.html#partial-derivatives",
    "title": "POLS 1600",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\nLet \\(f\\) be a function of the variables \\((x, \\dots, X_n)\\). The partial derivative of \\(f\\) with respect to \\(X_i\\) is\n\\[\\begin{align*}\n\\frac{\\partial f(x, \\dots, X_n)}{\\partial X_i}=\\lim_{h\\to0}\\frac{f(x, \\dots X_i+h \\dots, X_n)-f(x, \\dots X_i \\dots, X_n)}{h}\n\\end{align*}\\]\n\nSource"
  },
  {
    "objectID": "slides/05-slides.html#minimizing-the-sum-of-squared-errors",
    "href": "slides/05-slides.html#minimizing-the-sum-of-squared-errors",
    "title": "POLS 1600",
    "section": "Minimizing the sum of squared errors",
    "text": "Minimizing the sum of squared errors\nOur model\n\\[y_i =\\beta_0+\\beta_1x_{i}+\\epsilon_i\\]\nFinds coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to to minimize the sum of squared residuals, \\(\\hat{\\epsilon}_i\\):\n\\[\\begin{aligned}\n\\sum \\hat{\\epsilon_i}^2 &= \\sum (y_i-\\beta_0-\\beta_1 x_{i})^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#minimizing-the-sum-of-squared-errors-1",
    "href": "slides/05-slides.html#minimizing-the-sum-of-squared-errors-1",
    "title": "POLS 1600",
    "section": "Minimizing the sum of squared errors",
    "text": "Minimizing the sum of squared errors\nWe solve for \\(\\beta_0\\) and \\(\\beta_1\\), by taking the partial derivatives with respect to \\(\\beta_0\\) and \\(\\beta_1\\), and setting them equal to zero\n\\[\\begin{aligned}\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial \\beta_0} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})=0 & f'(-x^2) = -2x\\\\\n\\frac{\\partial \\sum \\hat{\\epsilon_i}^2}{\\partial\\beta_1} &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})x_{i}=0 & \\text{chain rule}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_0",
    "href": "slides/05-slides.html#solving-for-beta_0",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_0\\)",
    "text": "Solving for \\(\\beta_0\\)\nFirst, we’ll solve for \\(\\beta_0\\), by multiplying both sides by -1/2 and distributing the \\(\\sum\\):\n\\[\\begin{aligned}\n0 &= -2\\sum (y_i-\\beta_0-\\beta_1 x_{i})\\\\\n\\sum \\beta_0 &= \\sum y_i - \\sum \\beta_1 x_{i}\\\\\nN \\beta_0 &= \\sum y_i -\\sum \\beta_1 x_{i}\\\\\n\\beta_0 &= \\frac{\\sum y_i}{N} - \\frac{\\beta_1 \\sum x_{i}}{N}\\\\\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_1",
    "href": "slides/05-slides.html#solving-for-beta_1",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_1\\)",
    "text": "Solving for \\(\\beta_1\\)\nNow, we can solve for \\(\\beta_1\\) plugging in \\(\\beta_0\\).\n\\[\\begin{aligned}\n0 &= -2\\sum [(y_i-\\beta_0-\\beta_1 x_{i})x_{i}]\\\\\n0 &= \\sum [y_ix_i-(\\bar{y} - \\beta_1 \\bar{x})x_{i}-\\beta_1 x_{i}^2]\\\\\n0 &= \\sum [y_ix_i-\\bar{y}x_{i} + \\beta_1 \\bar{x}x_{i}-\\beta_1 x_{i}^2]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_1-1",
    "href": "slides/05-slides.html#solving-for-beta_1-1",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_1\\)",
    "text": "Solving for \\(\\beta_1\\)\nNow we’ll rearrange some terms and pull out an \\(x_{i}\\) to get\n\\[\\begin{aligned}\n0 &= \\sum [(y_i -\\bar{y} + \\beta_1 \\bar{x}-\\beta_1 x_{i})x_{i}]\n\\end{aligned}\\]\nDividing both sides by \\(x_{i}\\) and distributing the summation, we can isolate \\(\\beta_1\\)\n\\[\\begin{aligned}\n\\beta_1 \\sum (x_{i}-\\bar{x}) &= \\sum (y_i -\\bar{y})\n\\end{aligned}\\]\nDividing by \\(\\sum (x_{i}-\\bar{x})\\) to get\n\\[\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})}{\\sum (x_{i}-\\bar{x})}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slides.html#solving-for-beta_1-2",
    "href": "slides/05-slides.html#solving-for-beta_1-2",
    "title": "POLS 1600",
    "section": "Solving for \\(\\beta_1\\)",
    "text": "Solving for \\(\\beta_1\\)\nFinally, by multiplying by \\(\\frac{(x_{i}-\\bar{x})}{(x_{i}-\\bar{x})}\\) we get\n\\[\\begin{aligned}\n\\beta_1  &= \\frac{\\sum (y_i -\\bar{y})(x_{i}-\\bar{x})}{\\sum (\\bar{x}-x_{i})^2}\n\\end{aligned}\\]\nWhich has a nice interpretation:\n\\[\\begin{aligned}\n\\beta_1 &= \\frac{Cov(x,y)}{Var(x)}\n\\end{aligned}\\]\nSo the coefficient in a simple linear regression of \\(Y\\) on \\(X\\) is simply the ratio of the covariance between \\(X\\) and \\(Y\\) over the variance of \\(X\\). Neat!"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing",
    "href": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing",
    "title": "POLS 1600",
    "section": "Linear Regression is a many splendored thing",
    "text": "Linear Regression is a many splendored thing\nTimothy Lin provides a great overview of the various interpretations/motivations for linear regression.\n\nA least squares estimator\nA linear projection of \\(y\\) on the subspace spanned by \\(X\\beta\\)\nA method of moments estimator\nA maximum likelihood estimator\nA singular vector decomposition\nA linear approximation of the conditional expectation function"
  },
  {
    "objectID": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing-1",
    "href": "slides/05-slides.html#linear-regression-is-a-many-splendored-thing-1",
    "title": "POLS 1600",
    "section": "Linear Regression is a many splendored thing",
    "text": "Linear Regression is a many splendored thing\n\nTimothy Lin provides a great overview of various interpretations/motivations for linear regression.\n\nA least squares estimator\nA linear projection of \\(y\\) on the subspace spanned by \\(X\\beta\\)\nA method of moments estimator\nA maximum likelihood estimator\nA singular vector decomposition\nA linear approximation of the conditional expectation function"
  },
  {
    "objectID": "slides/05-slides.html#the-conditional-expectation-function",
    "href": "slides/05-slides.html#the-conditional-expectation-function",
    "title": "POLS 1600",
    "section": "The Conditional Expectation Function",
    "text": "The Conditional Expectation Function\nOf all the functions we could choose to describe the relationship between \\(Y\\) and \\(X\\),\n\\[\nY_i = f(X_i) + \\epsilon_i\n\\]\nthe conditional expectation of \\(Y\\) given \\(X\\) \\((E[Y|X])\\), has some appealing properties\n\\[\nY_i = E[Y_i|X_i] + \\epsilon\n\\]\nThe error, by definition, is uncorrelated with X and \\(E[\\epsilon|X]=0\\)\n\\[\nE[\\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0\n\\]\nOf all the possible functions \\(g(X)\\), we can show that \\(E[Y_i|X_i]\\) is the best predictor in terms of minimizing mean squared error\n\\[\nE[ (Y - g(Y))^2] \\geq E[(Y - E[Y|X])^2]\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#section-4",
    "href": "slides/05-slides.html#section-4",
    "title": "POLS 1600",
    "section": "",
    "text": "Linear Approximations to the Conditional Expectation Function\n\nConceptCEFOLS\n\n\n\n\nWe can then show (in a different class) that linear regression provides the best linear predictor of the CEF\n\nChapter 3, of Mostly Harmless Econometrics\nChapter 4 of Foundations of Agnostic Statistics\n\nFurthermore, when the CEF is linear, it’s equal exactly to OLS regression"
  },
  {
    "objectID": "slides/05-slides.html#what-you-need-to-know-about-regression",
    "href": "slides/05-slides.html#what-you-need-to-know-about-regression",
    "title": "POLS 1600",
    "section": "What you need to know about Regression",
    "text": "What you need to know about Regression\n\n\nConceptual\n\nSimple linear regression estimates a line of best fit that summarizes relationships between two variables\n\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n\\]\n\nPractical\n\nWe estimate linear models in R using the lm() function\n\n\n\nlm(y ~ x, data = df)"
  },
  {
    "objectID": "slides/05-slides.html#what-you-need-to-know-about-regression-1",
    "href": "slides/05-slides.html#what-you-need-to-know-about-regression-1",
    "title": "POLS 1600",
    "section": "What you need to know about Regression",
    "text": "What you need to know about Regression\n\n\nTechnical/Definitional\n\nLinear regression chooses \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the Sum of Squared Residuals (SSR):\n\n\n\\[\\textrm{Find }\\hat{\\beta_0},\\,\\hat{\\beta_1} \\text{ arg min}_{\\beta_0, \\beta_1} \\sum (y_i-(\\beta_0+\\beta_1x_i))^2\\]\n\nTheoretical\n\nLinear regression provides a linear estimate of the conditional expectation function (CEF): \\(E[Y|X]\\)"
  },
  {
    "objectID": "slides/05-slides.html#motivating-example-what-causes-cholera",
    "href": "slides/05-slides.html#motivating-example-what-causes-cholera",
    "title": "POLS 1600",
    "section": "Motivating Example: What causes Cholera?",
    "text": "Motivating Example: What causes Cholera?\n\nIn the 1800s, cholera was thought to be transmitted through the air.\nJohn Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.\nLeveraged a natural experiment in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location."
  },
  {
    "objectID": "slides/05-slides.html#notation",
    "href": "slides/05-slides.html#notation",
    "title": "POLS 1600",
    "section": "Notation",
    "text": "Notation\nLet’s adopt a little notation to help us think about the logic of Snow’s design:\n\n\\(D\\): treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)\n\\(T\\): period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).\n\\(Y_{di}(t)\\) the potential outcome of unit \\(i\\)\n\n\\(Y_{1i}(t)\\) the potential outcome of unit \\(i\\) when treated between the two periods\n\\(Y_{0i}(t)\\) the potential outcome of unit \\(i\\) when control between the two periods"
  },
  {
    "objectID": "slides/05-slides.html#causal-effects",
    "href": "slides/05-slides.html#causal-effects",
    "title": "POLS 1600",
    "section": "Causal Effects",
    "text": "Causal Effects\nThe individual causal effect for unit i at time t is:\n\\[\\tau_{it} = Y_{1i}(t) − Y_{0i}(t)\\]\nWhat we observe is\n\\[Y_i(t) = Y_{0i}(t)\\cdot(1 − D_i(t)) + Y_{1i}(t)\\cdot D_i(t)\\]\n\\(D\\) only equals 1, when \\(T\\) equals 1, so we never observe \\(Y_0i(1)\\) for the treated units.\nIn words, we don’t know what Lambeth’s outcome would have been in the second period, had they not been treated."
  },
  {
    "objectID": "slides/05-slides.html#average-treatment-on-treated",
    "href": "slides/05-slides.html#average-treatment-on-treated",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nThat is, what would have happened in Lambeth, had their water company not moved their pipes"
  },
  {
    "objectID": "slides/05-slides.html#average-treatment-on-treated-1",
    "href": "slides/05-slides.html#average-treatment-on-treated-1",
    "title": "POLS 1600",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nOur goal is to estimate the average effect of treatment on treated (ATT):\nWe we can observe is:\n\n\n\n\n\n\n\n\n\nPre-Period (T=0)\nPost-Period (T=1)\n\n\n\n\nTreated \\(D_{i}=1\\)\n\\(E[Y_{0i}(0)\\vert D_i = 1]\\)\n\\(E[Y_{1i}(1)\\vert D_i = 1]\\)\n\n\nControl \\(D_i=0\\)\n\\(E[Y_{0i}(0)\\vert D_i = 0]\\)\n\\(E[Y_{0i}(1)\\vert D_i = 0]\\)"
  },
  {
    "objectID": "slides/05-slides.html#data",
    "href": "slides/05-slides.html#data",
    "title": "POLS 1600",
    "section": "Data",
    "text": "Data\nBecause potential outcomes notation is abstract, let’s consider a modified description of the Snow’s cholera death data from Scott Cunningham:\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/05-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "href": "slides/05-slides.html#how-can-we-estimate-the-effect-of-moving-pumps-upstream",
    "title": "POLS 1600",
    "section": "How can we estimate the effect of moving pumps upstream?",
    "text": "How can we estimate the effect of moving pumps upstream?\nRecall, our goal is to estimate the effect of the the treatment on the treated:\n\\[\\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]\\]\nLet’s conisder some strategies Snow could take to estimate this quantity:"
  },
  {
    "objectID": "slides/05-slides.html#before-vs-after-comparisons",
    "href": "slides/05-slides.html#before-vs-after-comparisons",
    "title": "POLS 1600",
    "section": "Before vs after comparisons:",
    "text": "Before vs after comparisons:\n\n\nSnow could have compared Labmeth in 1854 \\((E[Y_i(1)|D_i = 1] = 19)\\) to Lambeth in 1849 \\((E[Y_i(0)|D_i = 1]=85)\\), and claimed that moving the pumps upstream led to 66 fewer cholera deaths.\nAssumes Lambeth’s pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn’t moved \\((E[Y_{0i}(1)|D_i = 1])\\).\nA skeptic might argue that Lambeth in 1849 \\(\\neq\\) Lambeth in 1854\n\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/05-slides.html#treatment-control-comparisons-in-the-post-period.",
    "href": "slides/05-slides.html#treatment-control-comparisons-in-the-post-period.",
    "title": "POLS 1600",
    "section": "Treatment-Control comparisons in the Post Period.",
    "text": "Treatment-Control comparisons in the Post Period.\n\n\nSnow could have compared outcomes between Lambeth and S&V in 1954 (\\(E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]\\)), concluding that the change in pump locations led to 128 fewer deaths.\nHere the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved \\((E[Y_{0i}(1)|D_i = 1])\\)\nAgain, our skeptic could argue Lambeth \\(\\neq\\) S&V\n\n\n\n\n\n\n\nCompany\n1849 (T=0)\n1854 (T=1)\n\n\n\n\nLambeth (D=1)\n85\n19\n\n\nSouthwark and Vauxhall (D=0)\n135\n147"
  },
  {
    "objectID": "slides/05-slides.html#difference-in-differences-1",
    "href": "slides/05-slides.html#difference-in-differences-1",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nTo address these concerns, Snow employed what we now call a difference-in-differences design,\nThere are two, equivalent ways to view this design.\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\]\n\nDifference 1: Average change between Treated and Control in Post Period\nDifference 2: Average change between Treated and Control in Pre Period"
  },
  {
    "objectID": "slides/05-slides.html#difference-in-differences-2",
    "href": "slides/05-slides.html#difference-in-differences-2",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\\}}_{\\text{1. Treat-Control |Post }}− \\overbrace{\\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Treated-Control|Pre}}\\] Is equivalent to:\n\\[\\underbrace{\\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\\}}_{\\text{Post - Pre |Treated }}− \\overbrace{\\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\\text{Post-Pre|Control}}\\]\n\nDifference 1: Average change between Treated over time\nDifference 2: Average change between Control over time"
  },
  {
    "objectID": "slides/05-slides.html#difference-in-differences-3",
    "href": "slides/05-slides.html#difference-in-differences-3",
    "title": "POLS 1600",
    "section": "Difference in Differences",
    "text": "Difference in Differences\nYou’ll see the DiD design represented both ways, but they produce the same result:\n\\[\n\\tau_{ATT} = (19-147) - (85-135) = -78\n\\]\n\\[\n\\tau_{ATT} = (19-85) - (147-135) = -78\n\\]"
  },
  {
    "objectID": "slides/05-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "href": "slides/05-slides.html#identifying-assumption-of-a-difference-in-differences-design",
    "title": "POLS 1600",
    "section": "Identifying Assumption of a Difference in Differences Design",
    "text": "Identifying Assumption of a Difference in Differences Design\nThe key assumption in this design is what’s known as the parallel trends assumption: \\(E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]\\)\n\nIn words: If Lambeth hadn’t moved its pumps, it would have followed a similar path as S&V"
  },
  {
    "objectID": "slides/05-slides.html#parralel-trends",
    "href": "slides/05-slides.html#parralel-trends",
    "title": "POLS 1600",
    "section": "Parralel Trends",
    "text": "Parralel Trends"
  },
  {
    "objectID": "slides/05-slides.html#summary",
    "href": "slides/05-slides.html#summary",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nA Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison\n\nTaking the pre-post difference removes any fixed differences between the units\nThen taking the difference between treated and control differences removes any common differences over time\n\nThe key identifying assumption of a DiD design is the “assumption of parallel trends”\n\nAbsent treatment, treated and control groups would see the same changes over time.\nHard to prove, possible to test"
  },
  {
    "objectID": "slides/05-slides.html#extensions-and-limitations",
    "href": "slides/05-slides.html#extensions-and-limitations",
    "title": "POLS 1600",
    "section": "Extensions and limitations",
    "text": "Extensions and limitations\n\nDiff-in-Diff easy to estimate with linear regression\nGeneralizes to multiple periods and treatment interventions\n\nMore pre-treatment periods allow you assess “parallel trends” assumption\n\nAlternative methods\n\nSynthetic control\nEvent Study Designs\n\nWhat if you have multiple treatments or treatments that come and go?\n\nPanel Matching\nGeneralized Synthetic control"
  },
  {
    "objectID": "slides/05-slides.html#applications",
    "href": "slides/05-slides.html#applications",
    "title": "POLS 1600",
    "section": "Applications",
    "text": "Applications\n\nCard and Krueger (1994) What effect did raising the minimum wage in NJ have on employment\nAbadie, Diamond, & Hainmueller (2014) What effect did German Unification have on economic development in West Germany\nMalesky, Nguyen and Tran (2014) How does decentralization influence public services?"
  },
  {
    "objectID": "slides/05-slides.html#references",
    "href": "slides/05-slides.html#references",
    "title": "POLS 1600",
    "section": "References",
    "text": "References\n\n\n\n\nPOLS 1600\n\n\n\n\nBlair, Graeme, Alexander Coppock, and Macartan Humphreys. 2023. Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign. Princeton University Press."
  },
  {
    "objectID": "slides/11-slides.html#general-plan",
    "href": "slides/11-slides.html#general-plan",
    "title": "Week 11:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nReview: Confidence Intervals\nLecture : Hypothesis Testing\nDemo: Final Projects"
  },
  {
    "objectID": "slides/11-slides.html#course-plan",
    "href": "slides/11-slides.html#course-plan",
    "title": "Week 11:",
    "section": "Course Plan",
    "text": "Course Plan\n\nApril 18: Lecture – Hypothesis Testing\nApril 20 Lab – Hypothesis Testing and Interval Estimation\nApril 25: Lecture – Course Review\nApril 27: Workshop:\nApril 30: Take Home Final Exam\nMay ?: Tacos or Pizza with POLS 1140?\nMay 7: Take Home Final Exam due\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/11-slides.html#new-packages",
    "href": "slides/11-slides.html#new-packages",
    "title": "Week 11:",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anser)"
  },
  {
    "objectID": "slides/11-slides.html#packages-for-today",
    "href": "slides/11-slides.html#packages-for-today",
    "title": "Week 11:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"modelr\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 11:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/11-slides.html#load-packages-for-today",
    "href": "slides/11-slides.html#load-packages-for-today",
    "title": "Week 11:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         purrr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign        modelr           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, middle, center # 🔍 # Review ## Confidence Intervals"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals",
    "href": "slides/11-slides.html#confidence-intervals",
    "title": "Week 11:",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nStatistical inference involves quantifying uncertainty about what could have happened\nWe can describe this uncertainty in terms of a sampling distribution (what could have happened had we had a different sample)\nThe standard deviation of a sampling distribution describes its width (spread) and is called a standard error\nStandard errors decrease with by the \\(\\sqrt{N}\\) where \\(N\\) is the size of our sample\nWe can estimate standard errors via simulation or analytically.\n\nSimulations require fewer assumptions, but take more time.\nAnalytic estimates are are quick, but require more assumptions.\n\nWe use standard errors to construct confidence intervals which we interpret as describing a range of plausible values for the thing we’re trying to estimate.\nAny one 95% confidence interval may or may not contain the truth, but in repeated sampling, 95% of the intervals we construct would contain the truth."
  },
  {
    "objectID": "slides/11-slides.html#standard-errors-of-regression-coefficients",
    "href": "slides/11-slides.html#standard-errors-of-regression-coefficients",
    "title": "Week 11:",
    "section": "Standard Errors of Regression Coefficients",
    "text": "Standard Errors of Regression Coefficients\nLast class, we calculated the standard error of a sample mean using bootstrapping.\nWe can take the same approach for regression coefficients:\n\nload(url(\"https://pols1600.paultesta.org/files/data/df_drww.rda\"))\n\n# Fit model\nm1 &lt;- lm(support_war01 ~ age + education_n + sex, df_drww)\n# set seed\nset.seed(123)\n# 1,000 bootstrap samples\nboot &lt;- modelr::bootstrap(df_drww, 1000)\n# Estimate Boostrapped Models\nm1_bs &lt;- purrr::map(boot$strap, ~ lm(support_war01 ~ age + education_n + sex, data =.))\n# Tidy coefficients\nm1_bs_df &lt;- map_df(m1_bs, tidy, .id = \"id\")"
  },
  {
    "objectID": "slides/11-slides.html#bootstrapped-standard-errors",
    "href": "slides/11-slides.html#bootstrapped-standard-errors",
    "title": "Week 11:",
    "section": "Bootstrapped Standard Errors",
    "text": "Bootstrapped Standard Errors\nThen we simply calculate the standard error of for the sampling distribution of each coefficient\n\nm1_bs_df %&gt;%\n  group_by(term)%&gt;%\n  summarize(\n    bs_se = sd(estimate)\n  ) -&gt; m1_bs_se\nm1_bs_se\n\n# A tibble: 4 × 2\n  term           bs_se\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept) 0.0564  \n2 age         0.000717\n3 education_n 0.0108  \n4 sexMale     0.0223"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.",
    "href": "slides/11-slides.html#analytic-standard-errors.",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nAlternatively, we can derive the standard error of the sampling distribution analytically using asymptotic theory (e.g. the Central Limit Theorem).\nThe process starts by defining the quantity we want to know: the variance of our estimated coefficients \\(\\hat{\\beta}\\) around their true values in the population \\(\\beta\\):\n\\[V(\\hat{\\beta}|X) =(E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)' |X]\\]\nYour textbook walks through the math to estimate this quantity on pages 375-380 for simple bivariate regression (i.e. a regression with 1 predictor)\nIn the notes, you’ll find some further discussion of the math for the more general case of with multiple predictors.\nThis is also an excellent walk through of the linear algebra"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.-1",
    "href": "slides/11-slides.html#analytic-standard-errors.-1",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nEssentially, if you expand terms from:\n\\[V(\\hat{\\beta}|X) =(E[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)' |X]\\]\nmake substitutions and some assumptions about the distribution of \\(\\epsilon\\), you arrive at a formula for the “Variance Covariance Matrix” of the model:\n\\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] Which is a function of\n\n\\(\\sigma^2\\) the “Sum of Square Errors”\n\\((X'X)^{-1}\\) roughly captures the underlying variance and covariance of the predictors in your model, where:\n\n\\(X\\) is a matrix of predictors (each row is an observation, each column a variable)\n\\(X'X\\) is a symmetric maxtrix (like squaring a variable, but in linear algerba)\n\\(X'X^{-1}\\) is the inverse of this matrix\n\\(sigma^2(X'X)^{-1}\\) is like dividing \\(\\sigma^2\\) by \\(X'X\\)"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.-2",
    "href": "slides/11-slides.html#analytic-standard-errors.-2",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nThe square root of the elements on the diagonal of \\(V(\\hat{\\beta}|X)\\) provides the standard error for each coefficient, which is larger if:\n\nif \\(\\sigma^2\\) is large (When there’s a lot of unexplained variance, our uncertainty is high)\nthe variance of \\(X\\) is small (When predictors don’t vary much our uncertainty increases)"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors.-3",
    "href": "slides/11-slides.html#analytic-standard-errors.-3",
    "title": "Week 11:",
    "section": "Analytic Standard Errors.",
    "text": "Analytic Standard Errors.\nIn practice, you will let your computer calculate these standard errors.\n\nsummary(m1)$coef\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.277540588 0.0529197543  5.244555 1.797036e-07\nage          0.009216952 0.0007129459 12.927982 2.827291e-36\neducation_n -0.015814493 0.0108357258 -1.459477 1.446491e-01\nsexMale      0.094320479 0.0225358310  4.185356 3.017253e-05"
  },
  {
    "objectID": "slides/11-slides.html#analytic-standard-errors-by-hand",
    "href": "slides/11-slides.html#analytic-standard-errors-by-hand",
    "title": "Week 11:",
    "section": "Analytic Standard Errors by Hand",
    "text": "Analytic Standard Errors by Hand\nJust for fun:\n\n# Sum of Squared Residuals\nsigma_2 &lt;- sum(resid(m1)^2)/m1$df.residual\n# X transpose X\nXtXinv &lt;- solve(t(model.matrix(m1))%*%model.matrix(m1))\n# SE is square root of diagonal of Variance Covariance Matrix\nsqrt(diag(sigma_2*XtXinv))\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310 \n\nsummary(m1)$coef[,2]\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310"
  },
  {
    "objectID": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients",
    "href": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nEstimate the model to obtain coefficients \\(\\hat{\\beta}\\)\nCalculate Standard Errors using simulation or asymptotic theory\nChoose desired confidence level \\(\\alpha\\) with a corresponding critical value \\(z_{\\alpha/2}\\) derived from an approximation of the hypotehtical sampling distribution\nConstruct a \\((1-\\alpha)\\times 100 \\%\\) percent confidence interval:\n\n\\[CI(\\alpha) = [\\hat{\\beta} - z_{\\alpha/2} \\times \\text{standard error}, \\hat{\\beta} + z_{\\alpha/2} \\times \\text{standard error}]\\]\n\nEstimate the model to obtain coefficients \\(\\hat{\\beta}\\)\n\n\nbeta &lt;- coef(m1)\nbeta\n\n (Intercept)          age  education_n      sexMale \n 0.277540588  0.009216952 -0.015814493  0.094320479 \n\n\n\nCalculate Standard Errors using simulation or asymptotic theory\n\n\nse &lt;- summary(m1)$coef[,2]\nse\n\n (Intercept)          age  education_n      sexMale \n0.0529197543 0.0007129459 0.0108357258 0.0225358310 \n\n# Similar to bs\nm1_bs_se\n\n# A tibble: 4 × 2\n  term           bs_se\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept) 0.0564  \n2 age         0.000717\n3 education_n 0.0108  \n4 sexMale     0.0223"
  },
  {
    "objectID": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-1",
    "href": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-1",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nCalculate critical value\n\n\nz_fs &lt;- abs(qt(.05/2, m1$df.residual))\nz_fs\n\n[1] 1.961591\n\n\n\nNote: For finite samples we use a \\(t\\) distribution…"
  },
  {
    "objectID": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-2",
    "href": "slides/11-slides.html#constructing-confidence-intervals-for-regression-coefficients-2",
    "title": "Week 11:",
    "section": "Constructing Confidence Intervals for Regression Coefficients",
    "text": "Constructing Confidence Intervals for Regression Coefficients\n\nll &lt;- beta - z_fs *se\nul &lt;- beta + z_fs *se\ncbind(ll,ul)\n\n                      ll          ul\n(Intercept)  0.173733660 0.381347516\nage          0.007818443 0.010615460\neducation_n -0.037069758 0.005440772\nsexMale      0.050114390 0.138526568\n\n# Compare to R:\nconfint(m1)\n\n                   2.5 %      97.5 %\n(Intercept)  0.173733660 0.381347516\nage          0.007818443 0.010615460\neducation_n -0.037069758 0.005440772\nsexMale      0.050114390 0.138526568"
  },
  {
    "objectID": "slides/11-slides.html#presenting-confidence-intervals-in-a-regression-table",
    "href": "slides/11-slides.html#presenting-confidence-intervals-in-a-regression-table",
    "title": "Week 11:",
    "section": "Presenting Confidence Intervals in a Regression Table:",
    "text": "Presenting Confidence Intervals in a Regression Table:\n\n```{r, results=\"asis\"}`r''`\ntexreg::htmlreg(m1, \n  digits = 3, \n  ci.force = T)\n```\n\ntexreg::htmlreg(m1,\n        digits = 3, \n        ci.force = T)\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\n(Intercept)\n\n\n0.278*\n\n\n\n\n \n\n\n[ 0.174; 0.381]\n\n\n\n\nage\n\n\n0.009*\n\n\n\n\n \n\n\n[ 0.008; 0.011]\n\n\n\n\neducation_n\n\n\n-0.016\n\n\n\n\n \n\n\n[-0.037; 0.005]\n\n\n\n\nsexMale\n\n\n0.094*\n\n\n\n\n \n\n\n[ 0.050; 0.138]\n\n\n\n\nR2\n\n\n0.107\n\n\n\n\nAdj. R2\n\n\n0.106\n\n\n\n\nNum. obs.\n\n\n1463\n\n\n\n\n\n\n* 0 outside the confidence interval."
  },
  {
    "objectID": "slides/11-slides.html#tidying-regression-models",
    "href": "slides/11-slides.html#tidying-regression-models",
    "title": "Week 11:",
    "section": "Tidying Regression Models",
    "text": "Tidying Regression Models\n\nm1 %&gt;%\n  tidy(., conf.int =  T) -&gt; m1_df\nm1_df\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.278    0.0529        5.24 1.80e- 7  0.174     0.381  \n2 age          0.00922  0.000713     12.9  2.83e-36  0.00782   0.0106 \n3 education_n -0.0158   0.0108       -1.46 1.45e- 1 -0.0371    0.00544\n4 sexMale      0.0943   0.0225        4.19 3.02e- 5  0.0501    0.139"
  },
  {
    "objectID": "slides/11-slides.html#coefficient-plots-as-an-alternative-to-regression-tables",
    "href": "slides/11-slides.html#coefficient-plots-as-an-alternative-to-regression-tables",
    "title": "Week 11:",
    "section": "Coefficient Plots as an Alternative to Regression Tables",
    "text": "Coefficient Plots as an Alternative to Regression Tables\n\nm1_df %&gt;%\n  filter(term !=  \"(Intercept)\")%&gt;%\n  ggplot(aes(x = estimate, \n             y= term, \n             xmin = conf.low,\n             xmax = conf.high,\n             label = round(estimate,3)\n             ))+\n  geom_pointrange()+\n  geom_text(vjust=-1.5)+\n  geom_vline(xintercept = 0, linetype =2)+\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals-summary",
    "href": "slides/11-slides.html#confidence-intervals-summary",
    "title": "Week 11:",
    "section": "Confidence Intervals: Summary",
    "text": "Confidence Intervals: Summary\nWhat is a sampling distribution?\n\nA distribution of values we would have observed upon repeated sampling\nBootstrapping (sampling from a sample) approximates the width of the sampling distribution\n\nWhat is a standard error?\n\nStandard deviation of the sampling distribution\n\nDescribes the width or range of plausible observations we would see.\nDecreases as the sample size increases"
  },
  {
    "objectID": "slides/11-slides.html#confidence-intervals-summary-1",
    "href": "slides/11-slides.html#confidence-intervals-summary-1",
    "title": "Week 11:",
    "section": "Confidence Intervals: Summary",
    "text": "Confidence Intervals: Summary\nWhat is a confidence interval\n\nCoverage interval for a sampling distribution\n\n“A confidence interval is a way of expressing the precision or repeatability of a statistic, how much variation would likely be present across the possible different random samples from the population”\n\nThree components:\n\nPoint Estimate (i.e. a mean, or coefficient)\nConfidence Level (Often 95 percent by convention)\nMargin of Error (+/- some range (typically 2*SD for 95 percent CI))\n\nConfidence is about the interval\n\n95 percent of the intervals construct in this manner would contain the truth.\n\n\nclass: inverse, center, middle # 💡 # Hypothesis Testing ## How likely is it that we would see what did if our hypothesis were true"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-hypothesis-test",
    "href": "slides/11-slides.html#what-is-a-hypothesis-test",
    "title": "Week 11:",
    "section": "What is a hypothesis test",
    "text": "What is a hypothesis test\n\nA formal way of assessing statistical evidence. Combines\n\nDeductive reasoning (distribution of a test statistic, if the a null hypothesis were true )\nInductive reasoning (based on the test statistic we observed, how likely is it that we would observe it if the null were true?)"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-test-statistic",
    "href": "slides/11-slides.html#what-is-a-test-statistic",
    "title": "Week 11:",
    "section": "What is a test statistic?",
    "text": "What is a test statistic?\n\nA way of summarizing data\n\ndifference of means\ncoefficients from a linear model\ncoefficients from a linear model divded by their standard errors\nR^2"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-null-hypothesis",
    "href": "slides/11-slides.html#what-is-a-null-hypothesis",
    "title": "Week 11:",
    "section": "What is a null hypothesis?",
    "text": "What is a null hypothesis?\n\nA statement about the world\n\nOnly interesting if we reject it\nWould yield a distribution of test statistics “under the null”\nTypically something like “X has no effect on Y” (Null = no effect)\nNever accept the null can only reject"
  },
  {
    "objectID": "slides/11-slides.html#what-is-a-p-value",
    "href": "slides/11-slides.html#what-is-a-p-value",
    "title": "Week 11:",
    "section": "What is a p-value?",
    "text": "What is a p-value?\n\nA p-value is a conditional probability summarizing the likelihood of observing a test statistic as far from our hypothesis or farther, if our hypothesis were true.\nIt’s the area in the “tails of the curve” of the distribution of the test statistic under the null."
  },
  {
    "objectID": "slides/11-slides.html#how-do-we-do-hypothesis-testing",
    "href": "slides/11-slides.html#how-do-we-do-hypothesis-testing",
    "title": "Week 11:",
    "section": "How do we do hypothesis testing?",
    "text": "How do we do hypothesis testing?\n\nPosit a hypothesis (e.g. \\(\\beta = 0\\))\n\n–\n\nCalculate the test statistic (e.g. \\((\\hat{\\beta}-\\beta)/se_\\beta\\))\n\n–\n\nDerive the distribution of the test statistic under the null via\n\n\nSimulation\nAsymptotic theory\n\n–\n\nCompare the test statistic to the distribution under the null\n\nIf it’s in the tails \\(\\to\\) very unlikely that we would observe what we did if our hypothesis were true\n\n\n–\n\nCalculate p-value\n\nQuantify how often we would see test statistics as big or bigger\nTwo-side tests: how often do we see test statics as big or bigger in absolute value as our observed test statistic\nOne-side test: how often do we see test statistics as extreme as our observed statistic in a particular direction (positive/negative)\n\n\n–\n\nReject or fail to reject/retain our hypothesis based on some threshold of statistical significance (e.g. p &lt; 0.05)"
  },
  {
    "objectID": "slides/11-slides.html#outcomes-of-hypothesis-tests",
    "href": "slides/11-slides.html#outcomes-of-hypothesis-tests",
    "title": "Week 11:",
    "section": "Outcomes of Hypothesis Tests",
    "text": "Outcomes of Hypothesis Tests\n\nTwo conclusions from of a hypothesis test: we can reject or fail to reject a hypothesis test.\nWe never “accept” a hypothesis, since there are, in theory, an infinite number of other hypotheses we could have tested.\n\nOur decision can produce four outcomes and two types of error:\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative"
  },
  {
    "objectID": "slides/11-slides.html#outcomes-of-hypothesis-tests-1",
    "href": "slides/11-slides.html#outcomes-of-hypothesis-tests-1",
    "title": "Week 11:",
    "section": "Outcomes of Hypothesis Tests",
    "text": "Outcomes of Hypothesis Tests\n\n\n\n\nReject \\(H_0\\)\nFail to Reject \\(H_0\\)\n\n\n\n\n\\(H_0\\) is true\nFalse Positive\nCorrect!\n\n\n\\(H_0\\) is false\nCorrect!\nFalse Negative\n\n\n\nSuppose we chose to reject a hypothesis if our p-value was less than 0.05.\nWhat we’re saying is that we’re willing to falsely reject our hypothesis 5 times out of 100.\nTypically we want to minimize this false positive rate (Type 1 error), but there’s a trade off:\n\nReducing Type 1 error means, we’re more likely to make a Type 2 error – failing to reject when our null is false.\n\nclass:inverse, middle, center # 💪 ## Application: Hypothesis Testing for Linear Models"
  },
  {
    "objectID": "slides/11-slides.html#posit-a-null-hypothesis",
    "href": "slides/11-slides.html#posit-a-null-hypothesis",
    "title": "Week 11:",
    "section": "0. Posit a Null Hypothesis",
    "text": "0. Posit a Null Hypothesis\n\nTypically, we will test a null hypothesis that the coefficient for variable \\(X\\) equals 0 (e.g. \\(H_0: \\beta_x = 0\\) )\nOur alternative hypothesis in a two sided test then is that \\(\\beta_0\\) doesn’t equal 0 (\\(H_1: \\beta_x \\neq 0\\))\nIn a one-sided test, our alternative is directional (\\(H_1: \\beta_x &gt; 0\\) or \\(H_1: \\beta_x &lt; 0\\) )\n\nOne-sided tests are rare in practice – need a strong substantive reason for directional expectation"
  },
  {
    "objectID": "slides/11-slides.html#calculate-the-test-statistic",
    "href": "slides/11-slides.html#calculate-the-test-statistic",
    "title": "Week 11:",
    "section": "1. Calculate the test statistic",
    "text": "1. Calculate the test statistic\nFor a linear regression, we could use the \\(\\hat{\\beta}\\) as our test statistic.\nIn practice, we we use a “t-stat” which is our observed coefficiet, \\(\\hat{\\beta}\\) minus our hypothesized value \\(\\beta\\) (e.g. 0), divided by the standard error of \\(\\hat{\\beta}\\).\n\\[t= \\frac{\\hat\\beta-\\beta}{\\widehat{SE}_{\\hat{\\beta}}} \\sim \\text{Students's } t \\text{ with } n-k \\text{ degrees of freedom}\\] Fisher showed that this statistic from a regression follows a \\(t\\) distribution – which looks like a normal distribution but with “fatter tails” (e.g. more probability assigned to extreme values)"
  },
  {
    "objectID": "slides/11-slides.html#calculate-the-test-statistic-in-r",
    "href": "slides/11-slides.html#calculate-the-test-statistic-in-r",
    "title": "Week 11:",
    "section": "1. Calculate the test statistic in R",
    "text": "1. Calculate the test statistic in R\nWe can caclulate test statistics from our model by dividing the coefficients by their standard errors\n\nt_stat &lt;- coef(m1) / summary(m1)$coef[,2]\nt_stat\n\n(Intercept)         age education_n     sexMale \n   5.244555   12.927982   -1.459477    4.185356 \n\n\nWhich is exactly what the third column of summary() shows\n\nsummary(m1)$coef\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  0.277540588 0.0529197543  5.244555 1.797036e-07\nage          0.009216952 0.0007129459 12.927982 2.827291e-36\neducation_n -0.015814493 0.0108357258 -1.459477 1.446491e-01\nsexMale      0.094320479 0.0225358310  4.185356 3.017253e-05"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-via",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-via",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null via",
    "text": "2. Derive the distribution of the test statistic under the null via\nTwo approaches:\n\nSimulation: Calculate test statistics in a world where \\(H_0\\) is true\nAsymptotic theory: Use statistics to approximate this distribution"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nWe can make the null true, by randomly permuting (sampling without replacement) the outcome of our model:\n\n# One perumtation\nset.seed(123)\ndf_drww$support_war01_null &lt;- sample(df_drww$support_war01)\n\nOur permuted outcome is now uncorrelated with any predictors, it’s just a random sample of 0s, and 1s.\n\ncor(df_drww$support_war01, df_drww$age, use = \"complete.obs\")\n\n[1] 0.3093656\n\ncor(df_drww$support_war01_null, df_drww$age, use = \"complete.obs\")\n\n[1] -0.0005132924"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-1",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-1",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nAs such when we estimate a model with data where we have made our null hypothesis true (e.g \\(\\beta = 0\\) for all predictors), we get coefficients that are close to 0. Of course, by chance some might be a little negative, or a little positive.\n\nm1_null &lt;- lm(support_war01_null ~ age + education_n + sex, df_drww)\n\ntidy(m1_null)%&gt;%mutate_if(is.numeric, round,2)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     0.74      0.05     13.6     0   \n2 age             0         0        -0.07    0.94\n3 education_n    -0.01      0.01     -0.8     0.43\n4 sexMale         0.02      0.02      0.65    0.52"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-2",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-2",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nThe logic of hypothesis testing is to compare what we observed, to what we could have observed when our null is true.\nBy repeatedly permuting our outcome, estimating models, caclulating test statistics to generate the distributions under the null\nTo do this, we’ll write a function to make our life easier:\n\nmy_null_fn &lt;- function(\n  df=df_drww, \n  y = \"support_war01\", \n  f = formula(m1)\n  ){\n  df[, y] &lt;- sample(df[,y])\n  m &lt;- lm(f, df)\n  stat &lt;- summary(m)$coef[,3]\n  return(stat)\n}\nmy_null_fn()\n\n(Intercept)         age education_n     sexMale \n 11.0493734   1.8113230   0.9787517   0.6637328 \n\nmy_null_fn()\n\n(Intercept)         age education_n     sexMale \n 13.8126681   0.3840811  -1.4405531  -0.9781048"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-3",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-3",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nThen we’ll use the replicate() function to repeat this process 1000, saving the distribution of test statistics \\(((\\hat{\\beta}-\\beta)/se_\\beta)\\) to an object called m1_null\n\nset.seed(123)\nm1_null &lt;- data.frame(t(\n  replicate(1000, my_null_fn(df_drww, \"support_war01\",formula(m1)))))\nhead(m1_null)\n\n  X.Intercept.         age education_n    sexMale\n1     13.58601 -0.06917489  -0.7952217  0.6460818\n2     11.04937  1.81132298   0.9787517  0.6637328\n3     13.81267  0.38408113  -1.4405531 -0.9781048\n4     13.15015  0.04525689  -0.9986349  1.0681923\n5     13.41492 -0.59487147   0.4026906 -1.5637179\n6     13.43857 -1.11574477   0.3126860 -0.7188458"
  },
  {
    "objectID": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-4",
    "href": "slides/11-slides.html#derive-the-distribution-of-the-test-statistic-under-the-null-4",
    "title": "Week 11:",
    "section": "2. Derive the distribution of the test statistic under the null",
    "text": "2. Derive the distribution of the test statistic under the null\nWe can visualize this distribution of test statistics for the education_n:\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density()+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#compare-permutation-distribution-to-asymptotic-t-distribution",
    "href": "slides/11-slides.html#compare-permutation-distribution-to-asymptotic-t-distribution",
    "title": "Week 11:",
    "section": "2. Compare Permutation Distribution to Asymptotic t-Distribution",
    "text": "2. Compare Permutation Distribution to Asymptotic t-Distribution\nThis simulated distribution (grey) can be approximated by a \\(t\\) distribution (black), with \\(n\\) (the number of observations) minus \\(k\\) (the number of coefficients in our model) degrees of freedom.\n\nRoughly, you can think of “degrees of freedom” as a way of reflecting the fact that we have to estimate parameters of the distribution, and so we use up pieces of information (degrees of freedom),\nBecause we’re estimating parameters our uncertainty (the spread of the distribution) increases as the degrees of freedom decreases\nFor a small \\(N\\) reducing degrees of freedom, can increase uncertainty a lot\nFor a large \\(N\\) the differences are minute\n\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col = \"grey\")+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  stat_function(fun =dt,args =list(df = m1$df.residual), #&lt;&lt;\n                xlim =c(-3.5,3.5))+ #&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#compare-the-test-statistic-to-the-distribution-assuming-the-null-were-true",
    "href": "slides/11-slides.html#compare-the-test-statistic-to-the-distribution-assuming-the-null-were-true",
    "title": "Week 11:",
    "section": "3. Compare the test statistic to the distribution assuming the null were true",
    "text": "3. Compare the test statistic to the distribution assuming the null were true\nNext we’ll compare our observed test statistic -1.45 to its hypothesized distribution under the null\n\nFor a two sided test, we interested in absolute distance from the null, so we’ll put lines at both -1.45 and 1.45\n\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col = \"grey\")+\n  geom_rug()+\n  geom_vline(xintercept = 0, linetype =2)+\n  stat_function(fun =dt,args =list(df = m1$df.residual),\n                xlim =c(-3.5,3.5))+\n  geom_vline(xintercept =t_stat[\"education_n\"], linetype = 3)+ #&lt;&lt;\n  geom_vline(xintercept =t_stat[\"education_n\"]*-1, linetype = 3)+ #&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#calculate-p-value",
    "href": "slides/11-slides.html#calculate-p-value",
    "title": "Week 11:",
    "section": "4. Calculate p-value",
    "text": "4. Calculate p-value\nTo quantify how often we would see test statistics as big or bigger, we can simply take the mean of a logical comparison of the absolute value of the test statistics under the null greater than the absolute value of our observed test statistic\n\n# Simulation\nmean(abs(m1_null$education_n) &gt; abs(t_stat[\"education_n\"]))\n\n[1] 0.145\n\n\nOr, we can calculate the area under the curve for a \\(t\\)-distribution, as with 1459 degrees of freedom, that is less than -1.45 and multiply this by 2 (because the distribution is symetric), to get the p-value\n\n# Asymptotic\n2*pt(t_stat[\"education_n\"], m1$df.residual)\n\neducation_n \n  0.1446491"
  },
  {
    "objectID": "slides/11-slides.html#calculate-p-value-1",
    "href": "slides/11-slides.html#calculate-p-value-1",
    "title": "Week 11:",
    "section": "4. Calculate p-value",
    "text": "4. Calculate p-value\n\n# Asymptotic\n2*pt(t_stat[\"education_n\"], m1$df.residual)\n\neducation_n \n  0.1446491 \n\n\nOs exactly what R’s summary() function returns:\n\n# R's summary\nsummary(m1)$coef[4,]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n9.432048e-02 2.253583e-02 4.185356e+00 3.017253e-05 \n\n\nVisually, these calculations look like this:\n\nm1_null %&gt;%\n  ggplot(aes(education_n))+\n  geom_density(col=\"grey\")+\n  geom_rug(col = ifelse(abs(m1_null$education_n) &gt;= abs(t_stat[\"education_n\"]), \"red\",\"black\"))+ #&lt;&lt;\n  geom_vline(xintercept =0, linetype = 2)+\n  geom_vline(xintercept =t_stat[\"education_n\"], linetype = 3)+\n  geom_vline(xintercept =t_stat[\"education_n\"]*-1, linetype = 3)+\n  stat_function(fun =dt,args =list(df = m1$df.residual),\n                xlim =c(-3.5,3.5))+\n  stat_function(fun =dt,args =list(df = m1$df.residual), #&lt;&lt;\n                geom =\"area\", xlim = c(-3.5,t_stat[\"education_n\"]),#&lt;&lt;\n                alpha = .5,fill =\"red\")+#&lt;&lt;\n  stat_function(fun =dt, args =list(df = m1$df.residual), #&lt;&lt;\n                geom =\"area\", xlim = c(abs(t_stat[\"education_n\"]),3.5),#&lt;&lt;\n                alpha = .5, fill =\"red\")+#&lt;&lt;\n  theme_bw()"
  },
  {
    "objectID": "slides/11-slides.html#reject-or-fail-to-rejectretain-our-hypothesis",
    "href": "slides/11-slides.html#reject-or-fail-to-rejectretain-our-hypothesis",
    "title": "Week 11:",
    "section": "Reject or fail to reject/retain our hypothesis",
    "text": "Reject or fail to reject/retain our hypothesis\nFor an estimated p-value of 0.145 for coefficient on education_n and a significance threshold of p &lt; 0.05, we would…\n–\nFail to reject the null hypothesis that \\(H_0: \\beta_{education} = 0\\)\nBecause in a world where the truth was \\(\\beta_{education} = 0\\) test statistics reflecting coefficients as far as \\(t-stat = |-1.459|\\) about 14.5 percent of the time.\nIf we were to reject the null, 14.5 percent of the time, we would be making a Type-1 error | False Positive | concluding there was a relationship when in fact their wasn’t.\nclass:inverse, middle, center # Break\nclass:inverse, middle, center # Final Projects"
  },
  {
    "objectID": "slides/11-slides.html#timelines-this-week",
    "href": "slides/11-slides.html#timelines-this-week",
    "title": "Week 11:",
    "section": "Timelines: This Week",
    "text": "Timelines: This Week\nNOTE: I’m leaving these slides in for nw\n\nFeedback on Data today\nQuestions/office hours today tomorrow/by appointment\nThursday: Work on analyzing your data\nDrafts are still due April 24th.\nYour draft need not, and probably will not, be a completed project.\nAt a minimum what you want is:\n\nA clean data set\nExploratory descriptives\nInitial results\n\nThe rest can come later. Our goal on Thursday is to move you from the data portions of your projects, into the analysis."
  },
  {
    "objectID": "slides/11-slides.html#timelines-next-week",
    "href": "slides/11-slides.html#timelines-next-week",
    "title": "Week 11:",
    "section": "Timelines: Next Week",
    "text": "Timelines: Next Week\n\nTuesday, April 26: Workshop: Review and Presentations\nThursday April 28: Workshop: Producing Presentations\n\n7-10 Slides\nDrafts/Analysis, just need to fill those slides\n\nSunday, May 1: Presentations due (Monday at the latests)"
  },
  {
    "objectID": "slides/11-slides.html#timelines-first-week-of-may",
    "href": "slides/11-slides.html#timelines-first-week-of-may",
    "title": "Week 11:",
    "section": "Timelines: First Week of May:",
    "text": "Timelines: First Week of May:\n\nTuesday, May 3: Presentations\nThursday, May 5: Last class and Food?\n\nClass at Dolores at 5pm?\nBagels/Treats in class\n\nSunday May 8: Final Papers Due"
  },
  {
    "objectID": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "href": "slides/11-slides.html#strucutre-of-final-paper-and-drafts",
    "title": "Week 11:",
    "section": "Strucutre of Final Paper and Drafts:",
    "text": "Strucutre of Final Paper and Drafts:\nSeven sections:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides.html#focus-on-for-sunday",
    "href": "slides/11-slides.html#focus-on-for-sunday",
    "title": "Week 11:",
    "section": "Focus on for Sunday",
    "text": "Focus on for Sunday\nSeven sections:\n\nIntroduction (5 percent, ~ 4 paragraphs)\nTheory and Expectations (10 percent, ~4+ paragraphs)\nData (20 percent ~ 4+ paragraphs)\nDesign (25 percent ~ 5+ paragraphs)\nResults (25 percent ~ 5+ paragraphs)\nConclusion (5 percent ~ 3+ paragraphs)\nAppendix (10 percent ~ Variable codebook and all the R code for your project)"
  },
  {
    "objectID": "slides/11-slides.html#motivating-questions",
    "href": "slides/11-slides.html#motivating-questions",
    "title": "Week 11:",
    "section": "Motivating Questions:",
    "text": "Motivating Questions:\nIn the reset of today’s class, we’ll get some practice putting together the various skills you need for your drafts by exploring the following:\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\nHave these perceptions about vaccines changed over time?"
  },
  {
    "objectID": "slides/11-slides.html#tasks",
    "href": "slides/11-slides.html#tasks",
    "title": "Week 11:",
    "section": "Tasks:",
    "text": "Tasks:\nTo explore these questions, we need to\n\nGet setup to work\nLoad our data\nRecode our data\nSummarize our data\nSpecify our expectations\nEstimate models to test these expectations\nPresent and interpret results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/11-slides.html#new-packages-1",
    "href": "slides/11-slides.html#new-packages-1",
    "title": "Week 11:",
    "section": "New packages",
    "text": "New packages\nTo easily load survey data for our question, we’ll need the anesr package, which loads data from the American National Election Studies into R\n\n# Uncomment to uninstall package to download NES survey data\n# library(devtools)\n# install_github(\"jamesmartherus/anesr\")\nrequire(anesr)"
  },
  {
    "objectID": "slides/11-slides.html#packages-for-today-1",
    "href": "slides/11-slides.html#packages-for-today-1",
    "title": "Week 11:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\"purrr\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"modelr\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages-1",
    "href": "slides/11-slides.html#define-a-function-to-load-and-if-needed-install-packages-1",
    "title": "Week 11:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/11-slides.html#load-packages-for-today-1",
    "href": "slides/11-slides.html#load-packages-for-today-1",
    "title": "Week 11:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         purrr         ggmap \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      ggrepel      ggridges      ggthemes        ggpubr        GGally \n         TRUE          TRUE          TRUE          TRUE          TRUE \n       scales       dagitty         ggdag       ggforce scatterplot3d \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      COVID19          maps       mapdata           qss    tidycensus \n         TRUE          TRUE          TRUE          TRUE          TRUE \n    dataverse DeclareDesign        modelr           zoo \n         TRUE          TRUE          TRUE          TRUE \n\n\nclass:inverse, center, middle # 💪 ## Loading Data"
  },
  {
    "objectID": "slides/11-slides.html#data",
    "href": "slides/11-slides.html#data",
    "title": "Week 11:",
    "section": "Data",
    "text": "Data\nNow that we have anesr installed, let’s load data from the 2016 and 2020 National Election Studies:\n\n# Load data\ndata(timeseries_2016, package = \"anesr\")\ndata(timeseries_2020, package = \"anesr\")\n\nAnd copy those data frames into new dataframes with shorter names\n\n# Rename datasets\nnes16 &lt;- timeseries_2016\nnes20 &lt;- timeseries_2020\n\nclass:inverse, center, middle # 💪 ## Recoding Data"
  },
  {
    "objectID": "slides/11-slides.html#finding-variables-of-interest",
    "href": "slides/11-slides.html#finding-variables-of-interest",
    "title": "Week 11:",
    "section": "Finding variables of interest",
    "text": "Finding variables of interest\nOur primary outcome of interest are beliefs about vaccines.\nVariables V162162x in the 2016 NES and V202383x in the 2020 NES will serve as our primary outcome of interest, summarizing respondents answer to the following question:\n\nDo the health benefits of vaccinations generally outweigh the risks, do the risks outweigh the benefits, or is there no difference?\n\nSimilarly, V161158x in the 2016 NES and V201231x in the 2020 NES will serve our key predictor (respondent’s partisanship).\nFinally, we’ll control respondents’ age, using V161267 in the 2016 NES and V201507x in the 2020 NES\nLet’s take a look at the values and distributions of these raw variables and think about what we need to do to recode these data so that they are suitable for analysis"
  },
  {
    "objectID": "slides/11-slides.html#look-at-the-distribution-and-coding-of-our-outcome-vaccine-beliefs",
    "href": "slides/11-slides.html#look-at-the-distribution-and-coding-of-our-outcome-vaccine-beliefs",
    "title": "Week 11:",
    "section": "Look at the distribution and coding of our outcome: Vaccine Beliefs",
    "text": "Look at the distribution and coding of our outcome: Vaccine Beliefs\nThe variables in the NES datasets are of a class labelled which allows numeric values to have substantive labels\n\nclass(nes16$V162162x)\n\n[1] \"haven_labelled\"\n\n\nOur outcome variable has the following labels:\n\nlabelled::val_labels(nes16$V162162x)\n\n                                   -9. Refused \n                                            -9 \n                                -8. Don't know \n                                            -8 \n-7. No post data, deleted due to incomplete IW \n                                            -7 \n                -6. No post-election interview \n                                            -6 \n                      1. Benefits much greater \n                                             1 \n                2. Benefits moderately greater \n                                             2 \n                  3. Benefits slightly greater \n                                             3 \n                              4. No difference \n                                             4 \n                     5. Risks slightly greater \n                                             5 \n                   6. Risks moderately greater \n                                             6 \n                         7. Risks much greater \n                                             7 \n\n\nAnd distribution of responses:\n\ntable(nes16$V162162x)\n\n\n  -9   -8   -7   -6    1    2    3    4    5    6    7 \n  21   28   86  536 1687  726  258  539   96  211   82"
  },
  {
    "objectID": "slides/11-slides.html#recoding-our-outcome-variable",
    "href": "slides/11-slides.html#recoding-our-outcome-variable",
    "title": "Week 11:",
    "section": "Recoding our outcome variable",
    "text": "Recoding our outcome variable\nWhat transformations do we need to make to V162162x in nes16 and V202383x in nes20 so that these variables are suitable for analysis?\n–\n\nRecode negative values to be NA\n\n–\n\nReverse code so that higher values indicate greater belief in the benefits of vaccines\n\n–\n\nCreate an indicator of people who are skeptical of the benefits of vaccines"
  },
  {
    "objectID": "slides/11-slides.html#recoding-v162162x-in-2016-nes",
    "href": "slides/11-slides.html#recoding-v162162x-in-2016-nes",
    "title": "Week 11:",
    "section": "Recoding V162162x in 2016 NES",
    "text": "Recoding V162162x in 2016 NES\n\nnes16 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V162162x &lt; 0, NA, (V162162x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes16 # Save recodes to nes16"
  },
  {
    "objectID": "slides/11-slides.html#recoding-v202383x-in-2020-nes",
    "href": "slides/11-slides.html#recoding-v202383x-in-2020-nes",
    "title": "Week 11:",
    "section": "Recoding V202383x in 2020 NES",
    "text": "Recoding V202383x in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    # Make Negative values NA, Reverse Code So Higher Values = Benefits &gt; Risks\n    vaccine_benefits = ifelse(V202383x &lt; 0, NA, (V202383x-8)*-1),\n    # Indicator of vaccine skepticism (Risks &gt; Benefits)\n    vaccine_skeptic01 = case_when(\n      vaccine_benefits &gt; 4 ~ 0,\n      vaccine_benefits &lt;= 4 ~ 1,\n      TRUE ~ NA_real_\n    )\n  ) -&gt; nes20 # Save recodes to nes20"
  },
  {
    "objectID": "slides/11-slides.html#recoding-predictors",
    "href": "slides/11-slides.html#recoding-predictors",
    "title": "Week 11:",
    "section": "Recoding Predictors",
    "text": "Recoding Predictors\nNow we repeat this process for our key predictor, partisanship.\n\nRecode the the summary partisanship variables V161158x in nes16 and V201231x in nes20\nCreate indicators from this recoded variable that classify partisanship as categorical variable (with Democrats as the reference category)\n\nAnd our covariate, age variables V161267 in nes16 and V201507x in nes20\n\nRecode negative values to be NA"
  },
  {
    "objectID": "slides/11-slides.html#recoding-partisanship-v161158x-in-2016-nes",
    "href": "slides/11-slides.html#recoding-partisanship-v161158x-in-2016-nes",
    "title": "Week 11:",
    "section": "Recoding Partisanship (V161158x) in 2016 NES",
    "text": "Recoding Partisanship (V161158x) in 2016 NES\n\nnes16 %&gt;%\n  mutate(\n    pid = ifelse(V161158x &lt; 0, NA, V161158x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V161267 &lt; 0, NA, V161267)\n  ) -&gt; nes16"
  },
  {
    "objectID": "slides/11-slides.html#recoding-partisanship-v201231x-in-2020-nes",
    "href": "slides/11-slides.html#recoding-partisanship-v201231x-in-2020-nes",
    "title": "Week 11:",
    "section": "Recoding Partisanship (V201231x) in 2020 NES",
    "text": "Recoding Partisanship (V201231x) in 2020 NES\n\nnes20 %&gt;%\n  mutate(\n    pid = ifelse(V201231x &lt; 0, NA, V201231x),\n    pid3cat = case_when(\n      pid &lt; 4 ~ \"Democrat\",\n      pid == 4 ~ \"Independent\",\n      pid &gt; 4 ~ \"Republican\",\n      TRUE ~ \"Independent\"\n    ) %&gt;% factor(., levels = c(\"Democrat\",\"Independent\",\"Republican\")),\n    age = ifelse(V201507x &lt; 0, NA, V201507x)\n  ) -&gt; nes20"
  },
  {
    "objectID": "slides/11-slides.html#progress-report",
    "href": "slides/11-slides.html#progress-report",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data📥\nSpecify our expectations\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Summarizing data"
  },
  {
    "objectID": "slides/11-slides.html#tables-of-descriptive-statistics",
    "href": "slides/11-slides.html#tables-of-descriptive-statistics",
    "title": "Week 11:",
    "section": "Tables of descriptive statistics",
    "text": "Tables of descriptive statistics\n\nCreate a object with the names of the variables you want to summarize\nSelect these variables\nPivot the data\nCalculate summary statistics\nFormat as an html table"
  },
  {
    "objectID": "slides/11-slides.html#tables-of-descriptive-statistics-1",
    "href": "slides/11-slides.html#tables-of-descriptive-statistics-1",
    "title": "Week 11:",
    "section": "Tables of descriptive statistics",
    "text": "Tables of descriptive statistics\n\n# 1. Create a object with the names of the variables you want to summarize\nthe_vars &lt;- c(\"vaccine_skeptic01\",\"pid\",\"age\")\n# 2. Select these variables\nnes16 %&gt;%\n  select(all_of(the_vars)) %&gt;%\n# 3. Pivot the data\n  pivot_longer(\n    cols = all_of(the_vars),\n    names_to = \"Variable\"\n  )%&gt;%\n  mutate(\n    Variable = factor(Variable, levels = the_vars)\n  )%&gt;%\n  arrange(Variable)%&gt;%\n  dplyr::group_by(Variable)%&gt;%\n  # 3. Calculate summary statistics\n  dplyr::summarise(\n    min = min(value, na.rm=T),\n    p25 = quantile(value, na.rm=T, prob = 0.25),\n    Median = quantile(value, na.rm=T, prob = 0.5),\n    mean = mean(value, na.rm=T),\n    p75 = quantile(value, na.rm=T, prob = 0.25),\n    max = max(value, na.rm=T),\n    missing = sum(is.na(value))\n  ) -&gt; sum_tab"
  },
  {
    "objectID": "slides/11-slides.html#format-table",
    "href": "slides/11-slides.html#format-table",
    "title": "Week 11:",
    "section": "Format Table",
    "text": "Format Table\n\nknitr::kable(sum_tab,\n             caption = \"Descriptive Statistics\",\n             digits = 2) %&gt;%\n  kableExtra::kable_styling() %&gt;%\n  kableExtra::pack_rows(\"Outcome\", start_row = 1, end_row =1) %&gt;%\n  kableExtra::pack_rows(\"Key Predictors\", start_row = 2, end_row =2) %&gt;%\n  kableExtra::pack_rows(\"Covariates\", start_row = 3, end_row =3)\n\n\n\n\n\nDescriptive Statistics\n\n\nVariable\nmin\np25\nMedian\nmean\np75\nmax\nmissing\n\n\n\n\nOutcome\n\n\nvaccine_skeptic01\n0\n0\n0\n0.26\n0\n1\n671\n\n\nKey Predictors\n\n\npid\n1\n2\n4\n3.86\n2\n7\n23\n\n\nCovariates\n\n\nage\n18\n34\n50\n49.58\n34\n90\n121"
  },
  {
    "objectID": "slides/11-slides.html#progress-report-1",
    "href": "slides/11-slides.html#progress-report-1",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSummarize our data ✅\nSpecify our expectations 📥\nEstimate models to test these expectations\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Specificying Expecations"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations",
    "href": "slides/11-slides.html#specificying-expecations",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nConsider our first two motivating questions\n\nHow does partisanship shape American’s perceptions of vaccines?\nWho is skeptical of the benefits of vaccination?\n\nAnd some illustrative stereotypes:\n\n“Republicans are anti-science”\n“Liberal always for Goopy pseudo-science”\n“Independents love to do their own research”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-1",
    "href": "slides/11-slides.html#specificying-expecations-1",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nSimilarly, consider our third question:\n\nHave these perceptions about vaccines changed over time?\n\nAnd some similar simplified claims:\n\n“The Covid-19 vaccine is a miracle of modern science”\n“Social media is rife with misinformation about the Covid-19 vaccine”\n“Politicians are politicizing vaccine politics for political benefits”\n\nWhat are the empirical implications of these claims?"
  },
  {
    "objectID": "slides/11-slides.html#specificying-expecations-2",
    "href": "slides/11-slides.html#specificying-expecations-2",
    "title": "Week 11:",
    "section": "Specificying Expecations",
    "text": "Specificying Expecations\nOur goal is to take claims/conventional wisdom/theories, and derive their empirical implications:\n\nH1: Partisan Differences in Vaccine Skepticism\n\nH1a: Republicans will be the most skeptical of vaccines\nH1b: Democrats will be the most skeptical of vaccines\nH1a: Independents will be the most skeptical of vaccines\n\nH2: Temporal Differences in Vaccine Skepticism\n\nH2a: Vaccine skepticism will decrease from 2016 to 2020 with the widespread roll out of the Covid-19 vaccine\nH2b: Vaccine skepticism will increase from 2016 to 2020 with increased amounts of misinformation about the Covid-19 vaccine\n\nH3: Partisan Difference in Vaccine Skepticism Over Time Partisan differences in Vaccine Skepticism will increase from 2016 to 2020 with the politicization of Covid-19 policies"
  },
  {
    "objectID": "slides/11-slides.html#motivating-your-expectations.",
    "href": "slides/11-slides.html#motivating-your-expectations.",
    "title": "Week 11:",
    "section": "Motivating your expectations.",
    "text": "Motivating your expectations.\nIn your papers, unlike in these slides, your expectations should be grounded in existing theory, research, and evidence. For the present question, we might cite sources such as:\n\nEnders, Adam M., and Steven M. Smallpage. “Informational cues, partisan-motivated reasoning, and the manipulation of conspiracy beliefs.” Political Communication 36.1 (2019): 83-102.\nStecula, Dominik A., and Mark Pickup. “How populism and conservative media fuel conspiracy beliefs about COVID-19 and what it means for COVID-19 behaviors.” Research & Politics 8.1 (2021): 2053168021993979.\nJennings, Will, et al. “Lack of trust, conspiracy beliefs, and social media use predict COVID-19 vaccine hesitancy.” Vaccines 9.6 (2021): 593.\nHollander, Barry A. “Partisanship, individual differences, and news media exposure as predictors of conspiracy beliefs.” Journalism & Mass Communication Quarterly 95.3 (2018): 691-713."
  },
  {
    "objectID": "slides/11-slides.html#model-specification",
    "href": "slides/11-slides.html#model-specification",
    "title": "Week 11:",
    "section": "Model Specification",
    "text": "Model Specification\nTranslate these expectations into empirical models requires choices about how to specify our models\n–\n\nHow should we measure/operationalize our outcome\n\nShould we measure beliefs about vaccines with 7-point ordinal scale or as a binary indicator of vaccine skepticism\n\nHow should we measure/operationalize our key predictor(s)\n\nShould we measure partisanship using a 7 point scale or as categorical variable?\n\nWhat should we control for in our model?\n\nFactors likely to predict both our outcome and our key predictor of interest\n\n\n–\n\nThere are rarely definitive answers to these questions.\nIn practice, we will often estimate multiple models to try and show that our findings are robust to alternative modeling strategies/specifications"
  },
  {
    "objectID": "slides/11-slides.html#model-specification-1",
    "href": "slides/11-slides.html#model-specification-1",
    "title": "Week 11:",
    "section": "Model Specification",
    "text": "Model Specification\nFor your projects, every group will almost surely estimate some form of the following:\n\nBaseline bivariate model: The simplest test of the relationship between your outcome and key predictor\nMultiple regression model: A test of the robustness of this relationship, controlling for alternative explanations\n\nIn practice, I suspect you may estimate multiple regression models such as:\n\nAlternative specifications/operationalizations of outcomes and predictors\nInteraction models to test conditional relationships\nPolynomial models to test non-linear relationships"
  },
  {
    "objectID": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models",
    "href": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models",
    "title": "Week 11:",
    "section": "Translating Theoretical Expectations into Empirical Models",
    "text": "Translating Theoretical Expectations into Empirical Models\nBefore we estimate our models in R, we will write down our models formally and empirical implications of our theoretical expectations in terms of the coefficients of our model.\nFor example, test for partisan differences in vaccine skepticism, we might fit the following baseline model:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} + X\\beta + \\epsilon\\] - If \\(\\beta_1\\) is positive this is consistent with H1a (greater skepticism among Republicans), - If \\(\\beta_2\\) is negative this is consistent with H1b (greater skepticism among Democrats),\nBut how could we test H1c – greater skepticism among Independents, who are “4s” on \\(\\text{PID}_{7pt}\\)?"
  },
  {
    "objectID": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models-1",
    "href": "slides/11-slides.html#translating-theoretical-expectations-into-empirical-models-1",
    "title": "Week 11:",
    "section": "Translating Theoretical Expectations into Empirical Models",
    "text": "Translating Theoretical Expectations into Empirical Models\nWe could fit a polynomial regression, including both partisanship and partinaship squared to allow the relationship between partisanship and vaccine skepticism to vary non-linearly\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{7pt} +  \\beta_2 \\text{PID}_{7pt}^2+ X\\beta+ \\epsilon\\] Or we could estimate a model treating Partisanship as a categorical variable rather than an ordinal interval variable. In our recoding, we set \"Democrat\" to be the first level of the variable pid3cat, so the model R will estimate by default is:\n\\[\\text{Vaccine Skepticism} = \\beta_0 + \\beta_1 \\text{PID}_{Ind} +  \\beta_2 \\text{PID}_{Rep}+ X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/11-slides.html#testing-differences-over-time.",
    "href": "slides/11-slides.html#testing-differences-over-time.",
    "title": "Week 11:",
    "section": "Testing differences over time.",
    "text": "Testing differences over time.\nTesting Hypotheses 2 and 3 involve making comparisons across models estimated on data from different surveys.\nFormally, testing these expectations is a little more complicated\n\nwe could pool our two surveys together include an interaction term for survey year\n\nFor our purposes, we’ll treat these as more qualitative/exploratory hypotheses:\n\nH2a/b implies overall rates of vaccine skepticism will be lower/higher in 2020 compared to 2016\nH3 implies that whatever partisan differences we find in 2016 should be larger in 2020."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-2",
    "href": "slides/11-slides.html#progress-report-2",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations 📥\nPresenting and interpreting results using\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis tests (new!)\n\n\nclass:inverse, center, middle # 💪 ## Estimating Empirical Models"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models",
    "href": "slides/11-slides.html#estimating-empirical-models",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nHaving derived empirical implications of our theoretical expectations expressed in terms of linear regressions, now we simply have to estimate our models in R.\nWhen estimating the same model on different datasets we can write the formulas once\n\nf1 &lt;- formula(vaccine_skeptic01 ~ pid + age)\nf2 &lt;- formula(vaccine_skeptic01 ~ pid + I(pid^2) + age)\nf3 &lt;- formula(vaccine_skeptic01 ~ pid3cat + age)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-1",
    "href": "slides/11-slides.html#estimating-empirical-models-1",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nAnd then pass it to lm() with different data arguments:\n\nm1_2016 &lt;- lm(formula = f1, data = nes16)\nm1_2020 &lt;- lm(formula = f1, data = nes20)\nm2_2016 &lt;- lm(formula = f2, data = nes16)\nm2_2020 &lt;- lm(formula = f2, data = nes20)\nm3_2016 &lt;- lm(formula = f3, data = nes16)\nm3_2020 &lt;- lm(formula = f3, data = nes20)"
  },
  {
    "objectID": "slides/11-slides.html#estimating-empirical-models-2",
    "href": "slides/11-slides.html#estimating-empirical-models-2",
    "title": "Week 11:",
    "section": "Estimating Empirical Models",
    "text": "Estimating Empirical Models\nIf you’ve\n\ncoded your data correctly\ndeveloped clear testable implications from your theoretical expectations\n\nSpecifying and estimating empirical models is straightforward. Literally a few lines of code."
  },
  {
    "objectID": "slides/11-slides.html#progress-report-3",
    "href": "slides/11-slides.html#progress-report-3",
    "title": "Week 11:",
    "section": "Progress Report",
    "text": "Progress Report\nTo explore these questions, we need to\n\nGet setup to work ✅\nLoad our data ✅\nRecode our data ✅\nSpecify our expectations ✅\nEstimate models to test these expectations ✅\nPresent our results 📥\n\nTables\nFigures\nConfidence intervals (review)\nHypothesis testing (new!)\n\n\nclass:inverse, center, middle # 💪 ## Presenting and Interpreting Your Results"
  },
  {
    "objectID": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "href": "slides/11-slides.html#presenting-and-interpreting-your-results",
    "title": "Week 11:",
    "section": "Presenting and Interpreting Your Results",
    "text": "Presenting and Interpreting Your Results\nPresenting and interpreting your results is requires both art and science.\nYour goal is to tell a story with your results, walking your reader through the substantive and statsitical interpretation of tables and figures.\nLet’s start by producing a regression table, which provides a concise summary of multiple regression models.\nLike figures, producing a good regression table is an interactive process."
  },
  {
    "objectID": "slides/11-slides.html#regression-tables-with-htmlreg",
    "href": "slides/11-slides.html#regression-tables-with-htmlreg",
    "title": "Week 11:",
    "section": "Regression Tables with htmlreg",
    "text": "Regression Tables with htmlreg\nThe following code produces a basic regression table.\n\ntexreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020)\n)"
  },
  {
    "objectID": "slides/11-slides.html#regression-tables-with-htmlreg-1",
    "href": "slides/11-slides.html#regression-tables-with-htmlreg-1",
    "title": "Week 11:",
    "section": "Regression Tables with htmlreg",
    "text": "Regression Tables with htmlreg\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\nModel 4\n\n\nModel 5\n\n\nModel 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.46***\n\n\n0.35***\n\n\n0.42***\n\n\n0.34***\n\n\n0.32***\n\n\n0.35***\n\n\n\n\n \n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\npid\n\n\n-0.00\n\n\n0.06***\n\n\n \n\n\n0.02***\n\n\n0.04***\n\n\n \n\n\n\n\n \n\n\n(0.00)\n\n\n(0.02)\n\n\n \n\n\n(0.00)\n\n\n(0.01)\n\n\n \n\n\n\n\nage\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n-0.00***\n\n\n\n\n \n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n\n\npid^2\n\n\n \n\n\n-0.01***\n\n\n \n\n\n \n\n\n-0.00\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n \n\n\n(0.00)\n\n\n \n\n\n\n\npid3catIndependent\n\n\n \n\n\n \n\n\n0.17***\n\n\n \n\n\n \n\n\n0.20***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.02)\n\n\n\n\npid3catRepublican\n\n\n \n\n\n \n\n\n-0.02\n\n\n \n\n\n \n\n\n0.10***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.02)\n\n\n \n\n\n \n\n\n(0.01)\n\n\n\n\nR2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.05\n\n\n\n\nAdj. R2\n\n\n0.02\n\n\n0.03\n\n\n0.04\n\n\n0.03\n\n\n0.03\n\n\n0.04\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#customizing-our-regression-table",
    "href": "slides/11-slides.html#customizing-our-regression-table",
    "title": "Week 11:",
    "section": "Customizing our Regression Table",
    "text": "Customizing our Regression Table\nLet’s make this regression table more reader friendly and informative by:\n\nGiving the variables in substantive names\nReporting coefficients to 3 decimal places\nUsing a single significance threshold of \\(p &lt; 0.05\\)\nGiving the models custom names\nAdding a header to group models by year\nChanging the caption of the table\n\n\ntexreg::htmlreg(\n  list(m1_2016,m2_2016,m3_2016,\n       m1_2020,m2_2020,m3_2020),\n  # Reporting coefficients to 3 decimal places\n  digits = 3,\n  # Using a single significance threshold \n  stars = 0.05,\n  # Giving the variables in substantive names\n  custom.coef.names = c(\n    \"(Intercept)\",\n    \"PID (7pt)\",\n    \"Age\",\n    \"PID&lt;sup&gt;2&lt;/sup&gt; (7pt)\",\n    \"Independent\",\n    \"Republican\"\n  ),\n  # Giving the models custom names\n  custom.model.names = paste(\"Model\",c(1:3,1:3)),\n  # Adding a header to group models by year\n  custom.header = list(\"NES 2016\" = 1:3, \"NES 2020\" = 4:6),\n  # Changing the caption of the table\n  caption = \"Partisanship and Vaccine Skepticism\"\n)\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#telling-a-story-with-regression",
    "href": "slides/11-slides.html#telling-a-story-with-regression",
    "title": "Week 11:",
    "section": "Telling a Story with Regression",
    "text": "Telling a Story with Regression\nFirst, provide an overview the models presented in the table\n\nExplain what each model is doing conceptually\n\nThen, start with your simplest model (Typically the first column in your table).\n\nUse this as a chance to explain core concepts from the course\n\nWhat is regression\nHow should I interpret a coefficient substantively\nHow should I interepret the statistical signficance of a give coefficient\n\nAs you move from left to right (simple to more complex)\n\nyou need not interpret every single coefficient in the model\ninstead highlight the factors that are important for the reader to note (e.g. a comparison between one coefficient in model or another.)\n\n\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05"
  },
  {
    "objectID": "slides/11-slides.html#overview",
    "href": "slides/11-slides.html#overview",
    "title": "Week 11:",
    "section": "Overview",
    "text": "Overview\n\nTable 1 presents the results of three specifications exploring the relationship between partisanship and vaccine skepticism using data from the 2016 (Models 1-3) and 2020 (Models 4-5) National Election Studies.\nModels 1 and 4 operationalize partisanship as a 7-point scale, where 1 corresponds to Strong Democrats, 4 to Indepndents, and 7 to Strong Republicans in the 2016 (Model 1) and 2020 (Model 2) surveys.\nModels 2 and 5 allow the relationship between partisanship and vaccine skepticism to vary non-linear again for the 2016 (Model 2) and 2020 (Model 5) elections.\nModels 3 and 6 treat partisanship as categorical variable, describing how Independents and Republicans differ from Democrats, the reference category in these models.\nAll models control age, since (put in substantive justification for controlling for age here)"
  },
  {
    "objectID": "slides/11-slides.html#story-testing-for-partisan-differences",
    "href": "slides/11-slides.html#story-testing-for-partisan-differences",
    "title": "Week 11:",
    "section": "Story: Testing for Partisan Differences",
    "text": "Story: Testing for Partisan Differences\n\nThe results from Model 1 provide little initial evidence for partisan differences in vaccine skepticism in the 2016 Election.\n\nThe coefficient on the partisanship variable is -0.005, suggesting that a unit increase in partisanship (going from being a Strong Democrat to just a Democrat, or an Independent to an independent who leans Republican), is associated with just a 0.5 percentage point increase in the probability of being a vaccine skeptic (believing that the risks of vaccination outweigh the benefits or that their is no difference in the risks versus benefits).\n\nFurthermore the 95-percent confidence interval for this estimate (-0.011, 0.002) brackets 0, suggesting the true population estimate from this model could be either positive or negative. Similarly, we fail to reject the null hypothesis that the true coefficient on partisanship in this model is 0 as the test statistic for this estimate ( -1.38) corresponds to a p-value of 0.168 suggesting that we would see test statistics this large or larger fairly often when the true relationship was 0.\nIn some the results from Model 1 provide little support for any of the expectations described by H1"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "href": "slides/11-slides.html#testing-for-partisan-differences-model-2",
    "title": "Week 11:",
    "section": "Testing for Partisan Differences: Model 2",
    "text": "Testing for Partisan Differences: Model 2\n\nWhile coefficients from Model 1 suggest little evidence of partisan differences in vaccine skepticism, the coefficients on both partisanship, and partisanship squared are statistically significant (p &lt; 0.05)."
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2",
    "href": "slides/11-slides.html#interpreting-model-2",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nThe coefficients from polynomial regressions can be difficult to interpret jointly and so Figure 1 presents the predicted values from Model 2, holding age constant at its sample mean.\n\n\npred_df_m2 &lt;- expand_grid(\n  pid = 1:7,\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m2 &lt;- cbind(pred_df_m2, predict(m2_2016,pred_df_m2, interval =\"confidence\"))\npred_df_m2\n\n  pid      age       fit       lwr       upr\n1   1 49.58231 0.2366157 0.2082979 0.2649335\n2   2 49.58231 0.2743408 0.2551659 0.2935157\n3   3 49.58231 0.2945841 0.2729532 0.3162151\n4   4 49.58231 0.2973457 0.2738012 0.3208902\n5   5 49.58231 0.2826255 0.2611041 0.3041469\n6   6 49.58231 0.2504236 0.2300546 0.2707925\n7   7 49.58231 0.2007398 0.1688900 0.2325897"
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2-1",
    "href": "slides/11-slides.html#interpreting-model-2-1",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\npred_df_m2 %&gt;%\n  ggplot(aes(pid, fit, ymin =lwr, ymax =upr))+\n  geom_line()+\n  geom_ribbon(alpha=.2, fill=\"grey\")+\n  theme_bw()+\n  labs(x = \"Partisanship\",\n       y = \"Predicted Vaccine Skepticism\",\n       title = \"Independents are the most skeptical of vaccines\",\n       subtitle = \"Data: 2016 NES\"\n       )"
  },
  {
    "objectID": "slides/11-slides.html#interpreting-model-2-2",
    "href": "slides/11-slides.html#interpreting-model-2-2",
    "title": "Week 11:",
    "section": "Interpreting Model 2",
    "text": "Interpreting Model 2\n\nWe see from Model 2 that 29.7 percent [27.3%, 32.1%] of Independents in the 2016 NES were predicted to be vaccine skeptics compared to 23.7 percent [20.8%, 26.5%] of Strong Democrats and only 20.1 percent [16.9%, 23.3%] of Strong Republicans."
  },
  {
    "objectID": "slides/11-slides.html#testing-for-partisan-differences-model-3",
    "href": "slides/11-slides.html#testing-for-partisan-differences-model-3",
    "title": "Week 11:",
    "section": "Testing for Partisan Differences: Model 3",
    "text": "Testing for Partisan Differences: Model 3\nModel 3 tells a similar story to model 2. Again, adjusting for differences in vaccine skepticism explained by age, Model 3 predicts that 41.7 percent [37.7%, 45.6%] of Independents in the 2016 NES are vaccine skeptics compared to 24.2 percent [22.1%, 26.2%] of Democrats, and 22.6 percent [20.4%, 24.8%] of Republicans.\nNote the coefficients from Model 3 imply that the differences between Independents and Democrats are statistically significant (\\(\\beta_{Ind} = 0.175, p &lt; 0.05\\)), the differences between Republicans and Democrats are not (\\(\\beta_{Rep} = -0.004, p = 0.31\\))\n\npred_df_m3 &lt;- expand_grid(\n  pid3cat = c(\"Democrat\", \"Independent\",\"Republican\"),\n  age = mean(nes16$age, na.rm=T)\n)\npred_df_m3 &lt;- cbind(pred_df_m3, predict(m3_2016,pred_df_m3, interval =\"confidence\"))\npred_df_m3\n\n      pid3cat      age       fit       lwr       upr\n1    Democrat 49.58231 0.2419547 0.2211228 0.2627867\n2 Independent 49.58231 0.4169043 0.3773539 0.4564547\n3  Republican 49.58231 0.2261496 0.2038046 0.2484947"
  },
  {
    "objectID": "slides/11-slides.html#testing-for-differences-over-time",
    "href": "slides/11-slides.html#testing-for-differences-over-time",
    "title": "Week 11:",
    "section": "Testing for Differences Over Time",
    "text": "Testing for Differences Over Time\nThe results for the 2016 NES suggest political independents are most skeptical of vaccines.\nThe results for 2020 suggest the relationship between partisanship and vaccine skepticism has changed overtime.\n\n\n\nPartisanship and Vaccine Skepticism\n\n\n\n\n \n\n\nNES 2016\n\n\nNES 2020\n\n\n\n\n \n\n\nMod 1\n\n\nMod 2\n\n\nMod 3\n\n\nMod 4\n\n\nMod 5\n\n\nMod 6\n\n\n\n\n\n\n(Intercept)\n\n\n0.458*\n\n\n0.350*\n\n\n0.417*\n\n\n0.343*\n\n\n0.318*\n\n\n0.352*\n\n\n\n\n \n\n\n(0.025)\n\n\n(0.035)\n\n\n(0.023)\n\n\n(0.018)\n\n\n(0.024)\n\n\n(0.016)\n\n\n\n\nPID (7pt)\n\n\n-0.005\n\n\n0.064*\n\n\n \n\n\n0.021*\n\n\n0.037*\n\n\n \n\n\n\n\n \n\n\n(0.003)\n\n\n(0.016)\n\n\n \n\n\n(0.002)\n\n\n(0.011)\n\n\n \n\n\n\n\nAge\n\n\n-0.004*\n\n\n-0.003*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.004*\n\n\n-0.003*\n\n\n\n\n \n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n(0.000)\n\n\n\n\nPID2 (7pt)\n\n\n \n\n\n-0.009*\n\n\n \n\n\n \n\n\n-0.002\n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.002)\n\n\n \n\n\n \n\n\n(0.001)\n\n\n \n\n\n\n\nIndependent\n\n\n \n\n\n \n\n\n0.175*\n\n\n \n\n\n \n\n\n0.200*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.023)\n\n\n \n\n\n \n\n\n(0.016)\n\n\n\n\nRepublican\n\n\n \n\n\n \n\n\n-0.016\n\n\n \n\n\n \n\n\n0.100*\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.016)\n\n\n \n\n\n \n\n\n(0.011)\n\n\n\n\nR2\n\n\n0.023\n\n\n0.028\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nAdj. R2\n\n\n0.022\n\n\n0.027\n\n\n0.042\n\n\n0.032\n\n\n0.032\n\n\n0.045\n\n\n\n\nNum. obs.\n\n\n3494\n\n\n3494\n\n\n3507\n\n\n7041\n\n\n7041\n\n\n7052\n\n\n\n\n\n\n*p &lt; 0.05\n\n\n\n\n\n\nThe coefficient on partisanship in model 4 is now positive and statistically significant (p &lt; 0.05), suggesting that as respondents become more Republican, they are more likely to be skeptical of vaccines\nThe coefficients from Model 5 suggest the relationship between partisanship skepticism is non linear, which is confirmed by model 6.\nIn Model 6, we see that independents remain the most skeptical of vaccines in 2020 \\((\\beta = 0.20,\\, p &lt;0.05)\\), but that Republicans now tend to be more skeptical of vaccines than Democrats \\((\\beta = 0.10,\\, p &lt;0.05)\\)\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/02-slides.html#class-plan",
    "href": "slides/02-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nAnnouncments\nSetup (5 minutes)\nReview\n\nTroubleshooting Errors (5 min)\nData wrangling in R (20 min)\nDescriptive Statistics (10 min)\n\nData Visualization (40 min)\n\nThe grammar of graphics\nBasic plots to describe:\n\nDistributions\nAssociations"
  },
  {
    "objectID": "slides/02-slides.html#announcements",
    "href": "slides/02-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\nManuel’s office hours today\nPaul’s office hours on Thursday\nSubmit tutorials from last week for full credit by this Sunday.\nGroups for the course assigned next week"
  },
  {
    "objectID": "slides/02-slides.html#setup-for-today",
    "href": "slides/02-slides.html#setup-for-today",
    "title": "POLS 1600",
    "section": "Setup for today",
    "text": "Setup for today"
  },
  {
    "objectID": "slides/02-slides.html#libraries",
    "href": "slides/02-slides.html#libraries",
    "title": "POLS 1600",
    "section": "Libraries",
    "text": "Libraries\nThis week we’ll use the following libraries.\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"tinytex\", \"kableExtra\",\n  \n  ## Tidyverse\n  \"tidyverse\",\"lubridate\", \"forcats\", \"haven\",\"labelled\",\n  \n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\",\"ggpubr\",\n  \"GGally\",\n  \n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"DT\"\n)\nthe_packages\n\n [1] \"tinytex\"    \"kableExtra\" \"tidyverse\"  \"lubridate\"  \"forcats\"   \n [6] \"haven\"      \"labelled\"   \"ggmap\"      \"ggrepel\"    \"ggridges\"  \n[11] \"ggthemes\"   \"ggpubr\"     \"GGally\"     \"COVID19\"    \"maps\"      \n[16] \"mapdata\"    \"DT\""
  },
  {
    "objectID": "slides/02-slides.html#installing-and-loading-new-packages",
    "href": "slides/02-slides.html#installing-and-loading-new-packages",
    "title": "POLS 1600",
    "section": "Installing and loading new packages",
    "text": "Installing and loading new packages\nNext we’ll create a function called ipak (thanks Steven) which:\n\nTakes a list of packages (pkg)\nChecks to see if these packages are installed\nInstalls any new packages\nLoads all the packages so we can use them\n\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}\n\nAgain, run this code on your machines"
  },
  {
    "objectID": "slides/02-slides.html#installing-and-loading-new-packages-1",
    "href": "slides/02-slides.html#installing-and-loading-new-packages-1",
    "title": "POLS 1600",
    "section": "Installing and loading new packages",
    "text": "Installing and loading new packages\nFinally, let’s use ipak to install and load the_packages\nWhat should we replace some_function and some_input with to do this?\n\nsome_function(some_input)\n\n\n\nipak(the_packages)\n\n   tinytex kableExtra  tidyverse  lubridate    forcats      haven   labelled \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n     ggmap    ggrepel   ggridges   ggthemes     ggpubr     GGally    COVID19 \n      TRUE       TRUE       TRUE       TRUE       TRUE       TRUE       TRUE \n      maps    mapdata         DT \n      TRUE       TRUE       TRUE \n\n\n\nR may ask you to install a package’s dependencies (other packages your package needs). Try entering the number 1 into your console\nR may tell you need to restart R Try saying yes. If it doesn’t start downloading, say no\nR may then ask if you want to compile some packages from source. Type Y into your console. If this doesn’t work, try again, but this time type N when asked"
  },
  {
    "objectID": "slides/02-slides.html#loading-the-covid-19-data",
    "href": "slides/02-slides.html#loading-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Loading the Covid-19 Data",
    "text": "Loading the Covid-19 Data\nLet’s load the Covid-19 data we worked with last week:\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))"
  },
  {
    "objectID": "slides/02-slides.html#section",
    "href": "slides/02-slides.html#section",
    "title": "POLS 1600",
    "section": "",
    "text": "XKCD"
  },
  {
    "objectID": "slides/02-slides.html#two-kinds-of-errors",
    "href": "slides/02-slides.html#two-kinds-of-errors",
    "title": "POLS 1600",
    "section": "Two kinds of errors:",
    "text": "Two kinds of errors:\n\nSyntactic\n\nR doesn’t understand how to run your code\nMost common, easy to fix (eventually…)\n\n\n\n\nSemantic\n\nR runs your code but doesn’t give you the expected result\nLess common, harder to fix\n\n\n\n\nMost errors happen because R is looking for something that isn’t there.\nMore discussion here and here"
  },
  {
    "objectID": "slides/02-slides.html#common-syntactic-errors",
    "href": "slides/02-slides.html#common-syntactic-errors",
    "title": "POLS 1600",
    "section": "Common Syntactic Errors",
    "text": "Common Syntactic Errors\n\nUnmatched parentheses or brackets\nMisspelled a name\nForgot a comma\nForgot to install a package or load a library\nForgot to set the working directory/path to a file you want R to use.\nTried to select a column or row that doesn’t exist"
  },
  {
    "objectID": "slides/02-slides.html#fixing-syntactic-errors",
    "href": "slides/02-slides.html#fixing-syntactic-errors",
    "title": "POLS 1600",
    "section": "Fixing Syntactic Errors",
    "text": "Fixing Syntactic Errors\n\nR Studio’s script editor will show a red circle with a white x in next to a line of code it thinks has an error in it.\nHave someone else look at your code (Fresh eyes, paired programming)\nCopy and paste the “general part” of error message into Google.\nKnit your document after each completed code chunk\n\nThis will run the code from top to bottom, and stop when it encounters an error\nTry commenting out the whole chunk, and then uncommenting successive lines of code\n\nBe patient. Don’t be hard are yourself. Remember, errors are portals of discovery."
  },
  {
    "objectID": "slides/02-slides.html#semantic-errors",
    "href": "slides/02-slides.html#semantic-errors",
    "title": "POLS 1600",
    "section": "Semantic Errors",
    "text": "Semantic Errors\n\nYour code runs, but doesn’t produce what you expected.\nLess common; can be harder to identify and fix\nOne example: Two packages have a function with the same name that do different things\n\n\n\n# dplyr::summarize\n# Hmisc::summarize"
  },
  {
    "objectID": "slides/02-slides.html#semantic-errors-1",
    "href": "slides/02-slides.html#semantic-errors-1",
    "title": "POLS 1600",
    "section": "Semantic Errors",
    "text": "Semantic Errors\n\nSome general solutions/practices to avoid semantic errors:\n\nSpecify the package and the function you want: package_name::function_name()\nWrite helpful comments in your code.\nInclude “sanity” checks in your code.\nIf a function should produce an output that’s a data.frame, check to see if it is a data frame\n\n\n\n\n# Here's some pseudo code:\n\n# I expect my_function produces a data frame\nx &lt;- my_function(y) \n\n# Check to see if x is a data frame\n# If x is not a data frame, return an Error\nstopifnot(is.data.frame(x))"
  },
  {
    "objectID": "slides/02-slides.html#why-do-we-need-to-wrangle-data",
    "href": "slides/02-slides.html#why-do-we-need-to-wrangle-data",
    "title": "POLS 1600",
    "section": "Why do we need to “wrangle” data",
    "text": "Why do we need to “wrangle” data\n\nRarely, if ever, do we get data in the exact format we need.\nInstead, before we can get to work, we often need to transform our data in various ways\nSometimes called:\n\nData cleaning/recoding\nData wrangling\nData carpentry\n\nThe end goal is the same: make messy data tidy"
  },
  {
    "objectID": "slides/02-slides.html#tidy-data",
    "href": "slides/02-slides.html#tidy-data",
    "title": "POLS 1600",
    "section": "Tidy data",
    "text": "Tidy data\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value"
  },
  {
    "objectID": "slides/02-slides.html#tools-for-transforming-our-data",
    "href": "slides/02-slides.html#tools-for-transforming-our-data",
    "title": "POLS 1600",
    "section": "Tools for transforming our data",
    "text": "Tools for transforming our data\nLast week we used the following functions:\n\nread_csv() and data() to read and load data in R\nlogical operators like &, |, %in% ==, !=, &gt;,&gt;=,&lt;,&lt;= to make comparisons\nthe pipe command %&gt;% to “pipe” the output of one function into another\nfilter() to pick observations (rows) by their values\narrange() to reorder rows\nselect() to pick variables by their names\nmutate() and case_when() command to create new variables in our data set\nsummarise() to collapse many values into a single value (like a mean or median)\ngroup_by() to apply functions like mutate() and summarise() on a group-by-group basis"
  },
  {
    "objectID": "slides/02-slides.html#common-functions-for-transforming-data",
    "href": "slides/02-slides.html#common-functions-for-transforming-data",
    "title": "POLS 1600",
    "section": "Common functions for transforming data",
    "text": "Common functions for transforming data\nAll of these “verb” functions from the dplyr package (e.g. filter(),mutate()) follow a similar format:\n\nTheir first argument is a data frame\nThe subsequent arguments tell R what to do with the data frame, using the variable names (without quotes)\nThe output is a new data frame\n\nMore"
  },
  {
    "objectID": "slides/02-slides.html#you-trying-to-get-the",
    "href": "slides/02-slides.html#you-trying-to-get-the",
    "title": "POLS 1600",
    "section": "You trying to get the %>%?",
    "text": "You trying to get the %&gt;%?"
  },
  {
    "objectID": "slides/02-slides.html#the-pipe-command",
    "href": "slides/02-slides.html#the-pipe-command",
    "title": "POLS 1600",
    "section": "The pipe command %>%",
    "text": "The pipe command %&gt;%\n\nThe pipe command is way of “chaining” lines of code together, piping the results of one tidyverse function into the next function.\nThe pipe command works because these functions always expect a data frame as their first argument, and always produce a data frame as their output."
  },
  {
    "objectID": "slides/02-slides.html#the-pipe-command-1",
    "href": "slides/02-slides.html#the-pipe-command-1",
    "title": "POLS 1600",
    "section": "The pipe command %>%",
    "text": "The pipe command %&gt;%\n\nsummarise(\n  data = df,\n  mean = mean(var1, na.rm = T),\n  median = median(var1, na.rm = T)\n )\n# Rewrite with a pipe:\n\ndf %&gt;% \n  summarize(\n    mean = mean(var1, na.rm = T),\n    median = median(var1, na.rm = T)    \n  )"
  },
  {
    "objectID": "slides/02-slides.html#wrangling-the-covid-19-data",
    "href": "slides/02-slides.html#wrangling-the-covid-19-data",
    "title": "POLS 1600",
    "section": "Wrangling the Covid-19 data",
    "text": "Wrangling the Covid-19 data\nTo work with the Covid-19 data we did the following:\n\nSubsetted/Filtered the data to exclude US Territories\nCreated new variables from existing variables in the data to use in our final analysis"
  },
  {
    "objectID": "slides/02-slides.html#wrangling-the-covid-19-data-1",
    "href": "slides/02-slides.html#wrangling-the-covid-19-data-1",
    "title": "POLS 1600",
    "section": "Wrangling the Covid-19 data",
    "text": "Wrangling the Covid-19 data\nSpecifically, we did the following:\n\nCreated an object called territories that is a vector containing the names of U.S. territories\nCreated a new dataframe, called covid_us, by filtering out observations from the U.S. territories\nCreated a state variable that is a copy of the administrative_area_level_2\nCreated a variable called new_cases from the confirmed. Create a variable called new_cases_pc that is the number of new Covid-19 cases per 100,000 citizens\nCreated a variable called face_masks from the facial_coverings variable.\nCalculated the average number of new cases, by different levels of face_masks\n\n\nLet’s take some time to make sure we understand everything that was happening."
  },
  {
    "objectID": "slides/02-slides.html#created-an-object-called-territories",
    "href": "slides/02-slides.html#created-an-object-called-territories",
    "title": "POLS 1600",
    "section": "Created an object called territories",
    "text": "Created an object called territories\n\n# - 1. Create territories object\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n\nThe object territories now exists in our environment."
  },
  {
    "objectID": "slides/02-slides.html#created-a-new-dataframe-called-covid_us",
    "href": "slides/02-slides.html#created-a-new-dataframe-called-covid_us",
    "title": "POLS 1600",
    "section": "Created a new dataframe, called covid_us",
    "text": "Created a new dataframe, called covid_us\n\n\nTaskCode\n\n\n\nUse the filter() command to select only the rows where the administrative_area_level_2 is not (!) in (%in%) the territories object\n\n\n\n\n# - 2. Create covid_us data frame\n# How many rows and columns in covid\ndim(covid)\n\n[1] 58809    47\n\n# Filter out obs from US territories\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories)\n\n# covid_us should have fewer rows than covid\ndim(covid_us)\n\n[1] 53678    47"
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-state",
    "href": "slides/02-slides.html#created-a-variable-called-state",
    "title": "POLS 1600",
    "section": "Created a variable called state",
    "text": "Created a variable called state\n\n\nTaskCode\n\n\nCopy administrative_area_level_2 into a new variable called state\n\n\n\n\n\n\nNote\n\n\nNote that we have to save the output of mutate back into covid_us for our state to exist as new column in covid_us\n\n\n\n\n\n\ndim(covid_us)\n\n[1] 53678    47\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2\n  ) -&gt; covid_us\ndim(covid_us)\n\n[1] 53678    48\n\nnames(covid_us)[48]\n\n[1] \"state\""
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-state-1",
    "href": "slides/02-slides.html#created-a-variable-called-state-1",
    "title": "POLS 1600",
    "section": "Created a variable called state",
    "text": "Created a variable called state\nNow there’s a new column in covid_us called state, that we can access by calling covid_us$state\n\ncovid_us$state[1:5] # Just show first 5 observations\n\n[1] \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\" \"Minnesota\"\n\n\n\nWe could have done the same thing in “Base” R\n\ncovid_us$state &lt;- covid_us$administrative_area_level_2\n\n\n\nWhy didn’t we?\n\nConsistent preference for tidyverse &gt; base R\nSaves time when recoding lots of variables\nmutate() plays nicely with functions like group_by()"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases from the confirmed variable",
    "text": "Create a variable called new_cases from the confirmed variable\nThe confirmed variable contains a running total of confirmed cases in a given state on a given day.\nVizualing data helps us understand how we might need to transform our data"
  },
  {
    "objectID": "slides/02-slides.html#visualize-confirmed-variable-for-rhode-island",
    "href": "slides/02-slides.html#visualize-confirmed-variable-for-rhode-island",
    "title": "POLS 1600",
    "section": "Visualize confirmed variable for Rhode Island",
    "text": "Visualize confirmed variable for Rhode Island\n\nCodePlotData\n\n\n\noptions(scipen = 999) # No scientific notation\ncovid_us %&gt;% \n  filter(state == \"Rhode Island\") %&gt;% \n  ggplot(aes(\n    x = date,\n    y = confirmed\n  ))+\n  geom_point()+\n  theme_bw() +\n  labs(title = \"Total Covid-19 cases in Rhode Island\",\n       y = \"Total Cases\",\n       x = \"Date\") -&gt; fig_ri_covid"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable-1",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases-from-the-confirmed-variable-1",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases from the confirmed variable",
    "text": "Create a variable called new_cases from the confirmed variable\n\nTaskCodeData\n\n\nTake the difference between a given day’s value of confirmed and yesterday’s value of confirmed to create a measure of new_cases on a given date for each state\n\n\n\n\n\n\nNote\n\n\n\nUse lag() to shift values in a column down one row in the data\nUse group_by() to respect the state-date structure of the data\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed)\n  ) -&gt; covid_us"
  },
  {
    "objectID": "slides/02-slides.html#create-a-variable-called-new_cases_pc",
    "href": "slides/02-slides.html#create-a-variable-called-new_cases_pc",
    "title": "POLS 1600",
    "section": "Create a variable called new_cases_pc",
    "text": "Create a variable called new_cases_pc\n\n\nTaskCode - WranglingCode - CheckingData\n\n\n\nScale new_cases by population to create a per capita measure (new_cases_pc)\n\n\n\n\n\n\n\nNote\n\n\nWe can create multiple variables in a single mutate() by separating lines of code with a ,\n\n\n\n\n\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases / population *100000\n    ) -&gt;covid_us\n\n\n\n\n# Check recoding\ncovid_us %&gt;% \n  # Look at two states\n  filter(state == \"Rhode Island\" | state == \"New York\") %&gt;% \n  # In a small date range\n  filter(date &gt; \"2021-01-01\" & date &lt; \"2021-01-05\") %&gt;% \n  # Select only the columns we want\n  select(state, date, new_cases, new_cases_pc) -&gt; hlo_df\n# save to object hlo_df\n\n\n\n\nhlo_df\n\n# A tibble: 6 × 4\n# Groups:   state [2]\n  state        date       new_cases new_cases_pc\n  &lt;chr&gt;        &lt;date&gt;         &lt;int&gt;        &lt;dbl&gt;\n1 Rhode Island 2021-01-02         0          0  \n2 Rhode Island 2021-01-03         0          0  \n3 Rhode Island 2021-01-04      4759        449. \n4 New York     2021-01-02     15849         81.5\n5 New York     2021-01-03     12232         62.9\n6 New York     2021-01-04     11242         57.8"
  },
  {
    "objectID": "slides/02-slides.html#created-a-variable-called-face_masks",
    "href": "slides/02-slides.html#created-a-variable-called-face_masks",
    "title": "POLS 1600",
    "section": "Created a variable called face_masks",
    "text": "Created a variable called face_masks\n\n\nTaskHLOCodeCheck\n\n\nCreate a variable called face_masks from the facial_coverings that describes the face mask policy experienced by most people in a given state on a given date.\n\n\n\n\n\n\nNote\n\n\n\nUse case_when() inside of mutate() to create a variable that takes certain values when certain logical statements are true\nSeting the levels = c(value1, value2, etc.) argument in factor() lets us control the ordering of categorical/character data.\n\n\n\n\n\n\nRecall, that the facial_coverings variable took on range of substantive values from 0 to 4, but empirically could take both positve and negative values\n\ntable(covid_us$facial_coverings)\n\n\n   -4    -3    -2    -1     0     1     2     3     4 \n  410  5897  7362   275  3893  8604 17424  9191   622 \n\n\n\n\n\ncovid_us %&gt;%\nmutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n    ) %&gt;% factor(.,\n      levels = c(\"No policy\",\"Recommended\",\n                 \"Some requirements\",\n                 \"Required shared places\",\n                 \"Required all times\")\n    ) \n    ) -&gt; covid_us\n\n\n\n\ncovid_us%&gt;%\n  filter(state == \"Illinois\", date &gt; \"2020-9-28\") %&gt;%\n  select(state, date, facial_coverings, face_masks) %&gt;% \n  slice(1:5)\n\n# A tibble: 5 × 4\n# Groups:   state [1]\n  state    date       facial_coverings face_masks        \n  &lt;chr&gt;    &lt;date&gt;                &lt;int&gt; &lt;fct&gt;             \n1 Illinois 2020-09-29                2 Some requirements \n2 Illinois 2020-09-30                2 Some requirements \n3 Illinois 2020-10-01               -4 Required all times\n4 Illinois 2020-10-02               -4 Required all times\n5 Illinois 2020-10-03               -4 Required all times"
  },
  {
    "objectID": "slides/02-slides.html#addtional-recoding",
    "href": "slides/02-slides.html#addtional-recoding",
    "title": "POLS 1600",
    "section": " Addtional recoding",
    "text": "Addtional recoding\nIn last week’s lab, we also added the following\n\ncovid_us %&gt;%\n  mutate(\n    year = year(date),\n    month = month(date),\n    year_month = paste(\n      year, \n      str_pad(month, width = 2, pad=0), \n      sep = \"-\"\n      ),\n    percent_vaccinated = people_fully_vaccinated/population*100  \n    ) -&gt; covid_us"
  },
  {
    "objectID": "slides/02-slides.html#working-with-dates",
    "href": "slides/02-slides.html#working-with-dates",
    "title": "POLS 1600",
    "section": " Working with dates",
    "text": "Working with dates\nR treat’s dates differently\n\ncovid_us$date[1:3]\n\n[1] \"2020-01-01\" \"2020-01-02\" \"2020-01-03\"\n\nclass(covid_us$date)\n\n[1] \"Date\"\n\n\nIf R knows a variable is a date, we can extract components of that date, using functions from the lubridate package\n\nyear(covid_us$date[1:3])\n\n[1] 2020 2020 2020\n\nmonth(covid_us$date[1:3])\n\n[1] 1 1 1"
  },
  {
    "objectID": "slides/02-slides.html#the-str_pad-and-paste-function",
    "href": "slides/02-slides.html#the-str_pad-and-paste-function",
    "title": "POLS 1600",
    "section": " The str_pad() and paste() function",
    "text": "The str_pad() and paste() function\n\nThe str_pad() function lets us ‘pad’ strings so that they’re all the same width\n\n\nmonth(covid_us$date[1:3])\n\n[1] 1 1 1\n\nstr_pad(month(covid_us$date[1:3]), width=2, pad = 0)\n\n[1] \"01\" \"01\" \"01\"\n\n\n\nThe paste function lets us paste objects together.\n\n\npaste(year(covid_us$date[1:3]),\n      str_pad(month(covid_us$date[1:3]), width=2, pad = 0),\n      sep = \"-\"\n      )\n\n[1] \"2020-01\" \"2020-01\" \"2020-01\""
  },
  {
    "objectID": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy",
    "href": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy",
    "title": "POLS 1600",
    "section": "Summarizing the averge number of new_cases by face_mask policy",
    "text": "Summarizing the averge number of new_cases by face_mask policy\n\nTaskCodeResults\n\n\nCalculate the mean (average) number of new_cases of Covid-19 when each type of face_mask policy was in effect\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() command will do each calculation inside of summarise() for each level of the grouping variable\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks)) %&gt;%\n  group_by(face_masks) %&gt;%\n  summarize(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  ) -&gt; face_mask_summary\n\n\n\n\nface_mask_summary\n\n# A tibble: 5 × 2\n  face_masks             new_cases_pc\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 No policy                      10.3\n2 Recommended                    16.6\n3 Some requirements              36.2\n4 Required shared places         29.4\n5 Required all times             32.2"
  },
  {
    "objectID": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy-by-month",
    "href": "slides/02-slides.html#summarizing-the-averge-number-of-new_cases-by-face_mask-policy-by-month",
    "title": "POLS 1600",
    "section": "Summarizing the averge number of new_cases by face_mask policy by month",
    "text": "Summarizing the averge number of new_cases by face_mask policy by month\n\nTaskCodeResults\n\n\nCalculate the mean (average) number of new_cases of Covid-19 when each type of face_mask policy was in effect for each year_month in our dataset\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() command can group on multiple variables\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  group_by(face_masks, year_month) %&gt;%\n  summarize(\n    new_cases_pc = mean(new_cases_pc, na.rm=T)\n  ) -&gt; cases_by_month_and_policy\n\n\n\n\ncases_by_month_and_policy\n\n# A tibble: 102 × 3\n# Groups:   face_masks [5]\n   face_masks year_month new_cases_pc\n   &lt;fct&gt;      &lt;chr&gt;             &lt;dbl&gt;\n 1 No policy  2020-01        0.000463\n 2 No policy  2020-02        0.00188 \n 3 No policy  2020-03        1.70    \n 4 No policy  2020-04        6.50    \n 5 No policy  2022-04       19.8     \n 6 No policy  2022-05       20.4     \n 7 No policy  2022-06       37.6     \n 8 No policy  2022-07       36.2     \n 9 No policy  2022-08       35.7     \n10 No policy  2022-09       19.0     \n# ℹ 92 more rows\n\n# In base R:\nmean(\n  covid_us$new_cases_pc[\n    covid_us$face_masks == \"No policy\" &\n      covid_us$year_month == \"2020-01\"], na.rm = T)\n\n[1] 0.0004626161"
  },
  {
    "objectID": "slides/02-slides.html#concept-check",
    "href": "slides/02-slides.html#concept-check",
    "title": "POLS 1600",
    "section": " Concept check",
    "text": "Concept check\nSuppose you want to do the following, what function or functions would you use:\n\nRead data into R\nLook at the data to get a high level overview of its structure\nSubset or filter the data to include just observations with certain values\nSelect specific columns from data\nAdd new columns to the data\nSummarize multiple values by collapsing them into a single value\nDoing some function group-by-group?"
  },
  {
    "objectID": "slides/02-slides.html#concept-check-1",
    "href": "slides/02-slides.html#concept-check-1",
    "title": "POLS 1600",
    "section": "Concept check",
    "text": "Concept check\nSuppose you want to do the following, what function or functions would you use:\n\nRead data into R\n\nread_xxx() (tidy), read.xxx() (base)\n\nLook at the data to get a high level overview of its structure\n\nhead(), tail(), glimpse(), table(), summary(), View()\n\nSubset the data to include just obersvations with certain values\n\ndata %&gt;% filter(x &gt; 0), data[data$x &gt; 0], subset(data, x &gt; 0)\n\nSelect specific columns from data\n\ndata$variable, data %&gt;% select(variable1, variable2), data[,c(\"x1\",\"x2\")]\n\nAdd new columns to the data\n\ndata %&gt;% mutate(x = y/10) data$x &lt;- data$y/10\n\nSummarize multiple values by collapsing them into a single value\n\ndata %&gt;% summarise(x_mn = mean(x, na.rm=T))\n\nDoing some function group-by-group?\n\ndata %&gt;% group_by(g) %&gt;% summarise(x_mn = mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/02-slides.html#concept-check-2",
    "href": "slides/02-slides.html#concept-check-2",
    "title": "POLS 1600",
    "section": "Concept check",
    "text": "Concept check\nShould you know exactly how to do all of this?\n\nNO! Of course not. For Pete’s sake, Paul, It’s only the second week\n\n\nWill you learn how to do much of this?\n\n\nMaybe, but I’m feeling pretty overwhelmed…\n\n\nHow will you learn how do these things?\n\n\nWith lots of practice, patience, and repetition motivated by a sense that these skills will help me learn about things I care about"
  },
  {
    "objectID": "slides/02-slides.html#advice-on-learning-how-to-code",
    "href": "slides/02-slides.html#advice-on-learning-how-to-code",
    "title": "POLS 1600",
    "section": "Advice on learning how to code",
    "text": "Advice on learning how to code\n\nIt takes lots of practice and lots of errors\n\nBreak long blocks of code into individual steps to see what’s happening\n\nCreate code chunks and FAFO\n\nJust clean up when you’re done…\n\nOnly dumb question is one you don’t ask\nGoogle, Stack Exchange are your friends\nTry writing out in comments what you want to do in code\nLearn to recognize patterns in the questions/tasks I give you:\n\nCopy and paste code I give\nChange one thing\nFix the error\nAdapt code from class to do a similar thing\n\nLearning to code is much less painful when you have a reason to do it\n\nLet me know what interests you"
  },
  {
    "objectID": "slides/02-slides.html#descriptive-statistics-1",
    "href": "slides/02-slides.html#descriptive-statistics-1",
    "title": "POLS 1600",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWhen social scientists talk about descriptive inference, we’re trying to summarize our data and make claims about what’s typical of our data\n\nWhat’s a typical value\n\nMeasures of central tendency\nmean, median, mode\n\nHow do our data vary around typical values\n\nMeasures of dispersion\nvariance, standard deviation, range, percentile ranges\n\nHow does variation in one variable relate to variation in another\n\nMeasures of association\ncovariance, correlation"
  },
  {
    "objectID": "slides/02-slides.html#using-r-to-summarize-data",
    "href": "slides/02-slides.html#using-r-to-summarize-data",
    "title": "POLS 1600",
    "section": "Using R to Summarize Data",
    "text": "Using R to Summarize Data\nHere are some common ways of summarizing data and how to calculate them with R\n\n\n\n\nDescription\nUsage\n\n\n\n\nsum\nsum(x)\n\n\nminimum\nmin(x)\n\n\nmaximum\nmax(x)\n\n\nrange\nrange(x)\n\n\nmean\nmean(x)\n\n\nmedian\nmedian(x)\n\n\npercentile\nquantile(x)\n\n\nvariance\nvar(x)\n\n\nstandard deviation\nsd(x)\n\n\nrank\nrank(x)\n\n\n\n\n\n\nAll of these functions have an argument called na.rm=F. If your data have missing values, you’ll need to set na.rm=F (e.g. mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of typical values\n\nMeans (mean()) all the time\nMedians (median()) useful for describing distributions of variables particularly those with extreme values\nMode useful for characterizing categorical data"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-1",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-1",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of typical variation\n\nvar() important for quantifying uncertainty, but rarely will you be calculating this directly\nsd() a good summary of a typical change in the data.\nrange(), min(), max() useful for exploring data, detecting outliers and potential values that need to be recoded"
  },
  {
    "objectID": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-2",
    "href": "slides/02-slides.html#what-you-need-to-know-for-pols-1600-2",
    "title": "POLS 1600",
    "section": "What you need to know for POLS 1600",
    "text": "What you need to know for POLS 1600\nMeasures of association\n\nCovariance (var()) central to describing relationships but generally not something you’ll calculate or interpret directly\nCorrelation (cor()) useful for describing [bivariate] relationships (positive or negative relationships)."
  },
  {
    "objectID": "slides/02-slides.html#what-you-dont-really-need-to-know-for-pols-1600-smaller",
    "href": "slides/02-slides.html#what-you-dont-really-need-to-know-for-pols-1600-smaller",
    "title": "POLS 1600",
    "section": "What you don’t really need to know for POLS 1600 {smaller}",
    "text": "What you don’t really need to know for POLS 1600 {smaller}\nWe won’t spend much time on the formal definitions, math, and proofs\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\n\\[\nM_x = X_i : \\int_{-\\infty}^{x_i} f_x(X)dx=\\int_{x_i}^\\infty f_x(X)dx=1/2\n\\]\n\nUseful eventually. Not necessary right now."
  },
  {
    "objectID": "slides/02-slides.html#data-visualizaiton",
    "href": "slides/02-slides.html#data-visualizaiton",
    "title": "POLS 1600",
    "section": "Data visualizaiton",
    "text": "Data visualizaiton\nData visualization is an incredibly valuable tool that helps us to\n\nExplore data, uncovering new relationships, as well as potential problems\nCommunicate our results clearly and precisely\n\nTake a look at how the BBC uses R to produce its graphics"
  },
  {
    "objectID": "slides/02-slides.html#data-visualization",
    "href": "slides/02-slides.html#data-visualization",
    "title": "POLS 1600",
    "section": "Data visualization",
    "text": "Data visualization\nToday, we will:\n\nIntroduce the grammar of graphics\nLearn how to apply this grammar with ggplot()\nIntroduce basic plots to describe\n\nUnivariate distributions\nBivariate relations"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics",
    "href": "slides/02-slides.html#the-grammar-of-graphics",
    "title": "POLS 1600",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nInspired by Wilkinson (2005)\n\nA statistical graphic is a mapping of data variables to aes thetic attributes of geom etric objects.\n\nAt a minimum, a graphic contains three core components:\n\ndata: the dataset containing the variables of interest.\naes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot For example: points, lines, and bars.\n\nIsmay and Kim (2022)"
  },
  {
    "objectID": "slides/02-slides.html#seven-layers-of-graphics",
    "href": "slides/02-slides.html#seven-layers-of-graphics",
    "title": "POLS 1600",
    "section": "Seven Layers of Graphics",
    "text": "Seven Layers of Graphics\nKesari (2018)"
  },
  {
    "objectID": "slides/02-slides.html#the-grammar-of-graphics-in-r",
    "href": "slides/02-slides.html#the-grammar-of-graphics-in-r",
    "title": "POLS 1600",
    "section": "The grammar of graphics in R",
    "text": "The grammar of graphics in R\nIn R, we’ll implement this grammar of graphics using the ggplot package\n\nLet’s take a look at your feedback to last week’s survey and see how we can visualize some of the in formation you provided"
  },
  {
    "objectID": "slides/02-slides.html#what-we-liked",
    "href": "slides/02-slides.html#what-we-liked",
    "title": "POLS 1600",
    "section": "What we liked",
    "text": "What we liked"
  },
  {
    "objectID": "slides/02-slides.html#what-we-disliked",
    "href": "slides/02-slides.html#what-we-disliked",
    "title": "POLS 1600",
    "section": "What we disliked",
    "text": "What we disliked"
  },
  {
    "objectID": "slides/02-slides.html#building-that-figure",
    "href": "slides/02-slides.html#building-that-figure",
    "title": "POLS 1600",
    "section": "Building that figure",
    "text": "Building that figure\n\nLook at the raw data\nRecode the raw data\nMake a basic plot, telling R the data, aesthetics, geometries, and statistics I want it to plot\nTinker with the data and plot’s scales, coordinates, labels and theme to make the figure look better"
  },
  {
    "objectID": "slides/02-slides.html#look-at-the-raw-data",
    "href": "slides/02-slides.html#look-at-the-raw-data",
    "title": "POLS 1600",
    "section": "1. Look at the raw data",
    "text": "1. Look at the raw data\n\ndf$trip\n\n&lt;labelled&lt;double&gt;[12]&gt;: You're on a road trip with friends. Who controls the music?\n [1] NA  3  1  2  3  2  2  2  1  2  2 NA\n\nLabels:\n value\n     1\n     2\n     3\n                                                                                                                               label\n                                                                                                                    The driver, duh.\n                                                                                                           The front seat, of course\n That jerk in the back who you don't even know but seems to have really strong feelings about Billy Joel's \"Only the good die young\""
  },
  {
    "objectID": "slides/02-slides.html#recode-the-raw-data",
    "href": "slides/02-slides.html#recode-the-raw-data",
    "title": "POLS 1600",
    "section": "2. Recode the raw data",
    "text": "2. Recode the raw data\n\ndf %&gt;%\n  mutate(\n    Playist = forcats::as_factor(trip)\n )%&gt;%\n  select(Playist)\n\n# A tibble: 12 × 1\n   Playist                                                                      \n   &lt;fct&gt;                                                                        \n 1  &lt;NA&gt;                                                                        \n 2 \"That jerk in the back who you don't even know but seems to have really stro…\n 3 \"The driver, duh.\"                                                           \n 4 \"The front seat, of course\"                                                  \n 5 \"That jerk in the back who you don't even know but seems to have really stro…\n 6 \"The front seat, of course\"                                                  \n 7 \"The front seat, of course\"                                                  \n 8 \"The front seat, of course\"                                                  \n 9 \"The driver, duh.\"                                                           \n10 \"The front seat, of course\"                                                  \n11 \"The front seat, of course\"                                                  \n12  &lt;NA&gt;"
  },
  {
    "objectID": "slides/02-slides.html#make-a-basic-plot",
    "href": "slides/02-slides.html#make-a-basic-plot",
    "title": "POLS 1600",
    "section": "3. Make a basic plot",
    "text": "3. Make a basic plot\n\nCodePlot\n\n\n\n#|\ndf %&gt;% #&lt;&lt; Raw data\n  mutate(\n    Playlist =forcats::as_factor(trip)\n  ) %&gt;% # Transformed data\n  ggplot(aes(x = Playlist, # Aesthetics\n             fill = Playlist))+\n  geom_bar( # Geometry\n    stat = \"count\" # Statistic\n    ) -&gt; fig_roadtrip\n\n\n\n\nfig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-data",
    "href": "slides/02-slides.html#tinker-with-data",
    "title": "POLS 1600",
    "section": "4.1 Tinker with data",
    "text": "4.1 Tinker with data\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\") -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-fill-aesthetic",
    "href": "slides/02-slides.html#tinker-with-fill-aesthetic",
    "title": "POLS 1600",
    "section": "4.2 Tinker with fill aesthetic",
    "text": "4.2 Tinker with fill aesthetic\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-coordinates",
    "href": "slides/02-slides.html#tinker-with-coordinates",
    "title": "POLS 1600",
    "section": "4.3 Tinker with coordinates",
    "text": "4.3 Tinker with coordinates\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer() +\n  coord_flip() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-labels",
    "href": "slides/02-slides.html#tinker-with-labels",
    "title": "POLS 1600",
    "section": "4.4 Tinker with labels",
    "text": "4.4 Tinker with labels\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n  scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")-&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#tinker-with-theme",
    "href": "slides/02-slides.html#tinker-with-theme",
    "title": "POLS 1600",
    "section": "4.4 Tinker with theme",
    "text": "4.4 Tinker with theme\n\nCodePlot\n\n\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n   scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")+\n  theme_bw() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#the-final-code",
    "href": "slides/02-slides.html#the-final-code",
    "title": "POLS 1600",
    "section": "The final code",
    "text": "The final code\n\ndf %&gt;%\n  filter(!is.na(trip)) %&gt;% \n  mutate(\n    Playlist =str_wrap(forcats::as_factor(trip),20)\n  ) %&gt;%\n  ggplot(aes(x = Playlist,\n             fill = Playlist))+\n  geom_bar(stat = \"count\")+\n   scale_fill_brewer(guide=\"none\")+\n  coord_flip()+\n  labs(title = \"Who controls the playlist\",\n       x= \"\",\n       y = \"\")+\n  theme_bw() -&gt; fig_roadtrip"
  },
  {
    "objectID": "slides/02-slides.html#describing-distributions-and-associations",
    "href": "slides/02-slides.html#describing-distributions-and-associations",
    "title": "POLS 1600",
    "section": "Describing Distributions and Associations",
    "text": "Describing Distributions and Associations\n\nIn the remaining slides, we’ see how to visualize some distributions and associations in the Covid data using:\n\nbarplots\nhistograms\ndensity plots\nboxplots\nline plots\nscatter plots"
  },
  {
    "objectID": "slides/02-slides.html#general-advice-for-making-figures",
    "href": "slides/02-slides.html#general-advice-for-making-figures",
    "title": "POLS 1600",
    "section": "General advice for making figures",
    "text": "General advice for making figures\n\nThink through conceptually how you want to figure to look\n\nDraw it out by hand\n\nMake a basic plot and iterate\nUse summarize() and other data wrangling skills to transform data for plotting\nUse factor() and related functions to control order of labels on axis\nUse google to figure out arcane options of ggplot\nDon’t let the perfect be the enemy of the good"
  },
  {
    "objectID": "slides/02-slides.html#barplots",
    "href": "slides/02-slides.html#barplots",
    "title": "POLS 1600",
    "section": "Barplots",
    "text": "Barplots\n\nQuestionBasic CodeBetter CodeFigure\n\n\nWhat was the most common face mask policy in the data?\n\n\n\ncovid_us %&gt;% \n  ggplot(aes(x=face_masks))+\n  geom_bar(stat = \"count\")\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  ungroup() %&gt;% \n  mutate(\n    face_masks = forcats::fct_infreq(face_masks)\n  ) %&gt;% \n  ggplot(aes(x=face_masks,\n             fill = face_masks))+\n  geom_bar()+\n  geom_text(stat='count', aes(label=..count..), \n            hjust=.5,vjust=-.5)+\n  guides(fill = \"none\")+\n  theme_bw()+\n  labs(\n    x = \"Face Mask Policy \",\n    title = \"\"\n  ) -&gt; fig_barplot"
  },
  {
    "objectID": "slides/02-slides.html#histogram",
    "href": "slides/02-slides.html#histogram",
    "title": "POLS 1600",
    "section": "Histogram",
    "text": "Histogram\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat does the distribution of new Covid-19 cases look like in June 2021\n\n\n\ncovid_us %&gt;% \n  filter(year_month == \"2021-06\") %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() -&gt; fig_hist1\n\n\n\n\nfig_hist1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(year_month == \"2021-06\") %&gt;% \n  filter(new_cases &gt; 0) %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() +\n  labs(\n    title = \"Exclude Negative Values\"\n  ) -&gt; fig_hist2a\n\ncovid_us %&gt;%\n  filter(year_month == \"2021-06\") %&gt;% \n  filter(new_cases &gt; 0) %&gt;% \n  ggplot(aes(x=new_cases))+\n  geom_histogram() +\n  scale_x_log10()+\n  labs(\n    title = \"Exclude Negative Values & Use log scale\"\n  ) -&gt; fig_hist2b\n\nfig_hist2 &lt;- ggarrange(fig_hist2a, fig_hist2b)"
  },
  {
    "objectID": "slides/02-slides.html#density-plots",
    "href": "slides/02-slides.html#density-plots",
    "title": "POLS 1600",
    "section": "Density Plots",
    "text": "Density Plots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat does the distribution of Covid-19 deaths look like?\n\n\n\ncovid_us %&gt;% \n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = deaths - lag(deaths)\n  ) %&gt;% \n  filter(new_deaths &gt; 0) %&gt;% \n  ggplot(aes(x=new_deaths_pc))+\n  geom_density() -&gt; fig_density1\n\n\n\n\nfig_density1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;% \n  mutate(\n    new_deaths = deaths - lag(deaths),\n    new_deaths_pc = deaths - lag(deaths),\n    year_f = factor(year)\n  ) %&gt;% \n  filter(new_deaths &gt; 0) %&gt;% \n  ggplot(aes(x=new_deaths_pc,\n             col = year_f))+\n  geom_density() +\n  geom_rug() +\n  scale_x_log10() +\n    facet_wrap(~month)+\n  theme(legend.position = \"bottom\")-&gt; \n  fig_density2"
  },
  {
    "objectID": "slides/02-slides.html#box-plots",
    "href": "slides/02-slides.html#box-plots",
    "title": "POLS 1600",
    "section": "Box plots",
    "text": "Box plots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nHow did the distribution of Covid-19 cases vary by face mask policy?\n\n\n\ncovid_us %&gt;%\n  filter(new_cases_pc &gt; 0) %&gt;% \n  ggplot(aes(x= face_masks, y=new_cases_pc))+\n  scale_y_log10()+\n  geom_boxplot() -&gt; fig_boxplot1\n\n\n\n\nfig_boxplot1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  mutate(\n    Month = lubridate::month(date, label = T)\n  ) %&gt;% \n  filter(new_cases_pc &gt; 0) %&gt;% \n  filter(year == 2020) %&gt;% \n ggplot(aes(x= face_masks, \n            y=new_cases_pc,\n            col = face_masks))+\n  scale_y_log10()+\n  coord_flip() +\n  geom_boxplot()  +\n    facet_wrap(~Month) +\n  theme(\n    legend.position = \"bottom\"\n  )-&gt; fig_boxplot2"
  },
  {
    "objectID": "slides/02-slides.html#line-graphs",
    "href": "slides/02-slides.html#line-graphs",
    "title": "POLS 1600",
    "section": "Line graphs",
    "text": "Line graphs\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nHow did vaccination rates vary by state?\n\n\n\ncovid_us %&gt;%\n  ggplot(\n    aes(x= date,\n        y=percent_vaccinated,\n        group = state\n        ))+\n  geom_line() -&gt; fig_line1\n\n\n\n\nfig_line1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Label = case_when(\n      date == max(date) & percent_vaccinated == max(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      date == max(date) & percent_vaccinated == median(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      date == max(date) & percent_vaccinated == min(percent_vaccinated[date == max(date)], na.rm = T) ~ state,\n      TRUE ~ NA_character_\n    ),\n    line_alpha = case_when(\n      state %in% c(\"District of Columbia\", \"Nebraska\", \"Wyoming\") ~ 1,\n      T ~ .3\n    ),\n    line_col = case_when(\n      state %in% c(\"District of Columbia\", \"Nebraska\", \"Wyoming\") ~ \"black\",\n      T ~ \"grey\"\n    )\n  ) %&gt;%\n  ggplot(\n    aes(x= date,\n        y=percent_vaccinated,\n        group = state\n        ))+\n  geom_line(\n    aes(alpha = line_alpha,\n        col =line_col)) +\n  geom_text_repel(aes(label = Label),\n                  direction = \"x\",\n                  nudge_y = 2) +\n  guides(\n    alpha = \"none\",\n    col = \"none\"\n  )+\n  xlim(ym(\"2021-01\"), ym(\"2023-01\")) +\n  labs(\n    y = \"Percent Vacinated\",\n    x = \"Date\"\n  ) +\n  theme_bw()-&gt; fig_line2"
  },
  {
    "objectID": "slides/02-slides.html#scatterplots",
    "href": "slides/02-slides.html#scatterplots",
    "title": "POLS 1600",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nQuestionBasic CodeEx 1Better CodeEx 2\n\n\nWhat’s the relationship between vaccination rates and new cases of Covid-19?\n\n\n\ncovid_us %&gt;%\n  ggplot(\n    aes(x= percent_vaccinated,\n        y=new_cases_pc,\n        ))+\n  geom_point() -&gt; fig_scatter1\n\n\n\n\nfig_scatter1\n\n\n\n\n\n\n\n\n\n\n\ncovid_us %&gt;%\n  filter(year &gt; 2020) %&gt;%\n  filter(month == 6) %&gt;%\n  filter(new_cases_pc &gt; 0) %&gt;%\n  ggplot(\n    aes(x= percent_vaccinated,\n        y=new_cases_pc,\n        ))+\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  facet_wrap(~year_month,ncol =1,\n             scales = \"free_y\")-&gt; fig_scatter2"
  },
  {
    "objectID": "slides/02-slides.html#summary-1",
    "href": "slides/02-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\n\nThe grammar of graphics provides a language for translating data into figures\nAt a minimum figures with ggplot() require three things:\n\ndata\naesthetic mappings\ngeometries\n\nTo produce a figure:\n\nthink about what the end product will look like\ntransform your data\nmap variables onto corresponding aesthetics\ntell R what to do with these aesthetic mappings\nRevise and iterate!\n\nLearning to code is hard, but the more errors you make now, the easier your life will be in the future\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/01-slides.html#class-plan",
    "href": "slides/01-slides.html#class-plan",
    "title": "POLS 1600",
    "section": "Class Plan",
    "text": "Class Plan\n\nLogistics (15 minutes)\n\nAnnouncements\nFeedback\n\nClass plan (60 minutes)\n\nIntroduction to R, R Studio and Quarto\nLoading and Looking at Data in R\nTransforming, Recoding, and Cleaning Data in R\nDescribing Data in R\nExploring Covid-19 Data for Lab"
  },
  {
    "objectID": "slides/01-slides.html#annoucements",
    "href": "slides/01-slides.html#annoucements",
    "title": "POLS 1600",
    "section": "Annoucements",
    "text": "Annoucements\n\nIf it’s your first time here you’ll need to work through Software Setup to follow along today\n\nTalk to me after class if you’re having installation issues\n\nIf you’re still on the waitlist on CAB, speak to me after class"
  },
  {
    "objectID": "slides/01-slides.html#announcements",
    "href": "slides/01-slides.html#announcements",
    "title": "POLS 1600",
    "section": "Announcements",
    "text": "Announcements\n\n\n“Uh ohhh, the Cavs are playing playoff basketball” pic.twitter.com/WrOrzeuEtW\n\n— Bottlegate ((Bottlegate?)) April 21, 2017"
  },
  {
    "objectID": "slides/01-slides.html#tutorials",
    "href": "slides/01-slides.html#tutorials",
    "title": "POLS 1600",
    "section": "Tutorials",
    "text": "Tutorials\nOnce you’ve done the following\n\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"PaulTestaBrown/qsslearnr\")\n\nYou can see the available problem sets by running the following code in your console:\n\nlearnr::run_tutorial(package = \"qsslearnr\")"
  },
  {
    "objectID": "slides/01-slides.html#tutorials-1",
    "href": "slides/01-slides.html#tutorials-1",
    "title": "POLS 1600",
    "section": "Tutorials",
    "text": "Tutorials\nAnd start a specific tutorial by running:\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nPlease upload tutorials 00-intro and 01-measurement1 to Canvas by Friday"
  },
  {
    "objectID": "slides/01-slides.html#youve-committed-a-murder",
    "href": "slides/01-slides.html#youve-committed-a-murder",
    "title": "POLS 1600",
    "section": "You’ve committed a murder",
    "text": "You’ve committed a murder"
  },
  {
    "objectID": "slides/01-slides.html#why",
    "href": "slides/01-slides.html#why",
    "title": "POLS 1600",
    "section": "Why",
    "text": "Why"
  },
  {
    "objectID": "slides/01-slides.html#why-do-you-ask",
    "href": "slides/01-slides.html#why-do-you-ask",
    "title": "POLS 1600",
    "section": "Why do you ask?",
    "text": "Why do you ask?\nIt’s cousin Nick’s fault…\n\nFunny icebreaker, but lots of assumptions…\n\nYou’re not a murderer\nYou don’t know someone who’s committed a murder or been murdered\nYou’ve got a mom and dad"
  },
  {
    "objectID": "slides/01-slides.html#why-do-you-ask-1",
    "href": "slides/01-slides.html#why-do-you-ask-1",
    "title": "POLS 1600",
    "section": "Why do you ask?",
    "text": "Why do you ask?\n\nHow might we make this question better?\n\nUse a screener question\n\n“Would you feel comfortable…”\n\n“Pipe” in responses from a prior question\n\n“Who are two people who raised you…”\n\n\n\n\nWhat questions we ask and how we ask them matters"
  },
  {
    "objectID": "slides/01-slides.html#hopes-and-dreams-fears-and-worries",
    "href": "slides/01-slides.html#hopes-and-dreams-fears-and-worries",
    "title": "POLS 1600",
    "section": "Hopes and Dreams, Fears and Worries",
    "text": "Hopes and Dreams, Fears and Worries\n\n\nWhat are we excited about?\n\nEngaging with social science\nLearning statistics and math\nLearning to code\n\n\nWhat are weworried about?\n\nEngaging with social science\nLearning statistics and math\nLearning to code"
  },
  {
    "objectID": "slides/01-slides.html#overview-1",
    "href": "slides/01-slides.html#overview-1",
    "title": "POLS 1600",
    "section": "Overview",
    "text": "Overview\n\nR, R Studio and Quarto\nGetting set up to work in R\nBasic Programming in R"
  },
  {
    "objectID": "slides/01-slides.html#r-r-studio-and-quarto",
    "href": "slides/01-slides.html#r-r-studio-and-quarto",
    "title": "POLS 1600",
    "section": "R, R Studio and Quarto",
    "text": "R, R Studio and Quarto\n\nR is an open source statistical programming language (cheatsheet)\nR Studio is an integrated development environment (IDE) that makes working in R much easier (cheatsheet)\nQuarto is a publishing system that allows us to write and present code in different formats (cheatsheet)"
  },
  {
    "objectID": "slides/01-slides.html#general-tuesday-workflow",
    "href": "slides/01-slides.html#general-tuesday-workflow",
    "title": "POLS 1600",
    "section": "General Tuesday Workflow",
    "text": "General Tuesday Workflow\n\nGo to https://pols1600.paultesta.org\nGo to class content for current week\nOpen slides in browser\nOpen R Studio\nCreate .qmd file titled wk01-notes.qmd and save in course folder\nGet set up to work\nTake notes and follow along"
  },
  {
    "objectID": "slides/01-slides.html#lets-create-a-.qmd-file",
    "href": "slides/01-slides.html#lets-create-a-.qmd-file",
    "title": "POLS 1600",
    "section": " Let’s create a .qmd file",
    "text": "Let’s create a .qmd file"
  },
  {
    "objectID": "slides/01-slides.html#three-components-of-a-.qmd",
    "href": "slides/01-slides.html#three-components-of-a-.qmd",
    "title": "POLS 1600",
    "section": "Three components of a .qmd",
    "text": "Three components of a .qmd\n\n\nControl output with YAML header\n\n  ---\n  title: \"Title here\"\n  author: \"Your name\"\n  format:\n    html:\n      toc: true\n  ---\n\nWrite code Blocks/Chunks\n\n```{r}\n#| echo: true\n2+2\n```\n\nDescribe code using Markdown\n\nSee Help &gt; Markdown quick reference"
  },
  {
    "objectID": "slides/01-slides.html#the-basics-of-r",
    "href": "slides/01-slides.html#the-basics-of-r",
    "title": "POLS 1600",
    "section": "The Basics of R",
    "text": "The Basics of R\n\nR is an interpreter (&gt;)\n“Everything that exists in R is an object”\n“Everything that happens in R is the result of a function”\nData come in different types, shapes, and sizes\nPackages make R great"
  },
  {
    "objectID": "slides/01-slides.html#r-is-an-interpreter",
    "href": "slides/01-slides.html#r-is-an-interpreter",
    "title": "POLS 1600",
    "section": "R is an interpreter (>)",
    "text": "R is an interpreter (&gt;)\nEnter commands line-by-line in the console\n\nThe &gt; means R is a ready for a command\nThe + means your last command isn’t complete\n\nIf you get stuck with a + use your escape key!\n\nSend code from .qmd file to the console:\n\ncntrl + Enter (PC) | cmd + Return (Mac) -&gt; run current line\ncntrl + shift + Enter (PC) | cmd + shift  + Return (Mac) -&gt; run all code in current chunk"
  },
  {
    "objectID": "slides/01-slides.html#r-is-a-calculator",
    "href": "slides/01-slides.html#r-is-a-calculator",
    "title": "POLS 1600",
    "section": "R is a Calculator",
    "text": "R is a Calculator\n\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n+\naddition\nx + y\n\n\n-\nsubtraction\nx - y\n\n\n*\nmultiplication\nx * y\n\n\n/\ndivision\nx / y\n\n\n^\nraised to the power of\nx ^ y\n\n\nabs\nabsolute value\nabs(x)\n\n\n%/%\ninteger division\nx %/% y\n\n\n%%\nremainder after division\nx %% y"
  },
  {
    "objectID": "slides/01-slides.html#r-is-logical",
    "href": "slides/01-slides.html#r-is-logical",
    "title": "POLS 1600",
    "section": "R is logical",
    "text": "R is logical\n\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n&\nand\nx & y\n\n\n|\nor\nx | y\n\n\nxor\nexactly x or y\nxor(x, y)\n\n\n!\nnot\n!x"
  },
  {
    "objectID": "slides/01-slides.html#r-is-logical-1",
    "href": "slides/01-slides.html#r-is-logical-1",
    "title": "POLS 1600",
    "section": "R is logical",
    "text": "R is logical\n\nx &lt;- T; y &lt;- F\n\nx == T\n\n[1] TRUE\n\nx == T & y == T\n\n[1] FALSE\n\nx == T | y == T\n\n[1] TRUE\n\n!x\n\n[1] FALSE"
  },
  {
    "objectID": "slides/01-slides.html#r-can-make-comparisons",
    "href": "slides/01-slides.html#r-can-make-comparisons",
    "title": "POLS 1600",
    "section": "R can make comparisons",
    "text": "R can make comparisons\n\n\n\n\nOperator\nDescription\nUsage\n\n\n\n\n&lt;\nless than\nx &lt; y\n\n\n&lt;=\nless than or equal to\nx &lt;= y\n\n\n&gt;\ngreater than\nx &gt; y\n\n\n&gt;=\ngreater than or equal to\nx &gt;= y\n\n\n==\nexactly equal to\nx == y\n\n\n!=\nnot equal to\nx != y\n\n\n%in%\ngroup membership*\nx %in% y\n\n\nis.na\nis missing\nis.na(x)\n\n\n!is.na\nis not missing\n!is.na(x)"
  },
  {
    "objectID": "slides/01-slides.html#everything-that-exists-in-r-is-an-object",
    "href": "slides/01-slides.html#everything-that-exists-in-r-is-an-object",
    "title": "POLS 1600",
    "section": "Everything that exists in R is an object",
    "text": "Everything that exists in R is an object\n\n\nThe number 5 is an object in R\n\n\n5\n\n[1] 5\n\n\n\nWe can assign the object 5, the name x, using the assignment operator &lt;-+\n\n\nx &lt;- 5 # Read this as \"x gets 5\"\n\n\nNow if we tell R to show us x, we’ll get\n\n\nx\n\n[1] 5\n\nprint(x)\n\n[1] 5"
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-types",
    "href": "slides/01-slides.html#data-come-in-different-types",
    "title": "POLS 1600",
    "section": "Data come in different types",
    "text": "Data come in different types"
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-types-1",
    "href": "slides/01-slides.html#data-come-in-different-types-1",
    "title": "POLS 1600",
    "section": "Data come in different types",
    "text": "Data come in different types\n\n\n\n# Create some data\n\n# Numeric\nx &lt;- 2 # Double\ny &lt;- 6L # Integer\n\n# Logical\nonly_two_types_of_people &lt;- TRUE \n\n# Character\nme &lt;- \"Paul\"\n\n# Factor\ngrades = factor(c(\"A\",\"B\",\"C\"))\n\n\n\n# What type are they?\nclass(x)\n\n[1] \"numeric\"\n\nclass(y)\n\n[1] \"integer\"\n\nclass(only_two_types_of_people)\n\n[1] \"logical\"\n\nclass(me)\n\n[1] \"character\"\n\nclass(grades)\n\n[1] \"factor\""
  },
  {
    "objectID": "slides/01-slides.html#data-come-in-different-shapes-and-sizes",
    "href": "slides/01-slides.html#data-come-in-different-shapes-and-sizes",
    "title": "POLS 1600",
    "section": "Data come in different “shapes” and “sizes”",
    "text": "Data come in different “shapes” and “sizes”\n\nSource: Gaurav Tiwari"
  },
  {
    "objectID": "slides/01-slides.html#section-1",
    "href": "slides/01-slides.html#section-1",
    "title": "POLS 1600",
    "section": "",
    "text": "Name\n“Size”\nType of Data\nR code\n\n\n\n\nscalar\n1\nnumeric, character, factor, logical\nx &lt;- 5\n\n\nvector\nN elements: length(x)\nall the same\nv &lt;- c(1, 2, T, \"false\")\n\n\nmatrix\nN rows by columns K: dim(x)\nall the same\nm &lt;- matrix(y,2,2)\n\n\narray\nN row by K column by J dimensions: dim(x)\nall the same\na &lt;- array(m,c(2,2,3))\n\n\ndata frames\nN row by K column matrix\ncan be different\nd &lt;-data.frame(x=x, y=y)\n\n\ntibbles\nN row by K column matrix\ncan be different\nd &lt;-tibble(x=x, y=y)\n\n\nlists\ncan vary\ncan be different\nl &lt;-list(x,y,m,a,d)"
  },
  {
    "objectID": "slides/01-slides.html#everything-that-happens-in-r-is-the-result-of-a-function",
    "href": "slides/01-slides.html#everything-that-happens-in-r-is-the-result-of-a-function",
    "title": "POLS 1600",
    "section": "Everything that happens in R is the result of a function",
    "text": "Everything that happens in R is the result of a function\n\nYou’ve already seen and used some R functions\n\nthe &lt;- is the assignement operator that assigns a value to a name\nc() is the combine function that combines elements together\ninstall.packages() installs packages\nlibrary() loads packages you’ve installed so you can use functions and data that are part of that package"
  },
  {
    "objectID": "slides/01-slides.html#three-sources-of-functions",
    "href": "slides/01-slides.html#three-sources-of-functions",
    "title": "POLS 1600",
    "section": "Three sources of functions",
    "text": "Three sources of functions\nThree sources of functions:\n\nbase R\n\n&lt;-; mean(x); library(\"package_name\")\n\npackages\n\ninstall.packages(\"packageName)\"\nremotes::intall_github(\"user/repository\")\n\nYou\n\nmy_function &lt;- function(x){x^2}"
  },
  {
    "objectID": "slides/01-slides.html#functions-are-like-recipes",
    "href": "slides/01-slides.html#functions-are-like-recipes",
    "title": "POLS 1600",
    "section": "Functions are like recipes",
    "text": "Functions are like recipes\nThey have:\n\n\n\nnames\ningredients (inputs)\nsteps that tell you what to do with the ingredients (statements/code)\ntasty results from applying those steps to given ingredients (outputs)\n\n\n (Source)"
  },
  {
    "objectID": "slides/01-slides.html#can-i-kick-it",
    "href": "slides/01-slides.html#can-i-kick-it",
    "title": "POLS 1600",
    "section": "Can I kick it?",
    "text": "Can I kick it?\n\ncan_x_kick_it &lt;- function(x){\n  # Determine if x can kick it\n  # If x in A Tribe Called Quest\n  if(x %in% c(\"Q-Tip\",\"Phife Dawg\",\n              \"Ali Shaheed Muhammad\", \n              \"Jarobi White\")){\n    return(\"Yes you can\")\n  }else{\n    return(\"Before this, did you really know what live was?\")\n  }\n\n}\ncan_x_kick_it(\"Q-Tip\")\n\n[1] \"Yes you can\"\n\ncan_x_kick_it(\"Paul\")\n\n[1] \"Before this, did you really know what live was?\""
  },
  {
    "objectID": "slides/01-slides.html#getting-setup-to-work-in-r",
    "href": "slides/01-slides.html#getting-setup-to-work-in-r",
    "title": "POLS 1600",
    "section": "Getting setup to work in R",
    "text": "Getting setup to work in R\nEach time you start a project in R, you will want to:\n\nSet your working directory\nLoad (and if needed, install) the R packages you will use\nSet any “global” options you want\nLoad the data you’ll be using"
  },
  {
    "objectID": "slides/01-slides.html#set-your-working-directory",
    "href": "slides/01-slides.html#set-your-working-directory",
    "title": "POLS 1600",
    "section": "Set your working directory",
    "text": "Set your working directory"
  },
  {
    "objectID": "slides/01-slides.html#load-and-if-needed-install-the-r-packages-you-will-use",
    "href": "slides/01-slides.html#load-and-if-needed-install-the-r-packages-you-will-use",
    "title": "POLS 1600",
    "section": "Load (and if needed, install) the R packages you will use",
    "text": "Load (and if needed, install) the R packages you will use\n\n\nInstall packages once1 with install.packages(\"package_name\")\nLoad packages every session with library(\"package_name\")\n\n\nOccasionally, you’ll have to update packages to newer versions and will likely need to reinstall when you upgrade R"
  },
  {
    "objectID": "slides/01-slides.html#install-packages-for-the-lab",
    "href": "slides/01-slides.html#install-packages-for-the-lab",
    "title": "POLS 1600",
    "section": " Install packages for the lab",
    "text": "Install packages for the lab"
  },
  {
    "objectID": "slides/01-slides.html#install-packages-for-the-lab-1",
    "href": "slides/01-slides.html#install-packages-for-the-lab-1",
    "title": "POLS 1600",
    "section": "Install packages for the lab",
    "text": "Install packages for the lab\nLet’s install the tidyverse and COVID19.\n\n\nCreate a new code chunk\nLabel it libraries\nCopy and paste the following into your console\n\n\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"COVID19\")\n\n\n\nOnce you’ve installed these packages comment out the code\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"COVID19\")\n\n\n\n\n\n\n\nKeyboard Shortcuts to toggle # comments\n\n\nmacOS: CMD + SHIFT + C\nPC: CTRL + SHIFT + C"
  },
  {
    "objectID": "slides/01-slides.html#loading-the-tidyverse-and-covid19-packages",
    "href": "slides/01-slides.html#loading-the-tidyverse-and-covid19-packages",
    "title": "POLS 1600",
    "section": "Loading the tidyverse and COVID19 packages",
    "text": "Loading the tidyverse and COVID19 packages\n\nType the following into your code chunk:\n\n\nlibrary(\"tidyverse\")\nlibrary(COVID19)"
  },
  {
    "objectID": "slides/01-slides.html#set-any-global-options-you-want",
    "href": "slides/01-slides.html#set-any-global-options-you-want",
    "title": "POLS 1600",
    "section": "Set any “global” options you want",
    "text": "Set any “global” options you want\nHere are the global options for these slides:1\n\n# Options for these slides\nknitr::opts_chunk$set(\n  warning = FALSE,       # Don't display warnings\n  message = FALSE,       # Don't display messages\n  comment = NA,          # No prefix before line of text\n  dpi = 300,             # Figure resolution\n  fig.align = \"center\",  # Figure alignment\n  out.width = \"80%\",     # Figure width\n  cache = FALSE          # Don't cache code chunks\n  )\n\nI’ll generally do this for you, but it’s useful to know that you can change options globally (for every chunk) and locallys (for specific chunks)"
  },
  {
    "objectID": "slides/01-slides.html#load-the-data-youll-be-using",
    "href": "slides/01-slides.html#load-the-data-youll-be-using",
    "title": "POLS 1600",
    "section": "Load the data you’ll be using",
    "text": "Load the data you’ll be using\nThere are three ways to load data.\n\nLoad a pre-existing dataset\n\ndata(\"dataset\") will load the dataset named “dataset”\ndata() will list all datasets\n\nLoad a .Rdata/.rda file using load(\"dataset.rda\")\nRead data of a different format (.csv, .dta, .spss) into R using specific functions from packages like haven and readr"
  },
  {
    "objectID": "slides/01-slides.html#overview-working-with-data-in-r",
    "href": "slides/01-slides.html#overview-working-with-data-in-r",
    "title": "POLS 1600",
    "section": "Overview: Working with Data in R",
    "text": "Overview: Working with Data in R\n\nLoading data into R\nLooking at your data\nCleaning and transforming your data"
  },
  {
    "objectID": "slides/01-slides.html#loading-data-into-r",
    "href": "slides/01-slides.html#loading-data-into-r",
    "title": "POLS 1600",
    "section": "Loading data into R",
    "text": "Loading data into R\nThere are three ways to load data.\n\nLoad a pre-existing dataset\n\ndata(\"dataset\") will load the dataset named “dataset”\ndata() will list all datasets\nUseful for tutorials, working through examples/help\n\nLoad a .Rdata/.rda file using load(\"dataset.rda\")\nRead data of a different format (.csv, .dta, .spss) into R using specific functions\n\nWe will use functions from the haven and readr packages to read data from the web and stored locally on your computer"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data",
    "href": "slides/01-slides.html#loading-state-level-covid-data",
    "title": "POLS 1600",
    "section": " Loading state-level Covid data",
    "text": "Loading state-level Covid data"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data-1",
    "href": "slides/01-slides.html#loading-state-level-covid-data-1",
    "title": "POLS 1600",
    "section": "Loading state-level Covid data",
    "text": "Loading state-level Covid data\nThe code below downloads two years of daily state-level Covid data:\n\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  start = \"2020-01-01\",\n  end = \"2022-12-31\",\n  level = 2,\n  verbose = F\n    \n)\n\nPlease run the following1\n\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n\n30 people trying to download this data live sometimes causes errors with the server"
  },
  {
    "objectID": "slides/01-slides.html#loading-state-level-covid-data-2",
    "href": "slides/01-slides.html#loading-state-level-covid-data-2",
    "title": "POLS 1600",
    "section": "Loading state-level Covid data",
    "text": "Loading state-level Covid data\n\n\ncountry = US tells the function we want data for the US\nstart = \"2020-01-01\" sets the start date for the data\nstart = \"2020-01-01\" sets the end date for the data\nlevel = 2 tells the function we want state-level data\nverbose = F tells the function not to print other stuff\n\n\n\ncovid &lt;- COVID19::covid19(\n  country = \"US\",\n  start = \"2020-01-01\",\n  end = \"2022-12-31\",\n  level = 2,\n  verbose = F\n    \n)"
  },
  {
    "objectID": "slides/01-slides.html#looking-at-your-data",
    "href": "slides/01-slides.html#looking-at-your-data",
    "title": "POLS 1600",
    "section": "Looking at your data",
    "text": "Looking at your data\nAnytime you load data into R, try some combination of the following to get a high-level overview (HLO) of the data\n\ndim(data) gives you the dimensions (# of rows and columns)\nView(data) opens data in a separate pane\nprint(data); data will display a truncated view of data in your console\nglimpse(data) will show a transposed (switch columns and rows) version of data with information on variable type\nhead(data) shows you the first 5 rows\ntail(data) shows you the last 5 rows\ndata$variable extracts variable from data\ntable(data$variable) creates a frequency table\n\nGood for categorical data\n\nsummary(data$variable) summary statistics\n\nGood for numeric data"
  },
  {
    "objectID": "slides/01-slides.html#hlos-allow-you-to",
    "href": "slides/01-slides.html#hlos-allow-you-to",
    "title": "POLS 1600",
    "section": "HLOs allow you to",
    "text": "HLOs allow you to\n\nDescribe the structure of your data:\n\nHow many observations (rows)\nHow many variables (columns)\n\nDescribe the unit of analysis\n\nWhat does a row in your data correspond to\n\nIdentify the class and type of variables (columns)\n\nNumeric, character, logical\nIs there missing data (NAs)\n\nFigure out what transformations, cleaning, and recoding you need to do"
  },
  {
    "objectID": "slides/01-slides.html#the-tidyverse",
    "href": "slides/01-slides.html#the-tidyverse",
    "title": "POLS 1600",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nFor more check out R for Data Science"
  },
  {
    "objectID": "slides/01-slides.html#tidy-data",
    "href": "slides/01-slides.html#tidy-data",
    "title": "POLS 1600",
    "section": "Tidy Data",
    "text": "Tidy Data\nTidy data is a standard way of mapping the meaning of a dataset to its structure.\nA dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nMore"
  },
  {
    "objectID": "slides/01-slides.html#dplyr-functions-for-data-wrangling",
    "href": "slides/01-slides.html#dplyr-functions-for-data-wrangling",
    "title": "POLS 1600",
    "section": "dplyr functions for data wrangling",
    "text": "dplyr functions for data wrangling\nToday and this week will begin learning some tools for selecting and transforming data:\n\nselect() to select columns from a dataframe\nfilter() to select rows from a dataframe when some statement is TRUE\nmutate() to create new colums\n\ncase_when() to recode values when some statement is TRUE\n\nsummarise() to transform many values into one value\ngroup_by() to create a grouped table so that other functions are applied separately to each group and then combined"
  },
  {
    "objectID": "slides/01-slides.html#the-pipe-operator",
    "href": "slides/01-slides.html#the-pipe-operator",
    "title": "POLS 1600",
    "section": "The %>% (“pipe” operator)",
    "text": "The %&gt;% (“pipe” operator)\nThe %&gt;% lets us chain functions together so we can read left to right\n\nslice(filter(covid, administrative_area_level_2 == \"Rhode Island\"), 1)\n\nBecomes\n\ncovid %&gt;% \n  filter(administrative_area_level_2 == \"Rhode Island\") %&gt;% \n  slice(1)\n\n\n\n\n\n\n\nKeyboard Shortcuts for %&gt;%\n\n\nmacOS: CMD + SHIFT + M\nPC: CTRL + SHIFT + M"
  },
  {
    "objectID": "slides/01-slides.html#descriptive-statistics",
    "href": "slides/01-slides.html#descriptive-statistics",
    "title": "POLS 1600",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWhen social scientists talk about descriptive inference, we’re trying to summarize our data and make claims about what’s typical of our data\n\nWhat’s a typical value\n\nMeasures of central tendency\nmean, median, mode\n\nHow do our data vary around typical values\n\nMeasures of dispersion\nvariance, standard deviation, range, percentile ranges\n\nHow does variation in one variable relate to variation in another\n\nMeasures of association\ncovariance, correlation"
  },
  {
    "objectID": "slides/01-slides.html#using-r-to-summarize-data",
    "href": "slides/01-slides.html#using-r-to-summarize-data",
    "title": "POLS 1600",
    "section": "Using R to Summarize Data",
    "text": "Using R to Summarize Data\nHere are some common ways of summarizing data and how to calculate them with R\n\n\n\n\nDescription\nUsage\n\n\n\n\nsum\nsum(x)\n\n\nminimum\nmin(x)\n\n\nmaximum\nmax(x)\n\n\nrange\nrange(x)\n\n\nmean\nmean(x)\n\n\nmedian\nmedian(x)\n\n\npercentile\nquantile(x)\n\n\nvariance\nvar(x)\n\n\nstandard deviation\nsd(x)\n\n\nrank\nrank(x)\n\n\n\n\n\n\nAll of these functions have an argument called na.rm=F. If your data have missing values, you’ll need to set na.rm=F (e.g. mean(x, na.rm=T))"
  },
  {
    "objectID": "slides/01-slides.html#exploring-covid-19-data-for-lab-1",
    "href": "slides/01-slides.html#exploring-covid-19-data-for-lab-1",
    "title": "POLS 1600",
    "section": "Exploring Covid-19 Data for Lab",
    "text": "Exploring Covid-19 Data for Lab\nLet’s spend the rest of class, exploring what seems like a simple question\n\nOn average, did states that adopted mask mandates have lower rates of new cases?"
  },
  {
    "objectID": "slides/01-slides.html#tasks",
    "href": "slides/01-slides.html#tasks",
    "title": "POLS 1600",
    "section": "Tasks",
    "text": "Tasks\n\nGet a high level overview of our data\nSubset the data to just U.S. States\nRecode our data to get a measure of new Covid cases and what face mask policy policy was in place\nSummarize the average number of new cases by face mask policy."
  },
  {
    "objectID": "slides/01-slides.html#get-a-high-level-overview-of-our-data",
    "href": "slides/01-slides.html#get-a-high-level-overview-of-our-data",
    "title": "POLS 1600",
    "section": "1. Get a high level overview of our data",
    "text": "1. Get a high level overview of our data\n\n\n\n\nCreate a new chunk\nLabel it #| label:\"HLO\"\nRun the following code\nComment code with #\n\n\n\n\ndim(covid)\nhead(covid)\nglimpse(covid)\ntable(covid$administrative_area_level_2)\nlength(unique(covid$administrative_area_level_2))\ncovid$confirmed[1:10]\ncovid %&gt;%\n  select(administrative_area_level_2, date, confirmed) %&gt;%\n  slice(10:20)\nsummary(covid$confirmed)\ntable(covid$facial_coverings)"
  },
  {
    "objectID": "slides/01-slides.html#answer-the-following",
    "href": "slides/01-slides.html#answer-the-following",
    "title": "POLS 1600",
    "section": "Answer the following",
    "text": "Answer the following\n\n\nHow many observations are there (rows)\nHow many variables (columns)\nWhat’s the unit of analysis?\n\nIn words, how would you describe what a row in your data set corresponds to?\n\nAre there any missing values for confirmed\nWhat range of values can facial_coverings take?1\n\n\nSee: https://covid19datahub.io/articles/docs.html"
  },
  {
    "objectID": "slides/01-slides.html#subsetting-our-data-to-only-u.s.-states",
    "href": "slides/01-slides.html#subsetting-our-data-to-only-u.s.-states",
    "title": "POLS 1600",
    "section": "Subsetting our data to only U.S. States",
    "text": "Subsetting our data to only U.S. States\nGoal: Subset our Covid data to include only the 50 states + DC\nSteps:\n\nCreate a vector of the territories we don’t want\nUse the filter() command to “filter” out these territories"
  },
  {
    "objectID": "slides/01-slides.html#create-a-vector-of-the-territories-we-dont-want",
    "href": "slides/01-slides.html#create-a-vector-of-the-territories-we-dont-want",
    "title": "POLS 1600",
    "section": "1. Create a vector of the territories we don’t want",
    "text": "1. Create a vector of the territories we don’t want\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )"
  },
  {
    "objectID": "slides/01-slides.html#use-the-filter-command-to-filter-out-these-territories",
    "href": "slides/01-slides.html#use-the-filter-command-to-filter-out-these-territories",
    "title": "POLS 1600",
    "section": "2. Use the filter() command to “filter” out these territories",
    "text": "2. Use the filter() command to “filter” out these territories\n\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories )\n\ndim(covid)\n\n[1] 58809    47\n\ndim(covid_us)\n\n[1] 53678    47"
  },
  {
    "objectID": "slides/01-slides.html#creating-new-variables-for-analysis",
    "href": "slides/01-slides.html#creating-new-variables-for-analysis",
    "title": "POLS 1600",
    "section": "Creating new variables for analysis",
    "text": "Creating new variables for analysis\n\nGoal: We need new variables that describe:\n\nthe number of new Covid-19 cases on a given date\nthe face mask policy in place\n\nSteps:\n\nUse mutate(), group_by() and lag() to calculate new_cases from total confirmed cases\nUse mutate(), case_when() and abs() to turn numeric facial_coverings into categorical factor variable"
  },
  {
    "objectID": "slides/01-slides.html#calculate-new-covid-19-cases",
    "href": "slides/01-slides.html#calculate-new-covid-19-cases",
    "title": "POLS 1600",
    "section": "Calculate new Covid-19 cases",
    "text": "Calculate new Covid-19 cases\nPlease run and comment the following code:\n\ndim(covid_us)\n\n[1] 53678    47\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases/population *100000\n    ) -&gt; covid_us\ndim(covid_us)\n\n[1] 53678    50"
  },
  {
    "objectID": "slides/01-slides.html#create-face-mask-policy-variable",
    "href": "slides/01-slides.html#create-face-mask-policy-variable",
    "title": "POLS 1600",
    "section": "Create Face Mask Policy variable",
    "text": "Create Face Mask Policy variable\n\ncovid_us %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\",\n      \n    ) \n  ) -&gt; covid_us\n\nlevels(factor(covid_us$face_masks))\n\n[1] \"No policy\"              \"Recommended\"            \"Required all times\"    \n[4] \"Required shared places\" \"Some requirements\""
  },
  {
    "objectID": "slides/01-slides.html#make-face_masks-a-factor-to-reflect-order-of-policies",
    "href": "slides/01-slides.html#make-face_masks-a-factor-to-reflect-order-of-policies",
    "title": "POLS 1600",
    "section": "Make face_masks a factor to reflect order of policies",
    "text": "Make face_masks a factor to reflect order of policies\n\nlevels(factor(covid_us$face_masks))\n\n[1] \"No policy\"              \"Recommended\"            \"Required all times\"    \n[4] \"Required shared places\" \"Some requirements\"     \n\ncovid_us %&gt;%\n  mutate(\n    face_masks = factor(\n      face_masks,\n      levels = c(\n        \"No policy\", \n        \"Recommended\", \n        \"Some requirements\",\n        \"Required shared places\",\n        \"Required all times\"\n        )\n    )\n  ) -&gt; covid_us\n\nlevels(covid_us$face_masks)\n\n[1] \"No policy\"              \"Recommended\"            \"Some requirements\"     \n[4] \"Required shared places\" \"Required all times\""
  },
  {
    "objectID": "slides/01-slides.html#calculate-the-average-number-of-covid-19-cases-by-face-mask-policy",
    "href": "slides/01-slides.html#calculate-the-average-number-of-covid-19-cases-by-face-mask-policy",
    "title": "POLS 1600",
    "section": "Calculate the Average Number of Covid-19 cases by Face Mask Policy",
    "text": "Calculate the Average Number of Covid-19 cases by Face Mask Policy\n\nGoal: On average, did states that adopted mask mandates have lower rates of new cases?\nSteps: use filter(), group_by() and summarise() and mean() to calculate the average number of cases for each level of the face_masks policy variable"
  },
  {
    "objectID": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k",
    "href": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k",
    "title": "POLS 1600",
    "section": "Face Masks and New Covid-19 Cases (per 100k)",
    "text": "Face Masks and New Covid-19 Cases (per 100k)\n\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(face_masks)%&gt;%\n  summarize(\n    `Average No. of New Cases` = round(mean(new_cases_pc, na.rm=T),2)\n  )%&gt;%\n  rename(\n    \"Face Mask Policy\" = face_masks\n  ) -&gt; face_mask_summary"
  },
  {
    "objectID": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k-1",
    "href": "slides/01-slides.html#face-masks-and-new-covid-19-cases-per-100k-1",
    "title": "POLS 1600",
    "section": "Face Masks and New Covid-19 Cases (per 100k)",
    "text": "Face Masks and New Covid-19 Cases (per 100k)\n\n\n\n\nWhat should we conclude?\nWhat’s wrong with this simple comparison?\nWhat’s a better comparison? (Thursday)\n\n\n\n\n\n\n\nFace Mask Policy\nAverage No. of New Cases\n\n\n\n\nNo policy\n10.26\n\n\nRecommended\n16.61\n\n\nSome requirements\n36.18\n\n\nRequired shared places\n29.38\n\n\nRequired all times\n32.18"
  },
  {
    "objectID": "slides/01-slides.html#commented-code",
    "href": "slides/01-slides.html#commented-code",
    "title": "POLS 1600",
    "section": "Commented Code",
    "text": "Commented Code\n\n# ---- Libraries ----\n## Uncomment to install\n# install.packages(\"tidyverse\")\n# install.packages(\"COVID19\")\n\nlibrary(\"tidyverse\")\nlibrary(\"COVID19\")\n\n# ---- Load data ----\nload(url(\"https://pols1600.paultesta.org/files/data/covid.rda\"))\n\n# ---- Subset to US states and DC ----\n\nterritories &lt;- c(\n  \"American Samoa\",\n  \"Guam\",\n  \"Northern Mariana Islands\",\n  \"Puerto Rico\",\n  \"Virgin Islands\"\n  )\n\n\ncovid_us &lt;- covid %&gt;%\n  filter(!administrative_area_level_2 %in% territories )\n\n## Check subsetting\ndim(covid)[1] &gt; dim(covid_us)[1]\n\n# ---- Recode covid_us ----\n\ncovid_us %&gt;%\n  mutate(\n    state = administrative_area_level_2,\n  ) %&gt;%\n  dplyr::group_by(state) %&gt;%\n  mutate(\n    new_cases = confirmed - lag(confirmed),\n    new_cases_pc = new_cases/population *100000\n    ) %&gt;%\n  mutate(\n    face_masks = case_when(\n      facial_coverings == 0 ~ \"No policy\",\n      abs(facial_coverings) == 1 ~ \"Recommended\",\n      abs(facial_coverings) == 2 ~ \"Some requirements\",\n      abs(facial_coverings) == 3 ~ \"Required shared places\",\n      abs(facial_coverings) == 4 ~ \"Required all times\"\n      ) \n    ) %&gt;%\n  mutate(\n    face_masks = factor(\n      face_masks,\n      levels = c(\n        \"No policy\", \n        \"Recommended\", \n        \"Some requirements\",\n        \"Required shared places\",\n        \"Required all times\"\n        )\n      )\n    )-&gt; covid_us\n\n\n# ---- Calculate new cases per capita by facemask policy\ncovid_us %&gt;%\n  filter(!is.na(face_masks))%&gt;%\n  group_by(face_masks)%&gt;%\n  summarize(\n    `Average No. of New Cases` = round(mean(new_cases_pc, na.rm=T),2)\n  )%&gt;%\n  rename(\n    \"Face Mask Policy\" = face_masks\n  ) -&gt; face_mask_summary\n\nface_mask_summary"
  },
  {
    "objectID": "slides/01-slides.html#summary-1",
    "href": "slides/01-slides.html#summary-1",
    "title": "POLS 1600",
    "section": "Summary",
    "text": "Summary\nAfter today, you should have a better sense of\n\nHow to write R code using Quarto and R Markdown\nHow to install packages and load libraries\nSome of different types and shapes of data\nHow to get a high level overview of your data\nHow to transform, recode, and summarise data using dplyr and the tidyverse\nHow describe typical values and variation in data\nHow to explore substantive questions using these these typical values"
  },
  {
    "objectID": "slides/01-slides.html#congrats",
    "href": "slides/01-slides.html#congrats",
    "title": "POLS 1600",
    "section": "Congrats!",
    "text": "Congrats!\n\nWe covered A LOT\nIt’s OK to feel overwhelmed\n\nBut please don’t suffer in silence\n\nDon’t worry if everything didn’t make sense.\n\nEventually it will, but this takes time and practice\nTesta’s 50-50 rule\nFAAFO\n\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/09-slides.html#general-plan",
    "href": "slides/09-slides.html#general-plan",
    "title": "Week 09:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nFeedback\nReview\n\nProbability Distributions\n\nLecture\n\nThe Law of Large Numbers\nThe Central Limit Theorem\nGeneralized Linear Models (Maybe…)"
  },
  {
    "objectID": "slides/09-slides.html#goals",
    "href": "slides/09-slides.html#goals",
    "title": "Week 09:",
    "section": "Goals",
    "text": "Goals\n\nThe Law of Large Number’s says that as our sample size increases, our sample mean will converge to the population value\n\n–\n\nThe Central Limit Theorem says that the distribution of those sample means will follow a normal distribution\n\n–\n\nGeneralized Linear Models allow us to more accurately model different types of data-generating processes using Maximum Likelihood Estimation."
  },
  {
    "objectID": "slides/09-slides.html#emoji-slide-notation",
    "href": "slides/09-slides.html#emoji-slide-notation",
    "title": "Week 09:",
    "section": "Emoji Slide notation",
    "text": "Emoji Slide notation\n\n💪: Exercises\n📢: Feedback\n🔍: Review\n💡: Core concept\n🦉: In case you’re interested\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/09-slides.html#new-packages",
    "href": "slides/09-slides.html#new-packages",
    "title": "Week 09:",
    "section": "New packages",
    "text": "New packages\nNone!"
  },
  {
    "objectID": "slides/09-slides.html#packages-for-today",
    "href": "slides/09-slides.html#packages-for-today",
    "title": "Week 09:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/09-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/09-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 09:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/09-slides.html#load-packages-for-today",
    "href": "slides/09-slides.html#load-packages-for-today",
    "title": "Week 09:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce       COVID19          maps \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      mapdata           qss    tidycensus     dataverse DeclareDesign \n         TRUE          TRUE          TRUE          TRUE          TRUE \n          zoo \n         TRUE \n\n\nclass:inverse, center, middle # 💪 ## Load Data for today\nclass:inverse, middle, center # 🔍 # Review ## Random Variables and Probability Distributions"
  },
  {
    "objectID": "slides/09-slides.html#probability",
    "href": "slides/09-slides.html#probability",
    "title": "Week 09:",
    "section": "Probability",
    "text": "Probability\n\nProbability describes the likelihood of an event happening.\nStatistics uses probability to quantify uncertainty about estimates and hypotheses.\nThree rules of probability (Kolmogorov axioms)\n\nPositivity: \\[Pr(A) \\geq 0 \\]\nCertainty: \\[Pr(\\Omega) = 1 \\]\nAdditivity: \\[Pr(A \\text{ or } B) = Pr(A) + Pr(B)\\] iff A and B are mutually exclusive"
  },
  {
    "objectID": "slides/09-slides.html#probability-1",
    "href": "slides/09-slides.html#probability-1",
    "title": "Week 09:",
    "section": "Probability",
    "text": "Probability\n\nTwo interpretations interpreting probabilities (Frequentist and Bayesian)\nConditional Probability and Bayes Rule:\n\n\\[Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)} = \\frac{Pr(B|A)Pr(A)}{Pr(B|A)Pr(A)+Pr(B|A^\\complement)Pr(A^\\complement)}\\]"
  },
  {
    "objectID": "slides/09-slides.html#random-variables",
    "href": "slides/09-slides.html#random-variables",
    "title": "Week 09:",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variables assign numeric values to each event in an experiment.\n\nMutually exclusive and exhaustive, together cover the entire sample space.\n\nDiscrete random variables take on finite, or countably infinite distinct values.\nContinuous variables can take on an uncountably infinite number of values."
  },
  {
    "objectID": "slides/09-slides.html#example-toss-two-coins",
    "href": "slides/09-slides.html#example-toss-two-coins",
    "title": "Week 09:",
    "section": "Example: Toss Two Coins",
    "text": "Example: Toss Two Coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)"
  },
  {
    "objectID": "slides/09-slides.html#probability-distributions",
    "href": "slides/09-slides.html#probability-distributions",
    "title": "Week 09:",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nBroadly probability distributions provide mathematical descriptions of random variables in terms of the probabilities of events.\n\n\\[\\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities}\\]\nThe can be represented in terms of:\n\nProbability Mass/Density Functions\n\nDiscrete variables have probability mass functions (PMF)\nContinuous variables have probability density functions (PDF)\n\nCumulative Density Functions\n\nDiscrete: Summation of discrete probabilities\nContinuous: Integration over a range of values"
  },
  {
    "objectID": "slides/09-slides.html#discrete-distributions",
    "href": "slides/09-slides.html#discrete-distributions",
    "title": "Week 09:",
    "section": "Discrete distributions",
    "text": "Discrete distributions\n\nProbability Mass Function (pmf): \\(f(x)=p(X=x)\\)\n\nAssigns probabilities to each unique event such that Kolmogorov Axioms (Positivity, Certainty, and Additivity) still apply\n\nCumulative Distribution Function (cdf) \\(F(x_j)=p(X\\leq x)=\\sum_{i=1}^{j}p(x_i)\\)\n\nSum of the probability mass for events less than or equal to \\(x_j\\)"
  },
  {
    "objectID": "slides/09-slides.html#example-toss-two-coins-1",
    "href": "slides/09-slides.html#example-toss-two-coins-1",
    "title": "Week 09:",
    "section": "Example: Toss Two coins",
    "text": "Example: Toss Two coins\n\n\\(S={TT,TH,HT,HH}\\)\nLet \\(X\\) be the number of heads\n\n\\(X(TT)=0\\)\n\\(X(TH)=1\\)\n\\(X(HT)=1\\)\n\\(X(HH)=2\\)\n\n\\(f(X=0)=p(X=0)=1/4\\)\n\\(f(X=1)=p(X=1)=1/2\\)\n\\(F(X\\leq 1) = p(X \\leq 1)= 3/4\\)\n\n\nEach side has equal probability of occurring (1/6). The probability that you roll a 2 or less P(X&lt;=2) = 1/6 + 1/6 = 1/3"
  },
  {
    "objectID": "slides/09-slides.html#continuous-distributions",
    "href": "slides/09-slides.html#continuous-distributions",
    "title": "Week 09:",
    "section": "Continuous distributions",
    "text": "Continuous distributions\n\nProbability Density Functions (PDF): \\(f(x)\\)\n\nAssigns probabilities to events in the sample space such that Kolmogorov Axioms still apply\nBut… since their are an infinite number of values a continuous variable could take, p(X=x)=0, that is, the probability that X takes any one specific value is 0.\n\nCumulative Distribution Function (CDF) \\(F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx\\)\n\nInstead of summing up to a specific value (discrete) we integrate over all possible values up to \\(x\\)\nProbability of having a value less than x"
  },
  {
    "objectID": "slides/09-slides.html#integrals",
    "href": "slides/09-slides.html#integrals",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nFirst, a brief aside on integral calculus:\nWhat’s the area of the rectangle? \\(base\\times height\\)"
  },
  {
    "objectID": "slides/09-slides.html#integrals-1",
    "href": "slides/09-slides.html#integrals-1",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nHow would we find the area under a curve?"
  },
  {
    "objectID": "slides/09-slides.html#integrals-2",
    "href": "slides/09-slides.html#integrals-2",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nWell suppose we added up the areas of a bunch of rectangles roughly whose height’s approximated the height of the curve?\n\nCan we do any better?"
  },
  {
    "objectID": "slides/09-slides.html#integrals-3",
    "href": "slides/09-slides.html#integrals-3",
    "title": "Week 09:",
    "section": "🦉 Integrals",
    "text": "🦉 Integrals\nLet’s make the rectangles smaller\n\nWhat happens as the width of rectangles get even smaller, approaches 0? Our approximation get’s even better:"
  },
  {
    "objectID": "slides/09-slides.html#link-between-pdf-and-cdf",
    "href": "slides/09-slides.html#link-between-pdf-and-cdf",
    "title": "Week 09:",
    "section": "🦉 Link between PDF and CDF",
    "text": "🦉 Link between PDF and CDF\nIf \\[F(x)=p(X\\leq x)=\\int_{-\\infty}^{x}f(x)dx \\]\nThen by the fundamental theorem of calculus\n\\[\\frac{d}{dx}F(x)=f(x)\\]\nIn words\n\nthe PDF (\\(f(x)\\)) is the derivative (rate of change) of the CDF (\\(F(X)\\))\nthe CDF describes the area under the curve defined by f(x) up to x"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-the-cdf",
    "href": "slides/09-slides.html#properties-of-the-cdf",
    "title": "Week 09:",
    "section": "Properties of the CDF",
    "text": "Properties of the CDF\n\n\\(0\\leq F(x) \\leq 1\\)\n\\(F\\) is non-decreasing and right continuous\n\\(\\lim_{x\\to-\\infty}F(x)=0\\)\n\\(\\lim_{x\\to\\infty}F(x)=1\\)\nFor all \\(a,b \\in \\mathbb{R}\\) s.t. \\(a&lt;b\\)\n\n\\[p(a &lt; X \\leq b) = F(b)- F(a) = \\int_a^b f(x)dx \\]"
  },
  {
    "objectID": "slides/09-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "href": "slides/09-slides.html#recall-the-pmf-and-cdf-of-a-die",
    "title": "Week 09:",
    "section": "Recall the PMF and CDF of a die",
    "text": "Recall the PMF and CDF of a die"
  },
  {
    "objectID": "slides/09-slides.html#whats-the-probability",
    "href": "slides/09-slides.html#whats-the-probability",
    "title": "Week 09:",
    "section": "What’s the probability",
    "text": "What’s the probability\n\n\\(p(X=1)...p(X=6) = 1/6\\)\n\\(p( 2 &lt; X \\leq 5) = F(5)-F(2)=5/6-2/6=3/6=1/2\\)"
  },
  {
    "objectID": "slides/09-slides.html#common-probablity-distirbutions",
    "href": "slides/09-slides.html#common-probablity-distirbutions",
    "title": "Week 09:",
    "section": "Common Probablity Distirbutions",
    "text": "Common Probablity Distirbutions\nIn this course, we’ll use probability distributions to\n\nmodel the data generating process as a function of parameters we can estimate (using Generalized Linear Models)\nperform inference based on asymptotic theory (statements about how statistics would be have as our sample size approached infinity)\n\nThere are a lot of probability distributions:\n\n\n\n\n\n\n\n\n\nFortunately, the distributions you need to know to really master data science, are probably more something like\n\n\n\n\n\n\n\n\n\nAnd the distributions we’ll work with the most in this class are an even smaller subset.\n\nBernoulli: Coinflips with probability of heads, \\(p\\)\nUniform: Coinflip with more than two outcomes\nBinomial: Adding up coinflips\nPoisson: Counting the total number of events\nGeometric: Counting till a specific event occurs\nExponential: Counting till a specific event occurs in continous time\nNormal:\n\nThe limit of a Binomial distribution as \\(n\\to \\infty\\)\nThe maximum entropy when we only know the mean and variance\n\nt: A finite sample approximation of the normal\n\\(\\chi^2\\): Distribution of sums of squared variables from Normal distribution"
  },
  {
    "objectID": "slides/09-slides.html#bernoulli-random-variables",
    "href": "slides/09-slides.html#bernoulli-random-variables",
    "title": "Week 09:",
    "section": "Bernoulli Random Variables",
    "text": "Bernoulli Random Variables\nLet’s start with our old friend the coin flip\nA coin flip is an example of a Bernoulli random variable defined by 1 parameter \\(p\\), the probability of success. It has a pmf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } x=1 \\\\\n                1-p & \\mathrm{if\\ } x=0 \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF of\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                0 & \\mathrm{if\\ } x&lt;1 \\\\\n                1-p & \\mathrm{if\\ } 0\\leq x&lt;1 \\\\\n                1& \\mathrm{if\\ } x\\geq1 \\\\\n        \\end{array}\n    \\right.\\]\nNote that in our coin flip example \\(p=0.5\\) but it need not. Just imagine a weighted coin like the Patriots use at Foxborough"
  },
  {
    "objectID": "slides/09-slides.html#uniform-distribution",
    "href": "slides/09-slides.html#uniform-distribution",
    "title": "Week 09:",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nOur fair die examples represent a discrete uniform distribution: multiple outcomes, equally likely. We could even imagine an infinite number of possible outcomes within a range \\([a,b]\\), the key parameters for a uniform distribution, in which case our case our continuous uniform random variable has a pdf of\n\\[f(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                \\frac{1}{b-a}& \\mathrm{if\\ } a \\leq x\\leq b \\\\\n                0 & \\text{otherwise} \\\\\n        \\end{array}\n    \\right.\\]\nAnd a CDF:\n\\[F(x) =\n    \\left\\{\n        \\begin{array}{cc}\n                        0 & x &lt;a \\\\\n                \\frac{x-a}{b-a}& \\mathrm{if\\ } a \\leq x &lt; b \\\\\n                1 & x \\geq b \\\\\n        \\end{array}\n    \\right.\\]\nWe won’t run into uniform distributions all that often except in examples like rolling a fair sided die, but often they’re used in Bayesian analysis as a form of uninformative prior."
  },
  {
    "objectID": "slides/09-slides.html#binomial-distributions",
    "href": "slides/09-slides.html#binomial-distributions",
    "title": "Week 09:",
    "section": "Binomial Distributions",
    "text": "Binomial Distributions\nThe binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Toss a fair coin 20 times; how many times does it come up heads? This count is an outcome that follows the binomial distribution.\nThe key parameters are the number of trials \\(n\\) and the probability of success for each trial \\(p\\) and the pdf of a binomial distribution is:\n\\[f(x)=\\binom{n}{x}p^x (1-p) ^{1-x} \\ \\text{for x 0,1,2},\\dots n\\] So if we were to toss a fair coin 20 times and count up the number of heads, the most common outcome would be 10 heads\n\nThe binomial distribution will come in handy when trying to model binary outcomes."
  },
  {
    "objectID": "slides/09-slides.html#poisson-distributions",
    "href": "slides/09-slides.html#poisson-distributions",
    "title": "Week 09:",
    "section": "Poisson Distributions",
    "text": "Poisson Distributions\nWhat would happen if you let the \\(n\\) in a binomial distribution go to infinity and \\(p\\) go to 0 so that \\(np\\) stayed the same. A Poisson distribution is what would happen. We use Poisson and negative binomial distributions to describe counts using the parameter \\(\\lambda\\) which represents rate at which events occur.\n\\[f(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\nWe use these distributions to try and predict to predict the probability of a given number of events occurring in a fixed interval of time. Things like how many acts of political participation would a voter engage in over a year."
  },
  {
    "objectID": "slides/09-slides.html#geometric-distributions",
    "href": "slides/09-slides.html#geometric-distributions",
    "title": "Week 09:",
    "section": "Geometric Distributions",
    "text": "Geometric Distributions\nWhat if we wanted to know the number times a coin came up tails before heads occurred? This discrete random variable follows a geometric distribution:\n\\[f(x)=p(1-p) ^{x}\\]\nGeometric and related distributions are useful for describing the time until an event occurs"
  },
  {
    "objectID": "slides/09-slides.html#exponential-distributions",
    "href": "slides/09-slides.html#exponential-distributions",
    "title": "Week 09:",
    "section": "Exponential Distributions",
    "text": "Exponential Distributions\nTaking a geometric distribution to its limit, you arrive at the continuous exponential distribution, again described by a \\(\\lambda = \\frac{1}{\\beta}\\) rate parameter\n\\[f(x)=\\frac{1}{\\beta}\\exp\\left[-x/\\beta\\right]\\]\nCioffa-Revilla (1984) uses an exponential distribution to model the stability of Italian governments."
  },
  {
    "objectID": "slides/09-slides.html#normal-distribution",
    "href": "slides/09-slides.html#normal-distribution",
    "title": "Week 09:",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nFinally, there’s the distribution so ubiquitous we called it normal. The Normal distribution is defined by two parameters: a location parameter \\(\\mu\\) that determines the center of a distribution and a scale parameter \\(\\sigma^2\\) that determines the spread of a distribution\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[\n-\\frac{1}{2\\sigma^2}(x-\\mu)^2\n\\right]\\]\nStandard normal: \\(X \\sim N(\\mu =0,\\sigma^2=1)\\)\n\n\n\n\n\n\n\n\n\n\nAs we’ll see normal distributions tend to arise when ever you’re summing variables.\nThat is sum together a bunch of values from almost any distribution and the distribution of their sums tends to follow a normal distribution.\nSince lots of our statistics involve summation, lots of our statistics will tend to follow normal distributions in their limit (in finite samples like the world we live in they may follow related distributions like the t-distribution, but more on that later.)\n\nConsider a binomial distribution with N=100 and p=.5.\nThe pmf of this variable (black lollipops) follows a distribution that’s closely approximated by a normal distribution (red line) with a mean 50 and a standard deviation of 5.\nA relationship explained more generally by the Central Limit Theorem, which we’ll cover next week.\n\n\n\n\n\n\n\n\n\nWhat’s the \\(p(X \\leq 0)\\) for a normal distirbution with mean 0 and sd 1\nSince the normal distribution is so common, it’s useful to get practice working with it’s pdf and cdf.\nConsider the following question: If X is normally distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\), what’s the probability that X is less than 0 \\(p(X\\leq0)=?\\) We could solve:\n\\[\\int_{-\\infty}^{0}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.5\\]\nBut R’s pnorm() function will quickly tell us\n\n\\(p(X\\leq0)=\\) 0.5\n\nAnd we can visualize this as follows:\n\n\n\n\n\n\n\n\n\nConsider some other questions?\n\n\\(p(X=0)=0\\)\n\nThe probability that a continuous variable is exactly some value is always 0.\n\n\\(p(X&lt;0)=0.5\\)\n\\(p(-1&lt; X&lt; 1)\\)\n\\(p(-2&lt; X&lt; 2)\\)\n\np(-1 &lt; X &lt; 1)\n\n\\(p(-1&lt; X&lt; 1)=pr(X&lt;1)-pr(X&lt;-1)\\)\n\n\\[\\int_{-1}^{1}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx=0.841-0.158=0.682\\]\n\n\n\n\n\n\n\n\n\np(-2 &lt; X &lt; 2)\n\n\\(p(-2&lt; X\\leq 2)=\\) 0.9544997\n\n\n\n\n\n\n\n\n\n\nWe’ll use the fact that close 95 of the observations of a standard normal variable will be within 2 standard deviations of the the mean of 0 for assessing whether a given statistic is likely to have arisen if the true value of that statistic were 0."
  },
  {
    "objectID": "slides/09-slides.html#expected-value",
    "href": "slides/09-slides.html#expected-value",
    "title": "Week 09:",
    "section": "Expected Value",
    "text": "Expected Value\nA (probability) weighted average of the possible outcomes of a random variable, often labeled \\(\\mu\\)\nDiscrete:\n\\[\\mu_X=E(X)=\\sum xp(x)\\]\nContinuous\n\\[\\mu_X=E(X)=\\int_{-\\infty}^{\\infty}xf(x) dx\\]"
  },
  {
    "objectID": "slides/09-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "href": "slides/09-slides.html#whats-the-expected-value-of-a-1-roll-of-fair-die",
    "title": "Week 09:",
    "section": "What’s the expected value of a 1 roll of fair die?",
    "text": "What’s the expected value of a 1 roll of fair die?\n\\[\\begin{align*}\nE(X)&=\\sum_{i=1}^{6}x_ip(x_i)\\\\\n     &=1/6\\times(1+2+3+4+5+6)\\\\\n     &= 21/6\\\\\n     &=3.5\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-expected-values",
    "href": "slides/09-slides.html#properties-of-expected-values",
    "title": "Week 09:",
    "section": "Properties of Expected Values",
    "text": "Properties of Expected Values\n\n\\(E(c)=c\\)\n\\(E(a+bX)=a+bE[X]\\)\n\\(E[E[X]]=X\\)\n\\(E[E[Y|X]]=E[Y]\\)\n\\(E[g(X)]=\\int_{-\\infty}^\\infty g(x)f(x)dx\\)\n\\(E[g(X_1)+\\dots+g(X_n)]=E[g(X_1)]+\\dots E[g(X_n)\\)\n\\(E[XY]=E[X]E[Y]\\) if \\(X\\) and \\(Y\\) are independent"
  },
  {
    "objectID": "slides/09-slides.html#variance",
    "href": "slides/09-slides.html#variance",
    "title": "Week 09:",
    "section": "Variance",
    "text": "Variance\nIf \\(X\\) has a finite mean \\(E[X]=\\mu\\), the \\(E[(X-\\mu)^2]\\) is finite and called the variance of \\(X\\) which we write as \\(\\sigma^2\\) or \\(Var[X]\\).\nNote:\n\\[\\begin{align*}\n\\sigma^2=E[(X-\\mu)^2]&=E[(X^2-2\\mu X+\\mu^2)]\\\\\n&= E[X^2]-2\\mu E[X]+\\mu^2\\\\\n&= E[X^2]-2\\mu^2+\\mu^2\\\\\n&= E[X^2]-\\mu^2\\\\\n&= E[X^2]-E[X]^2\n\\end{align*}\\]\n\n“The variance of X is equal to the expected value of X-squared, minus the square of X’s expected value.”\n\\(\\sigma^2=E[X^2]-E[X]^2\\) is a useful identity in proofs and derivations"
  },
  {
    "objectID": "slides/09-slides.html#variance-and-standard-deviations",
    "href": "slides/09-slides.html#variance-and-standard-deviations",
    "title": "Week 09:",
    "section": "Variance and Standard Deviations",
    "text": "Variance and Standard Deviations\nWe often think of variances \\(Var[X]\\) as describing the spread of a distribution\n\\[\\sigma^2=Var[X]=E[(X-E[X])^2]=E(X^2)-E(X)^2\\]\nA standard deviation is just the square root of the variance\n\\[\\sigma=\\sqrt{Var[X]}\\]"
  },
  {
    "objectID": "slides/09-slides.html#covariance",
    "href": "slides/09-slides.html#covariance",
    "title": "Week 09:",
    "section": "Covariance",
    "text": "Covariance\nCovariance measures the degree to which two random variables vary together.\n\n\\(Cov[X,Y] \\to +\\) An increase in \\(X\\) tends to be larger than its mean when \\(Y\\) is larger than its mean\n\n\\[Cov[X,Y]=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\\]"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-variance-and-covariance",
    "href": "slides/09-slides.html#properties-of-variance-and-covariance",
    "title": "Week 09:",
    "section": "Properties of Variance and Covariance",
    "text": "Properties of Variance and Covariance\n\n\\(Cov[X,Y]=E[XY]-E[X]E[Y]\\)\n\\(Var[X]=E[X^2]-(E[X])^2\\)\n\\(Var[X|Y]=E[X^2|Y]-(E[X|Y])^2\\)\n\\(Cov[X,Y]=Cov[X,E[Y|X]]\\)\n\\(Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]\\)\n\\(Var[Y]=Var[E[Y|X]]+E[Var[Y|X]]\\)"
  },
  {
    "objectID": "slides/09-slides.html#correlation",
    "href": "slides/09-slides.html#correlation",
    "title": "Week 09:",
    "section": "Correlation",
    "text": "Correlation\n\nThe correlation between \\(X\\) and \\(Y\\) is simply the covariance of \\(X\\) and \\(Y\\) divided by the standard deviation of each.\n\n\\[\\rho=\\frac{Cov[X,Y]}{\\sigma_X\\sigma_Y}\\]\n\nNormalize covariance to a scale that runs between [-1,1]\n\nclass:inverse, center, middle # 💡 # The Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive",
    "title": "Week 09:",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we wanted to know the average height of our class.\nWe could pick someone at random, measure their height and get an estimate. It would be a pretty bad estimate (it would vary a lot from person to person), but it would be an unbiased estimate\nHow would we improve our estimate?"
  },
  {
    "objectID": "slides/09-slides.html#the-law-of-large-numbers-intuitive-1",
    "href": "slides/09-slides.html#the-law-of-large-numbers-intuitive-1",
    "title": "Week 09:",
    "section": "The Law of Large Numbers (Intuitive)",
    "text": "The Law of Large Numbers (Intuitive)\nSuppose we increased our sample size from N=1 to N = 5.\nNow our estimate reflects the average of 5 people’s heights as opposed to just 1. Both are are unbiased estimates of the truth, but the N=5 sample has a lower variance.\n–\nNow suppose we took a sample of size N = N-1. That is we measured everyone except one person. Our estimate will be quite close to the truth, varying slightly based on the height of the person left out.\n–\nFinally we took a sample of size N = 24 (e.g. the class size). Since our sample is the population, our estimate will be exactly equal to to the population. Each sample will give us the same “true” value. That is, it wil not vary at all.\n–\nThe idea that as the sample size increases, the distance of a sample mean from the population mean \\(\\mu\\) goes to 0 is called the Law of Large Numbers"
  },
  {
    "objectID": "slides/09-slides.html#the-weak-law-of-large-numbers-formally",
    "href": "slides/09-slides.html#the-weak-law-of-large-numbers-formally",
    "title": "Week 09:",
    "section": "The (Weak) Law of Large Numbers (Formally)",
    "text": "The (Weak) Law of Large Numbers (Formally)\nLet \\(X_1, X_2, \\dots\\) be independent and identically distributed (i.i.d.) random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nThen for every \\(\\epsilon&gt;0\\), as the sample size increases (1), the distance of a sample mean from the population mean \\(\\mu\\) (2) goes to 0 (3).\n\\[\\overbrace{Pr(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon)}^{\\text{2. The distance of the sample mean from the truth}} \\overbrace{\\to 0}^{\\text{3. Goes to 0}} \\underbrace{\\text{ as }n \\to \\infty}_{\\text{1. As the sample size increases}}\\]\nEquivalently:\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]"
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-lln",
    "href": "slides/09-slides.html#simulating-the-lln",
    "title": "Week 09:",
    "section": "💪 Simulating the LLN",
    "text": "💪 Simulating the LLN\nRhe expected value of rolling a die 3.5.\n\\[ E[X] = \\Sigma x_ip(X=x_i) = 1/6 * (1+2+3+4+5+6)\\]\nIn terms of the LLN, think of our sample size as the number of times we roll a die.\nIf we rolled the die just once and took the average of our role, we could get a 1, 2, 3, 4, 5, or 6. which would be pretty far from our expected value of 3.5\nIf we rolled the die two times and took an average, we could still get an value of 1 or 6 for average, but values closer to our expected value of 3.5, happen more often\n\n# Calculate the average from 2 rows\ntable(rowMeans(expand.grid(1:6, 1:6)))\n\n\n  1 1.5   2 2.5   3 3.5   4 4.5   5 5.5   6 \n  1   2   3   4   5   6   5   4   3   2   1 \n\n\nAs we increase our sample size (roll the die more times), the LLN says the chance that our sample average is far from the truth \\((p(\\left|\\frac{X_1+\\dots+X_n}{n}-\\mu\\right| &gt; \\epsilon))\\), gets vanishingly small.\n\ndie &lt;- 1:6\nroll_fn &lt;- function(n) {\n  rolls &lt;- data.frame(rolls = sample(die, size = n, replace = TRUE))\n  # summarize rolls \n  df &lt;- rolls %&gt;%\n    summarise(\n    # number of rolls\n      n_rolls = n(),\n    # number of times 1 was rolled\n      ones = sum(rolls == 1),\n    # number of times 2 was rolled, etc..\n      twos = sum(rolls == 2),\n      threes = sum(rolls == 3),\n      fours = sum(rolls == 4),\n      fives = sum(rolls == 5),\n      sixes = sum(rolls == 6),\n      # Average of all our rolls\n      average =  mean(rolls),\n      # Absolute difference between averages and rolls\n      abs_error = abs(3.5-average)\n    )\n  # Return summary df\n  df\n}\n\nThen we could use a for-loop to simulate rolling our die once and calculating the average all the way up to rolling our die a 1000 times.\n\n# Holder\nsim_df &lt;- NULL\n\n# Set seed\nset.seed(123)\n\nfor(i in 1:1000){\n  sim_df &lt;- rbind(sim_df,\n                  roll_fn(i)\n  )\n}\n\nWith only a few rolls, our average bounces around a lot\n\nhead(sim_df)\n\n  n_rolls ones twos threes fours fives sixes  average abs_error\n1       1    0    0      1     0     0     0 3.000000 0.5000000\n2       2    0    0      1     0     0     1 4.500000 1.0000000\n3       3    0    2      0     0     0     1 3.333333 0.1666667\n4       4    0    0      1     1     1     1 4.500000 1.0000000\n5       5    1    1      1     0     1     1 3.400000 0.1000000\n6       6    3    0      2     1     0     0 2.166667 1.3333333\n\n\nWith a lot of rolls, our average is very close to 3.5\n\ntail(sim_df)\n\n     n_rolls ones twos threes fours fives sixes  average  abs_error\n995      995  197  160    151   154   171   162 3.430151 0.06984925\n996      996  184  164    176   149   175   148 3.412651 0.08734940\n997      997  163  159    170   163   171   171 3.534604 0.03460381\n998      998  162  163    142   173   185   173 3.576152 0.07615230\n999      999  209  154    151   154   163   168 3.412412 0.08758759\n1000    1000  181  189    147   179   146   158 3.394000 0.10600000\n\n\nLet’s visualize see how our average changes with the number of rolls, using ggplot()\n\np_die_lln &lt;- ggplot(sim_df, aes(n_rolls, average))+\n  geom_line()\n\n\nYour turn! Plot how the absolute value of the error changes as the number of rolls increases. Does it increase or decrease? How does the rate at which it goes up or down seem to change?\n\n# Write your code here:\n\nclass: inverse, center, middle #🦉 ## ICYI: Proving the Weak LLN"
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln",
    "href": "slides/09-slides.html#proving-the-weak-lln",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nA proof of the LLN is as follows:\nFirst define \\(U\\) such that its a sample mean for sample of size \\(n\\)\n\\[U=\\frac{X_1+\\dots +X_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln-1",
    "href": "slides/09-slides.html#proving-the-weak-lln-1",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen show that the sample mean, \\(U\\) is an unbiased estimator of the population mean \\(\\mu\\)\n\\[\\begin{align*}\nE[U]&=E[\\frac{X_1+\\dots +X_n}{n}]=\\frac{1}{n}E[X_1+\\dots +X_n]\\\\\n&=\\frac{n\\mu}{n}=\\mu\n\\end{align*}\\]\nWith a variance\n\\[\\begin{align*}\nVar[U]&=Var[\\frac{X_1+\\dots +X_n}{n}]=\\\\\n    &=Var[\\frac{X_1}{n}]\\dots Var[\\frac{+X_n}{n}]\\\\\n    &\\frac{\\sigma^2}{n^2}\\dots \\frac{\\sigma^2}{n^2}\\\\\n    &\\frac{n \\sigma^2}{n^2}\\\\\n    &\\frac{\\sigma^2}{n}\\\\\n\\end{align*}\\]\nThat decreases with N."
  },
  {
    "objectID": "slides/09-slides.html#proving-the-weak-lln-2",
    "href": "slides/09-slides.html#proving-the-weak-lln-2",
    "title": "Week 09:",
    "section": "Proving the Weak LLN",
    "text": "Proving the Weak LLN\nThen, by Chebyshev’s inequality, a theorem specifying, for a given distribution, the maximum fraction of values that can be some distance from that distribution’s mean:\n\\[Pr(\\left|U-\\mu\\right| &gt; \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2}\\]\nWhich \\(\\to 0\\) as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "slides/09-slides.html#the-strong-law-of-large-numbers",
    "href": "slides/09-slides.html#the-strong-law-of-large-numbers",
    "title": "Week 09:",
    "section": "The Strong Law of Large Numbers",
    "text": "The Strong Law of Large Numbers\nAs you may have inferred, there is a weak law of large numbers and a strong law of large numbers.\nThe weak law of large numbers states that as the sample size increases, the sample mean converges in probability to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n - \\mu| &lt; \\epsilon) = 1\\]\nThe strong law of large numbers states that as the sample size increases, the sample mean converges almost surely to the population value \\(\\mu\\)\n\\[\\lim_{n \\to \\infty} Pr(|\\bar{X}_n = \\mu|) = 1\\] The differences in types of convergence won’t matter much for us in this course\nclass:inverse, center, middle # Break\nclass:inverse, center, middle # 💡 ## The Central Limit Theorem\nSo the LLN tells us that as our sample size grows, an unbiased estimator like the sample average, will get increasingly close to the to the “true” value of the population of mean.\nIif we took a bunch of samples of the same size and calculated the mean of each sample:\n\nthe distribution of those sample means (the sampling distribution) would be centered around the truth (because the estimator is unbiased).\nthe width of the distribution (its variance) would decrease as we increased the size of each sample (by the LLN)\n\nThe Central Limit Theorem tells us about the shape of that distribution."
  },
  {
    "objectID": "slides/09-slides.html#review-z-scores-and-standardization",
    "href": "slides/09-slides.html#review-z-scores-and-standardization",
    "title": "Week 09:",
    "section": "Review: Z-scores and Standardization",
    "text": "Review: Z-scores and Standardization\nGiven a R.V. \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we can define a new R.V. \\(Z\\) as the standardization of \\(X\\):\n\\[Z=\\frac{X-\\mu}{\\sigma}\\]\nWhere Z has \\(\\mu=0\\) and \\(\\sigma=1\\)."
  },
  {
    "objectID": "slides/09-slides.html#notation-for-the-clt",
    "href": "slides/09-slides.html#notation-for-the-clt",
    "title": "Week 09:",
    "section": "Notation for the CLT",
    "text": "Notation for the CLT\nNext let’s define some variables \\(S\\) and \\(\\bar{X}\\) that are the sum \\((S)\\) and sample mean \\((\\bar{X})\\) of \\(n\\) iid draws of \\(X\\)\nLet \\(X_1,X_2,\\dots,X_n\\) be independent and identically distributed RVs with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nDefine \\(S_n\\) and \\(\\bar{X}_n\\) as follows:\n\\[S_n= X_1,X_2,\\dots,X_n= \\sum_{i=1}^n X_i\\]\n\\[\\bar{X}=\\frac{X_1,X_2,\\dots,X_n}{n}= \\frac{S_n}{n}\\]"
  },
  {
    "objectID": "slides/09-slides.html#additional-facts-for-the-clt",
    "href": "slides/09-slides.html#additional-facts-for-the-clt",
    "title": "Week 09:",
    "section": "Additional facts for the CLT",
    "text": "Additional facts for the CLT\nWe can show that:\n\\[\\begin{alignat*}{3}\nE[S_n]&=n\\mu \\hspace{2em}Var[S_n]&=n\\sigma^2 \\hspace{2em} \\sigma_S&=\\sqrt{n}\\sigma\\\\\nE[\\bar{X}_n]&=\\mu \\hspace{2em}Var[\\bar{X}_n]&=\\frac{\\sigma^2}{n} \\hspace{2em}\\sigma_{\\bar{X}}&=\\frac{\\sigma}{\\sqrt{n}}\\\\\n\\end{alignat*}\\]\nBasically: the expected value and variance of the sum is just \\(n\\) times the population parameters (the true values for the distribution).\nSince the mean is just the sum divided by the sample size, the expected value of the mean is equal to the population value and the variance and standard deviations of the mean are decreasing in \\(n\\).\nFinally, we can define \\(Z\\) to be a function of either \\(S\\) or \\(\\bar{X}\\)\n\\[Z_n=\\frac{S_n-n\\mu}{\\sqrt{n}\\sigma}=\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/09-slides.html#central-limit-theorem",
    "href": "slides/09-slides.html#central-limit-theorem",
    "title": "Week 09:",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nFor a sufficiently large \\(n\\)\n\\[\\begin{align*}\n\\bar{X_n}&\\approx N(\\mu,\\sigma^2/n) \\\\\n\\bar{S_n} &\\approx N(n\\mu,n\\sigma^2) \\\\\n\\bar{Z_n}&\\approx N(0,1)\n\\end{align*}\\]\n\nThe distribution of means \\((\\bar{X_n})\\) from almost any distribution \\(X\\) is approximately normal (converges in distribution), but with a smaller variance than (\\(\\sigma^2/n\\))\nProof: Several ways, but requires a little more math than is required for this course"
  },
  {
    "objectID": "slides/09-slides.html#clt-why-it-matters",
    "href": "slides/09-slides.html#clt-why-it-matters",
    "title": "Week 09:",
    "section": "CLT: Why it matters",
    "text": "CLT: Why it matters\nWhy is this result so important?\nWell lots of our questions come of the form, how does a typical value of Y vary with X.\nWe may not know the true underlying distribution of Y, but we can often approximate the distribution of a typical value of Y \\((E[Y])\\) using a normal distribution."
  },
  {
    "objectID": "slides/09-slides.html#simulating-the-clt",
    "href": "slides/09-slides.html#simulating-the-clt",
    "title": "Week 09:",
    "section": "Simulating the CLT",
    "text": "Simulating the CLT\nFor almost any distribution, the distribution of means from a sample of that distribution will converge to some Normal distribution.\nLet’s consider a decidedly non-Normal Binomial distribution: with p = 0.2.\nThe expected value of Binomial Distribution \\(X \\sim B(n,p)\\) is \\(E[X] = n*p\\).\nIf we were to flip a coin 20 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 4.\nIf we were to flip a coin 100 times, whether the probability of heads was 0.2, then the most likely number of heads (the expected value) is 20.\n\n\n\n\n\n\n\n\n\nSimulating 10,000 draws from Binomial Distributions of Different Sizes\n\n# Probability of success\np &lt;- .2\n# Sample sizes\nsamp_sizes &lt;- c(20, 50, 100,1000)\n# Number of simulations\nnsims &lt;- 10000\n# Holder for simulations\ndf_sim &lt;- tibble(\n  expand_grid(\n    samp_size = samp_sizes,\n    sim = 1:nsims,\n    sample_mean = NA\n  )\n)\n\nSimulating 1,000 draws from Binomial Distributions of Different Sizes\nBelow we loop through each sample size in samp_sizes\n\nfor(i in samp_sizes){\n  df_sim$sample_mean[df_sim$samp_size == i] &lt;- replicate(nsims, i*mean(rbinom(i, 1, p)))\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s consider a decided non normal distribution:\n\ndist &lt;- sample(18:80,size=10000, replace = T, prob = runif(length(18:80)))\n\nsamp_mean25 &lt;- replicate(10000,mean(sample(dist,25, replace=F)))\nsamp_mean100 &lt;- replicate(10000,mean(sample(dist,100, replace=F)))\nsamp_mean500 &lt;- replicate(10000,mean(sample(dist,500, replace=F)))\n\nex_df &lt;- tibble(\n  distribution = dist,\n  samp_mean25 = samp_mean25,\n  samp_mean100 = samp_mean100,\n  samp_mean500 = samp_mean500\n)"
  },
  {
    "objectID": "slides/09-slides.html#summary",
    "href": "slides/09-slides.html#summary",
    "title": "Week 09:",
    "section": "Summary",
    "text": "Summary\n\nSo we see that our sampling distributions are centered on the truth, and as the sample size increases, the width of the distribution decreases (Law of Large Numbers)\nThe shapes of distributions of sample means can be approximated by a Normal Distribution \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\)\n\nclass: inverse, center, middle #🦉 ## ICYI: Maximum Likelihood Estimation"
  },
  {
    "objectID": "slides/09-slides.html#maximum-likelihood-estimation-1",
    "href": "slides/09-slides.html#maximum-likelihood-estimation-1",
    "title": "Week 09:",
    "section": "🦉 Maximum Likelihood Estimation",
    "text": "🦉 Maximum Likelihood Estimation\nFormally, consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as\n\\[\\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta)\\]\nwhere \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\).\nMLE tries to find \\(\\hat{\\theta}_{MLE}\\) that maximizes \\(\\mathcal{L}(\\theta \\mid X)\\)"
  },
  {
    "objectID": "slides/09-slides.html#properties-of-maximum-likelihood-estimators",
    "href": "slides/09-slides.html#properties-of-maximum-likelihood-estimators",
    "title": "Week 09:",
    "section": "🦉 Properties of Maximum Likelihood Estimators",
    "text": "🦉 Properties of Maximum Likelihood Estimators\nMLE Estimators are\n\nFunctionally Invariant (The “Plug in Principle”)\n\nIf \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\) than then the MLE of some function of \\(\\theta\\), \\(f(\\theta)\\) is \\(f(\\hat\\theta_{MLE})\\)\nIf we have the MLE of the variance, the square root of this will give us the MLE of the standard deviation\n\nConsistent (by the LLN)\n\n\\(\\hat\\theta_{MLE}\\) collapses to a spike over \\(\\theta\\) as \\(n \\to \\infty\\)\n\nAsympotically Normal (by the CLT)\n\nA \\(n \\to \\infty\\) the sampling distribution of \\(\\hat\\theta_{MLE}\\) becomes Normally distributed\nMakes calculating quantities for inference easy\n\nAsympotically Efficient\n\nAs \\(n \\to \\infty\\), \\(\\hat\\theta_{MLE}\\) tends to be the estimator with the lowest error\n\n\nclass: inverse, center, middle # 💡 # Generalized Linear Models"
  },
  {
    "objectID": "slides/09-slides.html#generalized-linear-models",
    "href": "slides/09-slides.html#generalized-linear-models",
    "title": "Week 09:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nOLS provides a linear estimate to the conditional mean function\n\n–\n\nIf the conditional mean function is linear and the errors are normally distributed, OLS is the MLE.\n\n–\n\nWhat if the conditional mean function is non-linear?\n\n–\n\nSometimes we can transform the mean function so that it is linear, and estimate a generalized linear model (GLM) using MLE\n\n–\n\nUsing a GLM often produces more “reasonable” estimates, and can make more efficient use of the data, although there are many cases where a linear estimate to conditional mean function works just fine (or better)"
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nWe can think some variable \\(y\\) as having a distribution \\(f\\) that contains both a stochastic (random) and systematic components\n\\[\\begin{aligned}\n\\text{Stochastic:    }&& y \\sim f(\\mu,\\alpha)\\\\\n\\text{Systematic:    }&&\\mu = g(X\\beta)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models-1",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models-1",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nIn the past we’ve described the process of modeling \\(y\\) using a linear regression:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nand with multiple predictors:\n\\[y = X\\beta + \\epsilon\\]"
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models-2",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models-2",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nWe haven’t really talked about the distribution of \\(\\epsilon\\), in part because OLS doesn’t require any distributional assumptions to be unbiased.\nBut if we assumed \\(\\epsilon\\) are normally distributed, with mean 0 and variance \\(\\sigma^2\\)\n\\[\\epsilon \\sim f_\\mathcal{N}(0,\\sigma^2)\\]\nThen we could write our model for \\(y\\) as follows:\n\\[\\begin{aligned} y &\\sim f_{\\mathcal{N}}(\\mu,\\sigma^2)\\\\\n\\mu &= X\\beta\\end{aligned}\\]\nWhere the systematic component of why is modeled by \\(X\\beta\\) (i.e. g() is the identity function), with errors that are Normally distributed.\nThe \\(\\beta\\)s that OLS estimates turn out to be the same values that would get by maximizing the likelihood of this function, given our data, \\(X\\), assuming normally distributed errors."
  },
  {
    "objectID": "slides/09-slides.html#generalized-linear-models-1",
    "href": "slides/09-slides.html#generalized-linear-models-1",
    "title": "Week 09:",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nBut what if our outcome doesn’t follow a normal distribution?\nSay for example, we have a binary outcome,that we think follows a Bernoulli distribution with \\(\\pi\\) probability of success.\nWe could model the systematic portion of this using the logistic function, \\(g()\\)\n\\[\\begin{aligned}y &\\sim f_{Bern}(\\pi)\\\\\n\\pi &= \\frac{1}{1+\\exp(-{X\\beta})}\\end{aligned}\\]\nAgain, we could estimate \\(\\beta\\) using the MLE to fit a logistic regression."
  },
  {
    "objectID": "slides/09-slides.html#mle-and-generalized-linear-models-3",
    "href": "slides/09-slides.html#mle-and-generalized-linear-models-3",
    "title": "Week 09:",
    "section": "MLE and Generalized Linear Models",
    "text": "MLE and Generalized Linear Models\nOr if we had a count variable, we might use a Poisson distribution:\n\\[\\begin{aligned}y &\\sim f_{Pois}(\\lambda)\\\\\n\\lambda &= \\exp(X\\beta)\\end{aligned}\\]\nAgain estimating \\(\\beta\\) using MLE.\nIn this class, we’ll let R handle mechanics of actually fitting these models, and instead focus on interpreting their substantive differences"
  },
  {
    "objectID": "slides/09-slides.html#ols-vs-logistic-regression",
    "href": "slides/09-slides.html#ols-vs-logistic-regression",
    "title": "Week 09:",
    "section": "OLS vs Logistic Regression",
    "text": "OLS vs Logistic Regression\nOne situation where we’d use MLE is the case of binary responses variable coded using \\(0\\) and \\(1\\).\nIn practice, these \\(0\\) and \\(1\\)s will code for two classes such as yes/no, non-voter/voter,, etc.\nHow should we model this relationship?\nWe could use OLS to produce a linear estimate of the conditional mean function \\((\\text{E}[Y \\mid {\\bf X} = {\\bf x}])\\), by finding \\(\\beta\\)s that minimize the sum of squared errors\nOr\nWe could use a logistic regression, to produce a linear estimate of the “log-odds” of the conditional mean function of our binary variable by finding \\(\\beta\\)s that maximize the likelihood of this function.\nLet’s simulate data from the following model:\n\\[\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -2 + 3 x\\]\nWe’ll codify this into a function:\n\nsim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {\n  x = rnorm(n = sample_size)\n  eta = beta_0 + beta_1 * x\n  p = 1 / (1 + exp(-eta))\n  y = rbinom(n = sample_size, size = 1, prob = p)\n  data.frame(y, x)\n}\n\nAnd use it to generate some data\n\nset.seed(1)\nexample_data = sim_logistic_data()\nhead(example_data)\n\n  y          x\n1 0 -0.6264538\n2 1  0.1836433\n3 0 -0.8356286\n4 1  1.5952808\n5 0  0.3295078\n6 0 -0.8204684\n\n\nAfter simulating a dataset, we’ll then fit both ordinary linear regression and logistic regression.\n\n# ordinary linear regression\nfit_lm  = lm(y ~ x, data = example_data)\n# logistic regression\nfit_glm = glm(y ~ x, data = example_data, family = binomial)\n\nNotice that the syntax is extremely similar. What’s changed?\n\nlm() has become glm()\nWe’ve added family = binomial argument\n\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\n\n\n\n\n(Intercept)\n\n\n0.31**\n\n\n-2.31*\n\n\n\n\n \n\n\n(0.08)\n\n\n(1.13)\n\n\n\n\nx\n\n\n0.30**\n\n\n3.66*\n\n\n\n\n \n\n\n(0.09)\n\n\n(1.65)\n\n\n\n\nR2\n\n\n0.34\n\n\n \n\n\n\n\nAdj. R2\n\n\n0.31\n\n\n \n\n\n\n\nNum. obs.\n\n\n25\n\n\n25\n\n\n\n\nAIC\n\n\n \n\n\n22.74\n\n\n\n\nBIC\n\n\n \n\n\n25.18\n\n\n\n\nLog Likelihood\n\n\n \n\n\n-9.37\n\n\n\n\nDeviance\n\n\n \n\n\n18.74\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nMaking predictions with an object of type glm is slightly different than making predictions after fitting with lm().\nIn the case of logistic regression, with family = binomial, we have:\n\n\n\n\n\n\n\ntype\nReturned\n\n\n\n\n\"link\" [default]\n\\(\\hat{\\eta}({\\bf x}) = \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right)\\)\n\n\n\"response\"\n\\(\\hat{p}({\\bf x})\\)\n\n\n\nThat is, type = \"link\" will get you the log odds, while type = \"response\" will return \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) for each observation.\n\nplot(y ~ x, data = example_data, \n     pch = 20, ylab = \"Estimated Probability\", \n     main = \"Ordinary vs Logistic Regression\")\nabline(fit_lm, col = \"darkorange\")\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \n      add = TRUE, col = \"dodgerblue\", lty = 2)\nlegend(\"topleft\", c(\"Ordinary\", \"Logistic\", \"Data\"), lty = c(1, 2, 0), \n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))"
  },
  {
    "objectID": "slides/09-slides.html#ols-vs-logistic-regression-1",
    "href": "slides/09-slides.html#ols-vs-logistic-regression-1",
    "title": "Week 09:",
    "section": "OLS vs Logistic Regression",
    "text": "OLS vs Logistic Regression\n\nOLS produces impossible predictions\nThe coefficients from logistic regression aren’t directly interpertable \\(\\to\\) need predicted values.\n\nCan also calculate things like odds-ratios but I find this convoluted.\n\nThe marginal effect of \\(X\\) varies in a logistic regression"
  },
  {
    "objectID": "slides/09-slides.html#interpreting-logistic-regression-coefficients",
    "href": "slides/09-slides.html#interpreting-logistic-regression-coefficients",
    "title": "Week 09:",
    "section": "Interpreting Logistic Regression Coefficients",
    "text": "Interpreting Logistic Regression Coefficients\nOur estimated model is then:\n\\[\\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right) = -2.3 + 3.7 x\\]\nBecause we’re not directly estimating the mean, but instead a function of the mean, we need to be careful with our interpretation of \\(\\hat{\\beta}_1 = 3.7\\).\nThis means that, for a one unit increase in \\(x\\), the log odds change (in this case increase) by \\(3.7\\). Also, since \\(\\hat{\\beta}_1\\) is positive, as we increase \\(x\\) we also increase \\(p({\\bf x})\\).\nFor example, we have:\n\\[\\hat{P}[Y = 1 \\mid X = -0.5] = \\frac{e^{-2.3 + 3.7 \\cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \\cdot (-0.5)}} \\approx 0.016\\]\n\npredict(fit_glm, newdata = data.frame(x=-0.5), type = \"response\")\n\n         1 \n0.01567416 \n\n\n\\[\\hat{P}[Y = 1 \\mid X = 0] = \\frac{e^{-2.3 + 3.7 \\cdot (0)}}{1 + e^{-2.3 + 3.7 \\cdot (0)}} \\approx 0.09\\]\n\npredict(fit_glm, newdata = data.frame(x=0), type = \"response\")\n\n         1 \n0.09016056 \n\n\n\\[\\hat{P}[Y = 1 \\mid X = 1] = \\frac{e^{-2.3 + 3.7 \\cdot (1)}}{1 + e^{-2.3 + 3.7 \\cdot (1)}} \\approx 0.38\\]\n\npredict(fit_glm, newdata = data.frame(x=.5), type = \"response\")\n\n        1 \n0.3814476 \n\n\nbackground-image:url(“https://resourcemoon.com/wp-content/uploads/2018/09/summery.png”) background-size:cover"
  },
  {
    "objectID": "slides/09-slides.html#summary-1",
    "href": "slides/09-slides.html#summary-1",
    "title": "Week 09:",
    "section": "Summary",
    "text": "Summary\n\nThe Law of Large Number’s says that as our sample size increases, our sample mean will converge to the population value\nThe Central Limit Theorem says that the distribution of those sample means will follow a normal distribution\nGeneralized Linear Models allow us to more accurately model different types of data-generating processes using Maxium Likelihood Estimation.\n\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "slides/12-slides.html#general-plan",
    "href": "slides/12-slides.html#general-plan",
    "title": "Week 12:",
    "section": "General Plan",
    "text": "General Plan\n\nSetup\nFeedback\nReview\n\nStatistical Inference\nCausal Inference\nLinear Models\nConfidence intervals and Hypothesis tests\n\nPresentations\n\nclass:inverse, middle, center # 💪 ## Get set up to work"
  },
  {
    "objectID": "slides/12-slides.html#new-packages",
    "href": "slides/12-slides.html#new-packages",
    "title": "Week 12:",
    "section": "New packages",
    "text": "New packages\nFirst we’ll install some packages that you will need for your presentations\n\n# Uncomment and run:\n# install.packages(remotes)\n# remotes::install_github('yihui/xaringan')\n# devtools::install_github(\"gadenbuie/xaringanExtra\")\n# install.packages(\"xaringanthemer\")"
  },
  {
    "objectID": "slides/12-slides.html#packages-for-today",
    "href": "slides/12-slides.html#packages-for-today",
    "title": "Week 12:",
    "section": "Packages for today",
    "text": "Packages for today\n\nthe_packages &lt;- c(\n  ## R Markdown\n  \"kableExtra\",\"DT\",\"texreg\",\n  ## Tidyverse\n  \"tidyverse\", \"lubridate\", \"forcats\", \"haven\", \"labelled\",\n  ## Extensions for ggplot\n  \"ggmap\",\"ggrepel\", \"ggridges\", \"ggthemes\", \"ggpubr\", \n  \"GGally\", \"scales\", \"dagitty\", \"ggdag\", \"ggforce\",\n  # Graphics:\n  \"scatterplot3d\", #&lt;&lt;\n  # Data \n  \"COVID19\",\"maps\",\"mapdata\",\"qss\",\"tidycensus\", \"dataverse\", \n  # Analysis\n  \"DeclareDesign\", \"easystats\", \"zoo\"\n)"
  },
  {
    "objectID": "slides/12-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "href": "slides/12-slides.html#define-a-function-to-load-and-if-needed-install-packages",
    "title": "Week 12:",
    "section": "Define a function to load (and if needed install) packages",
    "text": "Define a function to load (and if needed install) packages\n\nipak &lt;- function(pkg){\n    new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n    if (length(new.pkg)) \n        install.packages(new.pkg, dependencies = TRUE)\n    sapply(pkg, require, character.only = TRUE)\n}"
  },
  {
    "objectID": "slides/12-slides.html#load-packages-for-today",
    "href": "slides/12-slides.html#load-packages-for-today",
    "title": "Week 12:",
    "section": "Load packages for today",
    "text": "Load packages for today\n\nipak(the_packages)\n\n   kableExtra            DT        texreg     tidyverse     lubridate \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      forcats         haven      labelled         ggmap       ggrepel \n         TRUE          TRUE          TRUE          TRUE          TRUE \n     ggridges      ggthemes        ggpubr        GGally        scales \n         TRUE          TRUE          TRUE          TRUE          TRUE \n      dagitty         ggdag       ggforce scatterplot3d       COVID19 \n         TRUE          TRUE          TRUE          TRUE          TRUE \n         maps       mapdata           qss    tidycensus     dataverse \n         TRUE          TRUE          TRUE          TRUE          TRUE \nDeclareDesign     easystats           zoo \n         TRUE          TRUE          TRUE \n\n\nclass:inverse, middle, center # 📢 ## Feedback"
  },
  {
    "objectID": "slides/12-slides.html#feedback-on-drafts",
    "href": "slides/12-slides.html#feedback-on-drafts",
    "title": "Week 12:",
    "section": "Feedback on Drafts",
    "text": "Feedback on Drafts\n\nPosted before class on Thursday.\nIf you haven’t submitted a file on Canvas, do it by COB today.\nFor Thursday’s class:\n\nToday and tomorrow, work on translating your draft into a slide presentation\nCome to class with a set of tasks to work on for your presentation.\n\nSunday, May 1st, upload final presentations to Canvas\nSunday, May 8, upload final papers to Canvas\n\nYou can have a 1-week extension, just email, no questions asked, but must submit by May 15.\n\n\nclass:inverse, middle, center # 🔍 ## Review"
  },
  {
    "objectID": "slides/12-slides.html#what-i-hope-youve-learned",
    "href": "slides/12-slides.html#what-i-hope-youve-learned",
    "title": "Week 12:",
    "section": "What I hope you’ve learned:",
    "text": "What I hope you’ve learned:\nCore Concepts:\n\nStatistical Inference\nCausal Inference\nLinear Models\nConfidence intervals and Hypothesis tests\n\nKey Skills:\n\nHow to load, transform, summarize, and visualize data\nHow to estimate, evaluate, present, and interpret linear models\n\nclass:inverse, middle, center # 🔍 # Core Concepts"
  },
  {
    "objectID": "slides/12-slides.html#statistical-inference",
    "href": "slides/12-slides.html#statistical-inference",
    "title": "Week 12:",
    "section": "Statistical Inference:",
    "text": "Statistical Inference:\n\nStatistical inference involves quantifying uncertainty about what could have happened\nWe describe uncertainty about what could have happened with distributions\nWe can generate these distributions via\n\nsimulation (Bootstrapping and permutations)\nanalytic theory (Limit Theorems)\n\nWe quantify uncertainty using confidence intervals and hypothesis tests"
  },
  {
    "objectID": "slides/12-slides.html#causal-inference",
    "href": "slides/12-slides.html#causal-inference",
    "title": "Week 12:",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal inference involves making counterfactual claims about what would have happened had some causal factor \\((Z)\\) been present or absent.\nThe fundamental problem of causal inference is that for an individual observation, we only observe one of many potential outcomes \\((Y(Z))\\).\nThe statistical solution to this problem moves from individual causal effects (\\(\\tau_i = Y_i(1) - Y_i(0)\\)) to average causal effects \\((\\tau = ATE = E[Y(1)]-E[Y(0)])\\)\nExperimental designs identify the ATE by randomly assigning treatment \\(\\to\\) \\(Y(1), Y(0), X, U, \\perp Z\\)\nObservational designs approximate the experimental ideal based on identifying assumptions that claim conditional independence \\(\\to\\) \\(Y(1), Y(0), X, U, \\perp Z |X\\).\n\nDifference in Difference \\(\\to\\) Parallel Trends\nRegression Discontinuity \\(\\to\\) Continuity at the cut off\nInstrumental Variables \\(\\to\\) The exclusion restriction\nRegression \\(\\to\\) Selection on observable"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression",
    "href": "slides/12-slides.html#linear-regression",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear regression provides a linear estimate of the Conditional Expectation Function\n\nBivariate: \\(E[Y|x] = \\beta_0 + \\beta_1 x +\\epsilon\\)\nMultiple regression: \\(E[Y|X] = X\\beta +\\epsilon = \\beta_0 + \\beta_1 x_1 \\dots \\beta_k x_k +\\epsilon\\)\n\nOrdinary Least Linear regression finds coefficients \\(\\beta\\) by minimizing the sum of squared errors \\((\\sum \\epsilon^2)\\)\nLinear regressions partition variance in the outcome into variance explained by the model \\((X\\beta)\\) and variance not explained by the model (\\(\\epsilon\\)).\n\nA model’s \\(R^2\\) describes the proportion of the overall variance in outcome explained by the predictors"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression-1",
    "href": "slides/12-slides.html#linear-regression-1",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nThe coefficient on a predictor describes how the outcome is expected to change with a 1-unit change in that predictor\nControlling for multiple variables isolates the variation in the outcome explained by one predictor by removing (controlling for) the variation in the outcome and that predictor explained by the other predictors.\n\nWe control for covariates that are common causes of both our key predictor and our outcome to address omitted variable bias (spurious correlation)\nWe avoid controlling for covariates that our common consequences of our outcome and predictor (collider bias)"
  },
  {
    "objectID": "slides/12-slides.html#linear-regression-2",
    "href": "slides/12-slides.html#linear-regression-2",
    "title": "Week 12:",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nWe present linear regression using regression tables where:\n\nEach column corresponds to a model\nEach row corresponds to a coefficient in the model (with standard errors in parentheses and asterisks denoting p&lt;0.05)\n\nWe use generalized linear models to help us incorporate information about our outcomes to improve our models’ predictions\n\nLogistic regression is commonly used to model binary outcomes\nPoisson regression is commonly used to model counts\n\nPlots of predicted values can help us interpret more complicated regression models such as:\n\npolynomial regressions where the marginal effect of one predictor varies non-linearly\ninteraction models, where the marginal effect of one predictor varies with the value of another predictor\ngeneralized linear models"
  },
  {
    "objectID": "slides/12-slides.html#confidence-intervals",
    "href": "slides/12-slides.html#confidence-intervals",
    "title": "Week 12:",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nA confidence interval describes a range of plausible values for the true (population) value of our estimate\nOur confidence is about the interval:\n\n\\((1-\\alpha)\\times 100%\\) of the intervals we could construct in repeated sampling are expected to contain the true (population) value of the thing we’re estimating\n\nTo construct a confidence interval we need:\n\nAn estimate \\((\\hat{\\theta})\\)\nA standard error \\((\\hat{\\sigma_{\\hat{\\theta}}})\\) (the standard deviation of sampling distribution)\nA critical value derived from the hypothetical sampling \\((z_{\\alpha/2})\\)\n\nWith these three components the \\((1-\\alpha)\\times 100%\\) is \\(\\hat{\\theta}\\pm z_{\\alpha/2} \\times \\hat{\\sigma_{\\hat{\\theta}}}\\)\nWe report confidence intervals in text: \\(\\beta = 0.9\\) \\([0.7, 0.11]\\)\nWe interpret estimates as being statistically significant, if 0 is outside the confidence interval"
  },
  {
    "objectID": "slides/12-slides.html#hypothesis-tests",
    "href": "slides/12-slides.html#hypothesis-tests",
    "title": "Week 12:",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\nA hypothesis test quantifies how likely it is that we would observe what we did (our test statistic), if some claim about the world were true (our hypothesis).\nTypically, we test a null hypothesis that expresses our belief that their is no relationship between variables.\n\n\\(\\tau = E[Y|Z=1] - E[Y|Z=0] = 0 \\to\\) No average treatment effect\n\\(\\beta = 0 \\to\\) No relationship between predictor and outcome\n\nIf our claim were true, then under the null, our test statistic would have a distribution centered around the truth.\nWe can describe this distribution via:\n\nsimulation (e.g. permuting the outcome)\nanalytic theory (CLT)\n\nWe quantify our uncertainty using a p-value which describes the probability of observing a test statistic as extreme or more extreme in a world where our null hypothesis was true\n\nIf our p-value is small (p &lt; 0.05), we reject the null hypothesis\nIf our p-value is large (p &gt; 0.05), we fail to reject the null, or retain the null hypothesis\n\n\nclass:inverse, middle, center # 🔍 # Key Skills"
  },
  {
    "objectID": "slides/12-slides.html#how-to-load-explore-and-transform-data",
    "href": "slides/12-slides.html#how-to-load-explore-and-transform-data",
    "title": "Week 12:",
    "section": "How to load, explore and transform data",
    "text": "How to load, explore and transform data\n\n# Load data\nload(\"df.rda\")\ndf &lt;- readr::read_csv(\"df.rda\")\nlibrary(tidyverse)\n## Explore data\nhead(df)\ntable(df$y)\n\n# Transfrom data:\ndf %&gt;%\n  mutate(\n    dv = ifelse(var &lt; 0, NA, y),\n    iv = case_when(\n      x == 1 ~ \"Low\",\n      x == 2 ~ \"Medium\"\n      x == 3 ~ \"High\",\n      T ~ NA_character_\n    ),\n    covar_std = scale(z)\n  ) -&gt; df"
  },
  {
    "objectID": "slides/12-slides.html#how-to-summarize-data",
    "href": "slides/12-slides.html#how-to-summarize-data",
    "title": "Week 12:",
    "section": "How to summarize data",
    "text": "How to summarize data\n\nsummary(df$dv)\n\ndf %&gt;%\n  group_by(iv) %&gt;%\n  summarize(\n    min = min(dv,na.rm = T),\n    median = median(dv,na.rm = T),\n    mean = median(dv,na.rm = T),\n  )"
  },
  {
    "objectID": "slides/12-slides.html#how-to-visualize-data",
    "href": "slides/12-slides.html#how-to-visualize-data",
    "title": "Week 12:",
    "section": "How to visualize data",
    "text": "How to visualize data\n\n# data\ndf %&gt;%\n  # aesthetics\n  ggplot(aes(x = iv, y = dv))+\n  # geometries\n  geom_point() -&gt; figure1"
  },
  {
    "objectID": "slides/12-slides.html#how-to-estimate-and-evaluate",
    "href": "slides/12-slides.html#how-to-estimate-and-evaluate",
    "title": "Week 12:",
    "section": "How to estimate and evaluate",
    "text": "How to estimate and evaluate\n\n# Estimate models\nm1 &lt;- lm(dv ~ iv, df)\nm2 &lt;- lm(dv ~ iv + covar_std, df)\nm3 &lt;- glm(dv ~ iv + covar_std, df, family = binomial)\n\n# evaluate models\nsummary(m1)\nconfint(m2)"
  },
  {
    "objectID": "slides/12-slides.html#how-to-present-and-interpret-linear-models",
    "href": "slides/12-slides.html#how-to-present-and-interpret-linear-models",
    "title": "Week 12:",
    "section": "How to present, and interpret linear models",
    "text": "How to present, and interpret linear models\n\n# Regression Table\ntexreg::htmlreg(list(m1, m2, m3))\n\n\n# Produce Predicted values\n\npred_df &lt;- expand_grid(\n  iv = c(\"Low\",\"Medium\",\"High\"),\n  covar_std = 0\n)\n\npred_df_m2 &lt;- cbind(pred_df, predict(m2, newdata = pred_df), interval = \"confidence\")\n\n# Plot Predicted values\npred_df_m2 %&gt;%\n  ggplot(aes(iv, fit))+\n  geom_pointrange(aes(ymin = lwr, ymax = upr))\n\nclass: inverse, center, middle # 💡 # Final Presentations"
  },
  {
    "objectID": "slides/12-slides.html#final-presentations",
    "href": "slides/12-slides.html#final-presentations",
    "title": "Week 12:",
    "section": "Final Presentations",
    "text": "Final Presentations\n\nNext Tuesday your groups will present some of the findings from your projects\n\n10 Minutes per group\n8-12 slides (15 max)\n2 Minute Q&A\n\nOn Thursday, we will work through the templates you’ve been provided\nDon’t have to present the finished product"
  },
  {
    "objectID": "slides/12-slides.html#final-presentation-structure",
    "href": "slides/12-slides.html#final-presentation-structure",
    "title": "Week 12:",
    "section": "Final Presentation Structure",
    "text": "Final Presentation Structure\n\nMotivation\nResearch Question\nTheory\nExpectations\nData\n\n\nSummary\nDescriptive Table and/or Figure (Optional)\n\n\nDesign\nResults\n\n\nSummary\nTable (Optional)\nFigure (Optional)\n\n\nConclusion\n\n\nAppendices (Extra Slides Optional)"
  },
  {
    "objectID": "slides/12-slides.html#template",
    "href": "slides/12-slides.html#template",
    "title": "Week 12:",
    "section": "Template",
    "text": "Template\nLet’s open up the template and explore.\n\n\n\n\nPOLS 1600"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "More about this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "class/07-class.html",
    "href": "class/07-class.html",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4\n Grumbach and Hill (2022) download",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#readings",
    "href": "class/07-class.html#readings",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4\n Grumbach and Hill (2022) download",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#lecture",
    "href": "class/07-class.html#lecture",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#lab",
    "href": "class/07-class.html#lab",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s lab here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/07-class.html#assignments",
    "href": "class/07-class.html#assignments",
    "title": "Interpreting and Evaluating Linear Regression",
    "section": "Assignments",
    "text": "Assignments\n\n Work on Assignment 2: Data Questions\n No tutorial",
    "crumbs": [
      "Class",
      "Weekly Content",
      "7: Regression Extensions"
    ]
  },
  {
    "objectID": "class/00-class.html",
    "href": "class/00-class.html",
    "title": "Introductions",
    "section": "",
    "text": "Welcome! I’ll see you in class on Thursday.\nTune in via Zoom: https://brown.zoom.us/j/97039852954",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#readings",
    "href": "class/00-class.html#readings",
    "title": "Introductions",
    "section": "Readings",
    "text": "Readings\n\nNone this week",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#lecture",
    "href": "class/00-class.html#lecture",
    "title": "Introductions",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#lab",
    "href": "class/00-class.html#lab",
    "title": "Introductions",
    "section": "Lab",
    "text": "Lab\n\nNone",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/00-class.html#assignments",
    "href": "class/00-class.html#assignments",
    "title": "Introductions",
    "section": "Assignments",
    "text": "Assignments\n\nWork through Software Setup before class next week",
    "crumbs": [
      "Class",
      "Weekly Content",
      "0: Introductions"
    ]
  },
  {
    "objectID": "class/06-class.html",
    "href": "class/06-class.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#readings",
    "href": "class/06-class.html#readings",
    "title": "Multiple Regression",
    "section": "",
    "text": "Imai (2022) Chapter 4",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#lecture",
    "href": "class/06-class.html#lecture",
    "title": "Multiple Regression",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#lab",
    "href": "class/06-class.html#lab",
    "title": "Multiple Regression",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/06-class.html#assignments",
    "href": "class/06-class.html#assignments",
    "title": "Multiple Regression",
    "section": "Assignments",
    "text": "Assignments\n\n Complete Assignment 1: Research Questions\n Complete QSS Tutorial 6: Prediction 2\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial( package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"06-prediction2 \", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "6: Multiple Regression"
    ]
  },
  {
    "objectID": "class/03-class.html",
    "href": "class/03-class.html",
    "title": "Causation I",
    "section": "",
    "text": "Imai (2022) Chapter 2\n Broockman and Kalla (2016) pdf",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#readings",
    "href": "class/03-class.html#readings",
    "title": "Causation I",
    "section": "",
    "text": "Imai (2022) Chapter 2\n Broockman and Kalla (2016) pdf",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#slides",
    "href": "class/03-class.html#slides",
    "title": "Causation I",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#lecture",
    "href": "class/03-class.html#lecture",
    "title": "Causation I",
    "section": "Lecture",
    "text": "Lecture",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#lab",
    "href": "class/03-class.html#lab",
    "title": "Causation I",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/03-class.html#assignments",
    "href": "class/03-class.html#assignments",
    "title": "Causation I",
    "section": "Assignments",
    "text": "Assignments\n\n Complete QSS Tutorial 3: Causality 1\n\n\nlearnr::run_tutorial(\"03-causality1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "3: Causation -- Experiments"
    ]
  },
  {
    "objectID": "class/11-class.html",
    "href": "class/11-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#readings",
    "href": "class/11-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#lecture",
    "href": "class/11-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#lab",
    "href": "class/11-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/11-class.html#assignments",
    "href": "class/11-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "11: Inference -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "class/02-class.html",
    "href": "class/02-class.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Imai (2022) Chapter 3\n R 4 Data Science Chapter 3 (Optional but very helpful)",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#readings",
    "href": "class/02-class.html#readings",
    "title": "Data Visualization",
    "section": "",
    "text": "Imai (2022) Chapter 3\n R 4 Data Science Chapter 3 (Optional but very helpful)",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#slides",
    "href": "class/02-class.html#slides",
    "title": "Data Visualization",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#lecture",
    "href": "class/02-class.html#lecture",
    "title": "Data Visualization",
    "section": "Lecture",
    "text": "Lecture",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#lab",
    "href": "class/02-class.html#lab",
    "title": "Data Visualization",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/02-class.html#assignments",
    "href": "class/02-class.html#assignments",
    "title": "Data Visualization",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 2: Measurement II\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"02-measurement2\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "2: Data Visualization"
    ]
  },
  {
    "objectID": "class/04-class.html",
    "href": "class/04-class.html",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#readings",
    "href": "class/04-class.html#readings",
    "title": "Data and Measurement",
    "section": "",
    "text": "Imai (2022) Chapter 1 here’s a free version of an older, non-tidyverse version",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#lecture",
    "href": "class/04-class.html#lecture",
    "title": "Data and Measurement",
    "section": "Lecture",
    "text": "Lecture\n\n View all slides in new window",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#lab",
    "href": "class/04-class.html#lab",
    "title": "Data and Measurement",
    "section": "Lab",
    "text": "Lab\n\n Download this week’s here \n Upload the rendered html file to Canvas here\n Review the commented solutions after class here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "class/04-class.html#assignments",
    "href": "class/04-class.html#assignments",
    "title": "Data and Measurement",
    "section": "Assignments",
    "text": "Assignments\n\n Work through Software Setup before class next week\n Complete QSS Tutorial 0: Introduction to R; QSS Tutorial 1: Measurement 1\n\n\n# After completing the software setup you should be able to to run the following to launch the tutorials\n\nlearnr::run_tutorial(\"00-intro\", package = \"qsslearnr\")\n\nlearnr::run_tutorial(\"01-measurement1\", package = \"qsslearnr\")\n\n\n Upload the rendered html tutorial files to Canvas here",
    "crumbs": [
      "Class",
      "Weekly Content",
      "4: Causation -- Observational Studies"
    ]
  },
  {
    "objectID": "files/img/hexlogo.html",
    "href": "files/img/hexlogo.html",
    "title": "Hex Logo for POLS 1600",
    "section": "",
    "text": "Linking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  }
]