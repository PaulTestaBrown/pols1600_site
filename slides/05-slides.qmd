---
title: "POLS 1600"
subtitle: "Casual Inference in<br>Observational Designs & <br> Simple Linear Regression"
date: last-modified
date-format: "[Updated ]MMM D, YYYY"
format: 
  revealjs:
    theme: brownslides.scss
    logo: images/pols1600_hex.png
    footer: "POLS 1600"
    multiplex: false
    transition: fade
    slide-number: c
    incremental: true
    center: false
    menu: true
    scrollable: true
    highlight-style: github
    progress: true
    code-overflow: wrap
    chalkboard: true
    # include-after-body: title-slide.html
    title-slide-attributes:
      align: left
      data-background-image: images/pols1600_hex.png
      data-background-position: 90% 50%
      data-background-size: 40%
filters:
  - openlinksinnewpage
execute: 
  eval: true
  echo: true
  warning: false
  message: false
  cache: true
---

```{r}
#| label: init
#| echo: false
#| results: hide
#| warning: false 
#| message: false

library(tidyverse)
library(labelled)
library(haven)
library(DeclareDesign)
library(easystats)
```

# {{< fa map-location>}} Overview {.inverse}

## Overview

- Announcements
- Setup
- Feedback
- Review
- Class plan

## Learing goals {.smaller}

- Introduce the concept of [Directed Acyclic Graphs]{.blue} to describe causal relationships and illustrate potential bias from [confounders]{.blue} and [colliders]{.blue} 

- Discuss three approaches to [covariate adjustment]{.blue}

  - Subclassification
  - Matching
  - [Linear Regression]{.blue}

- Begin discussing three research designs to make causal claims with observational data

  - [Differences-in-Differences]{.blue} (If there's time)
  - Regression Discontinuity Designs
  - Instrumental Variables



## Annoucements

- Sit with your groups (for now)

- Updated timeline for final projects next week

## Group Assignments {.smaller}

```{r}
#| label: groups
#| echo: false
groups_df <- tibble::tibble(
`Group 1` = c("Maia Eng","Guadalupe Herrera","Stephen Robinson","Jeremiah Harrington"),
`Group 2` = c("Andrew Rovinsky", "Spencer Lorin","Lucinda Anderson","Serenity Hamilton"),
`Group 3` = c("Serafym Rybachkivskyi","Rachel Kim","Kai Blades","Emma Coleman"),
`Group 4` = c("Tiffany Eddy","Daniel Solomon","Zoe Smith","Lorena Calderon"),
`Group 5` = c("Christopher Maron","Daniel Baker","Neve Diaz-Carr","Olivia Hanley"),
`Group 6` = c("Mia Hamilton","Emily Colon","Davis Kelly","Talia Levine"),
`Group 7` = c("Mariana Melzer","Kahrie Langham", "Shannon Feerick-Hillenbrand","Jarret Fernandes"),
`Group 8` = c("Logan Szittai", "Keiley Thompson","Lydell Dyer","Mahir Arora")
)

groups_df |> 
  pivot_longer(cols = starts_with("Group"),
               names_to = "Group",
               values_to = "Name") |> 
  arrange(Group) |>
  group_by(Group) |> 
  mutate(
    id = 1:n()
  ) |> 
  pivot_wider(id_cols = Group,
              names_from = id,
              values_from = Name) -> groups_df
# write_csv(groups_df, file = "../files/groups.csv" )
DT::datatable(groups_df)
```




# {{< fa bullhorn >}} Feedback {top="50%" background-image="https://jplilley.com/images/easyblog_articles/145/holding-ears-300x196.jpg"}

```{r}
#| label: feeback
#| echo: false
df <- haven::read_spss("../files/data/class_surveys/wk03.sav")

df %>%
  mutate(
    Reincarnation = reincarnation,
    Why = reincarnation_why
  ) -> df

```

## What did we like {.smaller}

```{r}
#| label: likes
#| echo: false
DT::datatable(df %>% 
                select(Likes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```

## What did we dislike {.smaller}

```{r}
#| label: dislikes
#| echo: false

DT::datatable(df %>% 
                select(Dislikes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```

## Grinding an Iron Pestle into a Needle {.smaller background-image="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5OqZwz1hmoCY5v_31xQ9A@2x.jpeg" background-opacity=.5}


::::{.columns}

:::{.column .fragment width="45%"}
### Me

- Less is more
- Go slow
- Provide labs/code earlier
- Adapt assignments/policies 

:::

:::{.column .fragment width="45%"}
### You

- Active reading
- Do tutorials
- Review labs before class
- Review comments after class
- Ask for help
- Don't give up!



:::

::::

# {{< fa magnifying-glass >}} Review {.inverse}

## Review

- Data wrangling 

- Descriptive Statistics

- Levels of understanding

- Data visualization

# {{< fa magnifying-glass >}} Data Wrangling {.inverse}

## Data wrangling {.smaller}

```{r}
#| label: spdf
#| echo: false
sp_df <- tibble(
  Skill = c("Setup R",
            "Load data",
            "Get HLO of data",
            "Transform data",
            "Reshape data",
            "Summarize data numerically",
            "Summarize data graphically"),
  `Common Commands` = c("library(), ipak()",
               "read_csv(), load()",
               "df$x, glimpse(), table(), summary()",
               "<-, mutate(), ifelse(), case_when()",
               "pivot_longer(), left_join()",
               "mean(), median(), summarise(), group_by()",
               "ggplot(), aes(), geom_")
)

kable(sp_df, 
      caption = "You're learning how to map conceptual tasks to commands in R",
      caption.above=T)

```

## Mapping Concepts to Code

- Takes time and practice

- Don't be afraid to [FAAFO]{.blue}

- Don't worry about memorizing everything.

- Statistical programming is necessary to actually [do]{.blue} empirical research

- [Learning to code]{.blue} will help us [understand statistical concepts]{.blue}.

- Learning to [think programmatically]{.blue} and algorithmically will help us [tackle complex problems]{.blue}

# {{< fa magnifying-glass >}} Descriptive Statiscs{.inverse}

## Descriptive statistics {.smaller}

- Descriptive statistics help us describe what's typical of our data

- [What's a typical value in our data]{.blue}

  - [Mean](https://pols1600.paultesta.org/labs/01-lab-comments.html#mean)
  - [Median](https://pols1600.paultesta.org/labs/01-lab-comments.html#median)
  - [Mode](https://pols1600.paultesta.org/labs/01-lab-comments.html#modes)

- [How much do our data vary?]{.blue}

  - [Variance](https://pols1600.paultesta.org/labs/01-lab-comments.html#variance)
  - [Standard deviation](https://pols1600.paultesta.org/labs/01-lab-comments.html#standard-deviations)

- As one variable changes [how does another change]{.blue}?

  - [Covariance](https://pols1600.paultesta.org/labs/01-lab-comments.html#covariance)
  - [Correlation](https://pols1600.paultesta.org/labs/01-lab-comments.html#correlation)

- Descriptive statistics are:

  - Diagnostic
  - Generative

# {{< fa magnifying-glass>}} Levels of understanding{.inverse}

## Levels of understanding in POLS 1600

- Conceptual

- Practical

- Definitional

- Theoretical

. . .

Let's illustrate these different levels of understanding about our old friend the [mean]{.blue}

## Mean: Conceptual Understanding{.smaller}

A mean is:

- A common and important [measure of central tendency]{.blue} (what's typical)

- It's the [arithmetic average]{.blue} you learned in school

- We can think of it as the [balancing point]{.blue} of a distribution

- A [conditional mean]{.blue} is the average of one variable $X$, when some other variable, $Z$ takes a value $z$

  - Think about the average height in our class ([unconditional mean]{.blue}) vs the average height among men and women (\[conditional means\].{blue})

## Mean as a balancing point{.smaller}

![](https://mathbitsnotebook.com/Algebra1/StatisticsData/balancepoint1.jpg)

[Source](https://mathbitsnotebook.com/Algebra1/StatisticsData/STCenter.html)

## Mean: Practical{.smaller}

There are lots of ways to calculate means in `R`

- The simplest is to use the `mean()` function

  - If our data have missing values, we need to to tell `R` to remove them

```{r}
#| label: meanex
#| eval: false
mean(df$x, na.rm=T)
```

## Conditional Means: Practical{.smaller}

- To calculate a conditional mean we could us a logical index `[df$z == 1]`

```{r}
#| label: conmean
#| eval: false
mean(df$x[df$z == 1], na.rm=T)
```

- If we wanted to a calculate a lot of conditional means we could use the `mean()` in combination with `group_by()` and `summarise()`

```{r}
#| label: grpmean
#| eval: false
df %>% 
  group_by(z)%>%
  summarise(
    x = mean(x, na.rm=T)
  )
```

## Mean: Definitional{.smaller}

Formally, we define the arithmetic mean of $x$ as $\bar{x}$:

$$
\bar{x} = \frac{1}{n}\left (\sum_{i=1}^n{x_i}\right ) = \frac{x_1+x_2+\cdots +x_n}{n}
$$

In words, this formula says, to calculate the average of x, we sum up all the values of $x_i$ from observation $i=1$ to $i=n$ and then divide by the total number of observations $n$

## Mean: Definitional {.smaller}

- In this class, I don't put a lot of weight on memorizing definitions (that's what Google's for).

- But being comfortable with "the math" is important and useful

- Definitional knowledge is a prerequisite for understanding more theoretical claims.

## Mean: Theoretical{.smaller}

Suppose I asked you to show that the sum of deviations from a mean equals 0?

$$
\text{Claim:} \sum_{i=1}^n (x_i -\bar{x}) = 0
$$

## Mean: Theoretical {.smaller}

Knowing the definition of an arithmetic mean, we could write:

$$
\begin{aligned}
\sum_{i=1}^n (x_i -\bar{x}) &= \sum_{i=1}^n x_i - \sum_{i=1}^n\bar{x} & \text{Distribute Summation}\\
              &= \sum_{i=1}^n x_i - n\bar{x} & \text{Summing a constant, } \bar{x}\\
              &= \sum_{i=1}^n x_i - n\times \left ( \frac{1}{n} \sum_{i=1}^n{x_i}\right ) & \text{Definition of } \bar{x}\\
              &= \sum_{i=1}^n x_i - \sum_{i=1}^n{x_i} & n \times \frac{1}{n}=1\\
              &= 0             
\end{aligned}
$$

## Mean: Theoretical {.smaller}

Why do we care?

- Showing the deviations sum to 0 is another way of saying the mean is a [balancing point]{.blue}.

- This turns out to be a useful property of means that will reappear throughout the course

- If I asked you to make a prediction, $\hat{x}$ of a random person's height in this class, the mean would have the lowest [mean squared error]{.blue} (MSE $=\frac{1}{n}\sum (x_i - \hat{x_i})^2)$

## Mean: Theoretical

Occasionally, you'll read or here me say say things like:

> The sample mean is an unbiased estimator of the population mean

In a statistics class, we would take time to prove this.

## The sample mean is an unbiased estimator of the population mean{.smaller}

Claim:

Let $x_1, x_2, \dots x_n$ from a random sample from a population with mean $\mu$ and variance $\sigma^2$

Then:

$$
\bar{x} = \frac{1}{n}\left (\sum_{i=1}^n x_i\right )
$$

is an unbiased estimator of $\mu$

$$
E[\bar{x}] = \mu
$$

## The sample mean is an unbiased estimator of the population mean {.smaller}

Proof:

$$
\begin{aligned}
E\left [\bar{x} \right] &= E\left [\frac{1}{n}\left (\sum_{i=1}^n x_i \right) \right] & \text{Definition of } \bar{x} \\
&= \frac{1}{n} \sum_{i=1}^nE\left [ x_i \right]  & \text{Linearity of Expectations} \\
&= \frac{1}{n} \sum_{i=1}^n \mu  & E[x_i] = \mu \\
&= \frac{n}{n}  \mu  & \sum_{i=1}^n \mu = n\mu \\
&= \mu  & \blacksquare \\
\end{aligned}
$$

## Levels of understanding {.smaller}

::: nonincremental
In this course, we tend to emphasize the

- **Conceptual**

- **Practical**

Over

- Definitional

- Theoretical

In an intro statistics class, the ordering might be reversed.

Trade offs:
:::

- Pro: We actually get to *work with data* and *do empirical research* much sooner
- Cons: We substitute intuitive understandings for more rigorous proofs

# {{< fa magnifying-glass>}} Data Visualization {.inverse}

## Data Visualization {.smaller}

-   The grammar of graphics

-   At minimum you need:

    -   `data`
    -   `aesthetic` mappings
    -   `geometries`

-  Take a sad plot and make it better by:

    -   `labels`
    -   `themes`
    -   `statistics`
    -   `cooridnates`
    -   `facets`
    -   transforming your data before plotting

## You are about to be reincarnated: HLO

```{r hlo}
#| label: hlo
#| echo: true

df$reincarnation

table(df$reincarnation)
```




## Basic Plot

::: panel-tabset

## Code
```{r}
#| label: plot0code
#| eval: false

df %>%
  ggplot(aes(x = reincarnation, 
             fill = reincarnation))+
  geom_bar(
    stat = "count"
  ) 
```


## Figure
```{r plot0, echo=F}
df %>%
  ggplot(aes(x = reincarnation, 
             fill = reincarnation))+
  geom_bar(
    stat = "count"
  )
```
:::


##  Use a factor to label and order responses

::: panel-tabset

## Recode
```{r factors}

df %>%
  mutate(
    # Turn numeric values into factor labels 
    Reincarnation = forcats::as_factor(reincarnation),
    # Order factor in decreasing frequency of levels
    Reincarnation = forcats::fct_infreq(Reincarnation),
    # Reverse order so levels are increasing in frequency
    Reincarnation = forcats::fct_rev(Reincarnation),
    # Rename explanations
    Why = reincarnation_why
  ) -> df

```

## Check recoding

```{r recode}
table(recode= df$Reincarnation, original = df$reincarnation)
```
:::


## Revised figure

::: panel-tabset

## Code
```{r plot1code, }
#| label: plot1code

df %>% # Data
  # Aesthetics
  ggplot(aes(x = Reincarnation, 
             fill = Reincarnation))+
  # Geometry
  geom_bar(stat = "count")+ # Statistic
  ## Include levels of Reincarnation w/ no values
  scale_x_discrete(drop=FALSE)+
  # Don't include a legend
  scale_fill_discrete(drop=FALSE, guide="none")+
  # Flip x and y
  coord_flip()+
  # Remove lines
  theme_classic() -> fig1
```

## Revised Figure
```{r}
#| echo: false
# Display figure
fig1
```

:::


## {.smaller}
#### What creature and why?


```{r }
#| label: creatures
#| echo: false


DT::datatable(df %>%  select(Reincarnation,Why),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```





## Adding labelled values

::: panel-tabset


## Recodes
```{r labcode}
df %>%
  mutate(
    # Create numeric id
    id = 1:n(),
    # Create a label with 3 answers and NA elsewhere
    Label = case_when(
      id == 10 ~ str_wrap(reincarnation_why[10],30),
      id == 4 ~ str_wrap(reincarnation_why[4],30),
      id == 7 ~ str_wrap(reincarnation_why[7],30),
      TRUE ~ NA_character_

    )

  ) -> df
```

## Recode Output

```{r whytab, echo=F}
DT::datatable(df %>% 
                select(Label,Why),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```


## Aggregate Data
```{r}
#| label: aggregatedata

# Calculate totals before calling ggplot
plot_df <- df %>%
  group_by(Reincarnation)%>% 
  summarise( 
    Count = n(), 
    Why = na.omit(unique(Label)) 
  ) 
```

:::


## You're about to be reincarnated:{.smaller}

::: panel-tabset

## Aggregate df
```{r }
#| label: labtab
#| echo: false
DT::datatable(plot_df)
```

## Revised Figure Code

```{r }
#| label: plot2code
#| echo: true

plot_df %>%
  ggplot(aes(x = Reincarnation, 
             y = Count,
             fill = Reincarnation, 
             label=Why))+
  geom_bar(stat = "identity")+ #<<
  ## Include levels of Reincarnation w/ no values
  scale_x_discrete(drop=FALSE)+
  # Don't include a legend
  scale_fill_discrete(drop=FALSE, guide="none")+
  coord_flip()+
  labs(x = "",y="",title="You're about to be reincarnated.\nWhat do you want to come back as?")+
  theme_classic()+
  ggrepel::geom_label_repel(
    fill="white",
    nudge_y = 1, 
    hjust = "left",
    size=3,
    arrow = arrow(length = unit(0.015, "npc"))
    )+ 
  scale_y_continuous(
    breaks = c(0,2,4,6,8,10,12),
    expand = expansion(add =c(0,6))
    ) -> fig1
```

## Labelled Figure
```{r }
#| label: fig1labelled
#| echo: false

fig1
```
:::

## Data visualization is an iterative process

- Data visualization is an iterative process

- Good data viz requires lots of data transformations

- Start with a minimum working example and build from there

- Don't let the perfect be the enemy of the good enough.


# {{< fa code >}} Setup

## New packages

This week's lab we'll be using the `dataverse` package to download data on presidential elections

Next week's lab, we'll be using the `tidycensus` package to download census data. 

We'll also need to [install a census API]{.blue} to get the data.

Here's a [detailed guide](https://pols1600.paultesta.org/resources/04-packages) of what we'll do in class right now.

## Install new packages

These packages are easier to install live:

```{r}
#| label: installpacks
#| eval: false
install.packages("dataverse")
install.packages("tidycensus")
install.packages("easystats")
install.packages("DeclareDesign")

```

## Census API {.smaller}

Please follow these steps so you can download data directly from the U.S. Census [here](https://pols1600.paultesta.org/resources/04-packages.html#3_Install_a_Census_API_tidycensus_package):

1.  Install the `tidycensus` package
2.  Load the installed package
3.  Request an API key from the Census
4.  Check your email
5.  Activate your key
6.  Install your API key in R
7.  Check that everything worked


## Packages for today

```{r}
#| label: packages
#| echo: true

## Pacakges for today
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)

## Define a function to load (and if needed install) packages

#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

## Install (if needed) and load libraries in the_packages
ipak(the_packages)
```



# {{< fa code >}}Previewing the Lab

## Red Covid{.smaller}


::::{.columns}

:::{.column width="45%"}

![](images/05_redcovid_orig.png)


[Red Covid](https://www.nytimes.com/2021/09/27/briefing/covid-red-states-vaccinations.html) *New York Times*, 27 September, 2021
:::

:::{.column width="45%"}
![](images/05_covid.png)
[Red Covid, an Update](https://www.nytimes.com/2022/02/18/briefing/red-covid-partisan-deaths-vaccines.html) *New York Times*, 18 February, 2022
:::
::::

## Preview of the Lab{.smaller}

[Please download]{.blue} Thursday's lab [here](https://pols1600.paultesta.org/labs/05-lab.qmd)

- Conceptually, this lab is designed to help reinforce the relationship between linear models like $y=\beta_0 + \beta_1x$ and the conditional expectation function $E[Y|X]$.

- Substantively, we will explore whether David Leonhardt's claims about [Red Covid](https://www.nytimes.com/2021/09/27/briefing/covid-red-states-vaccinations.html) the political polarization of vaccines and its consequences

## Lab: Questions 1-5: Review{.smaller}

Questions 1-5 are designed to [reinforce]{.blue} your [data wrangling]{.blue} skills.  In particular, you will get practice:

  - Creating and recoding variables using `mutate()`
  - Calculating a [moving average](https://en.wikipedia.org/wiki/Moving_average) or rolling mean using the `rollmean()` function from the `zoo` package
  - Transforming the data on presidential elections so that it can be merged with the data on Covid-19 using the `pivot_wider()` function.
  - [Merging data](https://r4ds.had.co.nz/relational-data.html) together using the `left_join()` function.



## Lab: Questions 6-10: Simple Linear Regression{.smaller}

- In question 6, you will see how calculating conditional means provides a simple test of "Red Covid" claim.

- In question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)

- In question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)

- In question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.

- Question 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week's lab.

## Before Thursday

The following slides provide detailed explanations of all the code you'll need for each question. 

- [Please run this code before class on Thursday]{.blue}

- We will review this material together at the start of class, but you will spend most of our time on the Questions 6-10

## Q1: Setup your workspace

::: panel-tabset

## Task

Q1 asks you to setup your workspace

This means [loading]{.blue} and, if needed, [installing]{.blue} the packages you will use.



## Code for Q1

```{r}
#| label: extendedsetup
#| eval: false

## Pacakges for today
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)

## Define a function to load (and if needed install) packages

#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

## Install (if needed) and load libraries in the_packages
ipak(the_packages)

```


:::


## Q2 Load the data

To explore Leonhardt's claims about *Red Covid*, we'll need data on:

- Covid-19
- The 2020 Presidential Election


## Q2.1 Load the Covid-19 Data{.smaller}

To load data on Covid-19 just run this

```{r}
#| label: covid

load(url("https://pols1600.paultesta.org/files/data/covid.rda"))
```

## Q2.2 Load Election Data {.smaller}

::: panel-tabset

## Task

Q2.2. asks you to write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab's dataverse

- Once you've installed the `dataverse` package you should be able to do this:

## Code for Q2.2
```{r}
#| label: pres_data
# Try this code first
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

pres_df <- dataverse::get_dataframe_by_name(
  "1976-2020-president.tab",
  "doi:10.7910/DVN/42MVDX"
)

# If the code above fails, comment out and uncomment the code below:

# load(url("https://pols1600.paultesta.org/files/data/pres_df.rda"))
```
:::

## Q3 Describe the structure of each dataset{.smaller}

Question 3 asks you to [describe the structure]{.blue} of each dataset.

- Specifically, it asks you to get a high level overview of `covid` and `pres_df` and describe the [unit of analysis]{.blue} in each dataset:
  - Describe substantively what specific, observation each [row]{.blue} in the dataset corresponds to
  - In covid `covid` dataset, the [unit of analysis]{.blue} is a [state-date]{.blue}

## Q3 Describe the structure of each dataset{.smaller}

Here's some possible code you could use to get a quick [HLO]{.blue} of each dataset:

::: panel-tabset


## HLO `covid`
```{r}
#| label: hlocovid
#| eval: false

# check names in `covid`
names(covid)

# take a quick look values of each variable

glimpse(covid)

# Look at first few observations for:
# date, administrative_area_level_2, 

covid %>% 
  select(date, administrative_area_level_2) %>%
  head()

# Summarize data to get a better sense of the unit of observastion

covid %>% 
  group_by(administrative_area_level_2) %>%
  summarise(
    n = n(), # Number of observations for each state
    start_date = min(date, na.rm = T),
    end_date = max(date, na.rm=T)
  ) -> hlo_covid_df

hlo_covid_df


# How many unique values of date and state are their:

n_dates <- length(unique(covid$date))
n_states <- length(unique(covid$administrative_area_level_2))
n_dates
n_states

# If we had observations for every state on every date then the number of rows 
# in the data 
dim(covid)[1]
# Should equal
dim(covid)[1] == n_dates * n_states

# This is what economists would call an unbalanced panel


```

## HLO `pres_df`

```{r}
#| label: hlopresdf
#| eval: false

# check names in `pres_df`
names(pres_df)

# take a quick look values of each variable

glimpse(pres_df)

# Unit of analysis is a year-state-candidate
pres_df %>% 
  select(year, state_po, candidate) %>%
  head()

# How many states?
length(unique(pres_df$state_po))



# How many candidates and parties on the ballot in a given election year
pres_df %>% 
  group_by(year) %>%
  summarise(
    n_candidates = length(unique(candidate)),
    # Look at both party_detailed and party_simplified
    n_parties_detailed = length(unique(party_detailed)),
    n_parties_simplified = length(unique(party_simplified))
  ) -> hlo_pres_df
hlo_pres_df

# Look at 2020
# pres_df$candidate[pres_df$year == "2020"]

 

```
:::
 
 
## Q4 Recode the data for analysis

Using our understanding of the structure of the data, Q4 asks you to:


- Recode the Covid-19 data like we've done before [plus]{.blue}
- Calculate [rolling means]{.blue},  7 and 14 day averages
- Reshape, recode, and filter the presidential election data



## Q4.1 Recode the Covid-19{.smaller}

::: panel-tabset

## Task
This is the same code we've used before to create `covid_us` from `covid` with the addition of code to calculate a [rolling mean]{.blue} or [moving average]{.blue} of the number of new cases

## Code for Q4.1
```{r}
#| label: covid19recode
# Create a vector containing of US territories
territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )

# Filter out Territories and create state variable
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)%>%
  mutate(
    state = administrative_area_level_2
  )

# Calculate new cases, new cases per capita, and 7-day average

covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000,
    new_cases_pc_7da = zoo::rollmean(new_cases_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us

# Recode facemask policy

covid_us %>%
mutate(
  # Recode facial_coverings to create face_masks
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ),
    # Turn face_masks into a factor with ordered policy levels
    face_masks = factor(face_masks,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -> covid_us

# Create year-month and percent vaccinated variables

covid_us %>%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(year, 
                       str_pad(month, width = 2, pad=0), 
                       sep = "-"),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```

## Template Code for Q4.2

```{r}
# Calculate new cases, new cases per capita, and 7-day average

covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000,
    new_cases_pc_7day = zoo::rollmean(new_cases_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us
```

:::


## Q4.2 Calculate Rolling Means of Covid Deaths{.smaller}

::: panel-tabset

## Task

Q4.2 asks you to create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents

It encourages you to use the code `new_cases_pc_7da` as a template

To build your coding skills, try writing this yourself, then comparing it to the code in the next tab:

## Code for Q4.2
```{r}
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = new_deaths / population *100000,
    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA ),
    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, 
                                     k = 14, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us
```

:::



## Rolling Averages{.smaller} 

The next slides [aren't necessary for the lab]{.blue} but are designed to illustrate:

- the [concept of a rolling mean]{.blue}
- [what]{.blue} the code does
- [why]{.blue} might prefer rolling averages over daily values


## Look at the output of `zoo::rollmean()`
```{r}
#| label: rollmean
covid_us %>%
  filter(date > "2020-03-05") %>%
  select(date,new_cases_pc,new_cases_pc_7day)
```

## Comparing Daily Cases to Rolling Average{.smaller}

The following code illustrates how a 7-day rolling mean smooths (`new_cases_pc_7da`) over the [*noisiness*]{.blue} of the daily measure

::: panel-tabset

## Code
```{r }
#| label: plotmean
covid_us %>%
  filter(date > "2020-03-05", 
         state == "Minnesota") %>%
  select(date,
         new_cases_pc,
         new_cases_pc_7day)%>%
  ggplot(aes(date,new_cases_pc ))+
  geom_line(aes(col="Daily"))+
  # set y aesthetic for second line of rolling average
  geom_line(aes(y = new_cases_pc_7day,
                col = "7-day average")
            ) +
  theme(legend.position="bottom")+
    labs( col = "Measure",
    y = "New Cases Per 100k", x = "",
    title = "Minnesota"
  ) -> fig_covid_mn 
```

## Figure
```{r}
#| label: plotmean2
#| echo: false
fig_covid_mn

```
:::

## Q4.3 Recode Presidential data{.smaller}

::: panel-tabset

## Task

Q4.3 Gives you a long list of steps to recode, reshape, and filter `pres_df` to produce `pres_df2020`

Most of this is review but it can seem like a lot.

Walk through the provided code and see if you can [map each conceptual step]{.blue} in Q4.3 to its [implementation in the code]{.blue}

## Code for Q4.3

```{r}
#| label: pres_wrangle
pres_df %>%
  mutate(
    year_election = year,
    state = str_to_title(state),
    # Fix DC
    state = ifelse(state == "District Of Columbia", "District of Columbia", state)
  ) %>%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%>%
  filter(year == 2020) %>%
  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes
         ) %>%
  pivot_wider(names_from = party_simplified,
              values_from = candidatevotes) %>%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes *100,
    rep_voteshare = REPUBLICAN/totalvotes*100,
    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,"Trump","Biden")))
  ) -> pres2020_df

# Check Output:

glimpse(pres2020_df)
```

:::

## Q5 merging data{.smaller}

::: panel-tabset

## Task

Q5 asks you to merge the 2020 election data from `pres2020_df` into `covid_us` using the common `state` variable in each dataset using the function `left_join()`


## Merge election data into Covid data

```{r}
#| label: merge_pres
dim(covid_us)
dim(pres2020_df)
covid_us <- covid_us %>% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(covid_us) 
```
:::

## Advice for merging {.smaller}

::: panel-tabset

## Advice

When merging datasets:

- Check the matches in your joining variables
  - Make sure the values `state` are the same in each dataset 
  - Check for differences in spelling, punctuation, etc.
- Check the [dimensions]{.blue} of output of your `left_join()`
  - If there is a 1-1 match the number of rows should be the same before after

## Illustration

```{r}
#| label: merge_check
# Should be 51 states and DC in each
sum(unique(pres_df$state) %in% covid_us$state)

# Look at each state variable
## With [] index
pres_df$state[1:5]
covid_us$state[1:5]

# Matching is case sensitive 

# make pres_df$state title case

## Base R:
pres_df$state <- str_to_title(pres_df$state )
## Tidy R:
pres_df %>% 
  mutate(
    state = str_to_title(state )
  ) -> pres_df

# Should be 51
sum(unique(pres_df$state) %in% covid_us$state)

# Find the mismatch:
unique(pres_df$state[!pres_df$state %in% covid_us$state])

# Two equivalent ways to fix this mismatch
## Base R: Quick fix to change spelling of DC
pres_df$state[pres2020_df$state == "District Of Columbia"] <- "District of Columbia"

## Tidy R: Quick fix to change spelling of DC

pres_df %>% 
  mutate(
    state = ifelse(test = state == "District Of Columbia",
                   yes = "District of Columbia",
                   no = state
                   )
  ) -> pres_df


# Problem Solved
sum(unique(pres2020_df$state) %in% covid_us$state)

```


:::

# {{< fa lightbulb >}} Causal Inference {.inverse}

## Causal inference is about counterfactual comparisons

- Causal inference is about counterfactual comparisons

  - What would have happened if some aspect of the world either had or had not been present

## Causal Identification{.smaller}

- [Casual Identification]{.blue} refers to "the assumptions needed for statistical estimates to be given a causal interpretation" [Keele (2015)](http://lukekeele.com/wp-content/uploads/2016/03/causal.pdf)\]

  - What do we need to assume to make our claims about cause and effect credible

- [Experimental Designs]{.blue} rely on [randomization]{.blue} of treatment to justify their causal claims

- [Observational Designs]{.blue} require [additional assumptions]{.blue} and [knowledge]{.blue} to make causal claims 

## Experimental Designs {.smaller}

- [Experimental designs]{.blue} are studies in which a causal variable of interest, the *treatement*, is [manipulated by the researcher]{.blue} to examine its causal effects on some *outcome* of interest

- [Random assignment]{.blue} is the key to causal identification in experiments because it creates [statistical independence]{.blue} between [treatment]{.blue} and [potential outcomes]{.blue} any potential [confounding factors]{.blue}

:::{.fragment}

$$
Y_i(1),Y_i(0),\mathbf{X_i},\mathbf{U_i} \unicode{x2AEB} D_i
$$\

:::

## Randomization creates credible counterfactual comparisons{.smaller}

If treatment has been randomly assigned, then:

- The only thing that differs between treatment and control is that one group got the treatment, and another did not.
- We can estimate the Average Treatment Effect (ATE) using the difference of sample means

:::{.fragment}

$$
\begin{aligned}
E \left[ \frac{\sum_1^m Y_i}{m}-\frac{\sum_{m+1}^N Y_i}{N-m}\right]&=\overbrace{E \left[ \frac{\sum_1^m Y_i}{m}\right]}^{\substack{\text{Average outcome}\\
\text{among treated}\\ \text{units}}}
-\overbrace{E \left[\frac{\sum_{m+1}^N Y_i}{N-m}\right]}^{\substack{\text{Average outcome}\\
\text{among control}\\ \text{units}}}\\
&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]
\end{aligned}
$$

:::

## Observational Designs {.smaller}

- [Observational designs]{.blue} are studies in which a causal variable of interest is determined by someone/thing [other than the researcher]{.blue} (nature, governments, people, etc.)

- Since treatment has not been randomly assigned, observational studies typically require [stronger assumptions]{.blue} to make causal claims.

- Generally speaking, these assumptions amount to a claim about conditional independence

:::{.fragment}


$$
Y_i(1),Y_i(0),\mathbf{X_i},\mathbf{U_i} \unicode{x2AEB} D_i | K_i
$$

:::

- Where after conditioning on $K_i$, some [knowledge about the world]{.blue} and how the [data were generated]{.blue}, our [treatment]{.blue} is as good as (as-if) randomly assigned (hence [conditionally independent]{.blue})
  - Economists often call this assumption of [selection on observables]{.blue}

## Causal Inference in Observational Studies {.smaller}

To understand how to make causal claims in observational studies we will:

- Introduce the concept of [Directed Acyclic Graphs]{.blue} to describe causal relationships

- Discuss three approaches to [covariate adjustment]{.blue}

  - Subclassification
  - Matching
  - [Linear Regression]{.blue}

- Three research designs for observational data

  - [Differences-in-Differences]{.blue}
  - Regression Discontinuity Designs
  - Instrumental Variables
  
  


# {{< fa lightbulb >}} Directed Acyclic Graphs {.inverse}


## Two Ways to Describe Causal Claims

In this course, we will use two forms of notation to describe our causal claims.

- *Potential Outcomes Notation* (last lecture)

  - Illustrates the [fundamental problem of causal inference]{.blue}

- [**Directed Acyclic Graphs** (DAGs)]{.blue}

  - Illustrates potential bias from [confounders]{.blue} and [colliders]{.blue}

## Directed Acyclic Graphs

- Directed Acyclic Graphs provide a way of encoding assumptions about casual relationships

  - **Directed** Arrows $\to$ describe a direct causal effect

  - Arrow from $D\to Y$ means $Y_i(d) \neq Y_i(d^\prime)$ "The outcome ( $Y$) for person $i$ when D happens ( $Y_i(d)$ ) is different than the the outcome when $D$ doesn't happen ( $Y_i(d^\prime)$ )

  - No arrow = no effect ( $Y_i(d) = Y_i(d^\prime)$ )

  - **Acyclic:** No cycles. A variable can't cause itself

## Types of variables in a DAG{.smaller}


:::: panel-tabset

## DAG
![](https://book.declaredesign.org/figures/figure-6-2.svg)

@Blair2023-yg [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)

## Variables

:::{.nonincremental}

Causal Explanations Involve:

- `Y` our outcome
- `D` A possible cause of Y
- `M` A [mediator]{.blue} or mechanism through which `D` effects `Y`
- `Z` An [instrument]{.blue} that can help us isolate the effects of D on  `Y
- `X2` a [covariate]{.blue} that may [moderate]{.blue} the effect of `D` on `Y`

Threats to causal claims/Sources of bias:

- `X1` an [observed confounder]{.blue} that is a common cause of both `D` & `Y` 
- `U` an [unobserved confounder]{.blue} a common cause of both `D` & `Y` 
- `K` a [collider]{.blue} that is a common consequence of both `D` & `Y`

:::

::::

## DAGs illustrate two sources of bias:

:::{.nonincremental}
- **Confounder bias:** Failing to control for a common cause of `D` **and** `Y` (aka Omitted Variable Bias)

- **Collider bias:** Controlling for a common consequence (aka Selection Bias^[Note in practice there's some slippage/debate/disagreement around this nomenclature])

:::

## {.smaller}
#### Confounding Bias: The Coffee Example

:::: panel-tabset

## Confounding Bias
:::{.nonincremental}
- Drinking coffee doesn't cause lung cancer we might find correlation between them because they share a [common cause:]{.blue} smoking.

- Smoking is a [confounding] variable, that if [omitted]{.blue} will [bias our results]{.blue} producing a [spurious]{.blue} relationsip 

- [Adjusting] for [confounders] removes this source of bias
:::

```{r}
#| label: confounding_day
#| echo: false
n <- 1000
coffee_df <- tibble(
  smoking = ifelse(rnorm(n)>.5,.75,0),
  Smoker = ifelse(smoking >0, "Smoker","Non-Smoker"),
  Coffee = smoking + rnorm(n),
  Cancer = 2*smoking+ rnorm(n),
)

coffee_df %>% 
ggplot(aes(Coffee,Cancer))+
  geom_point()+
  stat_smooth(method = "lm")+
  labs(title = "Positive relationship between\ncoffee and cancer")+
  theme_minimal()-> coffee_lm1_fig

coffee_df %>% 
ggplot(aes(Coffee,Cancer,col = Smoker))+
  geom_point()+
  stat_smooth(method = "lm") +
  labs(title = "No relationship between coffee\nand cancer adjusting for smoking")+
  theme_minimal()-> coffee_lm2_fig

coffee_dag1 <- dagify(
  y ~ x,
  labels = c(
    "y" = "Cancer",
    "x" = "Coffee"
  ),
  outcome = "y",
  exposure = "x"
)
coffee_dag1 %>% tidy_dagitty(layout = "linear") %>% 
  ggplot(aes(x,y, xend = xend, yend = yend))+
  geom_dag_point()+
  geom_dag_edges(edge_linetype = "dashed")+
  geom_dag_label(aes(label =label),nudge_y =.4) +
  ylim(1,-1)+
  theme_dag() +
  labs(title ="Spurious association between\ncoffee and cancer") -> coffee_dag1_fig



coffee_dag2 <- dagify(
  x ~z,
  y ~ z,
  labels = c(
    "y" = "Cancer",
    "x" = "Coffee",
    "z" = "Smoking"
     
  ),
  outcome = "y",
  exposure = "x",
  coords = list(
    x = c(x = -1, y = 1, z = 0),
    y = c(x = 0, y = 0, z = 1)

  )
) |> tidy_dagitty() |> 
  mutate(
    fill_col = ifelse(name == "z","grey","black")
  )
coffee_dag2 |> 
  ggplot(aes(x,y, xend = xend, yend = yend))+
  geom_dag_point(aes(color = fill_col))+
  geom_dag_edges()+
  geom_dag_label(aes(label =label),nudge_y =.2) +
  guides(fill="none",color = "none")+
  theme_dag()+
  labs(title ="Adjusting for smoking, no relationship\nbetween coffee and cancer")+
  scale_color_manual(values=c("black","grey"))-> coffee_dag2_fig

confounded_fig1 <- ggarrange(coffee_dag1_fig,coffee_lm1_fig)
confounded_fig2 <- ggarrange(coffee_dag2_fig, coffee_lm2_fig)

```

## Coffee and Cancer
```{r}
#| label: confounded_fig1
#| echo: false

confounded_fig1

```


## Adjusting for Smoking

```{r}
#| label: confounded_fig2
#| echo: false

confounded_fig2

```



::::

## {.smaller} 
#### Collider Bias: The Dating Example

:::: panel-tabset

## Collider bias

:::{.nonincremental}



- Why are attractive people such jerks?


- Suppose [dating]{.blue} is a function of [looks]{.blue} and [personality]{.blue}

- Dating is a [common consequences]{.blue} of [looks]{.blue} and [personality]{.blue}

- Basing our claim off of who we date is an example of [selection bias]{.blue} created by [controlling for collider]{.blue}

 


:::

```{r}
#| label: collidercode
#| echo: false
dating_dag <- collider_triangle(
  x = "Looks",
  y = "Personality",
  m = "Dateability"
)
dating_dag %>% 
  tidy_dagitty() ->
  dating_dag
dating_dag %>% 
  mutate(colour = ifelse(name == "m", "Collider","Non-Collider"))->dating_dag
ggdag(dating_dag, 
      text = F,
      use_labels = "label")+
  theme_void() +labs(
    title = "Dating is collider"
  ) -> collider_dag_fig1




ggdag_dseparated(dating_dag, 
                 text = F, 
                 controlling_for = "m",
                 use_labels = "label")+
  theme_void()+
  guides(color="none",shape="none")+
  labs(title="Selection bias creates\nspurious relationship") -> collider_dag_fig2

n <- 100
set.seed(123)
collider_df <- tibble(
  Looks = rnorm(n),
  Personality = rnorm(n)
) %>% 
  mutate(
    date = case_when(
      Looks > .5  ~ 1,
      Looks < .5 & Personality >.75 ~ 1,
      T ~ 0
    ),
    Swipe = ifelse(date == 1, "Right","Left")
  )
collider_df %>% 
  filter(date==1) %>% 
  ggplot(aes(Looks, Personality, col=Swipe))+
  geom_point(alpha=.5)+
  stat_smooth(
    method = "lm",
    col = "red"
  )+
  guides(color=guide_legend(title= "Swipe"))+
  theme_minimal()+
  labs(title = "It looks like you date jerks")+
  scale_color_manual(values = "red")-> collider_lm_fig1
 
collider_lm_fig1+
  geom_point(
    data = collider_df,
    alpha = .5
  )+
  stat_smooth(
    data = collider_df,
    col = "black",
    method = "lm"
  )+scale_color_manual(values = c("grey","red"))+
  labs(title = "No relationship between looks\nand personality overall")->collider_lm_fig2

collider_fig1 <- ggarrange(collider_dag_fig2,collider_lm_fig1)
collider_fig2 <- ggarrange(collider_dag_fig1, collider_lm_fig2)



```

## Selection bias 

```{r}
#| echo: false
collider_fig1
```


## No relationship in population

```{r}
#| echo: false
collider_fig2
```

::::


## When to control for a variable:

![](https://book.declaredesign.org/figures/figure-16-3.svg)

[@Blair2023-yg] [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)


# {{< fa lightbulb >}} Covariate Adjustment {.inverse}

##  Covariate Adjustment

Covariate adjustment refers a broad class of procedures that try to make a comparison more credible or meaningful by adjusting for some other potentially confounding factor.


##  Covariate Adjustment

When you hear people talk about

- Controlling for age
- Conditional on income
- Holding age and income constant
- Ceteris paribus (All else equal)

They are typically talking about some sort of covariate adjustment.


##  Three approaches to covariate adjustment{.smallero}

- [Subclassification]{.blue}
  - 👍: Easy to implement and interpret
  - 👎: Curse of dimensionality, Selection on observables

- [Matching ]{.blue}
  - 👍: Balance on multiple covariates, Mirrors logic of experimental design 
  - 👎: Selection on observables, Only provides balance on observed variables, Lot's of technical details...

- [Regression]{.blue}
  - 👍: Easy to implement, control for many factors (good and bad)
  - 👎: Selection on observables, easy to fit "bad" models


# {{< fa lightbulb >}} Simple Linear Regression {.inverse}

## Understanding Linear Regression{.smaller}


:::{.nonincremental}
- **Conceptual**
  - Simple linear regression estimates "a line of best fit" that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function

```{r}
#| eval: false
lm(y ~ x, data = df)
```

:::

## Understanding Linear Regression{.smaller}

:::{.nonincremental}

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR):

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$

:::


# {{< fa lightbulb >}} Conceptual: Linear Regression {.inverse}

## Conceptual: Linear Regression

- Regression is a tool for describing relationships.

  - How does some outcome we're interested in tend to change as some predictor of that outcome changes?

  - How does economic development vary with democracy?

  - How does economic development vary with democracy, adjusting for natural resources like oil and gas


## Conceptual: Linear Regression {.smaller}

:::{.nonincremental}

More formally:

$$
y_i = f(x_i) + \epsilon
$$

- Y is a function of X plus some error, $\epsilon$

- Linear regression assumes that relationship between an outcome and a predictor can be by a [linear](https://en.wikipedia.org/wiki/Linearity) function

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$
:::

## Linear Regression and the Line of Best Fit {.smaller}

:::{.nonincremental}


- The goal of linear regression is to choose coefficients $\beta_0$ and $\beta_1$ to summarizes the relationship between $y$ and $x$

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- To accomplish this we need some sort of criteria.

- For linear regression, that criteria is minimizing the error between what our model predicts $\hat{y_i} = \beta_0 + \beta_1 x_i$ and what we actually observed $(y_i)$

- More on this to come. But first...

:::

## Regression Notation{.smaller}

- $y_i$ an **outcome variable** or thing we're trying to explain

  - AKA: The dependent variable, The response Variable, The left hand side of the model

- $x_i$ a **predictor variables** or things we think explain variation in our outcome

  - AKA: The independent variable, covariates, the right hand side of the model.

  - Cap or No Cap: I'll use $X$ (should be $\mathbf{X}$) to denote a set (matrix) of predictor variables. $y$ vs $Y$ can also have technical distinctions (Sample vs Population, observed value vs Random Variable, ...)

- $\beta$ a set of **unknown parameters** that describe the relationship between our outcome $y_i$ and our predictors $x_i$

- $\epsilon$ the **error term** representing variation in $y_i$ not explained by our model.

## Linear Regression {.smaller}

:::{.nonincremental}

- We call this a bivariate regression, because there are only two variables

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- We call this a linear regression, because $y_i = \beta_0 + \beta_1 x_i$ is the equation for a line, where:

  - $\beta_0$ corresponds to the $y$ intercept, or the model's prediction when $x = 0$.

  - $\beta_1$ corresponds to the slope, or how $y$ is predicted to change as $x$ changes.
:::

## Linear Regression {.smaller}

:::{.nonincremental}

- If you find this notation confusing, try plugging in substantive concepts for what $y$ and $x$ represent
- Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.

The generic linear model

$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$

Reflects:

$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$
:::

# {{< fa lightbulb >}} Practical: Estimating a Linear Regression {.inverse}

## Practical: Estimating a Linear Regression{.smaller}

:::{.nonincremental}

- We estimate linear regressions in `R` using the `lm()` function.
- `lm()` requires two arguments:
  - a `formula` argument of the general form `y ~ x` read as "Y modeled by X" or below "Transgender Feeling Thermometer (`y`) modeled by (`~`) Age (`x`)
  - a `data` argument telling R where to find the variables in the formula

```{r}
#| label: lmdata
load(url("https://pols1600.paultesta.org/files/data/03_lab.rda"))
m1 <- lm(therm_trans_t0 ~ vf_age, data = df)
m1

```

:::

## The `lm()` function{.smaller}

:::{.nonincremental}

The coefficients from `lm()` are saved in object called `m1`

```{r}
#| label: printm1
m1
```

`m1` actually contains a lot of information

```{r}
#| label: m1info
names(m1)
m1$coefficients
```

:::

## Practical: Interpreting a Linear Regression{.smaller}

We can extract the intercept and slope from this simple bivariate model, using the `coef()` function

```{r}
#| label: m1coef

# All the coefficients
coef(m1)
# Just the intercept
coef(m1)[1]
# Just the slope
coef(m1)[2]
```

## Practical: Interpreting a Linear Regression{.smaller}

The two coefficients from `m1` define a line of best fit, summarizing how feelings toward transgender individuals change with age

$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$

$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$

$$\text{Transgender Feeling Thermometer}_i = `r round(coef(m1)[1],2)` + `r round(coef(m1)[2],2)` \text{Age}_i + \epsilon_i$$

## Practical: Predicted values from a Linear Regression{.smaller}

:::{.nonincremental}

- Often it's useful for interpretation to obtain predicted values from a regression.

- To obtain predicted vales $(\hat{y})$, we simply plug in a value for $x$ (In this case, $Age$) and evaluate our equation.

- For example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?

$$\hat{FT}_{x=18} = `r round(coef(m1)[1],2)` + `r round(coef(m1)[2],2)` \times 18  = 59.16$$ $$\hat{FT}_{x=65} = `r round(coef(m1)[1],2)` + `r round(coef(m1)[2],2)` \times 68  = 49.01$$
:::

## Practical: Predicted values from a Linear Regression

We could do this by hand

```{r}
#| label: pred_man
coef(m1)[1] + coef(m1)[2] * 18
coef(m1)[1] + coef(m1)[2] * 68

```

## Practical: Predicted values from a Linear Regression{.smaller}

More often we will:

- Make a [prediction data frame]{.blue} (called `pred_df` below) with the values of interests
- Use the `predict()` function with our linear model (`m1`) and `pred_df`
- Save the predicted values to our new column in our prediction data frame

## Practical: Predicted values from a Linear Regression

```{r}
#| label: pred_df

# Make prediction data frame
pred_df <- data.frame(
  vf_age = c(18, 68)
)
# Predict FT for 18 and 68 year-olds
predict(m1, newdata = pred_df)

# Save predictions to data frame
pred_df$ft_trans_hat <- predict(m1, newdata = pred_df)
pred_df
```

##
#### Practical: Visualizing Linear Regression

::: panel-tabset

## Concept
We can visualize simple regression by:

- plotting a scatter plot of the outcome (y-axis) and predictors (x-axis)

- overlaying the line defined by `lm()`


## Code
```{r}
#| label: fig_lm_code

fig_lm <- df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  geom_abline(intercept = coef(m1)[1],
              slope = coef(m1)[2],
              col = "blue"
              )+
  geom_vline(xintercept = 0,linetype = 2)+
  xlim(0,100)+
  annotate("point",
           x = 0, y = coef(m1)[1],
           col= "red",
           )+
  annotate("text",
           label = expression(paste(beta[0],"= 62.81" )),
           x = 1, y = coef(m1)[1]+5,
           hjust = "left",
           )+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People"
  )+
  theme_classic() -> fig_lm
```


## Intercept
```{r}
#| label: fig_lm_plot
#| echo: false
fig_lm
```

## Slope

```{r}
#| label: figlm1
#| echo: false
#devtools::install_github("nicolash2/ggbrace")
# library(ggbrace)

fig_lm +
  ylim(48, 52)+
  xlim(58, 62)+
  geom_segment(aes(
    y = coef(m1)[1] + coef(m1)[2]*61,
    yend = coef(m1)[1] + coef(m1)[2]*60,
    x = 60, xend = 60),
    arrow = arrow(ends = "first",length = unit(0.015, "npc")),
    col = "red"
    )+
    geom_segment(aes(
    y = coef(m1)[1] + coef(m1)[2]*61,
    yend = coef(m1)[1] + coef(m1)[2]*61,
    x = 60, xend = 61),
    arrow = arrow(ends = "last",length = unit(0.015, "npc")),
    col = "red"
    )+
   geom_curve(aes(
    y = 49.7,
    yend = 50.53,
    x = 59.43, xend = 59.8),
    curvature = -.2,
    arrow = arrow(ends = "last",length = unit(0.015, "npc")),
    col = "black",
    )+
    annotate("text",
           label = expression(paste(beta[1],"= -0.21" )),
           x = 59, y = 49.5,
           hjust = "left",
           )
```

## Errors
```{r}
#| label: figlm1code
#| echo: false

fig_lm+
  geom_segment(aes(x=47, xend = 47,
                   y=53.27526, yend=74),
               col="red")+
  geom_segment(aes(x=47, xend = 55,
                   y=74, yend=74),
               col="darkgrey",linetype = 2)+
  annotate("text",label = expression(paste(Y[i],"= 74" )),
           x = 55.5, y = 74,
           hjust = "left")+
  geom_segment(aes(x=47, xend = 55,
                   y=63.5, yend=63.5),
               col="red",linetype = 2)+
  annotate("text",label = expression(paste(epsilon[i],"=", Y[i] - hat(Y[i]),"= 74 - 53.27 = 20.73" )),
           x = 55.5, y = 63.5,
           col = "red",
           hjust = "left")+
  geom_segment(aes(x=47, xend = 55,
                   y=53.27526, yend=53.27526),
               col="blue",linetype = 2)+
  annotate("text",label = expression(paste(hat(Y[i]),"=", beta[0] + beta[1],"Age = 62.81 -0.20*47 = 53.27" )),
           x = 55.5, y = 53.27526,
           col = "blue",
           hjust = "left")



```

:::

# {{< fa lightbulb >}} Technichal: Mechanics of Linear Regression {background-color="lightgrey"}

## How did `lm()` choose $\beta_0$ and $\beta_1${.smaller}

:::{.nonincremental}

- P: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression



- Q: Ok, that's not really that helpful...

  - What's an error?
  - Why would we square and sum them
  - How do we minimize them.

P: Good questions!

:::

## What's an error? {.smaller}

An error, $\epsilon_i$ is simply the difference between the observed value of $y_i$ and what our model would predict, $\hat{y_i}$ given some value of $x_i$. So for a model:

$$y_i=\beta_0+\beta_1 x_{i} + \epsilon_i$$

We simply subtract our model's prediction $\beta_0+\beta_1 x_{i}$ from the the observed value, $y_i$

$$\hat{\epsilon_i}=y_i-\hat{y_i}=(Y_i-(\beta_0+\beta_1 x_{i}))$$

To get $\epsilon_i$

## Why are we squaring and summing $\epsilon${.smaller}

:::{.nonincremental}

There are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)

- Squaring $\epsilon$ treats positive and negative residuals equally.

- Summing produces single value summarizing our models overall performance.

There are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties
:::

## How do we minimize $\sum \epsilon^2$ {.smaller}

OLS chooses $\beta_0$ and $\beta_1$ to minimize $\sum \epsilon^2$, the Sum of Squared Residuals (SSR)

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

## How did `lm()` choose $\beta_0$ and $\beta_1$ {.smaller}

In an intro stats course, we would walk through the process of finding

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$ Which involves a little bit of calculus. The big payoff is that

$$\beta_0 = \bar{y} - \beta_1 \bar{x}$$ And

$$ \beta_1 = \frac{Cov(x,y)}{Var(x)}$$ Which is never quite the epiphany, I think we think it is...

The following slides walk you through the mechanics of this exercise. We're gonna skip through them in class, but they're there for your reference

## How do we minimize $\sum \epsilon^2${.smaller}

To understand what's going on under the hood, you need a broad understanding of some basic calculus.

The next few slides provide a brief review of derivatives and differential calculus.

## Derivatives{.smaller}

:::{.nonincremental}

The derivative of $f$ at $x$ is its rate of change at $x$

- For a line: the slope
- For a curve: the slope of a line tangent to the curve

You'll see two notations for derivatives:

1.  Leibniz notation:

$$
\frac{df}{dx}(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

2.  Lagrange: $f^{\prime}(x)$

:::


## Some useful Facts about Derivatives{.smaller}

Derivative of a constant

$$
f^{\prime}(c)=0
$$

Derivative of a line f(x)=2x

$$
f^{\prime}(2x)=2
$$

Derivative of $f(x)=x^2$

$$
f^{\prime}(x^2)=2x
$$

Chain rule: y= f(g(x)). The derivative of y with respect to x is

$$
\frac{d}{dx}(f(g(x)))=f^{\prime}(g(x))g^{\prime}(x)
$$

The derivative of the "outside" times the derivative of the "inside," remembering that the derivative of the outside function is evaluated at the value of the inside function.

## Finding a Local Minimums{.smaller}

Local minimum:

$$
f^{\prime}(x)=0 \text{ and } f^{\prime\prime}(x)>0 
$$

```{r}
#| label: derivatives
#| echo: false
knitr::include_graphics("https://copingwithcalculus.com/SecondDeriv1.png")
```

[Source](https://copingwithcalculus.com/SecondDerivativeTest.html)

## Partial Derivatives{.smaller}

Let $f$ be a function of the variables $(x, \dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is

$$\begin{align*}
\frac{\partial f(x, \dots, X_n)}{\partial X_i}=\lim_{h\to0}\frac{f(x, \dots X_i+h \dots, X_n)-f(x, \dots X_i \dots, X_n)}{h}
\end{align*}$$

```{r}
#| label: partial
#| echo: false
knitr::include_graphics("https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png")
```

[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)

## Minimizing the sum of squared errors{.smaller}

Our model

$$y_i =\beta_0+\beta_1x_{i}+\epsilon_i$$

Finds coefficients $\beta_0$ and $\beta_1$ to to minimize the sum of squared residuals, $\hat{\epsilon}_i$:

$$\begin{aligned}
\sum \hat{\epsilon_i}^2 &= \sum (y_i-\beta_0-\beta_1 x_{i})^2
\end{aligned}$$

## Minimizing the sum of squared errors{.smaller}

We solve for $\beta_0$ and $\beta_1$, by taking the partial derivatives with respect to $\beta_0$ and $\beta_1$, and setting them equal to zero

$$\begin{aligned}
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial \beta_0} &= -2\sum (y_i-\beta_0-\beta_1 x_{i})=0 & f'(-x^2) = -2x\\
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial\beta_1} &= -2\sum (y_i-\beta_0-\beta_1 x_{i})x_{i}=0 & \text{chain rule}
\end{aligned}$$

## Solving for $\beta_0${.smaller}

First, we'll solve for $\beta_0$, by multiplying both sides by -1/2 and distributing the $\sum$:

$$\begin{aligned}
0 &= -2\sum (y_i-\beta_0-\beta_1 x_{i})\\
\sum \beta_0 &= \sum y_i - \sum \beta_1 x_{i}\\
N \beta_0 &= \sum y_i -\sum \beta_1 x_{i}\\
\beta_0 &= \frac{\sum y_i}{N} - \frac{\beta_1 \sum x_{i}}{N}\\
\beta_0 &= \bar{y} - \beta_1 \bar{x}
\end{aligned}$$

## Solving for $\beta_1${.smaller}

Now, we can solve for $\beta_1$ plugging in $\beta_0$.

$$\begin{aligned}
0 &= -2\sum [(y_i-\beta_0-\beta_1 x_{i})x_{i}]\\
0 &= \sum [y_ix_i-(\bar{y} - \beta_1 \bar{x})x_{i}-\beta_1 x_{i}^2]\\
0 &= \sum [y_ix_i-\bar{y}x_{i} + \beta_1 \bar{x}x_{i}-\beta_1 x_{i}^2]
\end{aligned}$$

## Solving for $\beta_1${.smaller}

Now we'll rearrange some terms and pull out an $x_{i}$ to get

$$\begin{aligned}
0 &= \sum [(y_i -\bar{y} + \beta_1 \bar{x}-\beta_1 x_{i})x_{i}]
\end{aligned}$$

Dividing both sides by $x_{i}$ and distributing the summation, we can isolate $\beta_1$

$$\begin{aligned}
\beta_1 \sum (x_{i}-\bar{x}) &= \sum (y_i -\bar{y})
\end{aligned}$$

Dividing by $\sum (x_{i}-\bar{x})$ to get

$$\begin{aligned}
\beta_1  &= \frac{\sum (y_i -\bar{y})}{\sum (x_{i}-\bar{x})}
\end{aligned}$$

## Solving for $\beta_1${.smaller}

Finally, by multiplying by $\frac{(x_{i}-\bar{x})}{(x_{i}-\bar{x})}$ we get

$$\begin{aligned}
\beta_1  &= \frac{\sum (y_i -\bar{y})(x_{i}-\bar{x})}{\sum (\bar{x}-x_{i})^2}
\end{aligned}$$

Which has a nice interpretation:

$$\begin{aligned}
\beta_1 &= \frac{Cov(x,y)}{Var(x)}
\end{aligned}$$

So the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat!

# {{< fa lightbulb >}} Theoretical:  OLS provides a linear estimate of CEF: E\[Y|X\]{.inverse}

## Linear Regression is a many splendored thing{.smaller}

[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of the various interpretations/motivations for linear regression.

- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)

- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\beta$

- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)

- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)

- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)

- A [linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)

## Linear Regression is a many splendored thing{.smaller}

:::{.nonincremental}

[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of various interpretations/motivations for linear regression.

- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)

- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\beta$

- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)

- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)

- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)

- A [**linear approximation of the conditional expectation function**](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)

:::



## The Conditional Expectation Function{.smaller}

Of all the functions we could choose to describe the relationship between $Y$ and $X$,

$$
Y_i = f(X_i) + \epsilon_i
$$

the conditional expectation of $Y$ given $X$ $(E[Y|X])$, has some appealing properties

$$
Y_i = E[Y_i|X_i] + \epsilon
$$

The error, by definition, is uncorrelated with X and $E[\epsilon|X]=0$

$$
E[\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0
$$

Of all the possible functions $g(X)$, we can show that $E[Y_i|X_i]$ is the best predictor in terms of minimizing **mean squared error**

$$
E[ (Y - g(Y))^2] \geq E[(Y - E[Y|X])^2] 
$$

## {.smaller}
#### Linear Approximations to the Conditional Expectation Function

::: panel-tabset

## Concept

:::{.nonincremental}

- We can then show (in a different class) that linear regression provides the best linear predictor of the CEF
  - Chapter 3, of [Mostly Harmless Econometrics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991028523169706966)
  - Chapter 4 of [Foundations of Agnostic Statistics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991000736119706966)
- Furthermore, when the CEF is linear, it's equal exactly to OLS regression
:::

## CEF
```{r}
#| label: cef1code
#| echo: false


df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  stat_summary(geom="point", aes(col="CEF"))+
  stat_summary(geom="line", aes(col="CEF"))+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People",
    col = ""
  )+
  theme_classic() -> plot_cef

plot_cef


```

## OLS
```{r}
#| label: cef2code
#| echo: false
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -> plot_cef
plot_cef




```

:::

## What you need to know about Regression {.smaller}

:::{.nonincremental}

- **Conceptual**
  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function

```{r}
#| label: ols
#| eval: false
lm(y ~ x, data = df)
```

:::

## What you need to know about Regression {.smaller}

:::{.nonincremental}

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR):

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$

:::

# {{< fa lightbulb >}} Difference-in-Differences {.inverse}

## Motivating Example: What causes Cholera? {.smaller background-image=https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc, background-opacity=.3}

- In the 1800s, cholera was thought to be transmitted through the air.

- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.

- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. 


## Notation {.smaller}

Let's adopt a little notation to help us think about the logic of Snow's design:

- $D$: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)

- $T$: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).

- $Y_{di}(t)$ the potential outcome of unit $i$ 

  - $Y_{1i}(t)$ the potential outcome of unit $i$ when treated between the two periods 

  - $Y_{0i}(t)$ the potential outcome of unit $i$ when control between the two periods 


## Causal Effects {.smaller}

The individual causal effect for unit i at time t is:

$$\tau_{it} = Y_{1i}(t) − Y_{0i}(t)$$

What we observe is 

$$Y_i(t) = Y_{0i}(t)\cdot(1 − D_i(t)) + Y_{1i}(t)\cdot D_i(t)$$

$D$ only equals 1, when $T$ equals 1, so we never observe $Y_0i(1)$ for the treated units. 

In words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.


## Average Treatment on Treated {.smaller}

Our goal is to estimate the average effect of treatment on treated (ATT):


$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

That is, what would have happened in Lambeth, had their water company not moved their pipes


## Average Treatment on Treated {.smaller}

Our goal is to estimate the average effect of treatment on treated (ATT):

We we can observe is:

|               | Pre-Period (T=0)  | Post-Period (T=1)  |
|-|--|-|
| Treated $D_{i}=1$  |  $E[Y_{0i}(0)\vert D_i = 1]$ | $E[Y_{1i}(1)\vert D_i = 1]$  |
| Control $D_i=0$  |  $E[Y_{0i}(0)\vert D_i = 0]$ | $E[Y_{0i}(1)\vert D_i = 0]$  |


## Data {.smaller}

Because potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):

```{r}
#| label = "choleradat",
#| echo = F
snow <- tibble(Company = c("Lambeth (D=1)", "Southwark and Vauxhall (D=0)"),
               `1849 (T=0)` = c(85,135),
               `1854 (T=1)` = c(19,147),

               )

knitr::kable(snow)

```


## How can we estimate the effect of moving pumps upstream? {.smaller}

Recall, our goal is to estimate the effect of the the treatment on the treated:

$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

Let's conisder some strategies Snow could take to estimate this quantity:


## Before vs after comparisons:{.smaller}

:::{.nonincremental}
- Snow could have compared Labmeth in 1854 $(E[Y_i(1)|D_i = 1] = 19)$ to Lambeth in 1849 $(E[Y_i(0)|D_i = 1]=85)$, and claimed that moving the pumps upstream led to **66 fewer cholera deaths.** 

- Assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved $(E[Y_{0i}(1)|D_i = 1])$.

- A skeptic might argue that Lambeth in 1849 $\neq$ Lambeth in 1854


```{r}
#| echo: false
knitr::kable(snow) |> 
  kable_styling() |> 
  row_spec(1, bold=T,color = "blue")
```

:::


## Treatment-Control comparisons in the Post Period. {.smaller}

:::{.nonincremental}

- Snow could have compared outcomes between Lambeth and S&V in 1954  ($E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to **128 fewer deaths.**

- Here the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved $(E[Y_{0i}(1)|D_i = 1])$

- Again, our skeptic could argue  Lambeth $\neq$ S&V 

```{r}
#| echo: false
knitr::kable(snow) |> 
  kable_styling() |> 
  kableExtra::column_spec(3, bold=T, color="red")
```

:::

## Difference in Differences {.smaller}

:::{.nonincremental}
To address these concerns, Snow employed what we now call a [difference-in-differences]{.blue} design, 

There are two, equivalent ways to view this design. 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$

- Difference 1: Average change between Treated and Control  in Post Period

- Difference 2: Average change between Treated and Control  in Pre Period

:::

## Difference in Differences {.smaller}

:::{.nonincremental}

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$
Is equivalent to: 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\}}_{\text{Post - Pre |Treated }}− \overbrace{\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Post-Pre|Control}}$$


- Difference 1: Average change between Treated over time
- Difference 2: Average change between Control over time

:::

## Difference in Differences {.smaller}


You'll see the DiD design represented both ways, but they produce the same result:

$$
\tau_{ATT} = (19-147) - (85-135) = -78
$$

$$
\tau_{ATT} = (19-85) - (147-135) = -78
$$


## Identifying Assumption of a Difference in Differences Design {.smaller}

The key assumption in this design is what's known as the parallel trends assumption: $E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]$ 

- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&V

## Parralel Trends

```{r}
#| label: paralleltrends
#| echo: false

snow_g <- tibble(
  Period = c(0,0,3,3,0,3),
  Treatment = c(0,1,0, 1,1,1),
  Line = c(1,1,1,1,2,2),
  Company = c("S&V","Lambeth","S&V","Lambeth","Lambeth (D=0)","Lambeth (D=0)"),
  Deaths = c(135,85,147,19,85,97)
)

snow_g %>%
  ggplot(aes(Period,Deaths,col = Company))+
  geom_point()+
  geom_line()+
  geom_segment(aes(x=3.1,xend=3.1,y=19,yend=147), linetype = 2, col= "gray")+
  annotate(geom="text",x = 3.3,y=125, label = "1",hjust=.5)+
  geom_segment(aes(x=3.2,xend=3.2,y=19,yend=97), linetype = 2,col="gray")+
  annotate(geom="text",x = 3.3,y=55, label = "3",hjust=-.5)+
  geom_segment(aes(x=-.1,xend=-.1,y=85,yend=135), linetype = 2,col="gray")+
  annotate(geom="text",x = -.1,y=120, label = "2",hjust=1.5)+
  xlim(-2,6)+
  scale_x_continuous(breaks = c(0,3),labels = c("Pre","Post"))+
  theme_bw() -> snow_p

snow_p


```




## Summary {.smaller}

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
  - Taking the pre-post difference removes any fixed differences between the units
  
  - Then taking the difference between treated and control differences removes any common differences over time

- The key identifying assumption of a DiD design is the "assumption of parallel trends"
  - Absent treatment, treated and control groups
would see the same changes over time.
  - Hard to prove, possible to test



## Extensions and limitations {.smaller}

- Diff-in-Diff easy to estimate with linear regression
- Generalizes to multiple periods and treatment interventions
  - More pre-treatment periods allow you assess "parallel trends" assumption
- Alternative methods 
  - Synthetic control
  - Event Study Designs
- What if you have multiple treatments or treatments that come and go?
  - Panel Matching
  - Generalized Synthetic control


## Applications{.smaller}

- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment

- [Abadie, Diamond, & Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany

- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?


## References

