---
title: "POLS 1600"
subtitle: "Casual Inference in<br>Observational Designs & <br> Simple Linear Regression"
date: last-modified
date-format: "[Updated ]MMM D, YYYY"
format: 
  revealjs:
    theme: brownslides.scss
    logo: images/pols1600_hex.png
    footer: "POLS 1600"
    multiplex: false
    transition: fade
    slide-number: c
    incremental: true
    center: false
    menu: true
    scrollable: true
    highlight-style: github
    progress: true
    code-overflow: wrap
    # include-after-body: title-slide.html
    title-slide-attributes:
      align: left
      data-background-image: images/pols1600_hex.png
      data-background-position: 90% 50%
      data-background-size: 40%
filters:
  - openlinksinnewpage
execute: 
  eval: true
  echo: true
  warning: false
  message: false
  cache: true
---

```{r}
#| label: init
#| echo: false
#| results: hide
#| warning: false 
#| message: false

library(tidyverse)
library(labelled)
library(haven)
library(DeclareDesign)
library(easystats)
```

# {{< fa map-location>}} Overview {.inverse}

## Overview

- Announcements
- Setup
- Feedback
- Review
- Class plan

## Annoucements

- Sit with your groups (for now)

## Group Assignments {.smaller}

```{r}
#| label: groups
#| echo: false
groups_df <- tibble::tibble(
`Group 1` = c("Maia Eng","Guadalupe Herrera","Stephen Robinson","Jeremiah Harrington"),
`Group 2` = c("Andrew Rovinsky", "Spencer Lorin","Lucinda Anderson","Serenity Hamilton"),
`Group 3` = c("Serafym Rybachkivskyi","Rachel Kim","Kai Blades","Emma Coleman"),
`Group 4` = c("Tiffany Eddy","Daniel Solomon","Zoe Smith","Lorena Calderon"),
`Group 5` = c("Christopher Maron","Daniel Baker","Neve Diaz-Carr","Olivia Hanley"),
`Group 6` = c("Mia Hamilton","Emily Colon","Davis Kelly","Talia Levine"),
`Group 7` = c("Mariana Melzer","Kahrie Langham", "Shannon Feerick-Hillenbrand","Jarret Fernandes"),
`Group 8` = c("Logan Szittai", "Keiley Thompson","Lydell Dyer","Mahir Arora")
)

groups_df |> 
  pivot_longer(cols = starts_with("Group"),
               names_to = "Group",
               values_to = "Name") |> 
  arrange(Group) |>
  group_by(Group) |> 
  mutate(
    id = 1:n()
  ) |> 
  pivot_wider(id_cols = Group,
              names_from = id,
              values_from = Name) -> groups_df
# write_csv(groups_df, file = "../files/groups.csv" )
DT::datatable(groups_df)
```



# {{< fa code >}} Setup

## New packages

This week's lab we'll be using the `dataverse` package to download data on presidential elections

Next week's lab, we'll be using the `tidycensus` package to download census data. 

We'll also need to [install a census API]{.blue} to get the data.

Here's a [detailed guide](https://pols1600.paultesta.org/slides/04-packages) of what we'll do in class right now.

## Install new packages

These packages are easier to install live:

```{r}
#| label: installpacks
#| eval: false
install.packages("dataverse")
install.packages("tidycensus")
install.packages("easystats")
install.packages("DeclareDesign")

```

## Census API {.smaller}

Please follow these steps so you can download data directly from the U.S. Census [here](https://pols1600.paultesta.org/slides/04-packages.html#3_Install_a_Census_API_tidycensus_package):

1.  Install the `tidycensus` package
2.  Load the installed package
3.  Request an API key from the Census
4.  Check your email
5.  Activate your key
6.  Install your API key in R
7.  Check that everything worked


## Packages for today

```{r}
#| label: packages
#| echo: true

## Pacakges for today
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)

## Define a function to load (and if needed install) packages

#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

## Install (if needed) and load libraries in the_packages
ipak(the_packages)
```


# {{< fa bullhorn >}} Feedback {top="50%" background-image="https://jplilley.com/images/easyblog_articles/145/holding-ears-300x196.jpg"}

```{r}
#| label: feeback
#| echo: false
df <- haven::read_spss("../files/data/class_surveys/wk03.sav")

df %>%
  mutate(
    Reincarnation = reincarnation,
    Why = reincarnation_why
  ) -> df

```

## What did we like {.smaller}

```{r}
#| label: likes
#| echo: false
DT::datatable(df %>% 
                select(Likes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```

## What did we dislike {.smaller}

```{r}
#| label: dislikes
#| echo: false

DT::datatable(df %>% 
                select(Dislikes),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```

## Grinding an Iron Pestle into a Needle {.smaller background-image="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5OqZwz1hmoCY5v_31xQ9A@2x.jpeg" background-opacity=.5}


::::{.columns}

:::{.column .fragment width="45%"}
### Me

- Less is more
- Go slow
- Provide labs/code earlier
- Adapt assignments/policies 

:::

:::{.column .fragment width="45%"}
### You

- Active reading
- Do tutorials
- Review labs before class
- Review comments after class
- Ask for help
- Don't give up!



:::

::::

# {{< fa magnifying-glass >}} Review {.inverse}

## Review

- Data wrangling 

- Descriptive Statistics

- Levels of understanding

- Data visualization

# {{< fa magnifying-glass >}} Data Wrangling {.inverse}

## Data wrangling {.smaller}

```{r}
#| label: spdf
#| echo: false
sp_df <- tibble(
  Skill = c("Setup R",
            "Load data",
            "Get HLO of data",
            "Transform data",
            "Reshape data",
            "Summarize data numerically",
            "Summarize data graphically"),
  `Common Commands` = c("library(), ipak()",
               "read_csv(), load()",
               "df$x, glimpse(), table(), summary()",
               "<-, mutate(), ifelse(), case_when()",
               "pivot_longer(), left_join()",
               "mean(), median(), summarise(), group_by()",
               "ggplot(), aes(), geom_")
)

kable(sp_df, 
      caption = "You're learning how to map conceptual tasks to commands in R",
      caption.above=T)

```

## Mapping Concepts to Code

- Takes time and practice

- Don't be afraid to [FAAFO]{.blue}

- Don't worry about memorizing everything.

- Statistical programming is necessary to actually [do]{.blue} empirical research

- [Learning to code]{.blue} will help us [understand statistical concepts]{.blue}.

- Learning to [think programmatically]{.blue} and algorithmically will help us [tackle complex problems]{.blue}

# {{< fa magnifying-glass >}} Descriptive Statiscs{.inverse}

## Descriptive statistics {.smaller}

- Descriptive statistics help us describe what's typical of our data

- [What's a typical value in our data]{.blue}

  - [Mean](https://pols1600.paultesta.org/labs/01-lab-comments.html#mean)
  - [Median](https://pols1600.paultesta.org/labs/01-lab-comments.html#median)
  - [Mode](https://pols1600.paultesta.org/labs/01-lab-comments.html#modes)

- [How much do our data vary?]{.blue}

  - [Variance](https://pols1600.paultesta.org/labs/01-lab-comments.html#variance)
  - [Standard deviation](https://pols1600.paultesta.org/labs/01-lab-comments.html#standard-deviations)

- As one variable changes [how does another change]{.blue}?

  - [Covariance](https://pols1600.paultesta.org/labs/01-lab-comments.html#covariance)
  - [Correlation](https://pols1600.paultesta.org/labs/01-lab-comments.html#correlation)

- Descriptive statistics are:

  - Diagnostic
  - Generative

# {{< fa magnifying-glass>}} Levels of understanding{.inverse}

## Levels of understanding in POLS 1600

- Conceptual

- Practical

- Definitional

- Theoretical

. . .

Let's illustrate these different levels of understanding about our old friend the [mean]{.blue}

## Mean: Conceptual Understanding{.smaller}

A mean is:

- A common and important [measure of central tendency]{.blue} (what's typical)

- It's the [arithmetic average]{.blue} you learned in school

- We can think of it as the [balancing point]{.blue} of a distribution

- A [conditional mean]{.blue} is the average of one variable $X$, when some other variable, $Z$ takes a value $z$

  - Think about the average height in our class ([unconditional mean]{.blue}) vs the average height among men and women (\[conditional means\].{blue})

## Mean as a balancing point{.smaller}

![](https://mathbitsnotebook.com/Algebra1/StatisticsData/balancepoint1.jpg)

[Source](https://mathbitsnotebook.com/Algebra1/StatisticsData/STCenter.html)

## Mean: Practical{.smaller}

There are lots of ways to calculate means in `R`

- The simplest is to use the `mean()` function

  - If our data have missing values, we need to to tell `R` to remove them

```{r}
#| label: meanex
#| eval: false
mean(df$x, na.rm=T)
```

## Conditional Means: Practical{.smaller}

- To calculate a conditional mean we could us a logical index `[df$z == 1]`

```{r}
#| label: conmean
#| eval: false
mean(df$x[df$z == 1], na.rm=T)
```

- If we wanted to a calculate a lot of conditional means we could use the `mean()` in combination with `group_by()` and `summarise()`

```{r}
#| label: grpmean
#| eval: false
df %>% 
  group_by(z)%>%
  summarise(
    x = mean(x, na.rm=T)
  )
```

## Mean: Definitional{.smaller}

Formally, we define the arithmetic mean of $x$ as $\bar{x}$:

$$
\bar{x} = \frac{1}{n}\left (\sum_{i=1}^n{x_i}\right ) = \frac{x_1+x_2+\cdots +x_n}{n}
$$

In words, this formula says, to calculate the average of x, we sum up all the values of $x_i$ from observation $i=1$ to $i=n$ and then divide by the total number of observations $n$

## Mean: Definitional {.smaller}

- In this class, I don't put a lot of weight on memorizing definitions (that's what Google's for).

- But being comfortable with "the math" is important and useful

- Definitional knowledge is a prerequisite for understanding more theoretical claims.

## Mean: Theoretical{.smaller}

Suppose I asked you to show that the sum of deviations from a mean equals 0?

$$
\text{Claim:} \sum_{i=1}^n (x_i -\bar{x}) = 0
$$

## Mean: Theoretical {.smaller}

Knowing the definition of an arithmetic mean, we could write:

$$
\begin{aligned}
\sum_{i=1}^n (x_i -\bar{x}) &= \sum_{i=1}^n x_i - \sum_{i=1}^n\bar{x} & \text{Distribute Summation}\\
              &= \sum_{i=1}^n x_i - n\bar{x} & \text{Summing a constant, } \bar{x}\\
              &= \sum_{i=1}^n x_i - n\times \left ( \frac{1}{n} \sum_{i=1}^n{x_i}\right ) & \text{Definition of } \bar{x}\\
              &= \sum_{i=1}^n x_i - \sum_{i=1}^n{x_i} & n \times \frac{1}{n}=1\\
              &= 0             
\end{aligned}
$$

## Mean: Theoretical {.smaller}

Why do we care?

- Showing the deviations sum to 0 is another way of saying the mean is a [balancing point]{.blue}.

- This turns out to be a useful property of means that will reappear throughout the course

- If I asked you to make a prediction, $\hat{x}$ of a random person's height in this class, the mean would have the lowest [mean squared error]{.blue} (MSE $=\frac{1}{n}\sum (x_i - \hat{x_i})^2)$

## Mean: Theoretical

Occasionally, you'll read or here me say say things like:

> The sample mean is an unbiased estimator of the population mean

In a statistics class, we would take time to prove this.

## The sample mean is an unbiased estimator of the population mean{.smaller}

Claim:

Let $x_1, x_2, \dots x_n$ from a random sample from a population with mean $\mu$ and variance $\sigma^2$

Then:

$$
\bar{x} = \frac{1}{n}\left (\sum_{i=1}^n x_i\right )
$$

is an unbiased estimator of $\mu$

$$
E[\bar{x}] = \mu
$$

## The sample mean is an unbiased estimator of the population mean {.smaller}

Proof:

$$
\begin{aligned}
E\left [\bar{x} \right] &= E\left [\frac{1}{n}\left (\sum_{i=1}^n x_i \right) \right] & \text{Definition of } \bar{x} \\
&= \frac{1}{n} \sum_{i=1}^nE\left [ x_i \right]  & \text{Linearity of Expectations} \\
&= \frac{1}{n} \sum_{i=1}^n \mu  & E[x_i] = \mu \\
&= \frac{n}{n}  \mu  & \sum_{i=1}^n \mu = n\mu \\
&= \mu  & \blacksquare \\
\end{aligned}
$$

## Levels of understanding {.smaller}

::: nonincremental
In this course, we tend to emphasize the

- **Conceptual**

- **Practical**

Over

- Definitional

- Theoretical

In an intro statistics class, the ordering might be reversed.

Trade offs:
:::

- Pro: We actually get to *work with data* and *do empirical research* much sooner
- Cons: We substitute intuitive understandings for more rigorous proofs

# {{< fa magnifying-glass>}} Data Visualization {.inverse}

## Data Visualization {.smaller}

-   The grammar of graphics

-   At minimum you need:

    -   `data`
    -   `aesthetic` mappings
    -   `geometries`

-  Take a sad plot and make it better by:

    -   `labels`
    -   `themes`
    -   `statistics`
    -   `cooridnates`
    -   `facets`
    -   transforming your data before plotting

## You are about to be reincarnated: HLO

```{r hlo}
#| label: hlo
#| echo: true

df$reincarnation

table(df$reincarnation)
```




## Basic Plot

::: panel-tabset

## Code
```{r}
#| label: plot0code
#| eval: false

df %>%
  ggplot(aes(x = reincarnation, 
             fill = reincarnation))+
  geom_bar(
    stat = "count"
  ) 
```


## Figure
```{r plot0, echo=F}
df %>%
  ggplot(aes(x = reincarnation, 
             fill = reincarnation))+
  geom_bar(
    stat = "count"
  )
```
:::


##  Use a factor to label and order responses

::: panel-tabset

## Recode
```{r factors}

df %>%
  mutate(
    # Turn numeric values into factor labels 
    Reincarnation = forcats::as_factor(reincarnation),
    # Order factor in decreasing frequency of levels
    Reincarnation = forcats::fct_infreq(Reincarnation),
    # Reverse order so levels are increasing in frequency
    Reincarnation = forcats::fct_rev(Reincarnation),
    # Rename explanations
    Why = reincarnation_why
  ) -> df

```

## Check recoding

```{r recode}
table(recode= df$Reincarnation, original = df$reincarnation)
```
:::


## Revised figure

::: panel-tabset

## Code
```{r plot1code, }
#| label: plot1code

df %>% # Data
  # Aesthetics
  ggplot(aes(x = Reincarnation, 
             fill = Reincarnation))+
  # Geometry
  geom_bar(stat = "count")+ # Statistic
  ## Include levels of Reincarnation w/ no values
  scale_x_discrete(drop=FALSE)+
  # Don't include a legend
  scale_fill_discrete(drop=FALSE, guide="none")+
  # Flip x and y
  coord_flip()+
  # Remove lines
  theme_classic() -> fig1
```

## Revised Figure
```{r}
#| echo: false
# Display figure
fig1
```

:::


## {.smaller}
#### What creature and why?


```{r }
#| label: creatures
#| echo: false


DT::datatable(df %>%  select(Reincarnation,Why),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```





## Adding labelled values

::: panel-tabset


## Recodes
```{r labcode}
df %>%
  mutate(
    # Create numeric id
    id = 1:n(),
    # Create a label with 3 answers and NA elsewhere
    Label = case_when(
      id == 10 ~ str_wrap(reincarnation_why[10],30),
      id == 4 ~ str_wrap(reincarnation_why[4],30),
      id == 7 ~ str_wrap(reincarnation_why[7],30),
      TRUE ~ NA_character_

    )

  ) -> df
```

## Recode Output

```{r whytab, echo=F}
DT::datatable(df %>% 
                select(Label,Why),
               fillContainer = F,
              height = "90%",
              options = list(
                pageLength = 5
              )
              )
```


## Aggregate Data
```{r}
#| label: aggregatedata

# Calculate totals before calling ggplot
plot_df <- df %>%
  group_by(Reincarnation)%>% 
  summarise( 
    Count = n(), 
    Why = na.omit(unique(Label)) 
  ) 
```

:::


## You're about to be reincarnated:{.smaller}

::: panel-tabset

## Aggregate df
```{r }
#| label: labtab
#| echo: false
DT::datatable(plot_df)
```

## Revised Figure Code

```{r }
#| label: plot2code
#| echo: true

plot_df %>%
  ggplot(aes(x = Reincarnation, 
             y = Count,
             fill = Reincarnation, 
             label=Why))+
  geom_bar(stat = "identity")+ #<<
  ## Include levels of Reincarnation w/ no values
  scale_x_discrete(drop=FALSE)+
  # Don't include a legend
  scale_fill_discrete(drop=FALSE, guide="none")+
  coord_flip()+
  labs(x = "",y="",title="You're about to be reincarnated.\nWhat do you want to come back as?")+
  theme_classic()+
  ggrepel::geom_label_repel(
    fill="white",
    nudge_y = 1, 
    hjust = "left",
    size=3,
    arrow = arrow(length = unit(0.015, "npc"))
    )+ 
  scale_y_continuous(
    breaks = c(0,2,4,6,8,10,12),
    expand = expansion(add =c(0,6))
    ) -> fig1
```

## Labelled Figure
```{r }
#| label: fig1labelled
#| echo: false

fig1
```
:::

## Data visualization is an iterative process

- Data visualization is an iterative process

- Good data viz requires lots of data transformations

- Start with a minimum working example and build from there

- Don't let the perfect be the enemy of the good enough.


# {{< fa code >}}Previewing the Lab

## Red Covid{.smaller}


::::{.columns}

:::{.column width="45%"}

![](images/05_redcovid_orig.png)


[Red Covid](https://www.nytimes.com/2021/09/27/briefing/covid-red-states-vaccinations.html) *New York Times*, 27 September, 2021
:::

:::{.column width="45%"}
![](images/05_covid.png)
[Red Covid, an Update](https://www.nytimes.com/2022/02/18/briefing/red-covid-partisan-deaths-vaccines.html) *New York Times*, 18 February, 2022
:::
:::

## Preview of the Lab{.smaller}

[Please download]{.blue} Thursday's lab [here](https://pols1600.paultesta.org/labs/05-lab.qmd)

- Conceptually, this lab is designed to help reinforce the relationship between linear models like $y=\beta_0 + \beta_1x$ and the conditional expectation function $E[Y|X]$.

- Substantively, we will explore whether David Leonhardt's claims about [Red Covid](https://www.nytimes.com/2021/09/27/briefing/covid-red-states-vaccinations.html) the political polarization of vaccines and its consequences

## Lab: Questions 1-5: Review{.smaller}

Questions 1-5 are designed to [reinforce]{.blue} your [data wrangling]{.blue} skills.  In particular, you will get practice:

  - Creating and recoding variables using `mutate()`
  - Calculating a [moving average](https://en.wikipedia.org/wiki/Moving_average) or rolling mean using the `rollmean()` function from the `zoo` package
  - Transforming the data on presidential elections so that it can be merged with the data on Covid-19 using the `pivot_wider()` function.
  - [Merging data](https://r4ds.had.co.nz/relational-data.html) together using the `left_join()` function.



## Lab: Questions 6-10: Simple Linear Regression{.smaller}

- In question 6, you will see how calculating conditional means provides a simple test of "Red Covid" claim.

- In question 7, you will see how a linear model returns the same information as these conditional means (in a sligthly different format)

- In question 8, you will get practice interpreting linear models with continuous predictors (i.e. predictors that take on a range of values)

- In question 9, you will get practice visualizing these models and using the figures help interpret your results substantively.

- Question 10 asks you to play the role of a skeptic and consider what other factors might explain the relationships we found in Questions 6-9. We will explore these factors in next week's lab.

## Before Thursday

The following slides provide detailed explanations of all the code you'll need for each question. 

- [Please run this code before class on Thursday]{.blue}

- We will review this material together at the start of class, but you will spend most of our time on the Questions 6-10

## Q1: Setup your workspace

::: panel-tabset

## Task

Q1 asks you to setup your workspace

This means [loading]{.blue} and, if needed, [installing]{.blue} the packages you will use.



## Code for Q1

```{r}
#| label: extendedsetup
#| eval: false

## Pacakges for today
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)

## Define a function to load (and if needed install) packages

#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

## Install (if needed) and load libraries in the_packages
ipak(the_packages)

```


:::


## Q2 Load the data

To explore Leonhardt's claims about *Red Covid*, we'll need data on:

- Covid-19
- The 2020 Presidential Election


## Q2.1 Load the Covid-19 Data{.smaller}

To load data on Covid-19 just run this

```{r}
#| label: covid

load(url("https://pols1600.paultesta.org/files/data/covid.rda"))
```

## Q2.2 Load Election Data {.smaller}

::: panel-tabset

## Task

Q2.2. asks you to write code that will download data presidential elections from 1976 to 2020 from the MIT Election Lab's dataverse

- Once you've installed the `dataverse` package you should be able to do this:

## Code for Q2.2
```{r}
#| label: pres_data
# Try this code first
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

pres_df <- dataverse::get_dataframe_by_name(
  "1976-2020-president.tab",
  "doi:10.7910/DVN/42MVDX"
)

# If the code above fails, comment out and uncomment the code below:

# load(url("https://pols1600.paultesta.org/files/data/pres_df.rda"))
```
:::

## Q3 Describe the structure of each dataset{.smaller}

Question 3 asks you to [describe the structure]{.blue} of each dataset.

- Specifically, it asks you to get a high level overview of `covid` and `pres_df` and describe the [unit of analysis]{.blue} in each dataset:
  - Describe substantively what specific, observation each [row]{.blue} in the dataset corresponds to
  - In covid `covid` dataset, the [unit of analysis]{.blue} is a [state-date]{.blue}

## Q3 Describe the structure of each dataset{.smaller}

Here's some possible code you could use to get a quick [HLO]{.blue} of each dataset:

::: panel-tabset


## HLO `covid`
```{r}
#| label: hlocovid
#| eval: false

# check names in `covid`
names(covid)

# take a quick look values of each variable

glimpse(covid)

# Look at first few observations for:
# date, administrative_area_level_2, 

covid %>% 
  select(date, administrative_area_level_2) %>%
  head()

# Summarize data to get a better sense of the unit of observastion

covid %>% 
  group_by(administrative_area_level_2) %>%
  summarise(
    n = n(), # Number of observations for each state
    start_date = min(date, na.rm = T),
    end_date = max(date, na.rm=T)
  ) -> hlo_covid_df

hlo_covid_df


# How many unique values of date and state are their:

n_dates <- length(unique(covid$date))
n_states <- length(unique(covid$administrative_area_level_2))
n_dates
n_states

# If we had observations for every state on every date then the number of rows 
# in the data 
dim(covid)[1]
# Should equal
dim(covid)[1] == n_dates * n_states

# This is what economists would call an unbalanced panel


```

## HLO `pres_df`

```{r}
#| label: hlopresdf
#| eval: false

# check names in `pres_df`
names(pres_df)

# take a quick look values of each variable

glimpse(pres_df)

# Unit of analysis is a year-state-candidate
pres_df %>% 
  select(year, state_po, candidate) %>%
  head()

# How many states?
length(unique(pres_df$state_po))



# How many candidates and parties on the ballot in a given election year
pres_df %>% 
  group_by(year) %>%
  summarise(
    n_candidates = length(unique(candidate)),
    # Look at both party_detailed and party_simplified
    n_parties_detailed = length(unique(party_detailed)),
    n_parties_simplified = length(unique(party_simplified))
  ) -> hlo_pres_df
hlo_pres_df

# Look at 2020
# pres_df$candidate[pres_df$year == "2020"]

 

```
:::
 
 
## Q4 Recode the data for analysis

Using our understanding of the structure of the data, Q4 asks you to:


- Recode the Covid-19 data like we've done before [plus]{.blue}
- Calculate [rolling means]{.blue},  7 and 14 day averages
- Reshape, recode, and filter the presidential election data



## Q4.1 Recode the Covid-19{.smaller}

::: panel-tabset

## Task
This is the same code we've used before to create `covid_us` from `covid` with the addition of code to calculate a [rolling mean]{.blue} or [moving average]{.blue} of the number of new cases

## Code for Q4.1
```{r}
#| label: covid19recode
# Create a vector containing of US territories
territories <- c(
  "American Samoa",
  "Guam",
  "Northern Mariana Islands",
  "Puerto Rico",
  "Virgin Islands"
  )

# Filter out Territories and create state variable
covid_us <- covid %>%
  filter(!administrative_area_level_2 %in% territories)%>%
  mutate(
    state = administrative_area_level_2
  )

# Calculate new cases, new cases per capita, and 7-day average

covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000,
    new_cases_pc_7da = zoo::rollmean(new_cases_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us

# Recode facemask policy

covid_us %>%
mutate(
  # Recode facial_coverings to create face_masks
    face_masks = case_when(
      facial_coverings == 0 ~ "No policy",
      abs(facial_coverings) == 1 ~ "Recommended",
      abs(facial_coverings) == 2 ~ "Some requirements",
      abs(facial_coverings) == 3 ~ "Required shared places",
      abs(facial_coverings) == 4 ~ "Required all times",
    ),
    # Turn face_masks into a factor with ordered policy levels
    face_masks = factor(face_masks,
      levels = c("No policy","Recommended",
                 "Some requirements",
                 "Required shared places",
                 "Required all times")
    ) 
    ) -> covid_us

# Create year-month and percent vaccinated variables

covid_us %>%
  mutate(
    year = year(date),
    month = month(date),
    year_month = paste(year, 
                       str_pad(month, width = 2, pad=0), 
                       sep = "-"),
    percent_vaccinated = people_fully_vaccinated/population*100  
    ) -> covid_us
```

## Template Code for Q4.2

```{r}
# Calculate new cases, new cases per capita, and 7-day average

covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_cases = confirmed - lag(confirmed),
    new_cases_pc = new_cases / population *100000,
    new_cases_pc_7day = zoo::rollmean(new_cases_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us
```

:::


## Q4.2 Calculate Rolling Means of Covid Deaths{.smaller}

::: panel-tabset

## Task

Q4.2 asks you to create new measures of the 7-day and 14-day averages of new deaths from Covid-19 per 100,000 residents

It encourages you to use the code `new_cases_pc_7da` as a template

To build your coding skills, try writing this yourself, then comparing it to the code in the next tab:

## Code for Q4.2
```{r}
covid_us %>%
  dplyr::group_by(state) %>%
  mutate(
    new_deaths = deaths - lag(deaths),
    new_deaths_pc = new_deaths / population *100000,
    new_deaths_pc_7day = zoo::rollmean(new_deaths_pc, 
                                     k = 7, 
                                     align = "right",
                                     fill=NA ),
    new_deaths_pc_14day = zoo::rollmean(new_deaths_pc, 
                                     k = 14, 
                                     align = "right",
                                     fill=NA )
    ) -> covid_us
```

:::



## Rolling Averages{.smaller} 

The next slides [aren't necessary for the lab]{.blue} but are designed to illustrate:

- the [concept of a rolling mean]{.blue}
- [what]{.blue} the code does
- [why]{.blue} might prefer rolling averages over daily values


## Look at the output of `zoo::rollmean()`
```{r}
#| label: rollmean
covid_us %>%
  filter(date > "2020-03-05") %>%
  select(date,new_cases_pc,new_cases_pc_7day)
```

## Comparing Daily Cases to Rolling Average{.smaller}

The following code illustrates how a 7-day rolling mean smooths (`new_cases_pc_7da`) over the [*noisiness*]{.blue} of the daily measure

::: panel-tabset

## Code
```{r }
#| label: plotmean
covid_us %>%
  filter(date > "2020-03-05", 
         state == "Minnesota") %>%
  select(date,
         new_cases_pc,
         new_cases_pc_7day)%>%
  ggplot(aes(date,new_cases_pc ))+
  geom_line(aes(col="Daily"))+
  # set y aesthetic for second line of rolling average
  geom_line(aes(y = new_cases_pc_7day,
                col = "7-day average")
            ) +
  theme(legend.position="bottom")+
    labs( col = "Measure",
    y = "New Cases Per 100k", x = "",
    title = "Minnesota"
  ) -> fig_covid_mn 
```

## Figure
```{r}
#| label: plotmean2
#| echo: false
fig_covid_mn

```
:::

## Q4.3 Recode Presidential data{.smaller}

::: panel-tabset

## Task

Q4.3 Gives you a long list of steps to recode, reshape, and filter `pres_df` to produce `pres_df2020`

Most of this is review but it can seem like a lot.

Walk through the provided code and see if you can [map each conceptual step]{.blue} in Q4.3 to its [implementation in the code]{.blue}

## Code for Q4.3

```{r}
#| label: pres_wrangle
pres_df %>%
  mutate(
    year_election = year,
    state = str_to_title(state),
    # Fix DC
    state = ifelse(state == "District Of Columbia", "District of Columbia", state)
  ) %>%
  filter(party_simplified %in% c("DEMOCRAT","REPUBLICAN"))%>%
  filter(year == 2020) %>%
  select(state, state_po, year_election, party_simplified, candidatevotes, totalvotes
         ) %>%
  pivot_wider(names_from = party_simplified,
              values_from = candidatevotes) %>%
  mutate(
    dem_voteshare = DEMOCRAT/totalvotes *100,
    rep_voteshare = REPUBLICAN/totalvotes*100,
    winner = forcats::fct_rev(factor(ifelse(rep_voteshare > dem_voteshare,"Trump","Biden")))
  ) -> pres2020_df

# Check Output:

glimpse(pres2020_df)
```

:::

## Q5 merging data{.smaller}

::: panel-tabset

## Task

Q5 asks you to merge the 2020 election data from `pres2020_df` into `covid_us` using the common `state` variable in each dataset using the function `left_join()`


## Merge election data into Covid data

```{r}
#| label: merge_pres
dim(covid_us)
dim(pres2020_df)
covid_us <- covid_us %>% left_join(
  pres2020_df,
  by = c("state" = "state")
)
dim(covid_us) 
```
:::

## Advice for merging {.smaller}

::: panel-tabset

## Advice

When merging datasets:

- Check the matches in your joining variables
  - Make sure the values `state` are the same in each dataset 
  - Check for differences in spelling, punctuation, etc.
- Check the [dimensions]{.blue} of output of your `left_join()`
  - If there is a 1-1 match the number of rows should be the same before after

## Illustration

```{r}
#| label: merge_check
# Should be 51 states and DC in each
sum(unique(pres_df$state) %in% covid_us$state)

# Look at each state variable
## With [] index
pres_df$state[1:5]
covid_us$state[1:5]

# Matching is case sensitive 

# make pres_df$state title case

## Base R:
pres_df$state <- str_to_title(pres_df$state )
## Tidy R:
pres_df %>% 
  mutate(
    state = str_to_title(state )
  ) -> pres_df

# Should be 51
sum(unique(pres_df$state) %in% covid_us$state)

# Find the mismatch:
unique(pres_df$state[!pres_df$state %in% covid_us$state])

# Two equivalent ways to fix this mismatch
## Base R: Quick fix to change spelling of DC
pres_df$state[pres2020_df$state == "District Of Columbia"] <- "District of Columbia"

## Tidy R: Quick fix to change spelling of DC

pres_df %>% 
  mutate(
    state = ifelse(test = state == "District Of Columbia",
                   yes = "District of Columbia",
                   no = state
                   )
  ) -> pres_df


# Problem Solved
sum(unique(pres2020_df$state) %in% covid_us$state)

```


:::

# {{< fa lightbulb >}} Causal Inference {.inverse}

## Causal inference is about counterfactual comparisons

- Causal inference is about counterfactual comparisons

  - What would have happened if some aspect of the world either had or had not been present

## Causal Identification{.smaller}

- [Casual Identification]{.blue} refers to "the assumptions needed for statistical estimates to be given a causal interpretation" [Keele (2015)](http://lukekeele.com/wp-content/uploads/2016/03/causal.pdf)\]

  - What do we need to assume to make our claims about cause and effect credible

- [Experimental Designs]{.blue} rely on [randomization]{.blue} of treatment to justify their causal claims

- [Observational Designs]{.blue} require [additional assumptions]{.blue} and [knowledge]{.blue} to make causal claims 

## Experimental Designs {.smaller}

- [Experimental designs]{.blue} are studies in which a causal variable of interest, the *treatement*, is [manipulated by the researcher]{.blue} to examine its causal effects on some *outcome* of interest

- [Random assignment]{.blue} is the key to causal identification in experiments because it creates [statistical independence]{.blue} between [treatment]{.blue} and [potential outcomes]{.blue} any potential [confounding factors]{.blue}

:::{.fragment}

$$
Y_i(1),Y_i(0),\mathbf{X_i},\mathbf{U_i} \unicode{x2AEB} D_i
$$\

:::

## Randomization creates credible counterfactual comparisons{.smaller}

If treatment has been randomly assigned, then:

- The only thing that differs between treatment and control is that one group got the treatment, and another did not.
- We can estimate the Average Treatment Effect (ATE) using the difference of sample means

:::{.fragment}

$$
\begin{aligned}
E \left[ \frac{\sum_1^m Y_i}{m}-\frac{\sum_{m+1}^N Y_i}{N-m}\right]&=\overbrace{E \left[ \frac{\sum_1^m Y_i}{m}\right]}^{\substack{\text{Average outcome}\\
\text{among treated}\\ \text{units}}}
-\overbrace{E \left[\frac{\sum_{m+1}^N Y_i}{N-m}\right]}^{\substack{\text{Average outcome}\\
\text{among control}\\ \text{units}}}\\
&= E [Y_i(1)|D_i=1] -E[Y_i(0)|D_i=0]
\end{aligned}
$$

:::

## Observational Designs {.smaller}

- [Observational designs]{.blue} are studies in which a causal variable of interest is determined by someone/thing [other than the researcher]{.blue} (nature, governments, people, etc.)

- Since treatment has not been randomly assigned, observational studies typically require [stronger assumptions]{.blue} to make causal claims.

- Generally speaking, these assumptions amount to a claim about conditional independence

:::{.fragment}


$$
Y_i(1),Y_i(0),\mathbf{X_i},\mathbf{U_i} \unicode{x2AEB} D_i | K_i
$$

:::

- Where after conditioning on $K_i$, some [knowledge about the world]{.blue} and how the [data were generated]{.blue}, our [treatment]{.blue} is as good as (as-if) randomly assigned (hence [conditionally independent]{.blue})

## Causal Inference in Observational Studies {.smaller}

To understand how to make causal claims in observational studies we will:

- Introduce the concept of Directed Acyclic Graphs to describe causal relationships

- Discuss three approaches to [covariate adjustment]{.blue}

  - Subclassification
  - Matching
  - [Linear Regression]{.blue}

- Three research designs for observational data

  - [Differences-in-Differences]{.blue}
  - Regression Discontinuity Designs
  - Instrumental Variables
  
  
## Two Ways to Describe Causal Claims

In this course, we will use two forms of notation to describe our causal claims.

- *Potential Outcomes Notation* (last lecture)

- [**Directed Acyclic Graphs** (DAGs)]{.blue}

# {{< fa lightbulb >}} Directed Acyclic Graphs {.inverse}


## Directed Acyclic Graphs

- Directed Acyclic Graphs provide a way of encoding assumptions about casual relationships

  - **Directed** Arrows $\to$ describe a direct causal effect

  - Arrow from $D\to Y$ means $Y_i(d) \neq Y_i(d^\prime)$ "The outcome ( $Y$) for person $i$ when D happens ( $Y_i(d)$ ) is different than the the outcome when $D$ doesn't happen ( $Y_i(d^\prime)$ )

  - No arrow = no effect ( $Y_i(d) = Y_i(d^\prime)$ )

  - **Acyclic:** No cycles. A variable can't cause itself

## Types of variables in a DAG{.smaller}


::: panel-tabset

## DAG
![](https://book.declaredesign.org/figures/figure-6-2.svg)

@Blair2023-yg [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)

## Variables

:::{.nonincremental}

Causal Explanations Involve:

- `Y` our outcome
- `D` A possible cause of Y
- `M` A [mediator]{.blue} or mechanism through which `D` effects `Y`
- `Z` An [instrument]{.blue} that can help us isolate the effects of D on  `Y
- `X2` a [covariate]{.blue} that may [moderate]{.blue} the effect of `D` on `Y`

Threats to causal claims/Sources of bias:

- `X1` an [observed confounder]{.blue} that is a common cause of both `D` & `Y` 
- `U` an [unobserved confounder]{.blue} a common cause of both `D` & `Y` 
- `K` a [collider]{.blue} that is a common consequence of both `D` & `Y`

:::

:::

## DAGs illustrate two sources of bias:

- **Confounder bias:** Failing to control for a common cause of `D` **and** `Y` (aka Selection Bias, Omitted Variable Bias)

- **Collider bias:** Controlling for a common consequence

## Confounding Bias: The Coffee Example

```{r}
#| label: confounding_day
#| echo: false
#| eval: false



library(dagitty)
library(ggdag)
library(ggplot2)

coffee_dag <- confounder_triangle(
  x = "Coffee",
  y = "Lung Cancer",
  z = "Smoking"
)

ggdag_confounder_triangle(coffee_dag, 
                 text = F, 
                 # controlling_for = "z",
                 use_labels = "label")
```



## Collider Bias: The Dating Example

```{r}
#| eval: false
dating_dag <- collider_triangle(
  x = "Looks",
  y = "Personality",
  m = "Dateability"
)
dating_dag %>% 
  tidy_dagitty() ->
  dating_dag
dating_dag %>% 
  mutate(colour = ifelse(name == "m", "Collider","Non-Collider"))->dating_dag
ggdag(dating_dag, use_labels = "label")

ggdag(dating_dag,text = F, use_labels = "label")+
  theme_void()-> collider_fig1

ggdag_collider(dating_dag,text = F, use_labels = "label",
               ) -> tmp
tmp$layers
ggdag_dseparated(dating_dag, 
                 text = F, 
                 controlling_for = "m",
                 use_labels = "label")+
  theme_void()
```


```{r}
#| label: collider
#| eval: false
n <- 100
set.seed(123)
collider_df <- tibble(
  Looks = rnorm(n),
  Personality = rnorm(n)
) %>% 
  mutate(
    date = case_when(
      Looks > .5  ~ "Date",
      Looks < .5 & Personality >.75 ~ 1,
      T ~ 0
    )
  )
collider_df %>% 
  filter(date==1) %>% 
  ggplot(aes(Looks, Personality))+
  geom_point(aes(col = "People you date"))+
  stat_smooth(
    method = "lm"
  )+
  guides(color = "none")+
  theme_minimal() -> collider_fig1
 
collider_fig1+
  geom_point(
    data = collider_df,
    col = "grey",
    alpha = .5
  )+
  stat_smooth(
    data = collider_df,
    col = "grey",
    method = "lm"
  )
  geom_point(
    data = collider_df %>% 
      filter(date == 1),
    col = "black"
  )

```





## When to control for a variable:

![](https://book.declaredesign.org/figures/figure-16-3.svg)

[@Blair2023-yg] [(Chap. 6.2)](https://book.declaredesign.org/declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models)

## Describing Casual Claims with DAGs

[Directed Acyclic Graphs]{.blue} provide a way of encoding assumptions about casual relationships and are useful for illustrating [types of bias]{.blue}

- **Confounder bias:** Failing to control for a common cause (aka Selection Bias, Omitted Variable Bias)

- **Collider bias:** Controlling for a common consequence


# {{< fa lightbulb >}} Causal Indentification in Observational Studies {.inverse}




# {{< fa lightbulb >}} Covariate Adjustment {.inverse}

# {{< fa lightbulb >}} Simple Linear Regression {.inverse}

## Understanding Linear Regression

- **Conceptual**
  - Simple linear regression estimates "a line of best fit" that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function

```{r}
#| eval: false
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR):

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$

class: inverse, center, middle \# 💡\
\# Conceptual: Linear Regression \## Linear Regression Provides an Estimate of the Line of Best Fit

## Conceptual: Linear Regression

- Regression is a tool for describing relationships.

  - How does some outcome we're interested in tend to change as some predictor of that outcome changes?

  - How does economic development vary with democracy?

  - How does economic development vary with democracy, adjusting for natural resources like oil and gas

- Formally:

$$
y_i = f(x_i) + \epsilon
$$

- Y is a function of X plus some error, $\epsilon$

- Linear regression assumes that relationship between an outcome and a predictor can be by a [linear](https://en.wikipedia.org/wiki/Linearity) function

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

## Linear Regression and the Line of Best Fit

- The goal of linear regression is to choose coefficients $\beta_0$ and $\beta_1$ to summarizes the relationship between $y$ and $x$

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- To accomplish this we need some sort of criteria.

- For linear regression, that criteria is minimizing the error between what our model predicts $\hat{y_i} = \beta_0 + \beta_1 x_i$ and what we actually observed $(y_i)$

- More on this to come. But first...

## Regression Notation

- $y_i$ an **outcome variable** or thing we're trying to explain

  - AKA: The dependent variable, The response Variable, The left hand side of the model

- $x_i$ a **predictor variables** or things we think explain variation in our outcome

  - AKA: The independent variable, covariates, the right hand side of the model.

  - Cap or No Cap: I'll use $X$ (should be $\mathbf{X}$) to denote a set (matrix) of predictor variables. $y$ vs $Y$ can also have technical distinctions (Sample vs Population, observed value vs Random Variable, ...)

- $\beta$ a set of **unknown parameters** that describe the relationship between our outcome $y_i$ and our predictors $x_i$

- $\epsilon$ the **error term** representing variation in $y_i$ not explained by our model.

## Linear Regression

Let's return to the simple (bivariate) linear regressions we introduced last week:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon
$$

- We call this a bivariate regression, because there are only two variables.

- We call this a linear regression, because $y_i = \beta_0 + \beta_1 x_i$ is the equation for a line, where:

  - $\beta_0$ corresponds to the $y$ intercept, or the model's prediction when $x = 0$.

  - $\beta_1$ corresponds to the slope, or how $y$ is predicted to change as $x$ changes.

## Linear Regression

- If you find this notation confusing, try plugging in substantive concepts for what $y$ and $x$ represent
- Say we wanted to know how attitudes to transgender people varied with age in the baseline survey from Lab 03.

The generic linear model

$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$

Reflects:

$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$

## Practical: Estimating a Linear Regression

- We estimate linear regressions in `R` using the `lm()` function.
- `lm()` requires two arguments:
  - a `formula` argument of the general form `y ~ x` read as "Y modeled by X" or below "Transgender Feeling Thermometer (`y`) modeled by (`~`) Age (`x`)
  - a `data` argument telling R where to find the variables in the formula

```{r}
#| label: lmdata
load(url("https://pols1600.paultesta.org/files/data/03_lab.rda"))
m1 <- lm(therm_trans_t0 ~ vf_age, data = df)
m1

```

The coefficients from `lm()` are saved in object called `m1`

```{r}
#| label: printm1
m1
```

`m1` actually contains a lot of information

```{r}
#| label: m1info
names(m1)
m1$coefficients
```

## Practical: Interpreting a Linear Regression

We can extract the intercept and slope from this simple bivariate model, using the `coef()` function

```{r}
#| label: m1coef

# All the coefficients
coef(m1)
# Just the intercept
coef(m1)[1]
# Just the slope
coef(m1)[2]
```

## Practical: Interpreting a Linear Regression

The two coefficients from `m1` define a line of best fit, summarizing how feelings toward transgender individuals change with age

$$y_i = \beta_0 + \beta_1 x_i + \epsilon$$

$$\text{Transgender Feeling Thermometer}_i = \beta_0 + \beta_1\text{Age}_i + \epsilon_i$$

$$\text{Transgender Feeling Thermometer}_i = `r round(coef(m1)[1],2)` + `r round(coef(m1)[2],2)` \text{Age}_i + \epsilon_i$$

## Practical: Predicted values from a Linear Regression

- Often it's useful for interpretation to obtain predicted values from a regression.

- To obtain predicted vales $(\hat{y})$, we simply plug in a value for $x$ (In this case, $Age$) and evaluate our equation.

- For example, might we expect attitudes to differ among an 18-year-old college student and their 68-year-old grandparent?

$$\hat{FT}_{x=18} = `r round(coef(m1)[1],2)` + `r round(coef(m1)[2],2)` \times 18  = 59.16$$ $$\hat{FT}_{x=65} = `r round(coef(m1)[1],2)` + `r round(coef(m1)[2],2)` \times 68  = 49.01$$

## Practical: Predicted values from a Linear Regression

We could do this by hand

```{r}
#| label: pred_man
coef(m1)[1] + coef(m1)[2] * 18
coef(m1)[1] + coef(m1)[2] * 68

```

## Practical: Predicted values from a Linear Regression

More often we will:

- Make a prediction data frame (called `pred_df` below) with the values of interests
- Use the `predict()` function with our linear model (`m1`) and `pred_df`
- Save the predicted values to our new column in our prediction data frame

## Practical: Predicted values from a Linear Regression

```{r}
#| label: pred_df

# Make prediction data frame
pred_df <- data.frame(
  vf_age = c(18, 68)
)
# Predict FT for 18 and 68 year-olds
predict(m1, newdata = pred_df)

# Save predictions to data frame
pred_df$ft_trans_hat <- predict(m1, newdata = pred_df)
pred_df
```

## Practical: Visualizing Linear Regression

We can visualize simple regression by:

- plotting a scatter plot of the outcome (y-axis) and predictors (x-axis)

- overlaying the line defined by `lm()`

```{r}
#| label: fig_lm_code
#| eval: false

fig_lm <- df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  geom_abline(intercept = coef(m1)[1],
              slope = coef(m1)[2],
              col = "blue"
              )+
  geom_vline(xintercept = 0,linetype = 2)+
  xlim(0,100)+
  annotate("point",
           x = 0, y = coef(m1)[1],
           col= "red",
           )+
  annotate("text",
           label = expression(paste(beta[0],"= 62.81" )),
           x = 1, y = coef(m1)[1]+5,
           hjust = "left",
           )+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People"
  )+
  theme_classic()
fig_lm
```

```{r}
#| label: fig_lm_plot
#| echo: false
fig_lm <- df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  geom_abline(intercept = coef(m1)[1],
              slope = coef(m1)[2],
              col = "blue"
              )+
  geom_vline(xintercept = 0,linetype = 2)+
  xlim(0,100)+
  annotate("point",
           x = 0, y = coef(m1)[1],
           col= "red",
           )+
  annotate("text",
           label = expression(paste(beta[0],"= 62.81" )),
           x = 1, y = coef(m1)[1]+5,
           hjust = "left",
           )+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People"
  )+
  theme_classic()
fig_lm
```

```{r}
#| label: figlm1code
#| echo: false

fig_lm+
  geom_segment(aes(x=47, xend = 47,
                   y=53.27526, yend=74),
               col="red")+
  geom_segment(aes(x=47, xend = 55,
                   y=74, yend=74),
               col="darkgrey",linetype = 2)+
  annotate("text",label = expression(paste(Y[i],"= 74" )),
           x = 55.5, y = 74,
           hjust = "left")+
  geom_segment(aes(x=47, xend = 55,
                   y=63.5, yend=63.5),
               col="red",linetype = 2)+
  annotate("text",label = expression(paste(epsilon[i],"=", Y[i] - hat(Y[i]),"= 74 - 53.27 = 20.73" )),
           x = 55.5, y = 63.5,
           col = "red",
           hjust = "left")+
  geom_segment(aes(x=47, xend = 55,
                   y=53.27526, yend=53.27526),
               col="blue",linetype = 2)+
  annotate("text",label = expression(paste(hat(Y[i]),"=", beta[0] + beta[1],"Age = 62.81 -0.20*47 = 53.27" )),
           x = 55.5, y = 53.27526,
           col = "blue",
           hjust = "left")



```

```{r}
#| label: figlm1
#| echo: false
# devtools::install_github("nicolash2/ggbrace")
library(ggbrace)
fig_lm +
  ylim(48, 52)+
  xlim(58, 62)+
  geom_segment(aes(
    y = coef(m1)[1] + coef(m1)[2]*61,
    yend = coef(m1)[1] + coef(m1)[2]*60,
    x = 60, xend = 60),
    arrow = arrow(ends = "first",length = unit(0.015, "npc")),
    col = "red"
    )+
    geom_segment(aes(
    y = coef(m1)[1] + coef(m1)[2]*61,
    yend = coef(m1)[1] + coef(m1)[2]*61,
    x = 60, xend = 61),
    arrow = arrow(ends = "last",length = unit(0.015, "npc")),
    col = "red"
    )+
   geom_curve(aes(
    y = 49.7,
    yend = 50.53,
    x = 59.43, xend = 59.8),
    curvature = -.2,
    arrow = arrow(ends = "last",length = unit(0.015, "npc")),
    col = "black",
    )+
    annotate("text",
           label = expression(paste(beta[1],"= -0.21" )),
           x = 59, y = 49.5,
           hjust = "left",
           )+
  geom_brace(aes(c(59.9, 59.98),c(50.43, 50.63)), 
             inherit.data=F,
             rotate = 270)
```

## How did `lm()` choose $\beta_0$ and $\beta_1$

--

- P: By minimizing the sum of squared errors, in procedure called Ordinary Least Squares (OLS) regression

--

- Q: Ok, that's not really that helpful...

  - What's an error?
  - Why would we square and sum them
  - How do we minimize them.

P: Good questions!

## What's an error?

An error, $\epsilon_i$ is simply the difference between the observed value of $y_i$ and what our model would predict, $\hat{y_i}$ given some value of $x_i$. So for a model:

$$y_i=\beta_0+\beta_1 x_{i} + \epsilon_i$$

We simply subtract our model's prediction $\beta_0+\beta_1 x_{i}$ from the the observed value, $y_i$

$$\hat{\epsilon_i}=y_i-\hat{y_i}=(Y_i-(\beta_0+\beta_1 x_{i}))$$

To get $\epsilon_i$

## Why are we squaring and summing $\epsilon$

There are more mathy reasons for this, but at intuitive level, the Sum of Squared Residuals (SSR)

- Squaring $\epsilon$ treats positive and negative residuals equally.

- Summing produces single value summarizing our models overall performance.

There are other criteria we could use (e.g. minimizing the sum of absolute errors), but SSR has some nice properties

## How do we minimize $\sum \epsilon^2$

OLS chooses $\beta_0$ and $\beta_1$ to minimize $\sum \epsilon^2$, the Sum of Squared Residuals (SSR)

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

## How did `lm()` choose $\beta_0$ and $\beta_1$

In an intro stats course, we would walk through the process of finding

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$ Which involves a little bit of calculus. The big payoff is that

$$\beta_0 = \bar{y} - \beta_1 \bar{x}$$ And

$$ \beta_1 = \frac{Cov(x,y)}{Var(x)}$$ Which is never quite the epiphany, I think we think it is...

The following slides walk you through the mechanics of this exercise. We're gonna skip through them in class, but they're there for your reference

## How do we minimize $\sum \epsilon^2$

To understand what's going on under the hood, you need a broad understanding of some basic calculus.

The next few slides provide a brief review of derivatives and differential calculus.

## Derivatives

The derivative of $f$ at $x$ is its rate of change at $x$

- For a line: the slope
- For a curve: the slope of a line tangent to the curve

You'll see two notations for derivatives:

1.  Leibniz notation:

$$
\frac{df}{dx}(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

2.  Lagrange: $f^{\prime}(x)$

## Some useful Facts about Derivatives

Derivative of a constant

$$
f^{\prime}(c)=0
$$

Derivative of a line f(x)=2x

$$
f^{\prime}(2x)=2
$$

Derivative of $f(x)=x^2$

$$
f^{\prime}(x^2)=2x
$$

Chain rule: y= f(g(x)). The derivative of y with respect to x is

$$
\frac{d}{dx}(f(g(x)))=f^{\prime}(g(x))g^{\prime}(x)
$$

The derivative of the "outside" times the derivative of the "inside," remembering that the derivative of the outside function is evaluated at the value of the inside function.

## Finding a Local Minimums

Local minimum:

$$
f^{\prime}(x)=0 \text{ and } f^{\prime\prime}(x)>0 
$$

```{r}
#| label: derivatives
#| echo: false
knitr::include_graphics("https://copingwithcalculus.com/SecondDeriv1.png")
```

[Source](https://copingwithcalculus.com/SecondDerivativeTest.html)

## Partial Derivatives

Let $f$ be a function of the variables $(x, \dots, X_n)$. The partial derivative of $f$ with respect to $X_i$ is

$$\begin{align*}
\frac{\partial f(x, \dots, X_n)}{\partial X_i}=\lim_{h\to0}\frac{f(x, \dots X_i+h \dots, X_n)-f(x, \dots X_i \dots, X_n)}{h}
\end{align*}$$

```{r}
#| label: partial
#| echo: false
knitr::include_graphics("https://miro.medium.com/max/766/1*dToo8pNrhBmYfwmPLp6WrQ.png")
```

[Source](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)

## Minimizing the sum of squared errors

Our model

$$y_i =\beta_0+\beta_1x_{i}+\epsilon_i$$

Finds coefficients $\beta_0$ and $\beta_1$ to to minimize the sum of squared residuals, $\hat{\epsilon}_i$:

$$\begin{aligned}
\sum \hat{\epsilon_i}^2 &= \sum (y_i-\beta_0-\beta_1 x_{i})^2
\end{aligned}$$

## Minimizing the sum of squared errors

We solve for $\beta_0$ and $\beta_1$, by taking the partial derivatives with respect to $\beta_0$ and $\beta_1$, and setting them equal to zero

$$\begin{aligned}
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial \beta_0} &= -2\sum (y_i-\beta_0-\beta_1 x_{i})=0 & f'(-x^2) = -2x\\
\frac{\partial \sum \hat{\epsilon_i}^2}{\partial\beta_1} &= -2\sum (y_i-\beta_0-\beta_1 x_{i})x_{i}=0 & \text{chain rule}
\end{aligned}$$

## Solving for $\beta_0$

First, we'll solve for $\beta_0$, by multiplying both sides by -1/2 and distributing the $\sum$:

$$\begin{aligned}
0 &= -2\sum (y_i-\beta_0-\beta_1 x_{i})\\
\sum \beta_0 &= \sum y_i - \sum \beta_1 x_{i}\\
N \beta_0 &= \sum y_i -\sum \beta_1 x_{i}\\
\beta_0 &= \frac{\sum y_i}{N} - \frac{\beta_1 \sum x_{i}}{N}\\
\beta_0 &= \bar{y} - \beta_1 \bar{x}
\end{aligned}$$

## Solving for $\beta_1$

Now, we can solve for $\beta_1$ plugging in $\beta_0$.

$$\begin{aligned}
0 &= -2\sum [(y_i-\beta_0-\beta_1 x_{i})x_{i}]\\
0 &= \sum [y_ix_i-(\bar{y} - \beta_1 \bar{x})x_{i}-\beta_1 x_{i}^2]\\
0 &= \sum [y_ix_i-\bar{y}x_{i} + \beta_1 \bar{x}x_{i}-\beta_1 x_{i}^2]
\end{aligned}$$

## Solving for $\beta_1$

Now we'll rearrange some terms and pull out an $x_{i}$ to get

$$\begin{aligned}
0 &= \sum [(y_i -\bar{y} + \beta_1 \bar{x}-\beta_1 x_{i})x_{i}]
\end{aligned}$$

Dividing both sides by $x_{i}$ and distributing the summation, we can isolate $\beta_1$

$$\begin{aligned}
\beta_1 \sum (x_{i}-\bar{x}) &= \sum (y_i -\bar{y})
\end{aligned}$$

Dividing by $\sum (x_{i}-\bar{x})$ to get

$$\begin{aligned}
\beta_1  &= \frac{\sum (y_i -\bar{y})}{\sum (x_{i}-\bar{x})}
\end{aligned}$$

## Solving for $\beta_1$

Finally, by multiplying by $\frac{(x_{i}-\bar{x})}{(x_{i}-\bar{x})}$ we get

$$\begin{aligned}
\beta_1  &= \frac{\sum (y_i -\bar{y})(x_{i}-\bar{x})}{\sum (\bar{x}-x_{i})^2}
\end{aligned}$$

Which has a nice interpretation:

$$\begin{aligned}
\beta_1 &= \frac{Cov(x,y)}{Var(x)}
\end{aligned}$$

So the coefficient in a simple linear regression of $Y$ on $X$ is simply the ratio of the covariance between $X$ and $Y$ over the variance of $X$. Neat!

class: inverse, center, middle \# 💡\
\# Theoretical: Linear Regression \## OLS provides a linear estimate of CEF: E\[Y\|X\]

## Linear Regression is a many splendored thing

[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of the various interpretations/motivations for linear regression.

- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)

- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\beta$

- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)

- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)

- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)

- A [linear approximation of the conditional expectation function](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)

## Linear Regression is a many splendored thing

[Timothy Lin](https://www.timlrx.com/tags/ols) provides a great overview of various interpretations/motivations for linear regression.

- A [least squares estimator](https://www.timlrx.com/blog/notes-on-regression-ols)

- A [linear projection](https://www.timlrx.com/blog/notes-on-regression-geometry) of $y$ on the subspace spanned by $X\beta$

- A [method of moments estimator](https://www.timlrx.com/blog/notes-on-regression-method-of-moments)

- A [maximum likelihood estimator](https://www.timlrx.com/blog/notes-on-regression-maximum-likelihood)

- A [singular vector decomposition](https://www.timlrx.com/blog/notes-on-regression-singular-vector-decomposition)

- A [**linear approximation of the conditional expectation function**](https://www.timlrx.com/blog/notes-on-regression-approximation-of-the-conditional-expectation-function#fn1)

## The Conditional Expectation Function

Of all the functions we could choose to describe the relationship between $Y$ and $X$,

$$
Y_i = f(X_i) + \epsilon_i
$$

the conditional expectation of $Y$ given $X$ $(E[Y|X])$, has some appealing properties

$$
Y_i = E[Y_i|X_i] + \epsilon
$$

The error, by definition, is uncorrelated with X and $E[\epsilon|X]=0$

$$
E[\epsilon|X] = E[Y - E[Y|X]|X]= E[Y|X] - E[Y|X] = 0
$$

Of all the possible functions $g(X)$, we can show that \$E\[Y_i\|X_i\] \$ is the best predictor in terms of minimizing **mean squared error**

$$
E[ (Y - g(Y))^2] \geq E[(Y - E[Y|X])^2] 
$$

## Linear Approximations to the Conditional Expectation Function

- We can then show (in a different class) that linear regression provides the best linear predictor of the CEF
  - Chapter 3, of [Mostly Harmless Econometrics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991028523169706966)
  - Chapter 4 of [Foundations of Agnostic Statistics](https://bruknow.library.brown.edu/permalink/01BU_INST/9mvq88/alma991000736119706966)
- Furthermore, when the CEF is linear, it's equal exactly to OLS regression

```{r}
#| label: cef1code
#| echo: true
#| eval: false


df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  stat_summary(geom="point", aes(col="CEF"))+
  stat_summary(geom="line", aes(col="CEF"))+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People",
    col = ""
  )+
  theme_classic() -> plot_cef



```

```{r}
#| label: cef1
#| echo: false
df %>%
  ggplot(aes(vf_age,therm_trans_t0))+
  geom_point(size=.5, alpha=.5)+
  stat_summary(geom="point", aes(col="CEF"))+
  stat_summary(geom="line", aes(col="CEF"))+
  labs(
    x = "Age",
    y = "Feeling Thermometer toward\nTransgender People",
    col = ""
  )+
  theme_classic() -> plot_cef

plot_cef
```

```{r}
#| label: cef2code
#| echo: true
#| eval: false
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -> plot_cef
plot_cef




```

```{r}
#| label: cef2
#| echo: false
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -> plot_cef
plot_cef
```

```{r}
#| label: cef3code
#| echo: true
#| eval: false
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -> plot_cef
plot_cef




```

```{r}
#| label: cef3
#| echo: false
plot_cef+
  geom_smooth(method = "lm", se=F, aes(col = "OLS")) -> plot_cef
plot_cef
```

## What you need to know about Regression

- **Conceptual**
  - Simple linear regression estimates a line of best fit that summarizes relationships between two variables

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

- **Practical**
  - We estimate linear models in R using the `lm()` function

```{r}
#| label: ols
#| eval: false
lm(y ~ x, data = df)
```

- *Technical/Definitional*
  - Linear regression chooses $\beta_0$ and $\beta_1$ to minimize the Sum of Squared Residuals (SSR):

$$\textrm{Find }\hat{\beta_0},\,\hat{\beta_1} \text{ arg min}_{\beta_0, \beta_1} \sum (y_i-(\beta_0+\beta_1x_i))^2$$

- *Theoretical*
  - Linear regression provides a **linear** estimate of the conditional expectation function (CEF): $E[Y|X]$

# {{< fa lightbulb >}} Difference-in-Differences {.inverse}

## Summary

## References

