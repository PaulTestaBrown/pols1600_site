---
title: "POLS 1600"
subtitle: "Topic"
date: last-modified
date-format: "[Updated ]MMM D, YYYY"
format: 
  revealjs:
    theme: brownslides.scss
    logo: images/pols1600_hex.png
    footer: "POLS 1600"
    multiplex: false
    transition: fade
    slide-number: c
    incremental: true
    center: false
    menu: true
    scrollable: true
    highlight-style: github
    progress: true
    code-overflow: wrap
    chalkboard: true
    # include-after-body: title-slide.html
    title-slide-attributes:
      align: left
      data-background-image: images/pols1600_hex.png
      data-background-position: 90% 50%
      data-background-size: 40%
filters:
  - openlinksinnewpage
execute: 
  eval: true
  echo: true
  warning: false
  message: false
  cache: false
---


```{r}
#| label: init
#| echo: false
#| results: hide
#| warning: false 
#| message: false

library(tidyverse)
library(labelled)
library(haven)
library(DeclareDesign)
library(easystats)
library(texreg)

```



# {{< fa map-location>}} Overview {.inverse}

## Class Plan

- Announcements
- Setup
- Feedback
- Review
- Class plan


## Annoucements

## Setup: Packages for today

```{r}
#| label: packages
#| echo: true

## Pacakges for today
the_packages <- c(
  ## R Markdown
  "kableExtra","DT","texreg","htmltools",
  ## Tidyverse
  "tidyverse", "lubridate", "forcats", "haven", "labelled",
  ## Extensions for ggplot
  "ggmap","ggrepel", "ggridges", "ggthemes", "ggpubr", 
  "GGally", "scales", "dagitty", "ggdag", "ggforce",
  # Data 
  "COVID19","maps","mapdata","qss","tidycensus", "dataverse", 
  # Analysis
  "DeclareDesign", "easystats", "zoo"
)

## Define a function to load (and if needed install) packages

#| label = "ipak"
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

## Install (if needed) and load libraries in the_packages
ipak(the_packages)
```


## Feedback

## {{< fa lightbulb >}}  Concepts 

## {{< fa code>}} Code   

# {{< fa magnifying-glass>}} Review {.inverse}

## Review

# {{< fa lightbulb >}} What does it mean to "control for X" {.inverse}

## Regression models partition variance {.smaller}

Regression models [partition variance]{.blue}, separating the [variation in the outcome]{.blue} **into** [variation explained by the predictors]{.blue} in our model and the [remaining variation not explained]{.blue} by these predictors

$$\begin{aligned}
\textrm{Total Variance} &= \textrm{Variance Explained by Model} + \textrm{Unexplained Variance} \\
\textrm{Observed} &= \textrm{Predicted Value} + \textrm{Error}\\
\textrm{Y} &=  E[Y|X] + \epsilon\\
\textrm{Y} &=  X\hat{\beta} + \hat{\epsilon}\\
\textrm{Y} &= \hat{Y} + \hat{\epsilon}
\end{aligned}$$

## {.smaller}
#### Coefficients describe the unique variance in Y explained by X (and only X)

::: panel-tabset

## Task

The coefficients in a regression model describe the variation in the outcome explained by that predictor, and only that predictor.

Let's fit three models from last week's lab and look at how the coefficients change from model to model

## {{< fa code >}}
```{r}
#| label: labmodels
load(url("https://pols1600.paultesta.org/files/data/06_lab.rda"))
m1 <- lm(new_deaths_pc_14day ~ rep_voteshare_std, covid_lab)
m2 <- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std, covid_lab)
m3 <- lm(new_deaths_pc_14day ~ rep_voteshare_std + med_age_std + med_income_std, covid_lab)

```

## {{< fa table >}}

```{r}
#| label: labtab

htmlreg(list(m1, m2, m3)) %>% HTML() %>% browsable()

```

:::


## Why do coefficients change when we control for variables?{.center}


## Residualized Regression


Residualized regression is way of understanding what it means to [control for variables]{.blue} in a regression.

Residualized regression provides a way of illustrating what we mean when say the coefficients describe the unique variance in Y explained by some predictor $x$ (and only $x$)


## What's a residual


- [Residuals]{.blue} represent the part of the outcome variable, [not explained]{.blue} by the predictors in a model
  - Difference between the observed $y$ and the predicted $\hat{y}$
  

$$y = \overbrace{\beta_0 + \beta_1x_1 + \beta_2 x_2  + \dots \beta_j x_j}^{\text{Predictors}} + \underbrace{\epsilon}_{\text{Residuals}}$$

## Residuals are uncorrelated with $X$ and $\hat{y}${.smaller}

::::{.columns}

:::{.column width=40%}

Residuals are uncorrelated with (orthogonal to) the predictors $X$, and predicted values $X\beta$

```{r}
#| label: rescor

# Trust but verify
cor(resid(m2),covid_lab$rep_voteshare_std) 
cor(resid(m2),covid_lab$med_age_std)
cor(resid(m2),fitted(m2))
```


:::

:::{.column width=55%}
![](https://i.stack.imgur.com/UEOIP.png)
:::

::::

## {.smaller}
#### Residualized Regression 


:::: panel-tabset

## {{< fa lightbulb >}}

:::{.nonincremental}
For a model like `m2` we can recover the coefficient on `rep_voteshare_std` by:
  
1. Regressing `new_deaths_pc_14day` on `med_age_std` to get the [residual]{.blue} variation in Covid-19 deaths [not explained]{.blue} by median age 
2. Regressing `rep_voteshare_std` on `med_age_std` to get the [residual]{.blue} variation in Republican Vote Share [not explained]{.blue} by median age 
3. Regressing the residuals from 1. (Deaths not explained by age) on the residuals from 2. (Vote share not explained by age) to obtain the [same]{.blue} coefficient from `m2` for `rep_voteshare_std`

The same principle holds for `m3`

:::

## {{< fa code >}} m2

```{r}
#| label: resm2

# 1. Regressing `new_deaths_pc_14da` on `med_age_std`
m2_death_by_age <- lm(new_deaths_pc_14day ~ med_age_std, covid_lab)
# Save residuals
covid_lab$res_death_no_age <- resid(m2_death_by_age)

# 2. Regressing `rep_voteshare_std` on `med_age_std` 
m2_repvs_by_age <- lm(rep_voteshare_std ~ med_age_std, covid_lab)
# Save residuals
covid_lab$res_repvs_no_age <- resid(m2_repvs_by_age)

# 3. Residualized regression of deaths on Rep Vote Share
m2_res <- lm(res_death_no_age ~ res_repvs_no_age, covid_lab)

# Mutliple regression
coef(m2)[2]

# Residualized regression
coef(m2_res)[2]
```

## {{< fa code >}} m3

```{r}
#| label: resm3

# 1. Regressing `new_deaths_pc_14da` on `med_age_std` and med_income_std
m3_death_by_age_income <- lm(new_deaths_pc_14day ~ med_age_std + med_income_std, covid_lab)
# Save residuals
covid_lab$res_death_no_age_income <- resid(m3_death_by_age_income)

# 2. Regressing `rep_voteshare_std` on `med_age_std` and med_income_std
m3_repvs_by_age_income <- lm(rep_voteshare_std ~ med_age_std + med_income_std, covid_lab)
# Save residuals
covid_lab$res_repvs_no_age_income <- resid(m3_repvs_by_age_income)

# 3. Residualized regression of deaths on Rep Vote Share
m3_res <- lm(res_death_no_age_income ~ res_repvs_no_age_income, covid_lab)

# multiple regression coefficient
coef(m3)[2]
# Same as  residualized regression coefficient
coef(m3_res)[2]
```
::::

## Why did the coefficient on Rep Vote Share change in `m3` but not `m2`?{.center}

## {.smaller}

## {.smaller}

```{r}
#| label: ivtab1
#| echo: false
htmlreg(list(m1),
          custom.model.names = c("Baseline"),
        custom.header = list("DV: Death"=1)
        ) %>% HTML() %>% browsable()
```

##

$$
\text{Covid-19 Deaths} = \beta_0 + \beta_1 \text{Rep Vote Share}
$$

```{r}
#| label: venn_iv1
#| echo: false

# Colors (order: x1, x2, x3, y, z)
venn_colors <- c("grey", "blue", "grey", "red")
venn_lab_colors <- c("white", "blue", "white", "red")

# Line types (order: x1, x2, x3, y, z)
venn_lines <- c("solid", "solid", "solid", "solid")
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   0,    1.3,    -2),
  y  = c( 0.0,   -2.5,   -1.8,    -2.8)+1,
  r  = c( 1.9,    1.5,    1.5,      1.3),
  l  = c( "Deaths", "RepVS", "Income",   "Age"),
  cc = c("grey", "red", "black", "blue"),
  lc = c( "red", "blue", "white","white"),
  xl = c( 0.0,    0,    1.3,  -2),
  yl = c( 0.0,   -2.8,   -1.9,   -2.8)+1,
  a = c(.3, .3 , .7, .7)
)

# Venn
venn_df %>%
  filter(l %in% c( "Deaths", "RepVS")) %>%
ggplot(aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l, alpha = a), size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors[c(4,2)]) +
scale_color_manual(values = venn_colors[c(4,2)]) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), col=c("red","blue"), size = 9, parse = T ) +
annotate(
  x = 0, y = -0.5,
  geom = "text", label = expression(beta[1]), size = 10, hjust = .5
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

## {.smaller}

```{r}
#| label: ivtab2
#| echo: false

htmlreg(list(m1,m2, m2_death_by_age,m2_repvs_by_age, m2_res),
        custom.model.names = c("Baseline","Mutliple","Deaths","Vote Share","Deaths"),
        custom.header = list("DV: Death"=1:3, "DV: Vote Share"=4, "DV: Res. Deaths"=5)) %>% HTML() %>% browsable()
```


##
$$
\text{Deaths} = \beta_0 + \beta_1 \text{Rep VS} + \beta_2 \text{Age}
$$

```{r}
#| label: venn_iv2
#| echo: false

#| echo = F
venn_df %>%
  filter(l %in% c( "Deaths", "RepVS","Age")) %>%
ggplot(aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l, alpha = a), size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors[c(1,2,4)]) +
scale_color_manual(values = venn_colors[c(1,2,4)]) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), col=c("blue","red","white"), size = 9, parse = T ) +
annotate(
  x = 0, y = -.5,
  geom = "text", label = expression(beta[1]), size = 10, hjust = .5
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()

```

$\beta_1$ doesn't change because `age` has no relationship to `deaths` in these data

## {.smaller}

```{r}
#| label: ivtab3
#| echo: false
htmlreg(list(m1, m3, m3_death_by_age_income, m3_repvs_by_age_income,m3_res),
          custom.model.names = c("Baseline","Mutliple","Deaths","Vote Share","Deaths"),
        custom.header = list("DV: Death"=1:3, "DV: Vote Share"=4, "DV: Res. Death"=5)
        ) %>% HTML() %>% browsable()
```


##

$$
\text{Deaths} = \beta_0 + \beta_1 \text{Rep VS} + \beta_2 \text{Age} + \beta_3 \text{Income}
$$

```{r}
#| label: venn_iv3
#| echo: false

venn_df %>%
ggplot(aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l, alpha = a), size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), col=c("blue","red","white","white"), size = 9, parse = T ) +
annotate(
  x = -0.45, y = -.45,
  geom = "text", label = expression(beta[1]), size = 5, hjust = .5
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

$\beta_1$ decreases because after controlling for `income` there is less unique variation explained only by `republican vote share`


# {{< fa lightbulb >}} Using regression to make predictions {.inverse}

## Using regression to produce predicted values {.smaller}

Coefficients in a regression define a formula which produces a predicted value of the outcome $y$ when the predictors $X$ take particular values.

$$
\begin{aligned}y &= \overbrace{\beta_0 + \beta_1x_1 + \beta_2 x_2  + \dots \beta_j x_j}^{\text{Predictors}} + \underbrace{\epsilon}_{\text{Residuals}} &\\
y &= \beta_0 + \beta_1x_{rvs} + \beta_2x_{age}+ \beta_3x_{inc} + \epsilon & \text{m3}\\
y &= 0.56 + 0.07x_{rvs} - 0.02 x_{age} - 0.22 x_{inc} + \hat{\epsilon} & \text{estimated m3}\\
y &= 0.56 + 0.07(-0.87) - 0.02(0.62) - 0.22(0.38) + \hat{\epsilon} & \text{prediction for RI} \\
\overbrace{0.22}^{\text{Observed}} &= \underbrace{0.41}_{\text{Predicted}} + \overbrace{(-0.19)}^{\text{Residual}} &
\end{aligned}
$$

## Producing Predicted Values in R {.smaller}

The basic steps to producing predicted values in R as follows:

- Fit a model using `lm()`
- Produce a [prediction data frame]{.blue}, where one (sometimes two) predictor(s) vary, and others are held constant at a single value using `expand_grid()`
  - Every predictor in your model needs to have a value in your prediction dataframe 
- Input the model from `lm()` and [prediction data frame]{.blue}, into the `predict()` function to obtain [predicted values]. 
  - Save predictions as a new column in your [prediction data frame]{.blue} (I generally call them `fit`)
- Plot predicted values in your [prediction data frame]{.blue} to help interpret your model

## {.smaller}
#### Are their decreasing returns to vaccination?

:::: panel-tabset

## Task

Suppose we thought the [marginal effect]{.blue} of vaccines varied.

There might be large gains from going to low to average rates of vaccination, but after a certain threshold, the decreases in deaths would taper off.

We could test this by including a [polynomial term]{.blue} `I(percent_vaccinated)^2` in our model.

It's hard to the coefficients on [polynomial terms]{.blue}

Instead, we'll produce a plot of predicted values to test these claims

## {{<fa code >}} m4

```{r}
m4 <- lm(new_deaths_pc_14day ~ percent_vaccinated + I(percent_vaccinated^2) + rep_voteshare_std + med_age_std + med_income_std, covid_lab
           )
```

## {{<fa table >}} Table

```{r}
#| label: tabm4
#| echo: false
#| results: asis
htmlreg(m4, digits = 3)
```

## {{<fa code >}} Predict

```{r}
#| label: predm4

pred_df <- expand_grid(
  percent_vaccinated = sort(unique(covid_lab$percent_vaccinated)),
  # Set standardized predictors to their means of 0
  rep_voteshare_std = 0,
  med_age_std = 0,
  med_income_std = 0
)

pred_df$fit <- predict(m4, newdata = pred_df)

pred_df %>% 
  ggplot(aes(percent_vaccinated, fit))+
  geom_line()+
  labs(
    y = "Predicted Covid-19 Deaths\n(per capita, 14-day average)",
    x = "Percent of State's population that's Vaccinated"
  ) + 
  theme_minimal() -> fig_m4
```


## {{<fa chart-line>}} Fig

For a typical state, early increases in vaccination rate are associated with larger declines in predicted deaths from Covid-19

```{r}
#| echo: false

fig_m4
```


::::


# {{< fa lightbulb >}} Evaluating Model Fit {.inverse}

## Evaluating Model Fit

Models partition variance. We can summarize the overall fit of our model using measures like $R^2$

$$R^2 = \frac{\text{variance(predicted values )}}{ \text{variance(observed values )}}$$

## R^2

More formally, you'll see $R^2$ defined in terms of "Sums of Squares"

- TSS = [Total Summ of Squares]{.blue} = Variance of the Outcome
- ESS = [Explained Sum of Squares]{.blue} = Variance of the Predicted Values
- RSS = [Sum of Squared Residuals]{.blue} = Variance of the Residuals

$$R^2 = \frac{ESS}{TSS}= 1 - \frac{RSS}{TSS}$$

## Calculating $R^2$ in R{.smaller}

We could do it by hand, finding that our model explained about 43 percent of the observed variation deaths.

```{r}
# ESS / TSS
var(m3$fitted.values)/var(m3$model$new_deaths_pc_14day)
# 1 - RSS/TSS
1 - var(m3$residuals)/var(m3$model$new_deaths_pc_14day)

```

But generally we let the `summary()` function do it for us:

```{r}
summary(m3)
```


## Adjusted $R^2${.smaller}

:::: panel-tabset

## Adjusted $R^2$

:::{.nonincremental}
- One can show that a models $R^2$ [always increases]{.blue} as we [add predictors]{.blue}, even when they're unrelated to the outcome

- The [adjusted $R^2$]{.blue} adjusts for this by weighting the $R^2$ of a model by the number of predictors

$$\text{adj. }R^2 = 1 - \frac{RSS/(n-k)}{TSS/(n-1)}$$

:::

## {{< fa code >}} Example

```{r}

ex_df <- data.frame(
  y = rnorm(100) 
  ) %>%
    bind_cols(
      data.frame(matrix(rnorm(10000), ncol=100))
    ) %>% janitor::clean_names()


the_formulas <- list()
for(i in 2:51){
  vars <- names(ex_df)[2:i]
  the_formulas[[i-1]] <- paste("y~",paste(vars,collapse = "+"))
}

the_formulas %>% 
  purrr::map(as.formula) %>% 
  purrr::map(lm, data=ex_df) %>% 
  purrr::map(summary) %>% 
  purrr::map_df(glance) -> r2_df

r2_df %>% 
  ggplot(aes(df, r.squared))+
  geom_point(aes(col = "R^2"))+
  geom_line()+
  geom_point(aes(y=adj.r.squared,col = "Adjusted R^2"))+
  geom_line(aes(y=adj.r.squared))+
  labs(
    x = "Number of predictors",
    y = "Proportion of Variance Explained",
    title = "Adding unrelated predictors increases a model's R^2\nwhile the Adjusted R^2 provides a better indicator of poor fit ",
    col ="Model fit"
  ) -> fig_r2






```

## {{< fa chart-line >}} Figure

```{r}
#| echo: false
fig_r2

```

::::

## {.smaller}
#### Using $R^2$ to compare models

::::panel-tabset

## ANOVA

When models are *nested* (larger models contain all the predictors of smaller models), we can ask, does including the additional predictors in the larger model explain more variation in the outcome than we would expect would happen if we just added additional, random variable.

Formally we call this process an [Analysis of Variance (ANOVA)](https://towardsdatascience.com/anova-for-regression-fdb49cf5d684#:~:text=It%20is%20the%20same%20as,or%20more%20categorical%20predictor%20variables.)

Let's consider whether the gains in fit from `I(percent_vaccinated^2)` 

## {{<fa code >}} m5

```{r}
m5 <- lm(new_deaths_pc_14day ~ percent_vaccinated + I(percent_vaccinated^2) + rep_voteshare_std + med_age_std + med_income_std, covid_lab
           )
```

## {{<fa table >}} Table

```{r}
#| label: tabm5
#| echo: false
#| results: asis
htmlreg(list(m4,m5), digits = 3)
```


::::

# {{< fa lightbulb >}} Difference-in-Differences {.inverse}

## Motivating Example: What causes Cholera? {.smaller background-image=https://www.finebooksmagazine.com/sites/default/files/styles/gallery_item/public/media-images/2020-11/map-lead-4.jpg?h=2ded5a3f&itok=Mn-K5rQc, background-opacity=.3}

- In the 1800s, cholera was thought to be transmitted through the air.

- John Snow (the physician, not the snack), to explore the origins eventunally concluding that cholera was transmitted through living organisms in water.

- Leveraged a **natural experiment** in which one water company in London moved its pipes further upstream (reducing contamination for Lambeth), while other companies kept their pumps serving Southwark and Vauxhall in the same location. 


## Notation {.smaller}

Let's adopt a little notation to help us think about the logic of Snow's design:

- $D$: treatment indicator, 1 for treated neighborhoods (Lambeth), 0 for control neighborhoods (Southwark and Vauxhall)

- $T$: period indicator, 1 if post treatment (1854), 0 if pre-treatment (1849).

- $Y_{di}(t)$ the potential outcome of unit $i$ 

  - $Y_{1i}(t)$ the potential outcome of unit $i$ when treated between the two periods 

  - $Y_{0i}(t)$ the potential outcome of unit $i$ when control between the two periods 


## Causal Effects {.smaller}

The individual causal effect for unit i at time t is:

$$\tau_{it} = Y_{1i}(t) − Y_{0i}(t)$$

What we observe is 

$$Y_i(t) = Y_{0i}(t)\cdot(1 − D_i(t)) + Y_{1i}(t)\cdot D_i(t)$$

$D$ only equals 1, when $T$ equals 1, so we never observe $Y_0i(1)$ for the treated units. 

In words, we don't know what Lambeth's outcome would have been in the second period, had they not been treated.


## Average Treatment on Treated {.smaller}

Our goal is to estimate the average effect of treatment on treated (ATT):


$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

That is, what would have happened in Lambeth, had their water company not moved their pipes


## Average Treatment on Treated {.smaller}

Our goal is to estimate the average effect of treatment on treated (ATT):

We we can observe is:

|               | Pre-Period (T=0)  | Post-Period (T=1)  |
|-|--|-|
| Treated $D_{i}=1$  |  $E[Y_{0i}(0)\vert D_i = 1]$ | $E[Y_{1i}(1)\vert D_i = 1]$  |
| Control $D_i=0$  |  $E[Y_{0i}(0)\vert D_i = 0]$ | $E[Y_{0i}(1)\vert D_i = 0]$  |


## Data {.smaller}

Because potential outcomes notation is abstract, let's consider a modified description of the Snow's cholera death data from [Scott Cunningham](https://mixtape.scunning.com/difference-in-differences.html):

```{r}
#| label = "choleradat",
#| echo = F
snow <- tibble(Company = c("Lambeth (D=1)", "Southwark and Vauxhall (D=0)"),
               `1849 (T=0)` = c(85,135),
               `1854 (T=1)` = c(19,147),

               )

knitr::kable(snow)

```


## How can we estimate the effect of moving pumps upstream? {.smaller}

Recall, our goal is to estimate the effect of the the treatment on the treated:

$$\tau_{ATT} = E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$$

Let's conisder some strategies Snow could take to estimate this quantity:


## Before vs after comparisons:{.smaller}

:::{.nonincremental}
- Snow could have compared Labmeth in 1854 $(E[Y_i(1)|D_i = 1] = 19)$ to Lambeth in 1849 $(E[Y_i(0)|D_i = 1]=85)$, and claimed that moving the pumps upstream led to **66 fewer cholera deaths.** 

- Assumes Lambeth's pre-treatment outcomes in 1849 are a good proxy for what its outcomes would have been in 1954 if the pumps hadn't moved $(E[Y_{0i}(1)|D_i = 1])$.

- A skeptic might argue that Lambeth in 1849 $\neq$ Lambeth in 1854


```{r}
#| echo: false
knitr::kable(snow) |> 
  kable_styling() |> 
  row_spec(1, bold=T,color = "blue")
```

:::


## Treatment-Control comparisons in the Post Period. {.smaller}

:::{.nonincremental}

- Snow could have compared outcomes between Lambeth and S&V in 1954  ($E[Yi(1)|Di = 1] − E[Yi(1)|Di = 0]$), concluding that the change in pump locations led to **128 fewer deaths.**

- Here the assumption is that the outcomes in S&V and in 1854 provide a good proxy for what would have happened in Lambeth in 1954 had the pumps not been moved $(E[Y_{0i}(1)|D_i = 1])$

- Again, our skeptic could argue  Lambeth $\neq$ S&V 

```{r}
#| echo: false
knitr::kable(snow) |> 
  kable_styling() |> 
  kableExtra::column_spec(3, bold=T, color="red")
```

:::

## Difference in Differences {.smaller}

:::{.nonincremental}
To address these concerns, Snow employed what we now call a [difference-in-differences]{.blue} design, 

There are two, equivalent ways to view this design. 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$

- Difference 1: Average change between Treated and Control  in Post Period

- Difference 2: Average change between Treated and Control  in Pre Period

:::

## Difference in Differences {.smaller}

:::{.nonincremental}

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(1)|D_{i} = 0]\}}_{\text{1. Treat-Control |Post }}− \overbrace{\{E[Y_{i}(0)|D_{i} = 1] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Treated-Control|Pre}}$$
Is equivalent to: 

$$\underbrace{\{E[Y_{i}(1)|D_{i} = 1] − E[Y_{i}(0)|D_{i} = 1]\}}_{\text{Post - Pre |Treated }}− \overbrace{\{E[Y_{i}(1)|D_{i} = 0] − E[Y_{i}(0)|D_{i}=0 ]}^{\text{Post-Pre|Control}}$$


- Difference 1: Average change between Treated over time
- Difference 2: Average change between Control over time

:::

## Difference in Differences {.smaller}


You'll see the DiD design represented both ways, but they produce the same result:

$$
\tau_{ATT} = (19-147) - (85-135) = -78
$$

$$
\tau_{ATT} = (19-85) - (147-135) = -78
$$


## Identifying Assumption of a Difference in Differences Design {.smaller}

The key assumption in this design is what's known as the parallel trends assumption: $E[Y_{0i}(1) − Y_{0i}(0)|D_i = 1] = E[Y_{0i}(1) − Y_{0i}(0)|D_i = 0]$ 

- In words: If Lambeth hadn't moved its pumps, it would have followed a similar path as S&V

## Parallel Trends

```{r}
#| label: paralleltrends
#| echo: false

snow_g <- tibble(
  Time = c(0,0,3,3,0,3),
  Treated = c(0,1,0, 1,1,1),
  Line = c(1,1,1,1,2,2),
  Company = c("S&V","Lambeth","S&V","Lambeth","Lambeth (D=0)","Lambeth (D=0)"),
  Deaths = c(135,85,147,19,85,97),
  line_col = c("red","red","red","red","blue","blue")
)

line_df <- tibble(
  line_type = c("solid","solid","dotted","dotted","solid","solid"),
  line_col = c("red","red","red","red","blue","blue")
)

snow_g %>%
  ggplot(aes(Time,Deaths, col=Company))+
  geom_point()+
  geom_line(linetype=line_df$line_type
            )+
  scale_color_manual(values = c("red","red","blue"))+
  geom_segment(aes(x=3.1,xend=3.1,y=19,yend=147), linetype = 2, col= "gray")+
  annotate(geom="text",x = 3.3,y=125, label = "1",hjust=.5)+
  geom_segment(aes(x=3.2,xend=3.2,y=19,yend=97), linetype = 2,col="gray")+
  annotate(geom="text",x = 3.3,y=55, label = "3",hjust=-.5)+
  geom_segment(aes(x=-.1,xend=-.1,y=85,yend=135), linetype = 2,col="gray")+
  annotate(geom="text",x = -.1,y=120, label = "2",hjust=1.5)+
  xlim(-2,6)+
  scale_x_continuous(breaks = c(0,3),labels = c("Pre","Post"))+
  theme_bw() -> snow_p

snow_p


```

## Using lm to estimate Diff-in-Diff

:::: panel-tabset

## {{< fa table >}} Data

```{r}
#| label: cholera_df

cholera_df <- tibble(
  Location = c("S&V","Lambeth","S&V","Lambeth"),
  Treated = c(0,1,0, 1),
  Time = c(0,0,1,1),
  Deaths = c(135,85,147,19)
)
cholera_df

```

## {{< fa code >}} lm()
$$
\text{Deaths} = \beta_0 + \beta_1\text{Treated} + \beta_2\text{Time} + \beta_3 \text{Treated}\times\text{Time}
$$

```{r}
#| label: did
diff_in_diff <- lm(Deaths ~ Treated + Time + Treated:Time, cholera_df)
diff_in_diff

```

## {{< fa chart-line >}} Diff-in-Diff

```{r}
#| label: didplot
#| echo: false

snow_g %>%
  ggplot(aes(Time,Deaths, col=Company))+
  geom_point()+
  coord_cartesian(xlim = c(-.5,3.5))+
  geom_line(linetype=line_df$line_type
            )+
  scale_color_manual(values = c("red","red","blue"))+
  annotate(geom="text",x = 0 ,
           y=coef(diff_in_diff)[1], 
           label = expression(paste(beta[0])),vjust=-.5,
           col = "blue")+
  geom_segment(aes(x=0,xend=0,y=85,yend=135), linetype = 2,col="magenta")+
  annotate(geom="text",x = 2.9 ,
           y=122, 
           label = expression(paste(beta[1])),
           col = "magenta",
           hjust=.75)+
    geom_segment(aes(x=3,xend= 3,y=97,yend=147), linetype = 2,col="magenta")+
  annotate(geom="text",x = -.1 ,
           y=coef(diff_in_diff)[1]-25, 
           label = expression(paste(beta[1])),
           col = "magenta",
           hjust=.75)+
  geom_segment(aes(x=0,xend=3,y=84.9,yend=84.9), linetype = 2,col="pink")+
  geom_segment(aes(x=3,xend=3,y=85,yend=97), linetype = 2,col="pink")+
  annotate(geom="text",x = 2.9 ,
           y=91, 
           label = expression(paste(beta[2])),
           col = "pink",
           hjust=.75)+
  geom_segment(aes(x=0,xend=3,y=134.9,yend=134.9), linetype = 2,col="pink")+
  geom_segment(aes(x=3,xend=3,y=135,yend=147), linetype = 2,col="pink")+
  annotate(geom="text",x = 2.9 ,
           y=141, 
           label = expression(paste(beta[2])),
           col = "pink",
           hjust=.75)+
  geom_segment(aes(x=3,xend=3,y=97,yend=19), linetype = 2,col="red")+
  annotate(geom="text",x = 3 ,
           y=58, 
           label = expression(paste(beta[3])),
           col = "red",
           hjust=-.75)+
  xlim(-2,6)+
  scale_x_continuous(breaks = c(0,3),labels = c("Pre","Post"))+
  theme_bw()-> snow_lm



snow_lm


```


:::{.nonincremental}

- $\beta_0=$ Outcome in control (S&V) before treatment
- $\beta_1=$ Fixed, [time invariant]{.blue} differences between [treated]{.blue} and [control]{.blue}
- $\beta_2=$ Fixed, [unit invariant]{.blue} differences between [pre]{.blue} and [post]{.blue} periods
- $\beta_3=$ [Difference-in-Differences]{.blue} = $E[Y_{1i}(1) -  Y_{0i}(1)|D=1]$

:::

::::


## Summary {.smaller}

- A Difference in Differences (DiD, or diff-in-diff) design combines a pre-post comparison, with a treated and control comparison
  
- Differencing twice accounts for fixed differences [across units]{.blue} and [between periods]{.blue}
  - But not time varying differences across units...
  
- The key identifying assumption of a DiD design is the assumption of [parallel trends]{.blue}
  - Absent treatment, treated and control groups
would see the same changes over time.
  - Hard to prove, possible to test



## Extensions and limitations {.smaller}

- Diff-in-Diff easy to estimate with linear regression
- Generalizes to multiple periods and treatment interventions
  - More pre-treatment periods allow you assess "parallel trends" assumption
- Alternative methods 
  - Synthetic control
  - Event Study Designs
- What if you have multiple treatments or treatments that come and go?
  - Panel Matching
  - Generalized Synthetic control


## Applications{.smaller}

- [Card and Krueger (1994)](https://www.nber.org/papers/w4509) What effect did raising the minimum wage in NJ have on employment

- [Abadie, Diamond, & Hainmueller (2014)](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12116?casa_token=_ceCu4SwzTEAAAAA%3AP9aeaZpT_Zh1VdWKXx_tEmzaJTtMJ1n0eG7EaYlvJZYN000re33cfMAI2O8N8htFJjOsln2GyVeQql4) What effect did German Unification have on economic development in West Germany

- [Malesky, Nguyen and Tran (2014)](https://www.cambridge.org/core/journals/american-political-science-review/article/impact-of-recentralization-on-public-services-a-differenceindifferences-analysis-of-the-abolition-of-elected-councils-in-vietnam/3477854BAAFE152DC93C594169D64F58) How does decentralization influence public services?



# {{< fa code>}} Previewing Lab 7 {.inverse}

## Code

# {{< fa home >}} Summary {.inverse}

## Summary

## References